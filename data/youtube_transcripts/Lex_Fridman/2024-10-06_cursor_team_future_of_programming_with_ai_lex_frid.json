{
  "metadata": {
    "video_id": "oFfVt3S51T4",
    "channel_name": "Lex Fridman",
    "video_title": "Cursor Team: Future of Programming with AI | Lex Fridman Podcast #447",
    "published_at": "2024-10-06",
    "view_count": "553,256",
    "like_count": "10,258",
    "comment_count": "880"
  },
  "transcript": [
    {
      "text": "- The following is a conversation",
      "start": 0.09,
      "duration": 1.71
    },
    {
      "text": "with the founding members\nof the Cursor Team,",
      "start": 1.8,
      "duration": 2.76
    },
    {
      "text": "Michael Truell, Sualeh\nAsif, Arvid Lunnemark,",
      "start": 4.56,
      "duration": 3.45
    },
    {
      "text": "and Aman Sanger.",
      "start": 8.01,
      "duration": 2.4
    },
    {
      "text": "Cursor is a code editor based on VS Code",
      "start": 10.41,
      "duration": 3.9
    },
    {
      "text": "that adds a lot of powerful\nfeatures for AI-assisted coding.",
      "start": 14.31,
      "duration": 3.66
    },
    {
      "text": "It has captivated the\nattention and excitement",
      "start": 17.97,
      "duration": 3.09
    },
    {
      "text": "of the programming and AI communities.",
      "start": 21.06,
      "duration": 2.88
    },
    {
      "text": "So I thought, this is\nan excellent opportunity",
      "start": 23.94,
      "duration": 2.85
    },
    {
      "text": "to dive deep into the\nrole of AI in programming.",
      "start": 26.79,
      "duration": 3.66
    },
    {
      "text": "This is a super technical conversation",
      "start": 30.45,
      "duration": 2.64
    },
    {
      "text": "that is bigger than just\nabout one code editor.",
      "start": 33.09,
      "duration": 3.66
    },
    {
      "text": "It's about the future of programming,",
      "start": 36.75,
      "duration": 1.71
    },
    {
      "text": "and in general, the future\nof human AI collaboration",
      "start": 38.46,
      "duration": 3.69
    },
    {
      "text": "in designing and engineering complicated",
      "start": 42.15,
      "duration": 2.88
    },
    {
      "text": "and powerful systems.",
      "start": 45.03,
      "duration": 2.04
    },
    {
      "text": "This is the \"Lex Fridman Podcast.\"",
      "start": 47.07,
      "duration": 1.68
    },
    {
      "text": "To support it,",
      "start": 48.75,
      "duration": 1.17
    },
    {
      "text": "please check out our\nsponsors in the description.",
      "start": 49.92,
      "duration": 2.25
    },
    {
      "text": "And now, dear friends,",
      "start": 52.17,
      "duration": 1.71
    },
    {
      "text": "here's Michael, Sualeh, Arvid and Aman.",
      "start": 53.88,
      "duration": 4.173
    },
    {
      "text": "All right, this is awesome.",
      "start": 59.13,
      "duration": 0.9
    },
    {
      "text": "We have Michael, Aman, Sualeh, Arvid here",
      "start": 60.03,
      "duration": 3.09
    },
    {
      "text": "from the Cursor Team.",
      "start": 63.12,
      "duration": 1.8
    },
    {
      "text": "First up, big ridiculous question.",
      "start": 64.92,
      "duration": 2.46
    },
    {
      "text": "What's the point of a code editor?",
      "start": 67.38,
      "duration": 2.82
    },
    {
      "text": "- So the code editor is largely the place",
      "start": 70.2,
      "duration": 2.22
    },
    {
      "text": "where you build software.",
      "start": 72.42,
      "duration": 1.86
    },
    {
      "text": "And today or for a long\ntime, that's meant the place",
      "start": 74.28,
      "duration": 3.39
    },
    {
      "text": "where you text edit a\nformal programming language.",
      "start": 77.67,
      "duration": 3.42
    },
    {
      "text": "And for people who aren't programmers,",
      "start": 81.09,
      "duration": 1.65
    },
    {
      "text": "the way to think of a code editor",
      "start": 82.74,
      "duration": 0.93
    },
    {
      "text": "is a really souped up word\nprocessor for programmers,",
      "start": 83.67,
      "duration": 3.66
    },
    {
      "text": "where the reason it's souped up",
      "start": 87.33,
      "duration": 1.68
    },
    {
      "text": "is code has a lot of structure.",
      "start": 89.01,
      "duration": 2.46
    },
    {
      "text": "And so the, quote,\nunquote, \"word processor,\"",
      "start": 91.47,
      "duration": 2.85
    },
    {
      "text": "the code editor can\nactually do a lot for you",
      "start": 94.32,
      "duration": 3.0
    },
    {
      "text": "that word processors in the writing space",
      "start": 97.32,
      "duration": 2.43
    },
    {
      "text": "haven't been able to do for\npeople editing texts there.",
      "start": 99.75,
      "duration": 2.52
    },
    {
      "text": "And so that's everything",
      "start": 102.27,
      "duration": 1.62
    },
    {
      "text": "from giving you visual differentiation",
      "start": 103.89,
      "duration": 1.74
    },
    {
      "text": "of the actual tokens in the\ncode so you can scan it quickly,",
      "start": 105.63,
      "duration": 3.51
    },
    {
      "text": "to letting you navigate\naround the code base,",
      "start": 109.14,
      "duration": 2.16
    },
    {
      "text": "like you're navigating around\nthe internet with hyperlinks,",
      "start": 111.3,
      "duration": 1.89
    },
    {
      "text": "you're going to definitions\nof things you're using",
      "start": 113.19,
      "duration": 2.49
    },
    {
      "text": "to error checking to\ncatch rudimentary bugs.",
      "start": 115.68,
      "duration": 4.593
    },
    {
      "text": "And so traditionally, that's\nwhat a code editor has meant.",
      "start": 122.34,
      "duration": 4.26
    },
    {
      "text": "And I think that what a code editor is,",
      "start": 126.6,
      "duration": 3.45
    },
    {
      "text": "is going to change a lot\nover the next 10 years",
      "start": 130.05,
      "duration": 2.13
    },
    {
      "text": "as what it means to build software",
      "start": 132.18,
      "duration": 1.95
    },
    {
      "text": "maybe starts to look a bit different.",
      "start": 134.13,
      "duration": 2.67
    },
    {
      "text": "- I think also a code\neditor should just be fun.",
      "start": 136.8,
      "duration": 2.85
    },
    {
      "text": "- Yes, that is very important,\nthat is very important.",
      "start": 139.65,
      "duration": 2.58
    },
    {
      "text": "And it's actually an underrated aspect",
      "start": 142.23,
      "duration": 2.85
    },
    {
      "text": "of how we decide what to build.",
      "start": 145.08,
      "duration": 2.283
    },
    {
      "text": "A lot of the things that we\nbuild and then we try them out,",
      "start": 148.2,
      "duration": 3.75
    },
    {
      "text": "we do an experiment and then\nwe actually throw them out",
      "start": 151.95,
      "duration": 3.39
    },
    {
      "text": "because they're not fun.",
      "start": 155.34,
      "duration": 1.71
    },
    {
      "text": "And so, a big part of being fun",
      "start": 157.05,
      "duration": 1.44
    },
    {
      "text": "is being fast a lot of the time.",
      "start": 158.49,
      "duration": 3.09
    },
    {
      "text": "Fast is fun.",
      "start": 161.58,
      "duration": 1.2
    },
    {
      "text": "- Yeah, fast is. (chuckles)",
      "start": 162.78,
      "duration": 2.73
    },
    {
      "text": "Yeah, that should be a T-shirt.",
      "start": 165.51,
      "duration": 1.881
    },
    {
      "text": "(group chuckling)",
      "start": 167.391,
      "duration": 1.059
    },
    {
      "text": "- Fundamentally, I think one of the things",
      "start": 168.45,
      "duration": 2.49
    },
    {
      "text": "that draws a lot of people to\nbuilding stuff on computers",
      "start": 170.94,
      "duration": 2.73
    },
    {
      "text": "is this insane iteration speed,",
      "start": 173.67,
      "duration": 1.89
    },
    {
      "text": "where in other disciplines\nyou might be gate capped",
      "start": 175.56,
      "duration": 3.06
    },
    {
      "text": "by resources or the ability.",
      "start": 178.62,
      "duration": 2.52
    },
    {
      "text": "Even the ability to get\na large group together",
      "start": 181.14,
      "duration": 1.77
    },
    {
      "text": "and coding is this amazing thing",
      "start": 182.91,
      "duration": 1.576
    },
    {
      "text": "where it's you and the\ncomputer and that alone,",
      "start": 184.486,
      "duration": 3.254
    },
    {
      "text": "you can build really cool\nstuff really quickly.",
      "start": 187.74,
      "duration": 2.07
    },
    {
      "text": "- So for people who don't know,",
      "start": 189.81,
      "duration": 1.11
    },
    {
      "text": "Cursor is this super cool new editor",
      "start": 190.92,
      "duration": 3.84
    },
    {
      "text": "that's a fork of VS Code.",
      "start": 194.76,
      "duration": 1.503
    },
    {
      "text": "It would be interesting\nto get your explanation",
      "start": 197.274,
      "duration": 2.976
    },
    {
      "text": "of your own journey of editors.",
      "start": 200.25,
      "duration": 2.88
    },
    {
      "text": "I think all of you were big\nfans of VS Code with Copilot.",
      "start": 203.13,
      "duration": 5.0
    },
    {
      "text": "How did you arrive to VS Code",
      "start": 208.2,
      "duration": 1.77
    },
    {
      "text": "and how did that lead to\nyour journey with Cursor?",
      "start": 209.97,
      "duration": 3.15
    },
    {
      "text": "- Yeah, so I think a lot of us,",
      "start": 213.12,
      "duration": 4.53
    },
    {
      "text": "well, all of us were originally Vim users.",
      "start": 217.65,
      "duration": 2.34
    },
    {
      "text": "- Pure Vim.\n- Pure Vim, yeah.",
      "start": 219.99,
      "duration": 1.59
    },
    {
      "text": "No Neovim, just pure Vim and a terminal.",
      "start": 221.58,
      "duration": 2.61
    },
    {
      "text": "And at least for myself,",
      "start": 224.19,
      "duration": 3.18
    },
    {
      "text": "it was around the time\nthat Copilot came out,",
      "start": 227.37,
      "duration": 2.88
    },
    {
      "text": "so 2021 that I really wanted to try it.",
      "start": 230.25,
      "duration": 5.0
    },
    {
      "text": "So, I went into VS Code,",
      "start": 235.29,
      "duration": 1.413
    },
    {
      "text": "the only code editor in\nwhich it was available,",
      "start": 238.184,
      "duration": 1.906
    },
    {
      "text": "and even though I really\nenjoyed using Vim,",
      "start": 240.09,
      "duration": 4.59
    },
    {
      "text": "just the experience of\nCopilot with VS Code",
      "start": 244.68,
      "duration": 2.76
    },
    {
      "text": "was more than good enough\nto convince me to switch.",
      "start": 247.44,
      "duration": 3.39
    },
    {
      "text": "And so that kind of was the default",
      "start": 250.83,
      "duration": 1.5
    },
    {
      "text": "until we started working on Cursor.",
      "start": 252.33,
      "duration": 2.34
    },
    {
      "text": "- And maybe we should\nexplain what Copilot does.",
      "start": 254.67,
      "duration": 2.7
    },
    {
      "text": "It's a really nice auto complete.",
      "start": 257.37,
      "duration": 1.863
    },
    {
      "text": "As you start writing a thing,",
      "start": 261.044,
      "duration": 0.833
    },
    {
      "text": "it suggests one or two or three lines",
      "start": 261.877,
      "duration": 2.393
    },
    {
      "text": "how to complete the thing.",
      "start": 264.27,
      "duration": 1.71
    },
    {
      "text": "And there's a fun experience in that.",
      "start": 265.98,
      "duration": 3.03
    },
    {
      "text": "You know like when you\nhave a close friendship",
      "start": 269.01,
      "duration": 2.19
    },
    {
      "text": "and your friend completes your sentences?",
      "start": 271.2,
      "duration": 2.178
    },
    {
      "text": "(group chuckles)",
      "start": 273.378,
      "duration": 1.092
    },
    {
      "text": "When it's done well,\nthere's an intimate feeling.",
      "start": 274.47,
      "duration": 2.82
    },
    {
      "text": "There's probably a better\nword than intimate,",
      "start": 277.29,
      "duration": 1.41
    },
    {
      "text": "but there's a cool feeling of\nlike, \"Holy shit, it gets me.\"",
      "start": 278.7,
      "duration": 4.606
    },
    {
      "text": "(all chuckles)",
      "start": 283.306,
      "duration": 1.199
    },
    {
      "text": "And then, there's an unpleasant feeling",
      "start": 284.505,
      "duration": 1.785
    },
    {
      "text": "when it doesn't get you.",
      "start": 286.29,
      "duration": 1.98
    },
    {
      "text": "And so, there's that kind of friction.",
      "start": 288.27,
      "duration": 2.34
    },
    {
      "text": "But I would say for a lot of people,",
      "start": 290.61,
      "duration": 1.65
    },
    {
      "text": "the feeling that it gets me\noverpowers that it doesn't.",
      "start": 292.26,
      "duration": 2.88
    },
    {
      "text": "- And, I think, actually one\nof the underrated aspects",
      "start": 295.14,
      "duration": 1.92
    },
    {
      "text": "of Github Copilot is that\neven when it's wrong,",
      "start": 297.06,
      "duration": 2.73
    },
    {
      "text": "it's a little bit annoying,\nbut it's not that bad",
      "start": 299.79,
      "duration": 1.92
    },
    {
      "text": "because you just type another character,",
      "start": 301.71,
      "duration": 2.49
    },
    {
      "text": "and then maybe then it gets you,",
      "start": 304.2,
      "duration": 1.59
    },
    {
      "text": "or you type another character\nand then it gets you.",
      "start": 305.79,
      "duration": 2.28
    },
    {
      "text": "So even when it's wrong,\nit's not that bad.",
      "start": 308.07,
      "duration": 1.38
    },
    {
      "text": "- Yeah, you can iterate and fix it.",
      "start": 309.45,
      "duration": 2.4
    },
    {
      "text": "I mean, the other underrated\npart of Copilot for me",
      "start": 311.85,
      "duration": 3.45
    },
    {
      "text": "was just the first real AI product.",
      "start": 315.3,
      "duration": 2.7
    },
    {
      "text": "So the first language\nmodel consumer product.",
      "start": 318.0,
      "duration": 3.45
    },
    {
      "text": "- So, Copilot was like\nthe first killer app",
      "start": 321.45,
      "duration": 4.102
    },
    {
      "text": "for LLMs.\n- Yeah.",
      "start": 325.552,
      "duration": 0.833
    },
    {
      "text": "- Yeah, and the beta was out in 2021.",
      "start": 326.385,
      "duration": 2.655
    },
    {
      "text": "- Right, okay.",
      "start": 329.04,
      "duration": 1.263
    },
    {
      "text": "So, what's the origin story of Cursor?",
      "start": 331.41,
      "duration": 2.73
    },
    {
      "text": "- So around 2020,",
      "start": 334.14,
      "duration": 1.797
    },
    {
      "text": "the scaling loss papers\ncame out from OpenAI,",
      "start": 335.937,
      "duration": 3.153
    },
    {
      "text": "and that was a moment",
      "start": 339.09,
      "duration": 1.14
    },
    {
      "text": "where this looked like\nclear predictable progress",
      "start": 340.23,
      "duration": 2.7
    },
    {
      "text": "for the field where even if\nwe didn't have any more ideas,",
      "start": 342.93,
      "duration": 3.12
    },
    {
      "text": "it looked like you could make\nthese models a lot better",
      "start": 346.05,
      "duration": 1.35
    },
    {
      "text": "if you had more compute and more data.",
      "start": 347.4,
      "duration": 2.31
    },
    {
      "text": "- By the way, we'll probably\ntalk for three to four hours",
      "start": 349.71,
      "duration": 4.02
    },
    {
      "text": "on the topic of scaling loss.",
      "start": 353.73,
      "duration": 1.496
    },
    {
      "text": "(group chuckling)",
      "start": 355.226,
      "duration": 2.335
    },
    {
      "text": "But just to summarize,",
      "start": 357.561,
      "duration": 0.833
    },
    {
      "text": "it's a paper in a set of\npapers in a set of ideas",
      "start": 358.394,
      "duration": 1.846
    },
    {
      "text": "that say bigger might be better",
      "start": 360.24,
      "duration": 1.89
    },
    {
      "text": "for model size and data size",
      "start": 362.13,
      "duration": 1.98
    },
    {
      "text": "in the realm of machine learning.",
      "start": 364.11,
      "duration": 1.59
    },
    {
      "text": "- It's bigger and better,\nbut predictably better.",
      "start": 365.7,
      "duration": 3.09
    },
    {
      "text": "- Okay, that's another\ntopic of conversation.",
      "start": 368.79,
      "duration": 1.83
    },
    {
      "text": "- Yes.\n- Yeah.",
      "start": 370.62,
      "duration": 0.833
    },
    {
      "text": "- So around that time for some of us,",
      "start": 371.453,
      "duration": 1.627
    },
    {
      "text": "there were a lot of\nconceptual conversations",
      "start": 373.08,
      "duration": 1.59
    },
    {
      "text": "about what's this gonna look like?",
      "start": 374.67,
      "duration": 2.61
    },
    {
      "text": "What's the story gonna be",
      "start": 377.28,
      "duration": 1.23
    },
    {
      "text": "for all these different\nknowledge worker fields",
      "start": 378.51,
      "duration": 1.74
    },
    {
      "text": "about how they're gonna be made better",
      "start": 380.25,
      "duration": 2.94
    },
    {
      "text": "by this technology getting better?",
      "start": 383.19,
      "duration": 1.95
    },
    {
      "text": "And then, I think, there\nwere a couple of moments",
      "start": 385.14,
      "duration": 2.7
    },
    {
      "text": "where the theoretical gains predicted",
      "start": 387.84,
      "duration": 2.22
    },
    {
      "text": "in that paper started\nto feel really concrete,",
      "start": 390.06,
      "duration": 2.73
    },
    {
      "text": "and it started to feel like a moment",
      "start": 392.79,
      "duration": 0.96
    },
    {
      "text": "where you could actually\ngo and not do a PhD",
      "start": 393.75,
      "duration": 3.3
    },
    {
      "text": "if you wanted to do useful work in AI.",
      "start": 397.05,
      "duration": 3.02
    },
    {
      "text": "It actually felt like now\nthere was this whole set",
      "start": 400.07,
      "duration": 2.11
    },
    {
      "text": "of systems one could build\nthat were really useful.",
      "start": 402.18,
      "duration": 2.55
    },
    {
      "text": "And I think that the first moment",
      "start": 404.73,
      "duration": 1.23
    },
    {
      "text": "we already talked about a little bit,",
      "start": 405.96,
      "duration": 1.2
    },
    {
      "text": "which was playing with\nthe early beta of Copilot,",
      "start": 407.16,
      "duration": 1.8
    },
    {
      "text": "that was awesome and magical.",
      "start": 408.96,
      "duration": 1.45
    },
    {
      "text": "I think that the next big moment",
      "start": 411.84,
      "duration": 1.29
    },
    {
      "text": "where everything kind of clicked together",
      "start": 413.13,
      "duration": 2.16
    },
    {
      "text": "was actually getting\nearly access to GPT-4.",
      "start": 415.29,
      "duration": 2.447
    },
    {
      "text": "So, it was sort of end of 2022",
      "start": 417.737,
      "duration": 1.693
    },
    {
      "text": "was when we were\ntinkering with that model,",
      "start": 419.43,
      "duration": 2.97
    },
    {
      "text": "and the step-upping\ncapabilities felt enormous.",
      "start": 422.4,
      "duration": 3.24
    },
    {
      "text": "And previous to that,",
      "start": 425.64,
      "duration": 0.917
    },
    {
      "text": "we had been working on a\ncouple of different projects.",
      "start": 426.557,
      "duration": 2.7
    },
    {
      "text": "Because of Copilot,\nbecause of scaling odds,",
      "start": 430.68,
      "duration": 2.37
    },
    {
      "text": "because of our prior\ninterest in the technology,",
      "start": 433.05,
      "duration": 2.01
    },
    {
      "text": "we had been tinkering around\nwith tools for programmers,",
      "start": 435.06,
      "duration": 3.81
    },
    {
      "text": "but things that are very specific.",
      "start": 438.87,
      "duration": 1.62
    },
    {
      "text": "So, we were building tools\nfor financial professionals",
      "start": 440.49,
      "duration": 4.05
    },
    {
      "text": "who have to work within a Jupyter Notebook",
      "start": 444.54,
      "duration": 1.26
    },
    {
      "text": "or playing around with",
      "start": 445.8,
      "duration": 1.32
    },
    {
      "text": "can you do static analysis\nwith these models?",
      "start": 447.12,
      "duration": 2.13
    },
    {
      "text": "And then, the step-up in GPT-4 felt like,",
      "start": 449.25,
      "duration": 1.98
    },
    {
      "text": "look, that really made\nconcrete the theoretical gains",
      "start": 451.23,
      "duration": 3.93
    },
    {
      "text": "that we had predicted before.",
      "start": 455.16,
      "duration": 2.13
    },
    {
      "text": "It felt like you could build\na lot more just immediately",
      "start": 457.29,
      "duration": 2.52
    },
    {
      "text": "at that point in time.",
      "start": 459.81,
      "duration": 1.14
    },
    {
      "text": "And also, if we were being\nconsistent, it really felt like",
      "start": 460.95,
      "duration": 5.0
    },
    {
      "text": "this wasn't just gonna be\na point solution thing.",
      "start": 466.53,
      "duration": 1.59
    },
    {
      "text": "This was gonna be all of\nprogramming was gonna flow",
      "start": 468.12,
      "duration": 1.8
    },
    {
      "text": "through these models.",
      "start": 469.92,
      "duration": 0.833
    },
    {
      "text": "And it felt like that\ndemanded a different type",
      "start": 470.753,
      "duration": 2.317
    },
    {
      "text": "of programming environment, a\ndifferent type of programming.",
      "start": 473.07,
      "duration": 2.61
    },
    {
      "text": "And so, we set off to build\nthat larger vision around then.",
      "start": 475.68,
      "duration": 4.23
    },
    {
      "text": "- There's one that I distinctly remember.",
      "start": 479.91,
      "duration": 1.89
    },
    {
      "text": "So, my roommate is an IMO Gold winner",
      "start": 481.8,
      "duration": 3.39
    },
    {
      "text": "and there's a competition\nin the US called the PUTNAM,",
      "start": 485.19,
      "duration": 2.76
    },
    {
      "text": "which is the IMO for college people,",
      "start": 487.95,
      "duration": 2.13
    },
    {
      "text": "and it's this math competition.",
      "start": 490.08,
      "duration": 2.25
    },
    {
      "text": "It's exceptionally good.",
      "start": 492.33,
      "duration": 1.86
    },
    {
      "text": "So, Shengtong and Aman I\nremember, sort of June of 2022,",
      "start": 494.19,
      "duration": 5.0
    },
    {
      "text": "had this bet on whether\nthe 2024 June or July,",
      "start": 501.72,
      "duration": 5.0
    },
    {
      "text": "you were going to win a gold\nmedal in the IMO with models.",
      "start": 507.18,
      "duration": 4.08
    },
    {
      "text": "- IMO is the International Math Olympiad.",
      "start": 511.26,
      "duration": 2.31
    },
    {
      "text": "- Yeah, IMO is\nInternational Math Olympiad.",
      "start": 513.57,
      "duration": 2.163
    },
    {
      "text": "And so, Arvid and I are\nboth also competing in it.",
      "start": 515.733,
      "duration": 3.087
    },
    {
      "text": "So, it was sort of personal.",
      "start": 518.82,
      "duration": 3.172
    },
    {
      "text": "(group chuckling)",
      "start": 521.992,
      "duration": 1.606
    },
    {
      "text": "And I remember thinking, \"Matt,\nthis is not gonna happen.\"",
      "start": 523.598,
      "duration": 3.925
    },
    {
      "text": "Even though I sort of\nbelieved in progress,",
      "start": 528.99,
      "duration": 2.67
    },
    {
      "text": "I thought IMO Gold, Aman is delusional.",
      "start": 531.66,
      "duration": 4.143
    },
    {
      "text": "And to be honest, I mean, I\nwas, to be clear, very wrong.",
      "start": 537.18,
      "duration": 3.93
    },
    {
      "text": "But that was maybe the most\nprescient bet in the group.",
      "start": 541.11,
      "duration": 4.26
    },
    {
      "text": "- So the new results from DeepMind,",
      "start": 545.37,
      "duration": 2.79
    },
    {
      "text": "it turned out that you were correct.",
      "start": 548.16,
      "duration": 2.538
    },
    {
      "text": "(group chattering)",
      "start": 550.698,
      "duration": 1.122
    },
    {
      "text": "- [Arvid] Technically not.",
      "start": 551.82,
      "duration": 0.84
    },
    {
      "text": "- Technically incorrect\nbut one point away.",
      "start": 552.66,
      "duration": 2.37
    },
    {
      "text": "- Aman was very enthusiastic\nabout this stuff back then.",
      "start": 555.03,
      "duration": 2.93
    },
    {
      "text": "And before, Aman had\nthis scaling loss T-shirt",
      "start": 557.96,
      "duration": 3.19
    },
    {
      "text": "that he would wear around",
      "start": 561.15,
      "duration": 0.84
    },
    {
      "text": "where it had the charts\nand the formulas on it.",
      "start": 561.99,
      "duration": 3.21
    },
    {
      "text": "- So, you felt the AGI or\nyou felt the scaling loss.",
      "start": 565.2,
      "duration": 3.48
    },
    {
      "text": "- Yeah, I distinctly remember",
      "start": 568.68,
      "duration": 1.44
    },
    {
      "text": "there was this one\nconversation I had with Michael",
      "start": 570.12,
      "duration": 3.55
    },
    {
      "text": "before I hadn't thought super deeply",
      "start": 574.65,
      "duration": 1.56
    },
    {
      "text": "and critically about scaling laws.",
      "start": 576.21,
      "duration": 2.34
    },
    {
      "text": "And he kind of posed the question,",
      "start": 578.55,
      "duration": 2.01
    },
    {
      "text": "why isn't scaling all you need,",
      "start": 580.56,
      "duration": 2.1
    },
    {
      "text": "or why isn't scaling gonna result",
      "start": 582.66,
      "duration": 1.53
    },
    {
      "text": "in massive gains in progress?",
      "start": 584.19,
      "duration": 1.887
    },
    {
      "text": "And I think I went through\nthe stages of grief.",
      "start": 586.077,
      "duration": 3.363
    },
    {
      "text": "There is anger, denial,\nand then finally at the end",
      "start": 589.44,
      "duration": 2.43
    },
    {
      "text": "just thinking about it, acceptance.",
      "start": 591.87,
      "duration": 2.673
    },
    {
      "text": "And I think I've been quite hopeful",
      "start": 595.997,
      "duration": 2.97
    },
    {
      "text": "and optimistic about progress since.",
      "start": 600.03,
      "duration": 3.18
    },
    {
      "text": "I think one thing I'll caveat\nis, I think, it also depends",
      "start": 603.21,
      "duration": 3.24
    },
    {
      "text": "on which domains you're\ngonna see progress.",
      "start": 606.45,
      "duration": 2.16
    },
    {
      "text": "Math is a great domain\nespecially formal theorem proving",
      "start": 608.61,
      "duration": 4.14
    },
    {
      "text": "because you get this fantastic\nsignal of actually verifying",
      "start": 612.75,
      "duration": 4.14
    },
    {
      "text": "if the thing was correct.",
      "start": 616.89,
      "duration": 1.23
    },
    {
      "text": "And so this means",
      "start": 618.12,
      "duration": 0.84
    },
    {
      "text": "something like RL can\nwork really, really well,",
      "start": 618.96,
      "duration": 2.28
    },
    {
      "text": "and I think you could have systems",
      "start": 621.24,
      "duration": 1.62
    },
    {
      "text": "that are perhaps very superhuman in math",
      "start": 622.86,
      "duration": 2.49
    },
    {
      "text": "and still not technically have AGI.",
      "start": 625.35,
      "duration": 2.34
    },
    {
      "text": "- Okay, so can we take\nit all the way to Cursor.",
      "start": 627.69,
      "duration": 3.15
    },
    {
      "text": "And what is Cursor?",
      "start": 630.84,
      "duration": 1.44
    },
    {
      "text": "It's a fork of VS Code,",
      "start": 632.28,
      "duration": 2.25
    },
    {
      "text": "and VS Code is one of\nthe most popular editors",
      "start": 634.53,
      "duration": 3.03
    },
    {
      "text": "for a long time.",
      "start": 637.56,
      "duration": 0.833
    },
    {
      "text": "Everybody fell in love with it.",
      "start": 638.393,
      "duration": 1.147
    },
    {
      "text": "Everybody left Vim, I left DMAX for it.",
      "start": 639.54,
      "duration": 3.45
    },
    {
      "text": "Sorry.",
      "start": 642.99,
      "duration": 0.833
    },
    {
      "text": "(all laughing)",
      "start": 644.751,
      "duration": 1.959
    },
    {
      "text": "So, unified in some fundamental\nway the developer community.",
      "start": 646.71,
      "duration": 5.0
    },
    {
      "text": "And then, you look at the space of things,",
      "start": 652.98,
      "duration": 1.86
    },
    {
      "text": "you look at the scaling\nlaws, AI is becoming amazing.",
      "start": 654.84,
      "duration": 3.42
    },
    {
      "text": "And you decided, okay, it's not enough",
      "start": 658.26,
      "duration": 2.28
    },
    {
      "text": "to just write an extension via VS Code",
      "start": 660.54,
      "duration": 2.11
    },
    {
      "text": "because there's a lot\nof limitations to that.",
      "start": 663.78,
      "duration": 2.25
    },
    {
      "text": "If AI is gonna keep getting\nbetter and better and better,",
      "start": 667.35,
      "duration": 1.89
    },
    {
      "text": "we need to really rethink\nhow the AI is gonna be part",
      "start": 669.24,
      "duration": 3.317
    },
    {
      "text": "of the editing process.",
      "start": 672.557,
      "duration": 1.693
    },
    {
      "text": "And so, you decided to fork VS Code,",
      "start": 674.25,
      "duration": 2.43
    },
    {
      "text": "and start to build a lot\nof the amazing features",
      "start": 676.68,
      "duration": 2.79
    },
    {
      "text": "we'll be able to talk about.",
      "start": 679.47,
      "duration": 2.337
    },
    {
      "text": "But what was that decision like?",
      "start": 681.807,
      "duration": 1.503
    },
    {
      "text": "Because there's a lot of\nextensions, including Copilot,",
      "start": 683.31,
      "duration": 4.38
    },
    {
      "text": "of VS Code that are doing\nsort of AI type stuff.",
      "start": 687.69,
      "duration": 2.64
    },
    {
      "text": "What was the decision\nlike to just fork VS Code?",
      "start": 690.33,
      "duration": 3.0
    },
    {
      "text": "- So the decision to do an\neditor seemed self-evident to us",
      "start": 693.33,
      "duration": 4.59
    },
    {
      "text": "for at least what we\nwanted to do and achieve,",
      "start": 697.92,
      "duration": 2.55
    },
    {
      "text": "because when we started\nworking on the editor,",
      "start": 700.47,
      "duration": 1.92
    },
    {
      "text": "the idea was these models\nare gonna get much better,",
      "start": 702.39,
      "duration": 2.04
    },
    {
      "text": "their capabilities are gonna improve,",
      "start": 704.43,
      "duration": 1.11
    },
    {
      "text": "and it's gonna entirely\nchange how you build software,",
      "start": 705.54,
      "duration": 2.16
    },
    {
      "text": "both in a you will have\nbig productivity gains",
      "start": 707.7,
      "duration": 2.28
    },
    {
      "text": "but also radical and now\nthe active building software",
      "start": 709.98,
      "duration": 2.22
    },
    {
      "text": "is gonna change a lot.",
      "start": 712.2,
      "duration": 1.65
    },
    {
      "text": "And so, you're very limited",
      "start": 713.85,
      "duration": 1.92
    },
    {
      "text": "in the control you have over a code editor",
      "start": 715.77,
      "duration": 2.4
    },
    {
      "text": "if you're a plugin to an\nexisting coding environment,",
      "start": 718.17,
      "duration": 3.481
    },
    {
      "text": "and we didn't wanna get locked\nin by those limitations.",
      "start": 721.651,
      "duration": 3.269
    },
    {
      "text": "We wanted to be able to just\nbuild the most useful stuff.",
      "start": 724.92,
      "duration": 3.18
    },
    {
      "text": "- Okay.",
      "start": 728.1,
      "duration": 0.833
    },
    {
      "text": "Well then, the natural question is,",
      "start": 728.933,
      "duration": 1.39
    },
    {
      "text": "VS Code is kind of with\nCopilot a competitor,",
      "start": 731.82,
      "duration": 3.63
    },
    {
      "text": "so how do you win?",
      "start": 735.45,
      "duration": 1.89
    },
    {
      "text": "Is it basically just the\nspeed and the quality",
      "start": 737.34,
      "duration": 1.89
    },
    {
      "text": "of the features?",
      "start": 739.23,
      "duration": 0.99
    },
    {
      "text": "- Yeah, I mean, I think this is a space",
      "start": 740.22,
      "duration": 2.76
    },
    {
      "text": "that is quite interesting,\nperhaps quite unique",
      "start": 742.98,
      "duration": 3.3
    },
    {
      "text": "where if you look at previous tech waves,",
      "start": 746.28,
      "duration": 3.48
    },
    {
      "text": "maybe there's kind of one\nmajor thing that happened",
      "start": 749.76,
      "duration": 2.01
    },
    {
      "text": "and it unlocked a new wave of companies,",
      "start": 751.77,
      "duration": 2.43
    },
    {
      "text": "but every single year, every\nsingle model capability",
      "start": 754.2,
      "duration": 3.51
    },
    {
      "text": "or jump you get in model capabilities,",
      "start": 757.71,
      "duration": 2.19
    },
    {
      "text": "you now unlock this new wave of features,",
      "start": 759.9,
      "duration": 3.66
    },
    {
      "text": "things that are possible,\nespecially in programming.",
      "start": 763.56,
      "duration": 3.33
    },
    {
      "text": "And so, I think, in AI programming,",
      "start": 766.89,
      "duration": 2.91
    },
    {
      "text": "being even just a few months\nahead, let alone a year ahead,",
      "start": 769.8,
      "duration": 3.57
    },
    {
      "text": "makes your product much,\nmuch, much more useful.",
      "start": 773.37,
      "duration": 2.4
    },
    {
      "text": "I think the Cursor a year from now",
      "start": 775.77,
      "duration": 2.01
    },
    {
      "text": "will need to make the Cursor\nof today look obsolete.",
      "start": 777.78,
      "duration": 3.123
    },
    {
      "text": "And I think Microsoft has done\na number of fantastic things,",
      "start": 781.83,
      "duration": 4.65
    },
    {
      "text": "but I don't think they're in a great place",
      "start": 786.48,
      "duration": 1.86
    },
    {
      "text": "to really keep innovating",
      "start": 788.34,
      "duration": 1.92
    },
    {
      "text": "and pushing on this in the\nway that a startup can.",
      "start": 790.26,
      "duration": 2.85
    },
    {
      "text": "- Just rapidly implementing features.",
      "start": 793.11,
      "duration": 1.953
    },
    {
      "text": "- Yeah, and doing the research\nexperimentation necessary",
      "start": 796.41,
      "duration": 4.84
    },
    {
      "text": "to really push the ceiling.",
      "start": 802.92,
      "duration": 1.38
    },
    {
      "text": "- I don't know if I think\nof it in terms of features",
      "start": 804.3,
      "duration": 1.71
    },
    {
      "text": "as I think of it in terms of\ncapabilities for programmers.",
      "start": 806.01,
      "duration": 3.813
    },
    {
      "text": "As the new o1 model came out,",
      "start": 811.17,
      "duration": 3.75
    },
    {
      "text": "and I'm sure there are\ngonna be more models",
      "start": 814.92,
      "duration": 2.37
    },
    {
      "text": "of different types, like longer\ncontext and maybe faster,",
      "start": 817.29,
      "duration": 3.54
    },
    {
      "text": "there's all these crazy\nideas that you can try,",
      "start": 820.83,
      "duration": 3.87
    },
    {
      "text": "and hopefully 10% of the crazy ideas",
      "start": 824.7,
      "duration": 3.03
    },
    {
      "text": "will make it into something\nkind of cool and useful",
      "start": 827.73,
      "duration": 2.97
    },
    {
      "text": "and we want people to have that sooner.",
      "start": 830.7,
      "duration": 5.0
    },
    {
      "text": "To rephrase, an underrated fact",
      "start": 835.8,
      "duration": 1.8
    },
    {
      "text": "is we're making it for ourself.",
      "start": 837.6,
      "duration": 1.71
    },
    {
      "text": "When we started Cursor,",
      "start": 839.31,
      "duration": 1.5
    },
    {
      "text": "you really felt this\nfrustration that models,",
      "start": 840.81,
      "duration": 3.33
    },
    {
      "text": "you could see models getting better,",
      "start": 844.14,
      "duration": 2.55
    },
    {
      "text": "but the Copilot experience\nhad not changed.",
      "start": 846.69,
      "duration": 2.07
    },
    {
      "text": "It was like, \"Man, the\nceiling is getting higher,",
      "start": 848.76,
      "duration": 4.56
    },
    {
      "text": "why are they not making new things?",
      "start": 853.32,
      "duration": 1.771
    },
    {
      "text": "They should be making new things.",
      "start": 855.091,
      "duration": 2.399
    },
    {
      "text": "Where's all the alpha features?",
      "start": 857.49,
      "duration": 2.07
    },
    {
      "text": "There were no alpha features.\"",
      "start": 859.56,
      "duration": 1.5
    },
    {
      "text": "I'm sure it was selling well.",
      "start": 862.89,
      "duration": 1.8
    },
    {
      "text": "I'm sure it was a great business,",
      "start": 864.69,
      "duration": 1.65
    },
    {
      "text": "I'm one of these people",
      "start": 867.66,
      "duration": 1.29
    },
    {
      "text": "that really want to\ntry and use new things,",
      "start": 868.95,
      "duration": 2.85
    },
    {
      "text": "and there was no new thing\nfor a very long while.",
      "start": 871.8,
      "duration": 3.57
    },
    {
      "text": "- Yeah, it's interesting.",
      "start": 875.37,
      "duration": 1.92
    },
    {
      "text": "I don't know how you put that into words,",
      "start": 877.29,
      "duration": 1.44
    },
    {
      "text": "but when you compare\na Cursor with Copilot,",
      "start": 878.73,
      "duration": 2.7
    },
    {
      "text": "Copilot pretty quickly\nstarted to feel stale",
      "start": 881.43,
      "duration": 3.63
    },
    {
      "text": "for some reason.",
      "start": 885.06,
      "duration": 0.833
    },
    {
      "text": "- Yeah, I think one thing\nthat I think helps us",
      "start": 885.893,
      "duration": 3.667
    },
    {
      "text": "is that we're doing it all in one",
      "start": 889.56,
      "duration": 3.21
    },
    {
      "text": "where we're developing the UX",
      "start": 892.77,
      "duration": 2.61
    },
    {
      "text": "and the way you interact with the model",
      "start": 895.38,
      "duration": 2.1
    },
    {
      "text": "at the same time as we're developing",
      "start": 897.48,
      "duration": 2.94
    },
    {
      "text": "how we actually make the\nmodel give better answers.",
      "start": 900.42,
      "duration": 2.01
    },
    {
      "text": "So, how you build up the prompt",
      "start": 902.43,
      "duration": 2.7
    },
    {
      "text": "or how do you find the\ncontext and for a Cursor Tab,",
      "start": 905.13,
      "duration": 3.3
    },
    {
      "text": "how do you train the model?",
      "start": 908.43,
      "duration": 1.89
    },
    {
      "text": "So, I think that helps\nus to have all of it",
      "start": 910.32,
      "duration": 2.817
    },
    {
      "text": "the same people working on the\nentire experience end to end.",
      "start": 913.137,
      "duration": 4.263
    },
    {
      "text": "- Yeah, it's like the person making the UI",
      "start": 917.4,
      "duration": 2.01
    },
    {
      "text": "and the person training the\nmodel sit like 18 feet away.",
      "start": 919.41,
      "duration": 4.86
    },
    {
      "text": "- [Aman] Often the same person even.",
      "start": 924.27,
      "duration": 1.47
    },
    {
      "text": "- Yeah, often even the same person.",
      "start": 925.74,
      "duration": 2.749
    },
    {
      "text": "You can create things that\nare sort of not possible",
      "start": 928.489,
      "duration": 2.501
    },
    {
      "text": "if you're not talking,\nyou're not experimenting.",
      "start": 930.99,
      "duration": 3.33
    },
    {
      "text": "- And you're using, like you\nsaid, Cursor to write Cursor?",
      "start": 934.32,
      "duration": 2.85
    },
    {
      "text": "- Of course.",
      "start": 937.17,
      "duration": 0.833
    },
    {
      "text": "- Oh, yeah.\n- Yeah.",
      "start": 938.003,
      "duration": 0.833
    },
    {
      "text": "- Well, let's talk about\nsome of these features.",
      "start": 938.836,
      "duration": 1.904
    },
    {
      "text": "Let's talk about the all-knowing,",
      "start": 940.74,
      "duration": 2.4
    },
    {
      "text": "the all-powerful praise be to the Tab,",
      "start": 943.14,
      "duration": 3.248
    },
    {
      "text": "(group chuckles)",
      "start": 946.388,
      "duration": 1.162
    },
    {
      "text": "auto complete on steroids basically.",
      "start": 947.55,
      "duration": 3.21
    },
    {
      "text": "So how does Tab work?",
      "start": 950.76,
      "duration": 1.23
    },
    {
      "text": "What is Tab?",
      "start": 951.99,
      "duration": 1.2
    },
    {
      "text": "- To highlight and\nsummarize at a high level,",
      "start": 953.19,
      "duration": 1.65
    },
    {
      "text": "I'd say that there are two things",
      "start": 954.84,
      "duration": 2.16
    },
    {
      "text": "that Cursor is pretty good at right now.",
      "start": 957.0,
      "duration": 1.95
    },
    {
      "text": "There are other things that it does,",
      "start": 958.95,
      "duration": 2.46
    },
    {
      "text": "but two things that it\nhelps programmers with.",
      "start": 961.41,
      "duration": 3.48
    },
    {
      "text": "One is this idea of\nlooking over your shoulder,",
      "start": 964.89,
      "duration": 3.42
    },
    {
      "text": "and being a really fast colleague",
      "start": 968.31,
      "duration": 2.07
    },
    {
      "text": "who can kind of jump\nahead of you, and type,",
      "start": 970.38,
      "duration": 2.28
    },
    {
      "text": "and figure out what you're gonna do next.",
      "start": 972.66,
      "duration": 2.05
    },
    {
      "text": "That was the kernel of the idea\nbehind a good auto complete",
      "start": 979.113,
      "duration": 2.127
    },
    {
      "text": "was predicting what you're gonna do next,",
      "start": 981.24,
      "duration": 1.95
    },
    {
      "text": "but you can make that\nconcept even more ambitious",
      "start": 983.19,
      "duration": 2.91
    },
    {
      "text": "by not just predicting the\ncharacters after your Cursor",
      "start": 986.1,
      "duration": 3.267
    },
    {
      "text": "but actually predicting\nthe next entire change",
      "start": 989.367,
      "duration": 1.713
    },
    {
      "text": "you're gonna make, the next diff,",
      "start": 991.08,
      "duration": 0.99
    },
    {
      "text": "next place you're gonna jump to.",
      "start": 992.07,
      "duration": 1.6
    },
    {
      "text": "And the second thing Cursor is\npretty good at right now too",
      "start": 995.16,
      "duration": 5.0
    },
    {
      "text": "is helping you sometimes\njump ahead of the AI",
      "start": 1000.17,
      "duration": 2.5
    },
    {
      "text": "and tell it what to do and\ngo from instructions to code.",
      "start": 1003.71,
      "duration": 3.39
    },
    {
      "text": "And on both of those,\nwe've done a lot of work",
      "start": 1007.1,
      "duration": 1.44
    },
    {
      "text": "on making the editing experience\nfor those things ergonomic",
      "start": 1008.54,
      "duration": 3.18
    },
    {
      "text": "and also making those\nthings smart and fast.",
      "start": 1011.72,
      "duration": 2.76
    },
    {
      "text": "- One of the things we really wanted,",
      "start": 1014.48,
      "duration": 1.74
    },
    {
      "text": "was we wanted the model to\nbe able to edit code for us.",
      "start": 1016.22,
      "duration": 3.0
    },
    {
      "text": "That was kind of a wish and\nwe had multiple attempts at it",
      "start": 1019.22,
      "duration": 2.91
    },
    {
      "text": "before we had a good model\nthat could edit code for you.",
      "start": 1022.13,
      "duration": 4.173
    },
    {
      "text": "Then after we had a good model,",
      "start": 1027.71,
      "duration": 2.07
    },
    {
      "text": "I think there've been a lot of effort",
      "start": 1029.78,
      "duration": 1.89
    },
    {
      "text": "to make the inference fast\nfor having a good experience,",
      "start": 1031.67,
      "duration": 5.0
    },
    {
      "text": "and we've been starting to incorporate,",
      "start": 1038.96,
      "duration": 3.6
    },
    {
      "text": "I mean, Michael sort of\nmentioned this ability",
      "start": 1042.56,
      "duration": 1.95
    },
    {
      "text": "to jump to different places,",
      "start": 1044.51,
      "duration": 1.74
    },
    {
      "text": "and that jump to different\nplaces I think came",
      "start": 1046.25,
      "duration": 2.04
    },
    {
      "text": "from a feeling of once you accept an edit,",
      "start": 1048.29,
      "duration": 4.053
    },
    {
      "text": "it's like, \"Man, it should\nbe just really obvious",
      "start": 1054.29,
      "duration": 2.46
    },
    {
      "text": "where to go next.\"",
      "start": 1056.75,
      "duration": 1.05
    },
    {
      "text": "It's like, \"I'd made this change,\nthe model should just know",
      "start": 1057.8,
      "duration": 3.48
    },
    {
      "text": "that the next place to\ngo to is 18 lines down.\"",
      "start": 1061.28,
      "duration": 3.87
    },
    {
      "text": "If you're a WIM user, you\ncould press 18JJ or whatever,",
      "start": 1065.15,
      "duration": 3.783
    },
    {
      "text": "but why am I doing this?",
      "start": 1069.92,
      "duration": 2.349
    },
    {
      "text": "The model should just know it.",
      "start": 1072.269,
      "duration": 1.524
    },
    {
      "text": "So the idea was you just press Tab,",
      "start": 1074.658,
      "duration": 2.162
    },
    {
      "text": "it would go 18 lines down,",
      "start": 1076.82,
      "duration": 1.26
    },
    {
      "text": "and then show you the next\nedit and you would press Tab,",
      "start": 1078.08,
      "duration": 3.63
    },
    {
      "text": "so as long as you could keep pressing Tab.",
      "start": 1081.71,
      "duration": 2.91
    },
    {
      "text": "And so the internal competition was,",
      "start": 1084.62,
      "duration": 1.62
    },
    {
      "text": "how many Tabs can we make someone press?",
      "start": 1086.24,
      "duration": 2.25
    },
    {
      "text": "Once you have the idea, more abstractly,",
      "start": 1088.49,
      "duration": 3.8
    },
    {
      "text": "the thing to think about is\nhow are the edits zero entropy?",
      "start": 1094.07,
      "duration": 4.758
    },
    {
      "text": "There's no new bits of information\nto finish your thought,",
      "start": 1102.727,
      "duration": 4.693
    },
    {
      "text": "but you still have to type some characters",
      "start": 1107.42,
      "duration": 2.22
    },
    {
      "text": "to make the computer understand",
      "start": 1109.64,
      "duration": 1.71
    },
    {
      "text": "what you're actually thinking,",
      "start": 1111.35,
      "duration": 1.74
    },
    {
      "text": "then maybe the model\nshould just read your mind",
      "start": 1113.09,
      "duration": 2.821
    },
    {
      "text": "and all the zero entropy bits\nshould just be tabbed away.",
      "start": 1115.911,
      "duration": 4.292
    },
    {
      "text": "That was sort of the abstract version.",
      "start": 1121.22,
      "duration": 1.797
    },
    {
      "text": "- There's this interesting thing",
      "start": 1123.017,
      "duration": 0.833
    },
    {
      "text": "where if you look at language model loss",
      "start": 1123.85,
      "duration": 1.72
    },
    {
      "text": "on different domains, I\nbelieve the bits per byte,",
      "start": 1125.57,
      "duration": 3.78
    },
    {
      "text": "which is a kind of\ncharacter normalize loss",
      "start": 1129.35,
      "duration": 2.49
    },
    {
      "text": "for code is lower than language,",
      "start": 1131.84,
      "duration": 3.39
    },
    {
      "text": "which means in general, there\nare a lot of tokens in code",
      "start": 1135.23,
      "duration": 1.98
    },
    {
      "text": "that are super predictable,",
      "start": 1137.21,
      "duration": 1.65
    },
    {
      "text": "a lot of characters that\nare super predictable.",
      "start": 1138.86,
      "duration": 2.1
    },
    {
      "text": "And this is I think even magnified",
      "start": 1140.96,
      "duration": 2.1
    },
    {
      "text": "when you're not just trying\nto auto complete code,",
      "start": 1143.06,
      "duration": 2.52
    },
    {
      "text": "but predicting what the\nuser's going to do next",
      "start": 1145.58,
      "duration": 2.97
    },
    {
      "text": "in their editing of existing code.",
      "start": 1148.55,
      "duration": 2.34
    },
    {
      "text": "And so, the goal of Cursor\nTab is let's eliminate",
      "start": 1150.89,
      "duration": 2.73
    },
    {
      "text": "all the low entropy actions\nyou take inside of the editor.",
      "start": 1153.62,
      "duration": 3.12
    },
    {
      "text": "When the intent is effectively determined,",
      "start": 1156.74,
      "duration": 2.94
    },
    {
      "text": "let's just jump you forward\nin time, skip you forward.",
      "start": 1159.68,
      "duration": 2.73
    },
    {
      "text": "- Well, what's the intuition",
      "start": 1162.41,
      "duration": 1.56
    },
    {
      "text": "and what's the technical details",
      "start": 1163.97,
      "duration": 1.11
    },
    {
      "text": "of how to do next Cursor prediction?",
      "start": 1165.08,
      "duration": 2.91
    },
    {
      "text": "That jump, that's not so\nintuitive I think to people.",
      "start": 1167.99,
      "duration": 3.45
    },
    {
      "text": "- Yeah.",
      "start": 1171.44,
      "duration": 0.833
    },
    {
      "text": "I think I can speak to\na few of the details",
      "start": 1172.273,
      "duration": 3.277
    },
    {
      "text": "on how to make these things work.",
      "start": 1175.55,
      "duration": 1.77
    },
    {
      "text": "They're incredibly low latency,",
      "start": 1177.32,
      "duration": 1.2
    },
    {
      "text": "so you need to train\nsmall models on this task.",
      "start": 1178.52,
      "duration": 4.71
    },
    {
      "text": "In particular, they're\nincredibly pre-fill token hungry.",
      "start": 1183.23,
      "duration": 5.0
    },
    {
      "text": "What that means is they have these really,",
      "start": 1188.33,
      "duration": 1.56
    },
    {
      "text": "really long prompts where\nthey see a lot of your code",
      "start": 1189.89,
      "duration": 2.76
    },
    {
      "text": "and they're not actually\ngenerating that many tokens.",
      "start": 1192.65,
      "duration": 2.25
    },
    {
      "text": "And so, the perfect fit for\nthat is using a sparse model,",
      "start": 1194.9,
      "duration": 3.81
    },
    {
      "text": "meaning an MOE model.",
      "start": 1198.71,
      "duration": 1.173
    },
    {
      "text": "So that was one breakthrough we made",
      "start": 1200.78,
      "duration": 2.58
    },
    {
      "text": "that substantially\nimproved its performance",
      "start": 1203.36,
      "duration": 1.65
    },
    {
      "text": "at longer context.",
      "start": 1205.01,
      "duration": 1.29
    },
    {
      "text": "The other being a variant\nof speculative decoding",
      "start": 1206.3,
      "duration": 3.69
    },
    {
      "text": "that we built out called\nspeculative edits.",
      "start": 1209.99,
      "duration": 3.33
    },
    {
      "text": "These are two, I think, important pieces",
      "start": 1213.32,
      "duration": 2.01
    },
    {
      "text": "of what make it quite high\nquality and very fast.",
      "start": 1215.33,
      "duration": 5.0
    },
    {
      "text": "- Okay, so MoE, Mixture of Experts,",
      "start": 1220.37,
      "duration": 2.43
    },
    {
      "text": "the input is huge, the output is small.",
      "start": 1222.8,
      "duration": 2.16
    },
    {
      "text": "- Yeah.\n- Okay.",
      "start": 1224.96,
      "duration": 0.933
    },
    {
      "text": "Does caching play a role-",
      "start": 1229.303,
      "duration": 1.279
    },
    {
      "text": "- Oh, caching plays a huge role.",
      "start": 1230.582,
      "duration": 2.271
    },
    {
      "text": "Because you're dealing with\nthis many input tokens,",
      "start": 1233.99,
      "duration": 2.49
    },
    {
      "text": "if every single keystroke that\nyou're typing in a given line",
      "start": 1236.48,
      "duration": 3.3
    },
    {
      "text": "you had to rerun the model on\nall of those tokens passed in,",
      "start": 1239.78,
      "duration": 4.35
    },
    {
      "text": "you're just going to one,\nsignificantly degrade latency,",
      "start": 1244.13,
      "duration": 3.27
    },
    {
      "text": "two, you're gonna kill\nyour GPUs with load.",
      "start": 1247.4,
      "duration": 2.46
    },
    {
      "text": "So, you need to design\nthe actual prompts you use",
      "start": 1249.86,
      "duration": 3.93
    },
    {
      "text": "for the model such that\nthey're caching aware.",
      "start": 1253.79,
      "duration": 3.36
    },
    {
      "text": "And then yeah, you need\nto reuse the KV cache",
      "start": 1257.15,
      "duration": 2.67
    },
    {
      "text": "across requests",
      "start": 1259.82,
      "duration": 1.53
    },
    {
      "text": "just so that you're spending\nless work, less compute.",
      "start": 1261.35,
      "duration": 3.09
    },
    {
      "text": "- Again, what are the\nthings that Tab is supposed",
      "start": 1264.44,
      "duration": 2.91
    },
    {
      "text": "to be able to do in the near\nterm, just to linger on that?",
      "start": 1267.35,
      "duration": 5.0
    },
    {
      "text": "Generate code, fill empty space,",
      "start": 1273.53,
      "duration": 3.753
    },
    {
      "text": "also edit code across multiple lines,",
      "start": 1278.45,
      "duration": 3.12
    },
    {
      "text": "and then jump to different\nlocations inside the same file,",
      "start": 1281.57,
      "duration": 3.174
    },
    {
      "text": "and then-\n- Hopefully,",
      "start": 1284.744,
      "duration": 0.833
    },
    {
      "text": "jump to different files also.",
      "start": 1285.577,
      "duration": 1.303
    },
    {
      "text": "So if you make an edit in one file,",
      "start": 1286.88,
      "duration": 2.077
    },
    {
      "text": "and maybe you have to go to another file",
      "start": 1288.957,
      "duration": 3.503
    },
    {
      "text": "to finish your thought,",
      "start": 1292.46,
      "duration": 1.11
    },
    {
      "text": "it should go to the second file also.",
      "start": 1293.57,
      "duration": 1.85
    },
    {
      "text": "- The full generalization\nis next action prediction.",
      "start": 1296.9,
      "duration": 4.41
    },
    {
      "text": "Sometimes, you need to run\na command in the terminal",
      "start": 1301.31,
      "duration": 2.67
    },
    {
      "text": "and it should be able to\nsuggest the command based",
      "start": 1303.98,
      "duration": 3.15
    },
    {
      "text": "on the code that you wrote too.",
      "start": 1307.13,
      "duration": 1.803
    },
    {
      "text": "It suggests something,",
      "start": 1313.37,
      "duration": 1.1
    },
    {
      "text": "but it's hard for you\nto know if it's correct",
      "start": 1315.306,
      "duration": 1.814
    },
    {
      "text": "because you actually need some\nmore information to learn.",
      "start": 1317.12,
      "duration": 2.79
    },
    {
      "text": "You need to know the\ntype to be able to verify",
      "start": 1319.91,
      "duration": 2.04
    },
    {
      "text": "that it's correct.",
      "start": 1321.95,
      "duration": 0.833
    },
    {
      "text": "And so maybe it should\nactually take you to a place",
      "start": 1322.783,
      "duration": 2.797
    },
    {
      "text": "that's the definition of something,",
      "start": 1325.58,
      "duration": 1.92
    },
    {
      "text": "and then take you back",
      "start": 1327.5,
      "duration": 1.65
    },
    {
      "text": "so that you have all\nthe requisite knowledge",
      "start": 1329.15,
      "duration": 2.01
    },
    {
      "text": "to be able to accept the next completion.",
      "start": 1331.16,
      "duration": 2.07
    },
    {
      "text": "- So providing the human the knowledge.",
      "start": 1333.23,
      "duration": 2.43
    },
    {
      "text": "- [Arvid] Yes.",
      "start": 1335.66,
      "duration": 1.53
    },
    {
      "text": "- Right.\n- Mm-hmm, yeah.",
      "start": 1337.19,
      "duration": 1.25
    },
    {
      "text": "- I just gotten to know\na guy named Primeagen.",
      "start": 1339.583,
      "duration": 3.094
    },
    {
      "text": "You can order coffee via SSH.",
      "start": 1344.93,
      "duration": 2.467
    },
    {
      "text": "- (chuckles) Oh, yeah.",
      "start": 1348.513,
      "duration": 0.887
    },
    {
      "text": "- We did that.\n- We did that.",
      "start": 1349.4,
      "duration": 1.83
    },
    {
      "text": "- So, can also the model do that",
      "start": 1351.23,
      "duration": 1.771
    },
    {
      "text": "and provide you with caffeine?",
      "start": 1353.001,
      "duration": 4.349
    },
    {
      "text": "Okay, so that's the general framework.",
      "start": 1357.35,
      "duration": 2.044
    },
    {
      "text": "- Yeah.",
      "start": 1359.394,
      "duration": 5.0
    },
    {
      "text": "- Programming is this weird discipline",
      "start": 1364.67,
      "duration": 1.47
    },
    {
      "text": "where sometimes the next\nfive minutes, not always,",
      "start": 1366.14,
      "duration": 4.47
    },
    {
      "text": "but sometimes the next five\nminutes of what you're gonna do",
      "start": 1370.61,
      "duration": 1.53
    },
    {
      "text": "is actually predictable from\nthe stuff you've done recently.",
      "start": 1372.14,
      "duration": 2.22
    },
    {
      "text": "And so, can you get to a world\nwhere that next five minutes",
      "start": 1374.36,
      "duration": 2.31
    },
    {
      "text": "either happens by you disengaging\nand it taking you through?",
      "start": 1376.67,
      "duration": 2.79
    },
    {
      "text": "Or maybe a little bit more\nof just you seeing next step",
      "start": 1379.46,
      "duration": 3.48
    },
    {
      "text": "what it's gonna do and you're like,",
      "start": 1382.94,
      "duration": 0.833
    },
    {
      "text": "\"Okay, that's good, that's\ngood, that's good, that's good,\"",
      "start": 1383.773,
      "duration": 1.597
    },
    {
      "text": "and you can just tap, tap\nthrough these big changes.",
      "start": 1385.37,
      "duration": 3.66
    },
    {
      "text": "- As we're talking about this,",
      "start": 1389.03,
      "duration": 1.093
    },
    {
      "text": "I should mention one of the really cool",
      "start": 1390.123,
      "duration": 2.597
    },
    {
      "text": "and noticeable things about Cursor is that",
      "start": 1392.72,
      "duration": 2.19
    },
    {
      "text": "there's this whole diff\ninterface situation going on.",
      "start": 1394.91,
      "duration": 2.88
    },
    {
      "text": "So, the model suggests\nwith the red and the green",
      "start": 1397.79,
      "duration": 4.8
    },
    {
      "text": "of here's how we're gonna modify the code,",
      "start": 1402.59,
      "duration": 1.86
    },
    {
      "text": "and in the chat window you can apply",
      "start": 1404.45,
      "duration": 2.85
    },
    {
      "text": "and it shows you the diff\nand you can accept the diff.",
      "start": 1407.3,
      "duration": 2.61
    },
    {
      "text": "So, maybe can you speak to\nwhatever direction of that?",
      "start": 1409.91,
      "duration": 2.79
    },
    {
      "text": "- We'll probably have four or\nfive different kinds of diffs.",
      "start": 1412.7,
      "duration": 4.994
    },
    {
      "text": "So we have optimized the\ndiff for the auto complete,",
      "start": 1417.694,
      "duration": 3.226
    },
    {
      "text": "so that has a different diff interface",
      "start": 1420.92,
      "duration": 1.9
    },
    {
      "text": "than when you're reviewing\nlarger blocks of code.",
      "start": 1424.687,
      "duration": 3.013
    },
    {
      "text": "And then we're trying to\noptimize another diff thing",
      "start": 1427.7,
      "duration": 3.09
    },
    {
      "text": "for when you're doing\nmultiple different files.",
      "start": 1430.79,
      "duration": 2.583
    },
    {
      "text": "And at a high level, the difference is",
      "start": 1434.42,
      "duration": 3.09
    },
    {
      "text": "for when you're doing auto-complete,",
      "start": 1437.51,
      "duration": 3.0
    },
    {
      "text": "it should be really, really fast to read.",
      "start": 1440.51,
      "duration": 2.05
    },
    {
      "text": "Actually, it should be really fast to read",
      "start": 1443.78,
      "duration": 1.44
    },
    {
      "text": "in all situations,",
      "start": 1445.22,
      "duration": 1.47
    },
    {
      "text": "but in auto-complete your\neyes are focused in one area.",
      "start": 1446.69,
      "duration": 4.773
    },
    {
      "text": "The humans can't look in\ntoo many different places.",
      "start": 1453.925,
      "duration": 1.525
    },
    {
      "text": "- So, you're talking about\non the interface side?",
      "start": 1455.45,
      "duration": 1.74
    },
    {
      "text": "- On the interface side.",
      "start": 1457.19,
      "duration": 0.833
    },
    {
      "text": "So it currently has this box on this side.",
      "start": 1458.023,
      "duration": 2.467
    },
    {
      "text": "So we have the current box,\nand it you tries to delete code",
      "start": 1460.49,
      "duration": 3.54
    },
    {
      "text": "in some place and tries to add other code,",
      "start": 1464.03,
      "duration": 3.12
    },
    {
      "text": "it tries to show you a box on the side.",
      "start": 1467.15,
      "duration": 1.779
    },
    {
      "text": "- You can maybe show it if\nwe pull it up in cursor.com.",
      "start": 1468.929,
      "duration": 2.691
    },
    {
      "text": "This is what we're talking.",
      "start": 1471.62,
      "duration": 1.89
    },
    {
      "text": "- So that box-\n- Exactly here.",
      "start": 1473.51,
      "duration": 1.56
    },
    {
      "text": "- It was like three or\nfour different attempts",
      "start": 1475.07,
      "duration": 3.39
    },
    {
      "text": "at trying to make this thing work",
      "start": 1478.46,
      "duration": 2.34
    },
    {
      "text": "where first the attempt was\nthis blue crossed out line.",
      "start": 1480.8,
      "duration": 4.83
    },
    {
      "text": "So before it was a box on the side,",
      "start": 1485.63,
      "duration": 2.49
    },
    {
      "text": "it used to show you the code to delete",
      "start": 1488.12,
      "duration": 2.31
    },
    {
      "text": "by showing you Google Docs style,",
      "start": 1490.43,
      "duration": 2.91
    },
    {
      "text": "you would see a line through it",
      "start": 1493.34,
      "duration": 1.617
    },
    {
      "text": "and then you would see the new code.",
      "start": 1494.957,
      "duration": 2.943
    },
    {
      "text": "That was super distracting.",
      "start": 1497.9,
      "duration": 1.473
    },
    {
      "text": "There was deletions, there\nwas trying the red highlight.",
      "start": 1502.808,
      "duration": 3.345
    },
    {
      "text": "Then the next iteration of\nit, which is sort of funny,",
      "start": 1506.153,
      "duration": 3.477
    },
    {
      "text": "you would hold the, on\nMac, the Option button.",
      "start": 1509.63,
      "duration": 4.86
    },
    {
      "text": "So, it would highlight a region of code",
      "start": 1514.49,
      "duration": 2.67
    },
    {
      "text": "to show you that there\nmight be something coming.",
      "start": 1517.16,
      "duration": 2.49
    },
    {
      "text": "So, maybe in this example,",
      "start": 1519.65,
      "duration": 2.43
    },
    {
      "text": "the input and the value\nwould all get blue.",
      "start": 1522.08,
      "duration": 4.143
    },
    {
      "text": "And the blue was to highlight that the AI",
      "start": 1527.09,
      "duration": 1.92
    },
    {
      "text": "had a suggestion for you.",
      "start": 1529.01,
      "duration": 1.25
    },
    {
      "text": "So instead of directly\nshowing you the thing,",
      "start": 1531.2,
      "duration": 2.25
    },
    {
      "text": "it would just hint that\nthe AI had a suggestion",
      "start": 1535.026,
      "duration": 1.514
    },
    {
      "text": "and if you really wanted to see it,",
      "start": 1536.54,
      "duration": 1.71
    },
    {
      "text": "you would hold the Option button,",
      "start": 1538.25,
      "duration": 2.22
    },
    {
      "text": "and then you would see the new suggestion.",
      "start": 1540.47,
      "duration": 2.19
    },
    {
      "text": "And if you release the Option button,",
      "start": 1542.66,
      "duration": 2.58
    },
    {
      "text": "you would then see your original code.",
      "start": 1545.24,
      "duration": 1.71
    },
    {
      "text": "- Mm-hmm, by the way, that's pretty nice,",
      "start": 1546.95,
      "duration": 2.64
    },
    {
      "text": "but you have to know to\nhold the Option button.",
      "start": 1549.59,
      "duration": 1.62
    },
    {
      "text": "- Yeah.",
      "start": 1551.21,
      "duration": 0.87
    },
    {
      "text": "- And by the way, I'm not a\nMac user, but I got it, Option.",
      "start": 1552.08,
      "duration": 3.587
    },
    {
      "text": "It's a button I guess you people have.",
      "start": 1555.667,
      "duration": 3.106
    },
    {
      "text": "- Again, it's just not intuitive.",
      "start": 1560.27,
      "duration": 1.65
    },
    {
      "text": "I think that's the key thing.",
      "start": 1561.92,
      "duration": 1.86
    },
    {
      "text": "- And there's a chance",
      "start": 1563.78,
      "duration": 0.833
    },
    {
      "text": "this is also not the final version of it.",
      "start": 1564.613,
      "duration": 2.377
    },
    {
      "text": "- I am personally very excited",
      "start": 1566.99,
      "duration": 1.29
    },
    {
      "text": "for making a lot of\nimprovements in this area.",
      "start": 1568.28,
      "duration": 4.92
    },
    {
      "text": "We often talk about it as\nthe verification problem",
      "start": 1574.43,
      "duration": 3.57
    },
    {
      "text": "where these diffs are\ngreat for small edits.",
      "start": 1578.0,
      "duration": 3.75
    },
    {
      "text": "For large edits or when it's\nmultiple files or something,",
      "start": 1581.75,
      "duration": 3.96
    },
    {
      "text": "it's actually a little bit prohibitive",
      "start": 1585.71,
      "duration": 4.5
    },
    {
      "text": "to review these diffs.",
      "start": 1590.21,
      "duration": 1.853
    },
    {
      "text": "So, there are a couple\nof different ideas here.",
      "start": 1594.14,
      "duration": 2.58
    },
    {
      "text": "One idea that we have is,",
      "start": 1596.72,
      "duration": 1.56
    },
    {
      "text": "okay, parts of the diffs are important.",
      "start": 1598.28,
      "duration": 2.82
    },
    {
      "text": "They have a lot of information.",
      "start": 1601.1,
      "duration": 1.38
    },
    {
      "text": "And then parts of the diff\nare just very low entropy.",
      "start": 1602.48,
      "duration": 4.05
    },
    {
      "text": "They're the same thing\nover and over again.",
      "start": 1606.53,
      "duration": 3.06
    },
    {
      "text": "And so maybe you can\nhighlight the important pieces",
      "start": 1609.59,
      "duration": 3.09
    },
    {
      "text": "and then gray out the\nnot so important pieces.",
      "start": 1612.68,
      "duration": 2.58
    },
    {
      "text": "Or maybe you can have a\nmodel that looks at the diff",
      "start": 1615.26,
      "duration": 3.3
    },
    {
      "text": "and sees, oh, there's a likely bug here.",
      "start": 1618.56,
      "duration": 2.46
    },
    {
      "text": "I will mark this with a\nlittle red squiggly and say,",
      "start": 1621.02,
      "duration": 3.667
    },
    {
      "text": "\"You should probably review\nthis part of the diff.\"",
      "start": 1624.687,
      "duration": 2.55
    },
    {
      "text": "Ideas in that vein I think are exciting.",
      "start": 1628.67,
      "duration": 2.79
    },
    {
      "text": "- Yeah, that's a really fascinating space",
      "start": 1631.46,
      "duration": 2.34
    },
    {
      "text": "of UX design engineering.",
      "start": 1633.8,
      "duration": 2.22
    },
    {
      "text": "So you're, basically, trying\nto guide the human programmer",
      "start": 1636.02,
      "duration": 4.86
    },
    {
      "text": "through all the things they need to read",
      "start": 1640.88,
      "duration": 1.68
    },
    {
      "text": "and nothing more, optimally.",
      "start": 1642.56,
      "duration": 2.79
    },
    {
      "text": "- Yeah, and you want an\nintelligent model to do it.",
      "start": 1645.35,
      "duration": 2.73
    },
    {
      "text": "Currently, diff algorithms,",
      "start": 1648.08,
      "duration": 3.223
    },
    {
      "text": "they're just like normal algorithms.",
      "start": 1651.303,
      "duration": 4.817
    },
    {
      "text": "There's no intelligence.",
      "start": 1656.12,
      "duration": 1.92
    },
    {
      "text": "There's intelligence that went\ninto designing the algorithm,",
      "start": 1658.04,
      "duration": 2.31
    },
    {
      "text": "but then you don't care\nif it's about this thing",
      "start": 1660.35,
      "duration": 3.9
    },
    {
      "text": "or this thing as you want\nthe model to do this.",
      "start": 1664.25,
      "duration": 3.06
    },
    {
      "text": "- So, I think the\ngeneral question is like,",
      "start": 1667.31,
      "duration": 3.063
    },
    {
      "text": "man, these models are\ngoing to get much smarter.",
      "start": 1671.42,
      "duration": 2.25
    },
    {
      "text": "As the models get much smarter,",
      "start": 1673.67,
      "duration": 1.623
    },
    {
      "text": "changes they will be able\nto propose are much bigger.",
      "start": 1676.58,
      "duration": 3.21
    },
    {
      "text": "So as the changes gets\nbigger and bigger and bigger,",
      "start": 1679.79,
      "duration": 2.28
    },
    {
      "text": "the humans have to do more and more",
      "start": 1682.07,
      "duration": 1.5
    },
    {
      "text": "and more verification work.",
      "start": 1683.57,
      "duration": 1.383
    },
    {
      "text": "You need to help them out.",
      "start": 1687.35,
      "duration": 1.563
    },
    {
      "text": "I don't wanna spend all\nmy time reviewing code.",
      "start": 1690.44,
      "duration": 2.35
    },
    {
      "text": "- Can you say a little more\nacross multiple files diff?",
      "start": 1695.48,
      "duration": 4.62
    },
    {
      "text": "- Yeah, I mean, so GitHub\ntries to solve this, right,",
      "start": 1700.1,
      "duration": 3.48
    },
    {
      "text": "with code review.",
      "start": 1703.58,
      "duration": 1.56
    },
    {
      "text": "When you're doing code review,",
      "start": 1705.14,
      "duration": 0.99
    },
    {
      "text": "you're reviewing multiple\ndiffs across multiple files.",
      "start": 1706.13,
      "duration": 3.57
    },
    {
      "text": "But like Arvid said earlier,",
      "start": 1709.7,
      "duration": 2.43
    },
    {
      "text": "I think you can do much\nbetter than code review.",
      "start": 1712.13,
      "duration": 3.09
    },
    {
      "text": "Code review kind of sucks.",
      "start": 1715.22,
      "duration": 1.98
    },
    {
      "text": "You spend a lot of time\ntrying to grok this code",
      "start": 1717.2,
      "duration": 2.55
    },
    {
      "text": "that's often quite unfamiliar to you",
      "start": 1719.75,
      "duration": 2.52
    },
    {
      "text": "and it often doesn't even\nactually catch that many bugs.",
      "start": 1722.27,
      "duration": 5.0
    },
    {
      "text": "And I think you can significantly improve",
      "start": 1728.03,
      "duration": 2.28
    },
    {
      "text": "that review experience using\nlanguage models, for example,",
      "start": 1730.31,
      "duration": 2.34
    },
    {
      "text": "using the kinds of tricks\nthat Arvid had described",
      "start": 1732.65,
      "duration": 2.28
    },
    {
      "text": "of maybe pointing you towards the regions",
      "start": 1734.93,
      "duration": 2.73
    },
    {
      "text": "that actually matter.",
      "start": 1737.66,
      "duration": 1.293
    },
    {
      "text": "I think also if the code is produced",
      "start": 1741.38,
      "duration": 2.67
    },
    {
      "text": "by these language models",
      "start": 1744.05,
      "duration": 1.38
    },
    {
      "text": "and it's not produced by someone else.",
      "start": 1745.43,
      "duration": 1.9
    },
    {
      "text": "The code review experience is\ndesign for both the reviewer",
      "start": 1748.94,
      "duration": 5.0
    },
    {
      "text": "and the person that produced the code.",
      "start": 1754.28,
      "duration": 2.04
    },
    {
      "text": "In the case where the person",
      "start": 1756.32,
      "duration": 1.5
    },
    {
      "text": "that produced the code\nis a language model,",
      "start": 1757.82,
      "duration": 2.28
    },
    {
      "text": "you don't have to care that\nmuch about their experience",
      "start": 1760.1,
      "duration": 2.1
    },
    {
      "text": "and you can design the entire thing",
      "start": 1762.2,
      "duration": 1.92
    },
    {
      "text": "around the reviewer such that\nthe reviewer's job is as fun,",
      "start": 1764.12,
      "duration": 4.89
    },
    {
      "text": "as easy, as productive as possible.",
      "start": 1769.01,
      "duration": 2.043
    },
    {
      "text": "I think that feels like the\nissue with just naively trying",
      "start": 1773.06,
      "duration": 2.61
    },
    {
      "text": "to make these things\nlook like code review.",
      "start": 1775.67,
      "duration": 3.84
    },
    {
      "text": "I think you can be a lot more creative",
      "start": 1779.51,
      "duration": 1.56
    },
    {
      "text": "and push the boundary on what's possible.",
      "start": 1781.07,
      "duration": 1.53
    },
    {
      "text": "- And just one idea there\nis, I think ordering matters.",
      "start": 1782.6,
      "duration": 4.14
    },
    {
      "text": "Generally, when you review a\nPR, you have this list of files",
      "start": 1786.74,
      "duration": 3.42
    },
    {
      "text": "and you're reviewing\nthem from top to bottom,",
      "start": 1790.16,
      "duration": 2.22
    },
    {
      "text": "but you actually wanna\nunderstand this part first",
      "start": 1792.38,
      "duration": 3.48
    },
    {
      "text": "because that came logically first,",
      "start": 1795.86,
      "duration": 1.74
    },
    {
      "text": "and then you want to\nunderstand the next part.",
      "start": 1797.6,
      "duration": 1.5
    },
    {
      "text": "And you don't want to have\nto figure out that yourself.",
      "start": 1799.1,
      "duration": 3.63
    },
    {
      "text": "You want a model to guide\nyou through the thing.",
      "start": 1802.73,
      "duration": 2.643
    },
    {
      "text": "- And is the step of\ncreation going to be more",
      "start": 1806.432,
      "duration": 1.608
    },
    {
      "text": "and more natural language,",
      "start": 1808.04,
      "duration": 1.47
    },
    {
      "text": "is the goal versus with\nactual writing the book?",
      "start": 1809.51,
      "duration": 3.083
    },
    {
      "text": "- I think sometimes. I don't\nthink it's going to be the case",
      "start": 1812.593,
      "duration": 3.007
    },
    {
      "text": "that all of programming\nwill be natural language,",
      "start": 1815.6,
      "duration": 2.79
    },
    {
      "text": "and the reason for that is",
      "start": 1818.39,
      "duration": 2.16
    },
    {
      "text": "if I'm pair programming with Sualeh,",
      "start": 1820.55,
      "duration": 2.122
    },
    {
      "text": "and Sualeh is at the\ncomputer and the keyboard,",
      "start": 1822.672,
      "duration": 1.808
    },
    {
      "text": "and sometimes if I'm driving,\nI want to say to Sualeh,",
      "start": 1824.48,
      "duration": 5.0
    },
    {
      "text": "\"Hey, implement this\nfunction,\" and that works.",
      "start": 1829.527,
      "duration": 3.533
    },
    {
      "text": "And then sometimes it's just so annoying",
      "start": 1833.06,
      "duration": 2.79
    },
    {
      "text": "to explain to Sualeh\nwhat I want him to do,",
      "start": 1835.85,
      "duration": 2.04
    },
    {
      "text": "and so I actually take over\nthe keyboard and I show him.",
      "start": 1837.89,
      "duration": 3.66
    },
    {
      "text": "I write part of the example\nand then it makes sense",
      "start": 1841.55,
      "duration": 3.96
    },
    {
      "text": "and that's the easiest way to communicate.",
      "start": 1845.51,
      "duration": 1.95
    },
    {
      "text": "And so, I think that's\nalso the case for AI.",
      "start": 1847.46,
      "duration": 2.22
    },
    {
      "text": "Sometimes the easiest way\nto communicate with the AI",
      "start": 1849.68,
      "duration": 2.07
    },
    {
      "text": "will be to show an example",
      "start": 1851.75,
      "duration": 0.99
    },
    {
      "text": "and then it goes and does\nthe thing everywhere else.",
      "start": 1852.74,
      "duration": 2.22
    },
    {
      "text": "Or sometimes if you're making\na website, for example,",
      "start": 1854.96,
      "duration": 2.82
    },
    {
      "text": "the easiest way to show\nto the AI what you want",
      "start": 1857.78,
      "duration": 3.21
    },
    {
      "text": "is not to tell it what to do",
      "start": 1860.99,
      "duration": 1.41
    },
    {
      "text": "but drag things around or draw things,",
      "start": 1862.4,
      "duration": 2.643
    },
    {
      "text": "and maybe eventually,",
      "start": 1866.03,
      "duration": 2.76
    },
    {
      "text": "we will get to brain machine\ninterfaces or whatever",
      "start": 1868.79,
      "duration": 2.37
    },
    {
      "text": "and you can understand\nwhat you're thinking.",
      "start": 1871.16,
      "duration": 1.65
    },
    {
      "text": "And so, I think natural\nlanguage will have a place.",
      "start": 1872.81,
      "duration": 1.95
    },
    {
      "text": "I think it will definitely not be the way",
      "start": 1874.76,
      "duration": 3.03
    },
    {
      "text": "most people program most of the time.",
      "start": 1877.79,
      "duration": 2.79
    },
    {
      "text": "- I'm really feeling the\nAGI with this editor.",
      "start": 1880.58,
      "duration": 2.849
    },
    {
      "text": "(group chuckling) It\nfeels like there's a lot",
      "start": 1883.429,
      "duration": 1.666
    },
    {
      "text": "of machine learning going on underneath.",
      "start": 1885.095,
      "duration": 2.685
    },
    {
      "text": "Tell me about some of the ML\nstuff that makes it all work?",
      "start": 1887.78,
      "duration": 3.45
    },
    {
      "text": "- Where Cursor really works",
      "start": 1891.23,
      "duration": 1.74
    },
    {
      "text": "via this ensemble of custom models",
      "start": 1892.97,
      "duration": 2.58
    },
    {
      "text": "that we've trained alongside\nthe frontier models",
      "start": 1895.55,
      "duration": 2.91
    },
    {
      "text": "that are fantastic at the\nreasoning intense things.",
      "start": 1898.46,
      "duration": 2.187
    },
    {
      "text": "And so Cursor Tab, for\nexample, is a great example",
      "start": 1900.647,
      "duration": 3.213
    },
    {
      "text": "of where you can specialize\nthis model to be,",
      "start": 1903.86,
      "duration": 2.16
    },
    {
      "text": "even better than even frontier models",
      "start": 1906.02,
      "duration": 1.56
    },
    {
      "text": "if you look at evals on\nthe task we set it at.",
      "start": 1907.58,
      "duration": 2.82
    },
    {
      "text": "The other domain, which it's surprising",
      "start": 1910.4,
      "duration": 2.73
    },
    {
      "text": "that it requires custom models",
      "start": 1913.13,
      "duration": 1.44
    },
    {
      "text": "but it's necessary and works\nquite well, is in Apply.",
      "start": 1914.57,
      "duration": 3.573
    },
    {
      "text": "The frontier models are quite\ngood at sketching out plans",
      "start": 1922.37,
      "duration": 2.52
    },
    {
      "text": "for code and generating\nrough sketches of the change,",
      "start": 1924.89,
      "duration": 2.88
    },
    {
      "text": "but actually, creating diffs is quite hard",
      "start": 1927.77,
      "duration": 4.09
    },
    {
      "text": "for frontier models, for\nyour training models.",
      "start": 1933.14,
      "duration": 2.3
    },
    {
      "text": "You try to do this with Sonnet,\nwith o1, any frontier model",
      "start": 1937.85,
      "duration": 4.23
    },
    {
      "text": "and it really messes up stupid things",
      "start": 1942.08,
      "duration": 2.19
    },
    {
      "text": "like counting line numbers,",
      "start": 1944.27,
      "duration": 1.74
    },
    {
      "text": "especially in super, super large files.",
      "start": 1946.01,
      "duration": 2.433
    },
    {
      "text": "And so what we've done to alleviate this",
      "start": 1949.82,
      "duration": 2.07
    },
    {
      "text": "is we let the model sketch\nout this rough code block",
      "start": 1951.89,
      "duration": 3.36
    },
    {
      "text": "that indicates what the change will be",
      "start": 1955.25,
      "duration": 2.73
    },
    {
      "text": "and we train a model to then\nApply that change to the file.",
      "start": 1957.98,
      "duration": 4.317
    },
    {
      "text": "- And we should say that\nApply is the model looks",
      "start": 1962.297,
      "duration": 3.753
    },
    {
      "text": "at your code, it gives you a\nreally damn good suggestion",
      "start": 1966.05,
      "duration": 3.42
    },
    {
      "text": "of what new things to do.",
      "start": 1969.47,
      "duration": 2.94
    },
    {
      "text": "And the seemingly for humans trivial step",
      "start": 1972.41,
      "duration": 2.67
    },
    {
      "text": "of combining the two, you're\nsaying is not so trivial.",
      "start": 1975.08,
      "duration": 4.29
    },
    {
      "text": "- Contrary to popular perception,",
      "start": 1979.37,
      "duration": 1.8
    },
    {
      "text": "it is not a deterministic algorithm.",
      "start": 1981.17,
      "duration": 1.89
    },
    {
      "text": "- Yeah, I think you see shallow\ncopies of Apply elsewhere",
      "start": 1983.06,
      "duration": 5.0
    },
    {
      "text": "and it just breaks most of the time",
      "start": 1989.45,
      "duration": 2.04
    },
    {
      "text": "because you think you can try to do",
      "start": 1991.49,
      "duration": 1.41
    },
    {
      "text": "some deterministic matching,",
      "start": 1992.9,
      "duration": 1.08
    },
    {
      "text": "and then it fails at least 40% of the time",
      "start": 1993.98,
      "duration": 4.17
    },
    {
      "text": "and that just results in a\nterrible product experience.",
      "start": 1998.15,
      "duration": 3.273
    },
    {
      "text": "I think in general, this regime of you",
      "start": 2003.31,
      "duration": 3.15
    },
    {
      "text": "are going to get smarter\nand smarter models.",
      "start": 2006.46,
      "duration": 2.97
    },
    {
      "text": "So one other thing that Apply lets you do",
      "start": 2009.43,
      "duration": 2.34
    },
    {
      "text": "is it lets you use fewer tokens",
      "start": 2011.77,
      "duration": 3.42
    },
    {
      "text": "with the most intelligent models.",
      "start": 2015.19,
      "duration": 2.07
    },
    {
      "text": "This is both expensive in terms of latency",
      "start": 2017.26,
      "duration": 2.67
    },
    {
      "text": "for generating all these tokens and cost.",
      "start": 2019.93,
      "duration": 4.29
    },
    {
      "text": "So, you can give this\nvery, very rough sketch",
      "start": 2024.22,
      "duration": 2.94
    },
    {
      "text": "and then have your model\nmodels go and implement it",
      "start": 2027.16,
      "duration": 2.73
    },
    {
      "text": "because it's a much\neasier task to implement,",
      "start": 2029.89,
      "duration": 2.13
    },
    {
      "text": "this very, very sketched out code.",
      "start": 2032.02,
      "duration": 2.31
    },
    {
      "text": "And I think that this regime will continue",
      "start": 2034.33,
      "duration": 1.89
    },
    {
      "text": "where you can use smarter\nand smarter models",
      "start": 2036.22,
      "duration": 2.07
    },
    {
      "text": "to do the planning and then\nmaybe the implementation details",
      "start": 2038.29,
      "duration": 3.3
    },
    {
      "text": "can be handled by the\nless intelligent ones.",
      "start": 2041.59,
      "duration": 1.98
    },
    {
      "text": "Perhaps you'll have maybe o1,",
      "start": 2043.57,
      "duration": 2.28
    },
    {
      "text": "maybe it'll be even more capable models",
      "start": 2045.85,
      "duration": 2.52
    },
    {
      "text": "given an even higher level plan",
      "start": 2048.37,
      "duration": 2.43
    },
    {
      "text": "that is recursively applied by Sauna",
      "start": 2050.8,
      "duration": 4.83
    },
    {
      "text": "and then the Apply model.",
      "start": 2055.63,
      "duration": 0.99
    },
    {
      "text": "- Maybe we should talk\nabout how to make it fast",
      "start": 2056.62,
      "duration": 2.55
    },
    {
      "text": "if you like.",
      "start": 2059.17,
      "duration": 0.833
    },
    {
      "text": "Fast is always an interesting detail.",
      "start": 2060.003,
      "duration": 1.507
    },
    {
      "text": "- [Arvid] Fast is good.",
      "start": 2061.51,
      "duration": 1.26
    },
    {
      "text": "- Yeah, how do you make it fast?",
      "start": 2062.77,
      "duration": 2.34
    },
    {
      "text": "- Yeah, so one big component",
      "start": 2065.11,
      "duration": 1.92
    },
    {
      "text": "of making it fast is speculative edits.",
      "start": 2067.03,
      "duration": 2.91
    },
    {
      "text": "So, speculative edits are a\nvariant of speculative decoding,",
      "start": 2069.94,
      "duration": 3.18
    },
    {
      "text": "and maybe it'd be helpful",
      "start": 2073.12,
      "duration": 0.96
    },
    {
      "text": "to briefly describe speculative decoding.",
      "start": 2074.08,
      "duration": 2.763
    },
    {
      "text": "With speculative decoding,",
      "start": 2077.966,
      "duration": 1.274
    },
    {
      "text": "what you do is you can\ntake advantage of the fact",
      "start": 2079.24,
      "duration": 3.81
    },
    {
      "text": "that most of the time,\nand I'll add the caveat",
      "start": 2083.05,
      "duration": 3.57
    },
    {
      "text": "that it would be when you're memory bound",
      "start": 2086.62,
      "duration": 2.4
    },
    {
      "text": "in language model generation,",
      "start": 2089.02,
      "duration": 1.593
    },
    {
      "text": "if you process multiple tokens at once,",
      "start": 2091.9,
      "duration": 4.02
    },
    {
      "text": "it is faster than generating\none token at a time.",
      "start": 2095.92,
      "duration": 2.85
    },
    {
      "text": "So this is the same reason why",
      "start": 2098.77,
      "duration": 1.71
    },
    {
      "text": "if you look at tokens per\nsecond with prompt tokens",
      "start": 2100.48,
      "duration": 3.75
    },
    {
      "text": "versus generated tokens,",
      "start": 2104.23,
      "duration": 0.99
    },
    {
      "text": "it's much much faster for prompt tokens.",
      "start": 2105.22,
      "duration": 2.313
    },
    {
      "text": "So what we do is instead of using",
      "start": 2109.33,
      "duration": 2.88
    },
    {
      "text": "what speculative decoding normally does,",
      "start": 2112.21,
      "duration": 1.56
    },
    {
      "text": "which is using a really small model",
      "start": 2113.77,
      "duration": 1.98
    },
    {
      "text": "to predict these draft\ntokens that your larger model",
      "start": 2115.75,
      "duration": 2.25
    },
    {
      "text": "will then go in and verify,",
      "start": 2118.0,
      "duration": 2.613
    },
    {
      "text": "with code edits, we\nhave a very strong prior",
      "start": 2122.14,
      "duration": 1.86
    },
    {
      "text": "of what the existing code will look like,",
      "start": 2124.0,
      "duration": 1.95
    },
    {
      "text": "and that prior is literally\nthe same exact code.",
      "start": 2125.95,
      "duration": 3.66
    },
    {
      "text": "So you can do is you can just feed chunks",
      "start": 2129.61,
      "duration": 1.98
    },
    {
      "text": "of the original code back into the model,",
      "start": 2131.59,
      "duration": 2.223
    },
    {
      "text": "and then the model will\njust pretty much agree",
      "start": 2135.01,
      "duration": 2.88
    },
    {
      "text": "most of the time that,",
      "start": 2137.89,
      "duration": 1.087
    },
    {
      "text": "\"Okay, I'm just gonna\nspit this code back out.\"",
      "start": 2138.977,
      "duration": 1.426
    },
    {
      "text": "And so, you can process all\nof those lines in parallel",
      "start": 2140.403,
      "duration": 3.037
    },
    {
      "text": "and you just do this with\nsufficiently many chunks.",
      "start": 2143.44,
      "duration": 1.83
    },
    {
      "text": "And then eventually, you'll\nreach a point of disagreement",
      "start": 2145.27,
      "duration": 2.43
    },
    {
      "text": "where the model will now\npredict text that is different",
      "start": 2147.7,
      "duration": 3.48
    },
    {
      "text": "from the ground truth original code.",
      "start": 2151.18,
      "duration": 2.22
    },
    {
      "text": "It'll generate those tokens\nand then we will decide",
      "start": 2153.4,
      "duration": 2.85
    },
    {
      "text": "after enough tokens\nmatch the original code",
      "start": 2156.25,
      "duration": 3.0
    },
    {
      "text": "to re-start speculating in chunks of code.",
      "start": 2159.25,
      "duration": 3.0
    },
    {
      "text": "What this actually ends up looking like",
      "start": 2162.25,
      "duration": 2.73
    },
    {
      "text": "is just a much faster version\nof normal editing code.",
      "start": 2164.98,
      "duration": 4.02
    },
    {
      "text": "So, it looks like a much faster version",
      "start": 2169.0,
      "duration": 3.09
    },
    {
      "text": "of the model rewriting all the code.",
      "start": 2172.09,
      "duration": 1.62
    },
    {
      "text": "So, we can use the same exact interface",
      "start": 2173.71,
      "duration": 2.567
    },
    {
      "text": "that we use for diffs,",
      "start": 2176.277,
      "duration": 2.863
    },
    {
      "text": "but it will just stream down a lot faster.",
      "start": 2179.14,
      "duration": 2.79
    },
    {
      "text": "- And then the advantage is\nthat while it's streaming,",
      "start": 2181.93,
      "duration": 2.73
    },
    {
      "text": "you can just also start reviewing the code",
      "start": 2184.66,
      "duration": 3.69
    },
    {
      "text": "before it's done so there's\nno big loading screen.",
      "start": 2188.35,
      "duration": 2.853
    },
    {
      "text": "Maybe that is part of the advantage.",
      "start": 2193.18,
      "duration": 3.27
    },
    {
      "text": "- So, the human can start\nreading before the thing is done.",
      "start": 2196.45,
      "duration": 3.06
    },
    {
      "text": "- I think the interesting\nriff here is something like,",
      "start": 2199.51,
      "duration": 2.831
    },
    {
      "text": "I feel like speculation is a\nfairly common idea nowadays.",
      "start": 2202.341,
      "duration": 3.349
    },
    {
      "text": "It's not only in language models.",
      "start": 2205.69,
      "duration": 1.77
    },
    {
      "text": "There's obviously speculation in CPUs",
      "start": 2207.46,
      "duration": 1.77
    },
    {
      "text": "and there's speculation for databases",
      "start": 2209.23,
      "duration": 2.34
    },
    {
      "text": "and there's speculation\nall over the place.",
      "start": 2211.57,
      "duration": 3.09
    },
    {
      "text": "- Well, let me ask the ridiculous question",
      "start": 2214.66,
      "duration": 2.25
    },
    {
      "text": "of which LLM is better at coding?",
      "start": 2216.91,
      "duration": 3.03
    },
    {
      "text": "GPT, Claude, who wins in\nthe context of programming?",
      "start": 2219.94,
      "duration": 4.2
    },
    {
      "text": "And I'm sure the answer\nis much more nuanced",
      "start": 2224.14,
      "duration": 1.74
    },
    {
      "text": "because it sounds like every single part",
      "start": 2225.88,
      "duration": 2.55
    },
    {
      "text": "of this involves a different model.",
      "start": 2228.43,
      "duration": 3.6
    },
    {
      "text": "- Yeah, I think there's no model",
      "start": 2232.03,
      "duration": 3.15
    },
    {
      "text": "that Pareto dominates others,",
      "start": 2235.18,
      "duration": 3.75
    },
    {
      "text": "meaning, it is better in all categories",
      "start": 2238.93,
      "duration": 2.49
    },
    {
      "text": "that we think matter, the\ncategories being speed,",
      "start": 2241.42,
      "duration": 3.933
    },
    {
      "text": "ability to edit code, ability\nto process lots of code,",
      "start": 2247.57,
      "duration": 3.33
    },
    {
      "text": "long context, a couple of other things",
      "start": 2250.9,
      "duration": 1.993
    },
    {
      "text": "and coding capabilities.",
      "start": 2252.893,
      "duration": 2.657
    },
    {
      "text": "The one that I'd say right now\nis just net best is Sonnet.",
      "start": 2255.55,
      "duration": 3.93
    },
    {
      "text": "I think this is a consensus opinion.",
      "start": 2259.48,
      "duration": 1.89
    },
    {
      "text": "O1's really interesting and\nit's really good at reasoning.",
      "start": 2261.37,
      "duration": 3.6
    },
    {
      "text": "So if you give it really hard",
      "start": 2264.97,
      "duration": 1.75
    },
    {
      "text": "programming interview style\nproblems or lead code problems,",
      "start": 2267.7,
      "duration": 3.78
    },
    {
      "text": "it can do quite well on them,",
      "start": 2271.48,
      "duration": 1.773
    },
    {
      "text": "but it doesn't feel like it\nunderstands your rough intent",
      "start": 2274.12,
      "duration": 4.89
    },
    {
      "text": "as well as Sonnet does.",
      "start": 2279.01,
      "duration": 1.413
    },
    {
      "text": "If you look at a lot of\nthe other frontier models,",
      "start": 2282.25,
      "duration": 3.36
    },
    {
      "text": "one qualm I have is it feels like",
      "start": 2285.61,
      "duration": 2.49
    },
    {
      "text": "they're not necessarily over,",
      "start": 2288.1,
      "duration": 1.47
    },
    {
      "text": "I'm not saying they train on benchmarks,",
      "start": 2289.57,
      "duration": 2.49
    },
    {
      "text": "but they perform really\nwell in benchmarks relative",
      "start": 2292.06,
      "duration": 3.0
    },
    {
      "text": "to everything that's in the middle.",
      "start": 2295.06,
      "duration": 2.73
    },
    {
      "text": "So if you tried on all these benchmarks",
      "start": 2297.79,
      "duration": 1.47
    },
    {
      "text": "and things that are in the distribution",
      "start": 2299.26,
      "duration": 1.5
    },
    {
      "text": "of the benchmarks they're evaluated on,",
      "start": 2300.76,
      "duration": 1.83
    },
    {
      "text": "they'll do really well.",
      "start": 2302.59,
      "duration": 0.833
    },
    {
      "text": "But when you push them a\nlittle bit outside of that,",
      "start": 2303.423,
      "duration": 2.287
    },
    {
      "text": "Sonnet is I think the one that does best",
      "start": 2305.71,
      "duration": 2.76
    },
    {
      "text": "at maintaining that same capability.",
      "start": 2308.47,
      "duration": 3.0
    },
    {
      "text": "You have the same\ncapability in the benchmark",
      "start": 2311.47,
      "duration": 2.01
    },
    {
      "text": "as when you try to instruct\nit to do anything with coding.",
      "start": 2313.48,
      "duration": 3.363
    },
    {
      "text": "- Another ridiculous\nquestion is the difference",
      "start": 2318.31,
      "duration": 1.65
    },
    {
      "text": "between the normal programming experience",
      "start": 2319.96,
      "duration": 2.46
    },
    {
      "text": "versus what benchmarks represent?",
      "start": 2322.42,
      "duration": 2.64
    },
    {
      "text": "Where do benchmarks fall\nshort, do you think,",
      "start": 2325.06,
      "duration": 2.34
    },
    {
      "text": "when we're evaluating these models?",
      "start": 2327.4,
      "duration": 1.86
    },
    {
      "text": "- By the way, that's\na really, really hard,",
      "start": 2329.26,
      "duration": 2.15
    },
    {
      "text": "critically important detail",
      "start": 2332.41,
      "duration": 1.92
    },
    {
      "text": "of how different benchmarks\nare versus real coding,",
      "start": 2334.33,
      "duration": 4.29
    },
    {
      "text": "where real coding, it's\nnot interview style coding.",
      "start": 2338.62,
      "duration": 5.0
    },
    {
      "text": "Humans are saying\nhalf-broken English sometimes",
      "start": 2346.75,
      "duration": 3.63
    },
    {
      "text": "and sometimes you're saying,\n\"Oh, do what I did before.\"",
      "start": 2350.38,
      "duration": 4.26
    },
    {
      "text": "Sometimes you're saying,",
      "start": 2354.64,
      "duration": 1.473
    },
    {
      "text": "\"Go add this thing and then\ndo this other thing for me",
      "start": 2358.187,
      "duration": 2.273
    },
    {
      "text": "and then make this UI element.\"",
      "start": 2360.46,
      "duration": 1.41
    },
    {
      "text": "And then, it's just a lot of\nthings are context dependent.",
      "start": 2361.87,
      "duration": 5.0
    },
    {
      "text": "You really want to understand the human",
      "start": 2367.9,
      "duration": 2.16
    },
    {
      "text": "and then do what the human\nwants, as opposed to this,",
      "start": 2370.06,
      "duration": 3.18
    },
    {
      "text": "maybe the way to put it abstractly",
      "start": 2373.24,
      "duration": 2.37
    },
    {
      "text": "is the interview problems\nare very well specified.",
      "start": 2375.61,
      "duration": 5.0
    },
    {
      "text": "They lean a lot on specification",
      "start": 2382.15,
      "duration": 3.0
    },
    {
      "text": "while the human stuff is less specified.",
      "start": 2385.15,
      "duration": 3.483
    },
    {
      "text": "- Yeah.",
      "start": 2389.74,
      "duration": 0.833
    },
    {
      "text": "I think that this benchmark\nquestion is both complicated",
      "start": 2390.573,
      "duration": 2.257
    },
    {
      "text": "by what Sualeh just mentioned,",
      "start": 2392.83,
      "duration": 2.25
    },
    {
      "text": "and then also what Aman was getting into,",
      "start": 2395.08,
      "duration": 4.113
    },
    {
      "text": "there's this problem of the skew",
      "start": 2400.69,
      "duration": 1.17
    },
    {
      "text": "between what can you actually model",
      "start": 2401.86,
      "duration": 1.11
    },
    {
      "text": "in a benchmark versus real programming,",
      "start": 2402.97,
      "duration": 2.76
    },
    {
      "text": "and that can be sometimes\nhard to encapsulate",
      "start": 2405.73,
      "duration": 1.68
    },
    {
      "text": "because it's real programming's very messy",
      "start": 2407.41,
      "duration": 2.91
    },
    {
      "text": "and sometimes things\naren't super well specified",
      "start": 2410.32,
      "duration": 2.49
    },
    {
      "text": "what's correct or what isn't.",
      "start": 2412.81,
      "duration": 1.26
    },
    {
      "text": "But then it's also doubly hard",
      "start": 2414.07,
      "duration": 2.55
    },
    {
      "text": "because of this public benchmark problem.",
      "start": 2416.62,
      "duration": 1.68
    },
    {
      "text": "And that's both because public benchmarks",
      "start": 2418.3,
      "duration": 2.07
    },
    {
      "text": "are sometimes hill climbed on,",
      "start": 2420.37,
      "duration": 1.47
    },
    {
      "text": "then it's really, really\nhard to also get the data",
      "start": 2421.84,
      "duration": 2.91
    },
    {
      "text": "from the public benchmarks\nout of the models.",
      "start": 2424.75,
      "duration": 3.51
    },
    {
      "text": "And so for instance,",
      "start": 2428.26,
      "duration": 1.62
    },
    {
      "text": "one of the most popular\nagent benchmarks, SWE-Bench,",
      "start": 2429.88,
      "duration": 3.723
    },
    {
      "text": "is really, really contaminated\nin the training data",
      "start": 2434.92,
      "duration": 2.55
    },
    {
      "text": "of these foundation models.",
      "start": 2437.47,
      "duration": 1.89
    },
    {
      "text": "And so if you ask these foundation models",
      "start": 2439.36,
      "duration": 1.56
    },
    {
      "text": "to do a SWE-Bench problem,",
      "start": 2440.92,
      "duration": 1.167
    },
    {
      "text": "but you actually don't give\nthem the context of a code base,",
      "start": 2442.087,
      "duration": 2.103
    },
    {
      "text": "they can hallucinate the right file pass,",
      "start": 2444.19,
      "duration": 1.71
    },
    {
      "text": "they can hallucinate the\nright function names.",
      "start": 2445.9,
      "duration": 3.0
    },
    {
      "text": "And so, it's also just the public aspect",
      "start": 2448.9,
      "duration": 3.18
    },
    {
      "text": "of these things is tricky.",
      "start": 2452.08,
      "duration": 1.32
    },
    {
      "text": "- Yeah, in that case, it could be trained",
      "start": 2453.4,
      "duration": 2.01
    },
    {
      "text": "on the literal issues or\npull requests themselves,",
      "start": 2455.41,
      "duration": 3.54
    },
    {
      "text": "and maybe the labs will\nstart to do a better job",
      "start": 2458.95,
      "duration": 3.33
    },
    {
      "text": "or they've already done a good job",
      "start": 2462.28,
      "duration": 1.56
    },
    {
      "text": "at decontaminating those things,",
      "start": 2463.84,
      "duration": 1.56
    },
    {
      "text": "but they're not going to\nomit the actual training data",
      "start": 2465.4,
      "duration": 2.85
    },
    {
      "text": "of the repository itself.",
      "start": 2468.25,
      "duration": 1.74
    },
    {
      "text": "These are all some of the most\npopular Python repositories.",
      "start": 2469.99,
      "duration": 2.85
    },
    {
      "text": "SimPy is one example.",
      "start": 2472.84,
      "duration": 1.92
    },
    {
      "text": "I don't think they're going\nto handicap their models",
      "start": 2474.76,
      "duration": 2.97
    },
    {
      "text": "on SimPy and all these\npopular Python repositories",
      "start": 2477.73,
      "duration": 2.52
    },
    {
      "text": "in order to get true evaluation\nscores in these benchmarks.",
      "start": 2480.25,
      "duration": 3.93
    },
    {
      "text": "- I think that given\nthe dirts in benchmarks,",
      "start": 2484.18,
      "duration": 3.123
    },
    {
      "text": "there have been a few\ninteresting crutches that places",
      "start": 2488.26,
      "duration": 2.88
    },
    {
      "text": "that build systems with these models",
      "start": 2491.14,
      "duration": 1.41
    },
    {
      "text": "or build these models actually use",
      "start": 2492.55,
      "duration": 2.46
    },
    {
      "text": "to get a sense of are they going\nthe right direction or not.",
      "start": 2495.01,
      "duration": 2.13
    },
    {
      "text": "And in a lot of places,",
      "start": 2497.14,
      "duration": 2.28
    },
    {
      "text": "people will actually just have\nhumans play with the things",
      "start": 2499.42,
      "duration": 2.55
    },
    {
      "text": "and give qualitative feedback on these.",
      "start": 2501.97,
      "duration": 2.55
    },
    {
      "text": "One or two of the\nfoundation model companies,",
      "start": 2504.52,
      "duration": 1.74
    },
    {
      "text": "they have people that's\na big part of their role.",
      "start": 2506.26,
      "duration": 2.73
    },
    {
      "text": "And internally, we also\nqualitatively assess these models",
      "start": 2508.99,
      "duration": 4.05
    },
    {
      "text": "and actually lean on that a lot",
      "start": 2513.04,
      "duration": 0.99
    },
    {
      "text": "in addition to private\nemails that we have.",
      "start": 2514.03,
      "duration": 2.52
    },
    {
      "text": "- [Arvid] It's like the vibe.",
      "start": 2516.55,
      "duration": 0.833
    },
    {
      "text": "- The vibe, yeah, the vibe.\n- It's like the vibe.",
      "start": 2517.383,
      "duration": 2.257
    },
    {
      "text": "- The vibe benchmark, human\nbenchmark, the humans.",
      "start": 2519.64,
      "duration": 2.91
    },
    {
      "text": "You pull in the humans to do a vibe check.",
      "start": 2522.55,
      "duration": 3.06
    },
    {
      "text": "- Yeah. (chuckles)",
      "start": 2525.61,
      "duration": 1.442
    },
    {
      "text": "- Okay.",
      "start": 2527.052,
      "duration": 0.833
    },
    {
      "text": "That's what I do, just\nreading online forums",
      "start": 2527.885,
      "duration": 2.135
    },
    {
      "text": "and Reddit and X.",
      "start": 2530.02,
      "duration": 2.193
    },
    {
      "text": "Well, I don't know how to\nproperly load in people's opinions",
      "start": 2533.92,
      "duration": 5.0
    },
    {
      "text": "'cause they'll say things like,",
      "start": 2539.08,
      "duration": 1.417
    },
    {
      "text": "\"I feel like Claude or GPT has\ngotten dumber,\" or something.",
      "start": 2540.497,
      "duration": 4.703
    },
    {
      "text": "They'll say, \"I feel like.\"",
      "start": 2545.2,
      "duration": 2.49
    },
    {
      "text": "And then I sometimes feel like that too,",
      "start": 2547.69,
      "duration": 2.19
    },
    {
      "text": "but I wonder if it's the\nmodel's problem or mine.",
      "start": 2549.88,
      "duration": 4.14
    },
    {
      "text": "- With Claude, there's an\ninteresting take I heard",
      "start": 2554.02,
      "duration": 2.4
    },
    {
      "text": "where I think AWS has different chips,",
      "start": 2556.42,
      "duration": 3.963
    },
    {
      "text": "and I suspect they have\nslightly different numerics",
      "start": 2561.55,
      "duration": 2.91
    },
    {
      "text": "than Nvidia GPUs,",
      "start": 2564.46,
      "duration": 2.52
    },
    {
      "text": "and someone speculated that\nClaude's degraded performance",
      "start": 2566.98,
      "duration": 4.26
    },
    {
      "text": "had to do with maybe using\nthe quantized version",
      "start": 2571.24,
      "duration": 3.06
    },
    {
      "text": "that existed on AWS Bedrock",
      "start": 2574.3,
      "duration": 1.71
    },
    {
      "text": "versus whatever was\nrunning on Anthropics GPUs.",
      "start": 2576.01,
      "duration": 4.74
    },
    {
      "text": "- I interview a bunch of people",
      "start": 2580.75,
      "duration": 0.833
    },
    {
      "text": "that have conspiracy theories,",
      "start": 2581.583,
      "duration": 1.357
    },
    {
      "text": "so I'm glad you spoke to this conspiracy.",
      "start": 2582.94,
      "duration": 3.523
    },
    {
      "text": "- Well, it's not like conspiracy\ntheory as much as humans.",
      "start": 2586.463,
      "duration": 5.0
    },
    {
      "text": "Humans are humans and\nthere's these details.",
      "start": 2592.45,
      "duration": 2.01
    },
    {
      "text": "- [Lex] Yes.",
      "start": 2594.46,
      "duration": 0.935
    },
    {
      "text": "- And you're doing this\nqueasy amount of flops",
      "start": 2595.395,
      "duration": 3.805
    },
    {
      "text": "and chips are messy and\nman, you can just have bugs.",
      "start": 2599.2,
      "duration": 3.513
    },
    {
      "text": "It's hard to overstate how\nhard bugs are to avoid.",
      "start": 2604.27,
      "duration": 3.633
    },
    {
      "text": "- What's the role of a\ngood prompt in all of this?",
      "start": 2608.74,
      "duration": 4.38
    },
    {
      "text": "We mentioned that benchmarks\nhave really structured,",
      "start": 2613.12,
      "duration": 4.44
    },
    {
      "text": "well-formulated prompts.",
      "start": 2617.56,
      "duration": 1.293
    },
    {
      "text": "What should a human be\ndoing to maximize success",
      "start": 2619.859,
      "duration": 4.991
    },
    {
      "text": "and what's the importance\nof what the humans,",
      "start": 2624.85,
      "duration": 1.74
    },
    {
      "text": "you wrote a blog post, you\ncalled it Prompt Design.",
      "start": 2626.59,
      "duration": 4.17
    },
    {
      "text": "- Yeah, I think it depends\non which model you're using,",
      "start": 2630.76,
      "duration": 5.0
    },
    {
      "text": "and all of them are slightly different",
      "start": 2635.98,
      "duration": 1.41
    },
    {
      "text": "and they respond differently\nto different prompts,",
      "start": 2637.39,
      "duration": 2.61
    },
    {
      "text": "but I think the original GPT-4",
      "start": 2640.0,
      "duration": 5.0
    },
    {
      "text": "and the original (indistinct)\nmodels last year,",
      "start": 2645.07,
      "duration": 3.51
    },
    {
      "text": "they were quite sensitive to the prompts,",
      "start": 2648.58,
      "duration": 1.95
    },
    {
      "text": "and they also had a very\nsmall context window.",
      "start": 2650.53,
      "duration": 3.09
    },
    {
      "text": "And so, we have all of\nthese pieces of information",
      "start": 2653.62,
      "duration": 2.97
    },
    {
      "text": "around the code base",
      "start": 2656.59,
      "duration": 1.05
    },
    {
      "text": "that would maybe be\nrelevant in the prompt.",
      "start": 2657.64,
      "duration": 3.09
    },
    {
      "text": "You have the docs, you have\nthe files that you add,",
      "start": 2660.73,
      "duration": 2.37
    },
    {
      "text": "you have the conversation history,",
      "start": 2663.1,
      "duration": 1.53
    },
    {
      "text": "and then there's a problem\nlike how do you decide",
      "start": 2664.63,
      "duration": 2.7
    },
    {
      "text": "what you actually put in the prompt",
      "start": 2667.33,
      "duration": 1.47
    },
    {
      "text": "and when you have a limited space?",
      "start": 2668.8,
      "duration": 1.92
    },
    {
      "text": "And even for today's models,",
      "start": 2670.72,
      "duration": 1.14
    },
    {
      "text": "even when you have long context,",
      "start": 2671.86,
      "duration": 2.04
    },
    {
      "text": "filling out the entire\ncontext window means",
      "start": 2673.9,
      "duration": 2.25
    },
    {
      "text": "that it's slower.",
      "start": 2676.15,
      "duration": 1.77
    },
    {
      "text": "It means that sometimes the\nmodel actually gets confused",
      "start": 2677.92,
      "duration": 2.61
    },
    {
      "text": "and some models get more\nconfused than others.",
      "start": 2680.53,
      "duration": 2.64
    },
    {
      "text": "And we have this one system\ninternally that we call Preempt,",
      "start": 2683.17,
      "duration": 3.54
    },
    {
      "text": "which helps us with that a little bit.",
      "start": 2686.71,
      "duration": 3.33
    },
    {
      "text": "And I think it was\nbuilt for the era before",
      "start": 2690.04,
      "duration": 5.0
    },
    {
      "text": "where we had 8,000 token contact windows.",
      "start": 2696.37,
      "duration": 4.26
    },
    {
      "text": "And it's a little bit similar",
      "start": 2700.63,
      "duration": 2.79
    },
    {
      "text": "to when you're making a website.",
      "start": 2703.42,
      "duration": 2.493
    },
    {
      "text": "You want it to work on mobile,",
      "start": 2707.987,
      "duration": 1.853
    },
    {
      "text": "you want it to work on a desktop screen,",
      "start": 2709.84,
      "duration": 2.43
    },
    {
      "text": "and you have this dynamic\ninformation which you don't have.",
      "start": 2712.27,
      "duration": 5.0
    },
    {
      "text": "For example, if you're\ndesigning a print magazine,",
      "start": 2717.67,
      "duration": 2.25
    },
    {
      "text": "you know exactly where you can put stuff.",
      "start": 2719.92,
      "duration": 2.22
    },
    {
      "text": "But when you have a website\nor when you have a prompt,",
      "start": 2722.14,
      "duration": 2.1
    },
    {
      "text": "you have these inputs and\nthen you need to format them",
      "start": 2724.24,
      "duration": 2.73
    },
    {
      "text": "to always work, even if\nthe input is really big,",
      "start": 2726.97,
      "duration": 2.04
    },
    {
      "text": "then you might have to cut something down.",
      "start": 2729.01,
      "duration": 2.25
    },
    {
      "text": "And so the idea was, okay,\nlet's take some inspiration.",
      "start": 2731.26,
      "duration": 3.21
    },
    {
      "text": "What's the best way to design websites?",
      "start": 2734.47,
      "duration": 2.97
    },
    {
      "text": "Well, the thing that\nwe really like is React",
      "start": 2737.44,
      "duration": 3.54
    },
    {
      "text": "and the declarative approach",
      "start": 2740.98,
      "duration": 1.08
    },
    {
      "text": "where you use JSX in JavaScript,\nand then you declare,",
      "start": 2742.06,
      "duration": 5.0
    },
    {
      "text": "\"This is what I want and I\nthink this has higher priority",
      "start": 2749.147,
      "duration": 3.953
    },
    {
      "text": "or this has higher Z index\nthan something else.\"",
      "start": 2753.1,
      "duration": 3.36
    },
    {
      "text": "And then, you have this\nrendering engine in web design.",
      "start": 2756.46,
      "duration": 3.66
    },
    {
      "text": "It's like Chrome, and in our\ncase it's a preempt renderer,",
      "start": 2760.12,
      "duration": 3.63
    },
    {
      "text": "which then fits everything onto the page.",
      "start": 2763.75,
      "duration": 3.36
    },
    {
      "text": "And as you declare, decide what you want",
      "start": 2767.11,
      "duration": 1.47
    },
    {
      "text": "and then it figures out what you want.",
      "start": 2768.58,
      "duration": 2.133
    },
    {
      "text": "And so, we have found\nthat to be quite helpful",
      "start": 2771.73,
      "duration": 2.79
    },
    {
      "text": "and I think the role of\nit has shifted over time",
      "start": 2774.52,
      "duration": 4.53
    },
    {
      "text": "where initially it was to fit",
      "start": 2779.05,
      "duration": 1.08
    },
    {
      "text": "to these small context windows.",
      "start": 2780.13,
      "duration": 1.65
    },
    {
      "text": "Now, it's really useful",
      "start": 2781.78,
      "duration": 1.02
    },
    {
      "text": "because it helps us with\nsplitting up the data",
      "start": 2782.8,
      "duration": 4.92
    },
    {
      "text": "that goes into the prompt and\nthe actual rendering of it.",
      "start": 2787.72,
      "duration": 2.307
    },
    {
      "text": "And so, it's easier to debug",
      "start": 2790.027,
      "duration": 3.153
    },
    {
      "text": "because you can change the\nrendering of the prompt",
      "start": 2793.18,
      "duration": 2.31
    },
    {
      "text": "and then try it on old prompts",
      "start": 2795.49,
      "duration": 2.34
    },
    {
      "text": "because you have the raw data\nthat went into the prompt,",
      "start": 2797.83,
      "duration": 2.46
    },
    {
      "text": "and then you can see, \"Did\nmy change actually improve it",
      "start": 2800.29,
      "duration": 2.19
    },
    {
      "text": "for this entire eval set?\"",
      "start": 2802.48,
      "duration": 2.67
    },
    {
      "text": "- So, do you literally prompt with JSX?",
      "start": 2805.15,
      "duration": 2.91
    },
    {
      "text": "- Yes, yes.\n- Yeah.",
      "start": 2808.06,
      "duration": 1.01
    },
    {
      "text": "- So it looks like React,\nthere are components.",
      "start": 2809.07,
      "duration": 2.35
    },
    {
      "text": "We have one component\nthat's a file component",
      "start": 2811.42,
      "duration": 3.54
    },
    {
      "text": "and it takes in the Cursor.",
      "start": 2814.96,
      "duration": 2.82
    },
    {
      "text": "Usually, there's one line where\nthe Cursor is in your file",
      "start": 2817.78,
      "duration": 3.06
    },
    {
      "text": "and that's probably\nthe most important line",
      "start": 2820.84,
      "duration": 1.44
    },
    {
      "text": "because that's the one you're looking at.",
      "start": 2822.28,
      "duration": 1.17
    },
    {
      "text": "And so, then you can give priorities,",
      "start": 2823.45,
      "duration": 1.53
    },
    {
      "text": "so that line has the highest priority,",
      "start": 2824.98,
      "duration": 2.13
    },
    {
      "text": "and then you subtract one for every line",
      "start": 2827.11,
      "duration": 2.52
    },
    {
      "text": "that is farther away.",
      "start": 2829.63,
      "duration": 1.86
    },
    {
      "text": "And then eventually, when it's rendered,",
      "start": 2831.49,
      "duration": 1.68
    },
    {
      "text": "it figures out how many\nlines can actually fit",
      "start": 2833.17,
      "duration": 1.8
    },
    {
      "text": "and it centers around that thing.",
      "start": 2834.97,
      "duration": 2.07
    },
    {
      "text": "- That's amazing.\n- Yeah.",
      "start": 2837.04,
      "duration": 1.405
    },
    {
      "text": "- And you can do other fancy things",
      "start": 2838.445,
      "duration": 1.325
    },
    {
      "text": "where if you have lots of code blocks",
      "start": 2839.77,
      "duration": 2.04
    },
    {
      "text": "from the entire code base,\nyou could use retrieval",
      "start": 2841.81,
      "duration": 3.72
    },
    {
      "text": "and things like embedding\nand re-ranking scores",
      "start": 2845.53,
      "duration": 1.92
    },
    {
      "text": "to add priorities for you\nthrough these components.",
      "start": 2847.45,
      "duration": 3.45
    },
    {
      "text": "- So should humans when\nthey ask questions,",
      "start": 2850.9,
      "duration": 2.52
    },
    {
      "text": "also try to use something like that?",
      "start": 2853.42,
      "duration": 2.25
    },
    {
      "text": "Would it be beneficial to\nwrite JSX in the problem",
      "start": 2855.67,
      "duration": 4.511
    },
    {
      "text": "or the whole idea is this\nshould be loose and messy?",
      "start": 2860.181,
      "duration": 3.109
    },
    {
      "text": "- I think our goal is that\nyou should just do whatever",
      "start": 2863.29,
      "duration": 4.56
    },
    {
      "text": "is the most natural thing for you,",
      "start": 2867.85,
      "duration": 1.83
    },
    {
      "text": "and then our job is to figure out",
      "start": 2869.68,
      "duration": 3.48
    },
    {
      "text": "how do we actually retrieve\nthe relative event things",
      "start": 2873.16,
      "duration": 2.25
    },
    {
      "text": "so that your thinking\nactually makes sense?",
      "start": 2875.41,
      "duration": 1.53
    },
    {
      "text": "- Well, this is the discussion I had",
      "start": 2876.94,
      "duration": 1.89
    },
    {
      "text": "with Aravind of Perplexity\nis his whole idea",
      "start": 2878.83,
      "duration": 3.6
    },
    {
      "text": "is you should let the person\nbe as lazy as he wants.",
      "start": 2882.43,
      "duration": 3.63
    },
    {
      "text": "- Yeah.\n- Mm-hmm.",
      "start": 2886.06,
      "duration": 0.933
    },
    {
      "text": "- Yeah, that's a beautiful thing,",
      "start": 2888.34,
      "duration": 1.98
    },
    {
      "text": "but I feel like you're allowed",
      "start": 2890.32,
      "duration": 1.32
    },
    {
      "text": "to ask more of programmers, right?",
      "start": 2891.64,
      "duration": 2.64
    },
    {
      "text": "- Yes.",
      "start": 2894.28,
      "duration": 0.833
    },
    {
      "text": "- So if you say, \"Just do what you want,\"",
      "start": 2895.113,
      "duration": 1.597
    },
    {
      "text": "I mean, humans are lazy.",
      "start": 2896.71,
      "duration": 2.34
    },
    {
      "text": "There's a tension between just being lazy",
      "start": 2899.05,
      "duration": 2.43
    },
    {
      "text": "versus provide more as be prompted,",
      "start": 2901.48,
      "duration": 4.11
    },
    {
      "text": "almost like the system pressuring you",
      "start": 2905.59,
      "duration": 3.09
    },
    {
      "text": "or inspiring you to be articulate.",
      "start": 2908.68,
      "duration": 3.48
    },
    {
      "text": "Not in terms of the\ngrammar of the sentences,",
      "start": 2912.16,
      "duration": 2.25
    },
    {
      "text": "but in terms of the depth of thoughts",
      "start": 2914.41,
      "duration": 1.86
    },
    {
      "text": "that you convey inside the prompts.",
      "start": 2916.27,
      "duration": 2.7
    },
    {
      "text": "- I think even as a system gets closer",
      "start": 2918.97,
      "duration": 1.8
    },
    {
      "text": "to some level of perfection,",
      "start": 2920.77,
      "duration": 2.55
    },
    {
      "text": "often when you ask the\nmodel for something,",
      "start": 2923.32,
      "duration": 3.543
    },
    {
      "text": "not enough intent is\nconveyed to know what to do.",
      "start": 2928.901,
      "duration": 2.969
    },
    {
      "text": "And there are a few ways\nto resolve that intent.",
      "start": 2931.87,
      "duration": 2.34
    },
    {
      "text": "One is the simple thing of\nhaving the model just ask you,",
      "start": 2934.21,
      "duration": 3.757
    },
    {
      "text": "\"I'm not sure how to do these\nparts based on your query.",
      "start": 2937.967,
      "duration": 3.503
    },
    {
      "text": "Could you clarify that?\"",
      "start": 2941.47,
      "duration": 1.437
    },
    {
      "text": "I think the other could be maybe",
      "start": 2944.44,
      "duration": 2.68
    },
    {
      "text": "if there are five or six\npossible generations,",
      "start": 2949.655,
      "duration": 2.292
    },
    {
      "text": "\"Given the uncertainty\npresent in your query so far,",
      "start": 2951.947,
      "duration": 2.783
    },
    {
      "text": "why don't we just actually\nshow you all of those",
      "start": 2954.73,
      "duration": 1.8
    },
    {
      "text": "and let you pick them?\"",
      "start": 2956.53,
      "duration": 1.15
    },
    {
      "text": "- How hard is it for the\nmodel to choose to talk back?",
      "start": 2959.2,
      "duration": 4.173
    },
    {
      "text": "It's hard, how deal with the uncertainty.",
      "start": 2966.7,
      "duration": 3.39
    },
    {
      "text": "Do I choose to ask for more information",
      "start": 2970.09,
      "duration": 4.11
    },
    {
      "text": "to reduce the ambiguity?",
      "start": 2974.2,
      "duration": 1.89
    },
    {
      "text": "- So, I mean, one of the things we do,",
      "start": 2976.09,
      "duration": 2.133
    },
    {
      "text": "it's like a recent addition,",
      "start": 2979.42,
      "duration": 1.14
    },
    {
      "text": "is try to suggest files that you can add.",
      "start": 2980.56,
      "duration": 4.11
    },
    {
      "text": "And while you're typing,",
      "start": 2984.67,
      "duration": 2.34
    },
    {
      "text": "one can guess what the uncertainty is",
      "start": 2987.01,
      "duration": 3.72
    },
    {
      "text": "and maybe suggest that maybe\nyou're writing your API",
      "start": 2990.73,
      "duration": 5.0
    },
    {
      "text": "and we can guess using the commits",
      "start": 2997.24,
      "duration": 4.93
    },
    {
      "text": "that you've made\npreviously in the same file",
      "start": 3003.36,
      "duration": 2.25
    },
    {
      "text": "that the client and the\nserver is super useful",
      "start": 3005.61,
      "duration": 3.63
    },
    {
      "text": "and there's a hard technical problem",
      "start": 3009.24,
      "duration": 3.45
    },
    {
      "text": "of how do you resolve\nit across all commits?",
      "start": 3012.69,
      "duration": 2.64
    },
    {
      "text": "Which files are the most important",
      "start": 3015.33,
      "duration": 1.53
    },
    {
      "text": "given your current prompt?",
      "start": 3016.86,
      "duration": 2.25
    },
    {
      "text": "And we're still initial\nversion is ruled out",
      "start": 3019.11,
      "duration": 3.84
    },
    {
      "text": "and I'm sure we can make\nit much more accurate.",
      "start": 3022.95,
      "duration": 2.333
    },
    {
      "text": "It's very experimental, but\nthen the idea is we show you,",
      "start": 3026.73,
      "duration": 3.15
    },
    {
      "text": "do you just want to add this\nfile, this file, this file also",
      "start": 3029.88,
      "duration": 3.09
    },
    {
      "text": "to tell the model to\nedit those files for you?",
      "start": 3032.97,
      "duration": 3.303
    },
    {
      "text": "Because if maybe you're making the API,",
      "start": 3037.17,
      "duration": 2.34
    },
    {
      "text": "you should also edit the\nclient and the server",
      "start": 3039.51,
      "duration": 1.65
    },
    {
      "text": "that is using the API and the\nother one resolving the API.",
      "start": 3041.16,
      "duration": 3.03
    },
    {
      "text": "So that would be cool as\nboth there's the phase",
      "start": 3044.19,
      "duration": 3.87
    },
    {
      "text": "where you're writing a prompt.",
      "start": 3048.06,
      "duration": 1.89
    },
    {
      "text": "Before you even click, \"Enter,\"",
      "start": 3049.95,
      "duration": 2.07
    },
    {
      "text": "maybe we can help resolve\nsome of the uncertainty.",
      "start": 3052.02,
      "duration": 2.34
    },
    {
      "text": "- To what degree do you\nuse agentic approaches?",
      "start": 3054.36,
      "duration": 2.73
    },
    {
      "text": "How useful are agents?",
      "start": 3057.09,
      "duration": 2.01
    },
    {
      "text": "- We think agents are really, really cool.",
      "start": 3059.1,
      "duration": 3.601
    },
    {
      "text": "- [Lex] (chuckles) Okay.",
      "start": 3062.701,
      "duration": 1.661
    },
    {
      "text": "- I think agents, it's like\nresembles like a human.",
      "start": 3064.362,
      "duration": 3.801
    },
    {
      "text": "You can feel that you're\ngetting closer to AGI",
      "start": 3069.732,
      "duration": 2.868
    },
    {
      "text": "because you see a demo where\nit acts as a human would",
      "start": 3072.6,
      "duration": 5.0
    },
    {
      "text": "and it's really, really cool.",
      "start": 3077.67,
      "duration": 2.07
    },
    {
      "text": "I think agents are not yet\nsuper useful for many things.",
      "start": 3079.74,
      "duration": 5.0
    },
    {
      "text": "I think we're getting close",
      "start": 3087.12,
      "duration": 1.59
    },
    {
      "text": "to where they will actually be useful.",
      "start": 3088.71,
      "duration": 1.83
    },
    {
      "text": "And so, I think there are\ncertain types of tasks",
      "start": 3090.54,
      "duration": 5.0
    },
    {
      "text": "where having an agent\nwould be really nice.",
      "start": 3095.55,
      "duration": 4.23
    },
    {
      "text": "I would love to have an agent.\nFor example, if we have a bug",
      "start": 3099.78,
      "duration": 2.34
    },
    {
      "text": "where you sometimes can't\nCommand+C and Command+V",
      "start": 3102.12,
      "duration": 4.11
    },
    {
      "text": "inside our chat input box,",
      "start": 3106.23,
      "duration": 2.49
    },
    {
      "text": "and that's a task that's\nsuper well specified.",
      "start": 3108.72,
      "duration": 2.1
    },
    {
      "text": "I just want to say in two sentences,",
      "start": 3110.82,
      "duration": 1.717
    },
    {
      "text": "\"This does not work, please fix it.\"",
      "start": 3112.537,
      "duration": 1.883
    },
    {
      "text": "And then I would love to have an agent",
      "start": 3114.42,
      "duration": 1.8
    },
    {
      "text": "that just goes off, does it,",
      "start": 3116.22,
      "duration": 1.89
    },
    {
      "text": "and then a day later, I come\nback and I review the thing.",
      "start": 3118.11,
      "duration": 4.71
    },
    {
      "text": "- You mean it goes, finds the right file?",
      "start": 3122.82,
      "duration": 2.7
    },
    {
      "text": "- Yeah, it finds the right files,",
      "start": 3125.52,
      "duration": 1.38
    },
    {
      "text": "it tries to reproduce the bug,",
      "start": 3126.9,
      "duration": 1.89
    },
    {
      "text": "it fixes the bug and then it\nverifies that it's correct.",
      "start": 3128.79,
      "duration": 2.94
    },
    {
      "text": "And this could be a process\nthat takes a long time.",
      "start": 3131.73,
      "duration": 3.06
    },
    {
      "text": "And so, I think I would love to have that.",
      "start": 3134.79,
      "duration": 2.76
    },
    {
      "text": "And then I think a lot of programming,",
      "start": 3137.55,
      "duration": 3.21
    },
    {
      "text": "there is often this belief",
      "start": 3140.76,
      "duration": 1.26
    },
    {
      "text": "that agents will take\nover all of programming.",
      "start": 3142.02,
      "duration": 2.733
    },
    {
      "text": "I don't think we think\nthat that's the case",
      "start": 3146.19,
      "duration": 2.16
    },
    {
      "text": "because a lot of programming,",
      "start": 3148.35,
      "duration": 1.44
    },
    {
      "text": "a lot of the value is in iterating,",
      "start": 3149.79,
      "duration": 2.7
    },
    {
      "text": "or you don't actually want\nto specify something upfront",
      "start": 3152.49,
      "duration": 2.94
    },
    {
      "text": "because you don't really\nknow what you want",
      "start": 3155.43,
      "duration": 1.86
    },
    {
      "text": "until you have seen an initial version",
      "start": 3157.29,
      "duration": 1.8
    },
    {
      "text": "and then you want to iterate on that,",
      "start": 3159.09,
      "duration": 2.04
    },
    {
      "text": "and then you provide more information.",
      "start": 3161.13,
      "duration": 1.89
    },
    {
      "text": "And so, for a lot of programming,",
      "start": 3163.02,
      "duration": 1.77
    },
    {
      "text": "I think you actually want\na system that's instant,",
      "start": 3164.79,
      "duration": 2.49
    },
    {
      "text": "that gives you an initial\nversion instantly back",
      "start": 3167.28,
      "duration": 1.83
    },
    {
      "text": "and then you can iterate\nsuper, super quickly.",
      "start": 3169.11,
      "duration": 3.3
    },
    {
      "text": "- What about something like\nthat recently came out,",
      "start": 3172.41,
      "duration": 2.34
    },
    {
      "text": "replica agent, that does also setting up",
      "start": 3174.75,
      "duration": 3.99
    },
    {
      "text": "the development environment\nand solving software packages,",
      "start": 3178.74,
      "duration": 2.34
    },
    {
      "text": "configuring everything,\nconfiguring the databases",
      "start": 3181.08,
      "duration": 2.76
    },
    {
      "text": "and actually deploying the app.",
      "start": 3183.84,
      "duration": 2.04
    },
    {
      "text": "Is that also in the set\nof things you dream about?",
      "start": 3185.88,
      "duration": 4.05
    },
    {
      "text": "- I think so.",
      "start": 3189.93,
      "duration": 0.833
    },
    {
      "text": "I think that would be really cool.",
      "start": 3190.763,
      "duration": 1.267
    },
    {
      "text": "For certain types of programming,\nit would be really cool.",
      "start": 3192.03,
      "duration": 3.08
    },
    {
      "text": "- Is that within scope of Cursor?",
      "start": 3195.11,
      "duration": 2.65
    },
    {
      "text": "- Yeah, we aren't actively\nworking on it right now.",
      "start": 3197.76,
      "duration": 3.063
    },
    {
      "text": "We want to make the\nprogrammer's life easier",
      "start": 3202.86,
      "duration": 3.9
    },
    {
      "text": "and more fun and some things\nare just really tedious",
      "start": 3206.76,
      "duration": 3.27
    },
    {
      "text": "and you need to go\nthrough a bunch of steps",
      "start": 3210.03,
      "duration": 1.44
    },
    {
      "text": "and you want to delegate that to an agent.",
      "start": 3211.47,
      "duration": 2.67
    },
    {
      "text": "And then some things you\ncan actually have an agent",
      "start": 3214.14,
      "duration": 1.89
    },
    {
      "text": "in the background while you're working.",
      "start": 3216.03,
      "duration": 1.95
    },
    {
      "text": "Let's say you have a PR that's\nboth backend and frontend,",
      "start": 3217.98,
      "duration": 3.54
    },
    {
      "text": "and you're working in the frontend,",
      "start": 3221.52,
      "duration": 0.87
    },
    {
      "text": "and then you can have a background agent",
      "start": 3222.39,
      "duration": 1.71
    },
    {
      "text": "that doesn't work and figure\nout what you're doing.",
      "start": 3224.1,
      "duration": 2.97
    },
    {
      "text": "And then, when you get to\nthe backend part of your PR,",
      "start": 3227.07,
      "duration": 3.09
    },
    {
      "text": "then you have some initial piece of code",
      "start": 3230.16,
      "duration": 2.73
    },
    {
      "text": "that you can iterate on.",
      "start": 3232.89,
      "duration": 1.263
    },
    {
      "text": "And so that would also be really cool.",
      "start": 3235.92,
      "duration": 2.61
    },
    {
      "text": "- One of the things we\nalready talked about is speed,",
      "start": 3238.53,
      "duration": 2.91
    },
    {
      "text": "but I wonder if we can just\nlinger on that some more",
      "start": 3241.44,
      "duration": 3.17
    },
    {
      "text": "in the various places that\nthe technical details involved",
      "start": 3244.61,
      "duration": 5.0
    },
    {
      "text": "in making this thing really fast.",
      "start": 3250.02,
      "duration": 1.74
    },
    {
      "text": "So every single aspect of Cursor,",
      "start": 3251.76,
      "duration": 2.46
    },
    {
      "text": "most aspects of Cursor feel really fast.",
      "start": 3254.22,
      "duration": 2.16
    },
    {
      "text": "Like I mentioned, the Apply\nis probably the slowest thing.",
      "start": 3256.38,
      "duration": 2.723
    },
    {
      "text": "I'm sorry, the pain on\nArvid's face as I say that.",
      "start": 3260.239,
      "duration": 1.776
    },
    {
      "text": "- I know.",
      "start": 3262.015,
      "duration": 0.833
    },
    {
      "text": "It's a pain, it's a\npain that we're feeling",
      "start": 3262.848,
      "duration": 1.731
    },
    {
      "text": "and we're working on fixing it.",
      "start": 3264.579,
      "duration": 2.333
    },
    {
      "text": "(Arvid and Lex chuckling)",
      "start": 3266.912,
      "duration": 0.833
    },
    {
      "text": "- Yeah, it says something that feels,",
      "start": 3267.745,
      "duration": 2.465
    },
    {
      "text": "I don't know what it is, like\none second or two seconds,",
      "start": 3270.21,
      "duration": 2.82
    },
    {
      "text": "that feels slow.",
      "start": 3273.03,
      "duration": 0.93
    },
    {
      "text": "That means that actually shows",
      "start": 3273.96,
      "duration": 2.85
    },
    {
      "text": "that everything else is\njust really, really fast.",
      "start": 3276.81,
      "duration": 2.82
    },
    {
      "text": "So, is there some technical details about",
      "start": 3279.63,
      "duration": 1.62
    },
    {
      "text": "how to make some of these models,\nhow to make the chat fast,",
      "start": 3281.25,
      "duration": 2.88
    },
    {
      "text": "how to make the diffs fast?",
      "start": 3284.13,
      "duration": 3.21
    },
    {
      "text": "Is there something that\njust jumps to mind?",
      "start": 3287.34,
      "duration": 1.8
    },
    {
      "text": "- Yeah.",
      "start": 3289.14,
      "duration": 0.833
    },
    {
      "text": "So, we can go over a lot of\nthe strategies that we use.",
      "start": 3289.973,
      "duration": 1.867
    },
    {
      "text": "One interesting thing is cache warming.",
      "start": 3291.84,
      "duration": 2.043
    },
    {
      "text": "You're probably going to\nuse some piece of context",
      "start": 3301.74,
      "duration": 3.45
    },
    {
      "text": "and you can know that before\nthe user's done typing.",
      "start": 3305.19,
      "duration": 2.28
    },
    {
      "text": "So as we discussed before,",
      "start": 3307.47,
      "duration": 3.18
    },
    {
      "text": "reusing the KV cache\nresults in lower latency,",
      "start": 3310.65,
      "duration": 2.82
    },
    {
      "text": "lower costs, cross requests.",
      "start": 3313.47,
      "duration": 2.37
    },
    {
      "text": "So as the user starts typing,",
      "start": 3315.84,
      "duration": 1.17
    },
    {
      "text": "you can immediately warm the cache",
      "start": 3317.01,
      "duration": 1.44
    },
    {
      "text": "with let's say the current file contents,",
      "start": 3318.45,
      "duration": 2.73
    },
    {
      "text": "and then when they press Enter,",
      "start": 3321.18,
      "duration": 1.863
    },
    {
      "text": "there's very few tokens it\nactually has to pre-fill",
      "start": 3324.18,
      "duration": 2.91
    },
    {
      "text": "and compute before\nstarting the generation.",
      "start": 3327.09,
      "duration": 1.62
    },
    {
      "text": "This will significantly lower TTFT.",
      "start": 3328.71,
      "duration": 1.98
    },
    {
      "text": "- Can you explain how KV cache works?",
      "start": 3330.69,
      "duration": 2.32
    },
    {
      "text": "- [Aman] Yeah, so the\nway transformers work.",
      "start": 3333.01,
      "duration": 3.185
    },
    {
      "text": "(group chuckling)",
      "start": 3336.195,
      "duration": 1.759
    },
    {
      "text": "- I like it.",
      "start": 3337.954,
      "duration": 1.346
    },
    {
      "text": "(group chuckling)",
      "start": 3339.3,
      "duration": 2.833
    },
    {
      "text": "- The mechanism that allows transformers",
      "start": 3345.48,
      "duration": 1.41
    },
    {
      "text": "to not just independently\nlook at each token,",
      "start": 3346.89,
      "duration": 1.65
    },
    {
      "text": "but see previous tokens are the keys",
      "start": 3348.54,
      "duration": 2.79
    },
    {
      "text": "and values to attention.",
      "start": 3351.33,
      "duration": 1.26
    },
    {
      "text": "And generally, the way attention works",
      "start": 3352.59,
      "duration": 1.92
    },
    {
      "text": "is you have at your\ncurrent token some query,",
      "start": 3354.51,
      "duration": 3.99
    },
    {
      "text": "and then you've all the keys",
      "start": 3358.5,
      "duration": 1.41
    },
    {
      "text": "and values of all your previous tokens,",
      "start": 3359.91,
      "duration": 1.65
    },
    {
      "text": "which are some kind of representation",
      "start": 3361.56,
      "duration": 2.49
    },
    {
      "text": "that the model stores internally\nof all the previous tokens",
      "start": 3364.05,
      "duration": 2.73
    },
    {
      "text": "in the prompt.",
      "start": 3366.78,
      "duration": 1.29
    },
    {
      "text": "And by default, when you're\ndoing a chat, the model has to,",
      "start": 3368.07,
      "duration": 5.0
    },
    {
      "text": "for every single token,",
      "start": 3373.95,
      "duration": 1.89
    },
    {
      "text": "do this forward pass\nthrough the entire model.",
      "start": 3375.84,
      "duration": 3.51
    },
    {
      "text": "That's a lot of matrix\nmultiplies that happen,",
      "start": 3379.35,
      "duration": 2.16
    },
    {
      "text": "and that is really, really slow.",
      "start": 3381.51,
      "duration": 2.13
    },
    {
      "text": "Instead, if you have already done that",
      "start": 3383.64,
      "duration": 2.55
    },
    {
      "text": "and you stored the keys and values",
      "start": 3386.19,
      "duration": 1.86
    },
    {
      "text": "and you keep that in the GPU,",
      "start": 3388.05,
      "duration": 1.803
    },
    {
      "text": "let's say I have to sort\nit for the last N tokens.",
      "start": 3391.59,
      "duration": 1.92
    },
    {
      "text": "If I now wanna compute the output token",
      "start": 3393.51,
      "duration": 3.06
    },
    {
      "text": "for the N+1nth token,",
      "start": 3396.57,
      "duration": 2.1
    },
    {
      "text": "I don't need to pass those first N tokens",
      "start": 3398.67,
      "duration": 3.36
    },
    {
      "text": "through the entire model",
      "start": 3402.03,
      "duration": 0.93
    },
    {
      "text": "because I already have\nall those keys and values.",
      "start": 3402.96,
      "duration": 3.45
    },
    {
      "text": "And so, you just need\nto do the forward pass",
      "start": 3406.41,
      "duration": 1.89
    },
    {
      "text": "through that last token.",
      "start": 3408.3,
      "duration": 1.59
    },
    {
      "text": "And then when you're doing attention,",
      "start": 3409.89,
      "duration": 2.22
    },
    {
      "text": "you're reusing those keys and values",
      "start": 3412.11,
      "duration": 2.04
    },
    {
      "text": "that have been computed,",
      "start": 3414.15,
      "duration": 0.833
    },
    {
      "text": "which is the only kind of sequential part",
      "start": 3414.983,
      "duration": 2.707
    },
    {
      "text": "or sequentially dependent\npart of the transformer.",
      "start": 3417.69,
      "duration": 2.25
    },
    {
      "text": "- Is there higher level caching\nof caching of the prompts",
      "start": 3419.94,
      "duration": 4.35
    },
    {
      "text": "or that kind of stuff",
      "start": 3424.29,
      "duration": 1.35
    },
    {
      "text": "that could help?\n- I see.",
      "start": 3425.64,
      "duration": 0.93
    },
    {
      "text": "Yeah, there's other types\nof caching you can do.",
      "start": 3426.57,
      "duration": 4.023
    },
    {
      "text": "One interesting thing that\nyou can do for Cursor Tab",
      "start": 3432.6,
      "duration": 3.33
    },
    {
      "text": "is you can basically predict ahead",
      "start": 3435.93,
      "duration": 4.68
    },
    {
      "text": "as if the user would've\naccepted the suggestion",
      "start": 3440.61,
      "duration": 2.82
    },
    {
      "text": "and then trigger another request.",
      "start": 3443.43,
      "duration": 3.24
    },
    {
      "text": "And so then you've cached,\nyou've done the speculative.",
      "start": 3446.67,
      "duration": 2.52
    },
    {
      "text": "It's a mix of speculation\nand caching, right?",
      "start": 3449.19,
      "duration": 1.86
    },
    {
      "text": "Because speculating what would\nhappen if they accepted it.",
      "start": 3451.05,
      "duration": 3.0
    },
    {
      "text": "And then you have this value\nthat is cached this suggestion.",
      "start": 3454.05,
      "duration": 4.47
    },
    {
      "text": "And then when they press Tab,",
      "start": 3458.52,
      "duration": 1.08
    },
    {
      "text": "the next one would be\nwaiting for them immediately.",
      "start": 3459.6,
      "duration": 2.28
    },
    {
      "text": "It's a clever heuristic/trick",
      "start": 3461.88,
      "duration": 2.71
    },
    {
      "text": "that uses a higher level caching.",
      "start": 3465.51,
      "duration": 1.803
    },
    {
      "text": "It feels fast despite there\nnot actually being any changes",
      "start": 3470.04,
      "duration": 3.75
    },
    {
      "text": "in the model.",
      "start": 3473.79,
      "duration": 0.833
    },
    {
      "text": "- And if you can make\nthe KV cache smaller,",
      "start": 3474.623,
      "duration": 1.717
    },
    {
      "text": "one of the advantages you get",
      "start": 3476.34,
      "duration": 1.23
    },
    {
      "text": "is like maybe you can speculate even more.",
      "start": 3477.57,
      "duration": 2.37
    },
    {
      "text": "Maybe you can guess,",
      "start": 3479.94,
      "duration": 0.833
    },
    {
      "text": "\"Here's the 10 things\nthat could be useful,",
      "start": 3480.773,
      "duration": 3.07
    },
    {
      "text": "predict the next 10,\"",
      "start": 3485.79,
      "duration": 1.41
    },
    {
      "text": "and then it's possible the\nuser hits the one of the 10.",
      "start": 3487.2,
      "duration": 3.543
    },
    {
      "text": "It's much higher chance than\nthe user hits the exact one",
      "start": 3490.743,
      "duration": 2.787
    },
    {
      "text": "that you showed them.",
      "start": 3493.53,
      "duration": 1.53
    },
    {
      "text": "Maybe they type in other character",
      "start": 3495.06,
      "duration": 1.83
    },
    {
      "text": "and hit something else in the cache.",
      "start": 3496.89,
      "duration": 2.103
    },
    {
      "text": "The general phenomena here is,",
      "start": 3503.1,
      "duration": 1.743
    },
    {
      "text": "I think it's also super useful for RL",
      "start": 3505.77,
      "duration": 2.61
    },
    {
      "text": "is maybe a single sample from\nthe model isn't very good,",
      "start": 3508.38,
      "duration": 4.83
    },
    {
      "text": "but if you predict 10 different things,",
      "start": 3513.21,
      "duration": 4.74
    },
    {
      "text": "turns out that one of the 10",
      "start": 3517.95,
      "duration": 2.34
    },
    {
      "text": "that's right is the\nprobability is much higher.",
      "start": 3520.29,
      "duration": 2.34
    },
    {
      "text": "There's these passive K\ncurves and part of RL,",
      "start": 3522.63,
      "duration": 3.78
    },
    {
      "text": "what RL does is you can exploit\nthis passive K phenomena",
      "start": 3526.41,
      "duration": 4.931
    },
    {
      "text": "to make many different predictions.",
      "start": 3531.341,
      "duration": 2.212
    },
    {
      "text": "And one way to think about this,",
      "start": 3534.481,
      "duration": 2.789
    },
    {
      "text": "the model knows internally\nhas some uncertainty",
      "start": 3537.27,
      "duration": 3.51
    },
    {
      "text": "over which of the key things is correct",
      "start": 3540.78,
      "duration": 2.25
    },
    {
      "text": "or which of the key things\ndoes the human wants?",
      "start": 3543.03,
      "duration": 2.4
    },
    {
      "text": "When we RL our Cursor Tab model,",
      "start": 3545.43,
      "duration": 4.053
    },
    {
      "text": "one of the things we're\ndoing is we're predicting",
      "start": 3550.32,
      "duration": 2.5
    },
    {
      "text": "which of the 100 different\nsuggestions the model produces",
      "start": 3555.688,
      "duration": 3.362
    },
    {
      "text": "is more amenable for humans?",
      "start": 3559.05,
      "duration": 1.65
    },
    {
      "text": "Which of them do humans\nmore like than other things?",
      "start": 3560.7,
      "duration": 2.88
    },
    {
      "text": "Maybe there's something",
      "start": 3563.58,
      "duration": 2.37
    },
    {
      "text": "where the model can predict very far ahead",
      "start": 3565.95,
      "duration": 1.62
    },
    {
      "text": "versus a little bit, maybe\nsomewhere in the middle.",
      "start": 3567.57,
      "duration": 2.703
    },
    {
      "text": "And then you can give\na reward to the things",
      "start": 3572.841,
      "duration": 1.449
    },
    {
      "text": "that humans would like more",
      "start": 3574.29,
      "duration": 1.11
    },
    {
      "text": "and punish the things that it would like,",
      "start": 3575.4,
      "duration": 2.05
    },
    {
      "text": "and then train the model\nto output the suggestions",
      "start": 3577.45,
      "duration": 2.66
    },
    {
      "text": "that humans would like more.",
      "start": 3580.11,
      "duration": 0.84
    },
    {
      "text": "You have these RL loops\nthat are very useful",
      "start": 3580.95,
      "duration": 2.25
    },
    {
      "text": "that exploit these passive K curves.",
      "start": 3583.2,
      "duration": 1.8
    },
    {
      "text": "Aman, maybe can go into even more detail.",
      "start": 3585.84,
      "duration": 2.79
    },
    {
      "text": "- Yeah, it is a little\ndifferent than speed,",
      "start": 3588.63,
      "duration": 2.2
    },
    {
      "text": "but technically, you tie it back in",
      "start": 3592.17,
      "duration": 3.72
    },
    {
      "text": "because you can get away\nwith the smaller model",
      "start": 3595.89,
      "duration": 1.38
    },
    {
      "text": "if you RL your smaller model",
      "start": 3597.27,
      "duration": 1.257
    },
    {
      "text": "and it gets the same\nperformance as the bigger one.",
      "start": 3598.527,
      "duration": 2.55
    },
    {
      "text": "So while I was mentioning stuff about KV,",
      "start": 3604.209,
      "duration": 2.05
    },
    {
      "text": "about reducing the size of your KV cache,",
      "start": 3607.29,
      "duration": 1.53
    },
    {
      "text": "there are other techniques there as well",
      "start": 3608.82,
      "duration": 1.32
    },
    {
      "text": "that are really helpful for speed.",
      "start": 3610.14,
      "duration": 1.7
    },
    {
      "text": "So, kind of back in the day,\nall the way two years ago,",
      "start": 3612.69,
      "duration": 4.95
    },
    {
      "text": "people mainly use multi-head attention,",
      "start": 3617.64,
      "duration": 2.58
    },
    {
      "text": "and I think there's been a migration",
      "start": 3620.22,
      "duration": 1.26
    },
    {
      "text": "towards more efficient attention\nschemes like group query",
      "start": 3621.48,
      "duration": 4.24
    },
    {
      "text": "or multi-query attention,",
      "start": 3626.64,
      "duration": 2.07
    },
    {
      "text": "and this is really helpful for\nthen with larger batch sizes",
      "start": 3628.71,
      "duration": 4.86
    },
    {
      "text": "being able to generate\nthe tokens much faster.",
      "start": 3633.57,
      "duration": 3.06
    },
    {
      "text": "The interesting thing here\nis this now has no effect",
      "start": 3636.63,
      "duration": 4.38
    },
    {
      "text": "on that time to first\ntoken pre-fill speed.",
      "start": 3641.01,
      "duration": 4.26
    },
    {
      "text": "The thing this matters for\nis now generating tokens.",
      "start": 3645.27,
      "duration": 3.54
    },
    {
      "text": "And why is that?",
      "start": 3648.81,
      "duration": 1.02
    },
    {
      "text": "'Cause when you're generating tokens,",
      "start": 3649.83,
      "duration": 1.86
    },
    {
      "text": "instead of being bottlenecked",
      "start": 3651.69,
      "duration": 2.85
    },
    {
      "text": "by doing these super\nparallelizable matrix multiplies",
      "start": 3654.54,
      "duration": 3.12
    },
    {
      "text": "across all your tokens,\nyou're bottlenecked,",
      "start": 3657.66,
      "duration": 2.2
    },
    {
      "text": "for a long context with large batch sizes,",
      "start": 3661.2,
      "duration": 2.97
    },
    {
      "text": "by how quickly you can read\nthose cache, keys, and values.",
      "start": 3664.17,
      "duration": 3.66
    },
    {
      "text": "And so then that's memory bandwidth,",
      "start": 3667.83,
      "duration": 2.88
    },
    {
      "text": "and how can we make this faster?",
      "start": 3670.71,
      "duration": 1.8
    },
    {
      "text": "We can try to compress the\nsize of these keys and values.",
      "start": 3672.51,
      "duration": 2.73
    },
    {
      "text": "So multi-query attention is\nthe most aggressive of these.",
      "start": 3675.24,
      "duration": 3.003
    },
    {
      "text": "Where normally with multi-head attention,",
      "start": 3679.23,
      "duration": 1.08
    },
    {
      "text": "you have some number of, quote,\nunquote, \"attention heads,\"",
      "start": 3680.31,
      "duration": 4.017
    },
    {
      "text": "and some number of query heads.",
      "start": 3686.16,
      "duration": 3.15
    },
    {
      "text": "Multi-query just\npreserves the query heads,",
      "start": 3689.31,
      "duration": 2.85
    },
    {
      "text": "gets rid of all the key value heads.",
      "start": 3692.16,
      "duration": 2.313
    },
    {
      "text": "So there's only one\nkind of key value head,",
      "start": 3695.37,
      "duration": 2.49
    },
    {
      "text": "and there's all the remaining query heads.",
      "start": 3697.86,
      "duration": 3.45
    },
    {
      "text": "With group query, you instead\npreserve all the query heads.",
      "start": 3701.31,
      "duration": 5.0
    },
    {
      "text": "There are fewer heads\nfor the keys and values,",
      "start": 3712.2,
      "duration": 1.5
    },
    {
      "text": "but you're not reducing it to just one.",
      "start": 3713.7,
      "duration": 2.34
    },
    {
      "text": "But anyways, the whole point here",
      "start": 3716.04,
      "duration": 1.26
    },
    {
      "text": "is you're just reducing\nthe size of your KV cache.",
      "start": 3717.3,
      "duration": 3.18
    },
    {
      "text": "- And then there is MLA.",
      "start": 3720.48,
      "duration": 2.01
    },
    {
      "text": "- Yeah, multi-latent.",
      "start": 3722.49,
      "duration": 1.71
    },
    {
      "text": "That's a little more complicated.",
      "start": 3724.2,
      "duration": 1.86
    },
    {
      "text": "And the way that this works is\nit kind of turns the entirety",
      "start": 3726.06,
      "duration": 4.29
    },
    {
      "text": "of your keys and values\nacross all your heads",
      "start": 3730.35,
      "duration": 2.85
    },
    {
      "text": "into this one latent vector",
      "start": 3733.2,
      "duration": 3.54
    },
    {
      "text": "that has then kind of\nexpanded in for its time.",
      "start": 3736.74,
      "duration": 3.03
    },
    {
      "text": "- But MLA is from this\ncompany called DeepSeek.",
      "start": 3739.77,
      "duration": 4.2
    },
    {
      "text": "It's quite an interesting algorithm.",
      "start": 3743.97,
      "duration": 2.34
    },
    {
      "text": "Maybe the key idea is in\nboth MQA and in other places,",
      "start": 3746.31,
      "duration": 5.0
    },
    {
      "text": "what you're doing is you're\nreducing the number of KV heads.",
      "start": 3752.52,
      "duration": 5.0
    },
    {
      "text": "And the advantage you get from\nthat is there's less of them.",
      "start": 3758.67,
      "duration": 4.653
    },
    {
      "text": "You want each of the keys and values",
      "start": 3769.98,
      "duration": 1.89
    },
    {
      "text": "to actually be different.",
      "start": 3771.87,
      "duration": 0.96
    },
    {
      "text": "So, one way to reduce the size",
      "start": 3772.83,
      "duration": 1.59
    },
    {
      "text": "is you keep one big shared vector",
      "start": 3774.42,
      "duration": 4.89
    },
    {
      "text": "for all the keys and values,",
      "start": 3779.31,
      "duration": 2.04
    },
    {
      "text": "and then you have smaller\nvectors for every single token.",
      "start": 3781.35,
      "duration": 2.52
    },
    {
      "text": "So that you can store the\nonly the smaller thing",
      "start": 3783.87,
      "duration": 3.66
    },
    {
      "text": "as some sort of low-rank reduction.",
      "start": 3787.53,
      "duration": 2.283
    },
    {
      "text": "At the end of the time,",
      "start": 3791.703,
      "duration": 1.107
    },
    {
      "text": "when you eventually wanna\ncompute the final thing,",
      "start": 3792.81,
      "duration": 3.24
    },
    {
      "text": "remember that your memory band,",
      "start": 3796.05,
      "duration": 1.56
    },
    {
      "text": "which means that you still\nhave some compute left",
      "start": 3797.61,
      "duration": 2.7
    },
    {
      "text": "that you can use for these things.",
      "start": 3800.31,
      "duration": 1.287
    },
    {
      "text": "And if you can expand the\nlatent vector back out",
      "start": 3801.597,
      "duration": 5.0
    },
    {
      "text": "and somehow this is far more efficient",
      "start": 3807.39,
      "duration": 2.49
    },
    {
      "text": "because you're reducing, for example,",
      "start": 3809.88,
      "duration": 3.42
    },
    {
      "text": "maybe you're reducing vec 32 or something",
      "start": 3813.3,
      "duration": 2.88
    },
    {
      "text": "like the size of the\nvector that you're keeping.",
      "start": 3816.18,
      "duration": 1.74
    },
    {
      "text": "- Yeah, there's perhaps some richness",
      "start": 3817.92,
      "duration": 2.01
    },
    {
      "text": "in having a separate\nset of keys and values",
      "start": 3819.93,
      "duration": 3.99
    },
    {
      "text": "and query that kind of pairwise match up",
      "start": 3823.92,
      "duration": 2.04
    },
    {
      "text": "versus compressing that all into one",
      "start": 3825.96,
      "duration": 1.8
    },
    {
      "text": "in that interaction at least.",
      "start": 3829.452,
      "duration": 1.908
    },
    {
      "text": "- Okay, and all of that is\ndealing with being memory bound.",
      "start": 3831.36,
      "duration": 4.02
    },
    {
      "text": "- Yeah.",
      "start": 3835.38,
      "duration": 0.833
    },
    {
      "text": "- I mean, ultimately,",
      "start": 3837.96,
      "duration": 1.05
    },
    {
      "text": "how does that map to the user experience?",
      "start": 3839.01,
      "duration": 2.37
    },
    {
      "text": "Trying to get the-",
      "start": 3841.38,
      "duration": 1.2
    },
    {
      "text": "- Yeah, the two things that it maps to",
      "start": 3842.58,
      "duration": 1.32
    },
    {
      "text": "is you can now make\nyour cache a lot larger",
      "start": 3843.9,
      "duration": 2.73
    },
    {
      "text": "because you've less space\nallocated for the KV cache.",
      "start": 3846.63,
      "duration": 2.94
    },
    {
      "text": "You can maybe cache a\nlot more aggressively",
      "start": 3849.57,
      "duration": 1.68
    },
    {
      "text": "in a lot more things, so\nyou get more cache hits,",
      "start": 3851.25,
      "duration": 2.85
    },
    {
      "text": "which are helpful for reducing\nthe time to first token",
      "start": 3854.1,
      "duration": 3.21
    },
    {
      "text": "for the reasons that were\nkind of described earlier.",
      "start": 3857.31,
      "duration": 2.1
    },
    {
      "text": "And then the second being,",
      "start": 3859.41,
      "duration": 1.26
    },
    {
      "text": "when you start doing inference\nwith more and more requests",
      "start": 3860.67,
      "duration": 3.48
    },
    {
      "text": "and larger and larger batch sizes,",
      "start": 3864.15,
      "duration": 1.77
    },
    {
      "text": "you don't see much of a slowdown",
      "start": 3865.92,
      "duration": 2.11
    },
    {
      "text": "as it's generating the\ntokens at the speed of that.",
      "start": 3869.34,
      "duration": 1.86
    },
    {
      "text": "- Well, it also allows you\nto make your prompt bigger",
      "start": 3871.2,
      "duration": 2.73
    },
    {
      "text": "for certain-\n- Yeah, yeah.",
      "start": 3873.93,
      "duration": 0.9
    },
    {
      "text": "So, the size of your KV cache",
      "start": 3874.83,
      "duration": 3.45
    },
    {
      "text": "is both the size of all your prompts,",
      "start": 3878.28,
      "duration": 3.0
    },
    {
      "text": "multiplied by the number of prompts",
      "start": 3881.28,
      "duration": 1.2
    },
    {
      "text": "being processed in parallel.",
      "start": 3882.48,
      "duration": 1.14
    },
    {
      "text": "So you could increase either\nthose dimensions, right?",
      "start": 3883.62,
      "duration": 2.19
    },
    {
      "text": "The batch size or the size of your prompts",
      "start": 3885.81,
      "duration": 2.37
    },
    {
      "text": "without degrading the\nlatency of generating tokens.",
      "start": 3888.18,
      "duration": 3.75
    },
    {
      "text": "- Arvid, you wrote a blog post,",
      "start": 3891.93,
      "duration": 1.267
    },
    {
      "text": "\"Shadow Workspace: Iterating\non Code in the Background.\"",
      "start": 3893.197,
      "duration": 3.812
    },
    {
      "text": "So, what's going on?",
      "start": 3897.009,
      "duration": 1.608
    },
    {
      "text": "- So, to be clear, we want there",
      "start": 3898.617,
      "duration": 2.073
    },
    {
      "text": "to be a lot of stuff\nhappening in the background,",
      "start": 3900.69,
      "duration": 2.85
    },
    {
      "text": "and we're experimenting\nwith a lot of things.",
      "start": 3903.54,
      "duration": 2.25
    },
    {
      "text": "Right now, we don't have\nmuch stuff happening",
      "start": 3905.79,
      "duration": 3.36
    },
    {
      "text": "other than the cache warming",
      "start": 3909.15,
      "duration": 1.8
    },
    {
      "text": "or figuring out the right context",
      "start": 3910.95,
      "duration": 2.94
    },
    {
      "text": "that goes into your command\nkey prompts, for example.",
      "start": 3913.89,
      "duration": 2.61
    },
    {
      "text": "But the idea is if you can\nactually spend computation",
      "start": 3916.5,
      "duration": 2.91
    },
    {
      "text": "in the background, then\nyou can help the user",
      "start": 3919.41,
      "duration": 5.0
    },
    {
      "text": "maybe at a slightly longer time horizon",
      "start": 3924.75,
      "duration": 3.06
    },
    {
      "text": "than just predicting the next few lines",
      "start": 3927.81,
      "duration": 2.28
    },
    {
      "text": "that you're gonna make.",
      "start": 3930.09,
      "duration": 0.833
    },
    {
      "text": "But actually in the next 10 minutes,",
      "start": 3930.923,
      "duration": 1.957
    },
    {
      "text": "what are you going to make?",
      "start": 3932.88,
      "duration": 1.17
    },
    {
      "text": "And by doing it in background,",
      "start": 3934.05,
      "duration": 1.65
    },
    {
      "text": "you can spend more computation doing that.",
      "start": 3935.7,
      "duration": 3.09
    },
    {
      "text": "And so the idea of the Shadow\nWorkspace that we implemented,",
      "start": 3938.79,
      "duration": 3.93
    },
    {
      "text": "and we use it internally for experiments",
      "start": 3942.72,
      "duration": 3.03
    },
    {
      "text": "is that to actually get advantage",
      "start": 3945.75,
      "duration": 3.54
    },
    {
      "text": "of doing stuff in the background,",
      "start": 3949.29,
      "duration": 1.59
    },
    {
      "text": "you want some kind of feedback signal",
      "start": 3950.88,
      "duration": 2.55
    },
    {
      "text": "to give back to the model",
      "start": 3953.43,
      "duration": 1.41
    },
    {
      "text": "because otherwise, you\ncan get higher performance",
      "start": 3954.84,
      "duration": 2.64
    },
    {
      "text": "by just letting the\nmodel think for longer,",
      "start": 3957.48,
      "duration": 3.27
    },
    {
      "text": "and so o1 is a good example of that.",
      "start": 3960.75,
      "duration": 2.25
    },
    {
      "text": "But another way you\ncan improve performance",
      "start": 3963.0,
      "duration": 1.77
    },
    {
      "text": "is by letting the model\niterate and get feedback.",
      "start": 3964.77,
      "duration": 3.27
    },
    {
      "text": "And so, one very important\npiece of feedback",
      "start": 3968.04,
      "duration": 3.15
    },
    {
      "text": "when you're a programmer\nis the language server,",
      "start": 3971.19,
      "duration": 3.78
    },
    {
      "text": "which is this thing, it exists\nfor most different languages,",
      "start": 3974.97,
      "duration": 5.0
    },
    {
      "text": "and there's a separate\nlanguage server per language.",
      "start": 3980.07,
      "duration": 2.58
    },
    {
      "text": "And it can tell you, \"You're\nusing the wrong type here,\"",
      "start": 3982.65,
      "duration": 3.48
    },
    {
      "text": "and then gives you an error,",
      "start": 3986.13,
      "duration": 1.17
    },
    {
      "text": "or it can allow you to go to definition",
      "start": 3987.3,
      "duration": 2.07
    },
    {
      "text": "and understands the\nstructure of your code.",
      "start": 3989.37,
      "duration": 2.433
    },
    {
      "text": "There is a TypeScript\nlanguage server developed",
      "start": 3995.13,
      "duration": 1.65
    },
    {
      "text": "by the TypeScript people,",
      "start": 3996.78,
      "duration": 1.38
    },
    {
      "text": "a Rust language server\ndeveloped by the Rust people,",
      "start": 3998.16,
      "duration": 1.95
    },
    {
      "text": "and then they all interface",
      "start": 4000.11,
      "duration": 1.38
    },
    {
      "text": "over the language server\nprotocol to VS Code.",
      "start": 4001.49,
      "duration": 1.98
    },
    {
      "text": "So that VS Code doesn't need to have all",
      "start": 4003.47,
      "duration": 2.01
    },
    {
      "text": "of the different languages\nbuilt into VS Code",
      "start": 4005.48,
      "duration": 2.01
    },
    {
      "text": "but rather you can use the\nexisting compiler infrastructure.",
      "start": 4007.49,
      "duration": 3.599
    },
    {
      "text": "- For linting purposes, what-",
      "start": 4011.089,
      "duration": 1.186
    },
    {
      "text": "- It's for linting.",
      "start": 4012.275,
      "duration": 1.125
    },
    {
      "text": "It's for going to definition,",
      "start": 4013.4,
      "duration": 2.58
    },
    {
      "text": "and for seeing the right\ntypes that you're using.",
      "start": 4015.98,
      "duration": 3.0
    },
    {
      "text": "- So it's doing type checking also?",
      "start": 4018.98,
      "duration": 2.43
    },
    {
      "text": "- Yes, type checking\nand going to references.",
      "start": 4021.41,
      "duration": 2.553
    },
    {
      "text": "And that's like when you're\nworking in a big project,",
      "start": 4025.04,
      "duration": 2.01
    },
    {
      "text": "you kind of need that.",
      "start": 4027.05,
      "duration": 1.41
    },
    {
      "text": "If you don't have that,",
      "start": 4028.46,
      "duration": 1.14
    },
    {
      "text": "it's really hard to code in a big project.",
      "start": 4029.6,
      "duration": 3.15
    },
    {
      "text": "- Can you say, again, how\nthat's being used inside Cursor,",
      "start": 4032.75,
      "duration": 3.54
    },
    {
      "text": "the language server protocol\ncommunication thing?",
      "start": 4036.29,
      "duration": 4.17
    },
    {
      "text": "- So it's being used in Cursor\nto show to the programmer",
      "start": 4040.46,
      "duration": 2.07
    },
    {
      "text": "just like in VS Code, but then the idea is",
      "start": 4042.53,
      "duration": 2.16
    },
    {
      "text": "you want to show that same\ninformation to the models,",
      "start": 4044.69,
      "duration": 3.54
    },
    {
      "text": "the IM models, and you\nwant to do that in a way",
      "start": 4048.23,
      "duration": 3.6
    },
    {
      "text": "that doesn't affect the user",
      "start": 4051.83,
      "duration": 1.53
    },
    {
      "text": "because you want to do it in background.",
      "start": 4053.36,
      "duration": 1.287
    },
    {
      "text": "And so the idea behind\nthe Shadow Workspace was,",
      "start": 4054.647,
      "duration": 3.333
    },
    {
      "text": "okay, one way we can do this\nis we spawn a separate window",
      "start": 4057.98,
      "duration": 5.0
    },
    {
      "text": "of Cursor that's hidden, and\nso you can set this flag in it",
      "start": 4063.89,
      "duration": 4.05
    },
    {
      "text": "and like turn it's hidden.",
      "start": 4067.94,
      "duration": 0.99
    },
    {
      "text": "There is a window but you\ndon't actually see it.",
      "start": 4068.93,
      "duration": 1.8
    },
    {
      "text": "And inside of this window,\nthe AI agents can modify code",
      "start": 4070.73,
      "duration": 3.69
    },
    {
      "text": "however they want as long\nas they don't save it",
      "start": 4074.42,
      "duration": 2.67
    },
    {
      "text": "because it's still the same folder,",
      "start": 4077.09,
      "duration": 2.22
    },
    {
      "text": "and then can get feedback from the linters",
      "start": 4079.31,
      "duration": 2.46
    },
    {
      "text": "and go to definition and\niterate on their code.",
      "start": 4081.77,
      "duration": 2.31
    },
    {
      "text": "- So literally run\neverything in the background,",
      "start": 4084.08,
      "duration": 2.703
    },
    {
      "text": "right, maybe even run the code.",
      "start": 4087.62,
      "duration": 3.24
    },
    {
      "text": "- So that's the eventual version\nand that's what you want.",
      "start": 4090.86,
      "duration": 2.797
    },
    {
      "text": "And a lot of the blog\npost is actually about",
      "start": 4093.657,
      "duration": 2.603
    },
    {
      "text": "how do you make that happen",
      "start": 4096.26,
      "duration": 2.88
    },
    {
      "text": "because it's a little bit tricky.",
      "start": 4099.14,
      "duration": 1.59
    },
    {
      "text": "You want it to be on the user's machine",
      "start": 4100.73,
      "duration": 1.53
    },
    {
      "text": "so that it exactly mirrors\nthe user's environment.",
      "start": 4102.26,
      "duration": 3.63
    },
    {
      "text": "And then on Linux, you\ncan do this cool thing",
      "start": 4105.89,
      "duration": 3.18
    },
    {
      "text": "where you can actually\nmirror the file system",
      "start": 4109.07,
      "duration": 2.31
    },
    {
      "text": "and have the AI make changes to the files,",
      "start": 4111.38,
      "duration": 3.99
    },
    {
      "text": "and it thinks that it's\noperating on the file level,",
      "start": 4115.37,
      "duration": 3.3
    },
    {
      "text": "but actually, that's stored in memory",
      "start": 4118.67,
      "duration": 4.17
    },
    {
      "text": "and you can create this\nkernel-like extension",
      "start": 4122.84,
      "duration": 2.97
    },
    {
      "text": "to make it work.",
      "start": 4125.81,
      "duration": 1.38
    },
    {
      "text": "Whereas on Mac and Windows,",
      "start": 4127.19,
      "duration": 2.64
    },
    {
      "text": "it's a little bit more difficult,",
      "start": 4129.83,
      "duration": 2.073
    },
    {
      "text": "but it's a fun technical\nproblem, so that's why.",
      "start": 4134.24,
      "duration": 2.91
    },
    {
      "text": "- One may be hacky but interesting idea",
      "start": 4137.15,
      "duration": 1.98
    },
    {
      "text": "that I like is holding a lock on saving.",
      "start": 4139.13,
      "duration": 3.03
    },
    {
      "text": "And so basically, you can\nthen have the language model",
      "start": 4142.16,
      "duration": 2.55
    },
    {
      "text": "kind of hold the lock on saving to disk,",
      "start": 4144.71,
      "duration": 2.55
    },
    {
      "text": "and then instead of you operating",
      "start": 4147.26,
      "duration": 1.86
    },
    {
      "text": "in the ground truth version of the files",
      "start": 4149.12,
      "duration": 2.43
    },
    {
      "text": "that are saved to disk,\nyou actually are operating",
      "start": 4151.55,
      "duration": 1.74
    },
    {
      "text": "what was the Shadow Workspace before",
      "start": 4153.29,
      "duration": 1.716
    },
    {
      "text": "and these unsaved things\nthat only exist in memory",
      "start": 4155.006,
      "duration": 1.584
    },
    {
      "text": "that you still get linter\nerrors for, and you can code in.",
      "start": 4156.59,
      "duration": 2.49
    },
    {
      "text": "And then when you try to maybe run code,",
      "start": 4159.08,
      "duration": 2.22
    },
    {
      "text": "it's just like there's a small\nwarning that there's a lock,",
      "start": 4161.3,
      "duration": 2.64
    },
    {
      "text": "and then you kind of\nwill take back the lock",
      "start": 4163.94,
      "duration": 1.83
    },
    {
      "text": "from the language server",
      "start": 4165.77,
      "duration": 1.2
    },
    {
      "text": "if you're trying to do things concurrently",
      "start": 4166.97,
      "duration": 1.59
    },
    {
      "text": "or from the Shadow Workspace",
      "start": 4168.56,
      "duration": 1.23
    },
    {
      "text": "if you're trying to do\nthings concurrently.",
      "start": 4169.79,
      "duration": 1.95
    },
    {
      "text": "- That's such an exciting\nfuture by the way.",
      "start": 4171.74,
      "duration": 2.13
    },
    {
      "text": "It's a bit of a tangent,",
      "start": 4173.87,
      "duration": 0.99
    },
    {
      "text": "but to allow a model to change files,",
      "start": 4174.86,
      "duration": 3.54
    },
    {
      "text": "it's scary for people\nbut it's really cool,",
      "start": 4178.4,
      "duration": 3.75
    },
    {
      "text": "to be able to just let the\nagent do a set of tasks",
      "start": 4182.15,
      "duration": 3.87
    },
    {
      "text": "and you come back the next\nday and kind of observe,",
      "start": 4186.02,
      "duration": 3.66
    },
    {
      "text": "like it's a colleague\nor something like that.",
      "start": 4189.68,
      "duration": 2.73
    },
    {
      "text": "- And I think there may be\ndifferent versions of runability",
      "start": 4192.41,
      "duration": 3.93
    },
    {
      "text": "for the simple things\nwhere you're doing things",
      "start": 4196.34,
      "duration": 2.46
    },
    {
      "text": "in the span of a few minutes\non behalf of the user",
      "start": 4198.8,
      "duration": 2.07
    },
    {
      "text": "as they're programming, it makes sense",
      "start": 4200.87,
      "duration": 2.1
    },
    {
      "text": "to make something work\nlocally in their machine.",
      "start": 4202.97,
      "duration": 2.82
    },
    {
      "text": "I think for the more aggressive things",
      "start": 4205.79,
      "duration": 1.622
    },
    {
      "text": "where you're making larger changes",
      "start": 4207.412,
      "duration": 1.258
    },
    {
      "text": "that take longer periods of time,",
      "start": 4208.67,
      "duration": 1.68
    },
    {
      "text": "you'll probably wanna do this",
      "start": 4210.35,
      "duration": 1.29
    },
    {
      "text": "in some sandbox remote environment",
      "start": 4211.64,
      "duration": 1.83
    },
    {
      "text": "and that's another\nincredibly tricky problem",
      "start": 4213.47,
      "duration": 2.31
    },
    {
      "text": "of how do you exactly reproduce",
      "start": 4215.78,
      "duration": 3.09
    },
    {
      "text": "or mostly reproduce to the point of it",
      "start": 4218.87,
      "duration": 2.13
    },
    {
      "text": "being effectively\nequivalent for running code",
      "start": 4221.0,
      "duration": 2.52
    },
    {
      "text": "the user's environment\nwith this remote sandbox.",
      "start": 4223.52,
      "duration": 3.69
    },
    {
      "text": "- I'm curious what kind of\nagents you want for coding?",
      "start": 4227.21,
      "duration": 4.26
    },
    {
      "text": "Do you want them to find bugs?",
      "start": 4231.47,
      "duration": 1.44
    },
    {
      "text": "Do you want them to\nimplement new features?",
      "start": 4232.91,
      "duration": 2.193
    },
    {
      "text": "What agents do you want?",
      "start": 4236.3,
      "duration": 0.833
    },
    {
      "text": "- So by the way, when\nI think about agents,",
      "start": 4237.133,
      "duration": 1.447
    },
    {
      "text": "I don't think just about coding.",
      "start": 4238.58,
      "duration": 2.82
    },
    {
      "text": "I think so for this particular podcast,",
      "start": 4241.4,
      "duration": 3.72
    },
    {
      "text": "there's video editing\nand if you look in Adobe,",
      "start": 4245.12,
      "duration": 3.27
    },
    {
      "text": "there's code behind.",
      "start": 4248.39,
      "duration": 1.92
    },
    {
      "text": "It's very poorly documented code,",
      "start": 4250.31,
      "duration": 1.77
    },
    {
      "text": "but you can interact with\nPremiere, for example, using code,",
      "start": 4252.08,
      "duration": 4.38
    },
    {
      "text": "and basically all the uploading,\neverything I do on YouTube,",
      "start": 4256.46,
      "duration": 3.21
    },
    {
      "text": "everything as you could probably imagine,",
      "start": 4259.67,
      "duration": 1.8
    },
    {
      "text": "I do all of that through code\nand including translation",
      "start": 4261.47,
      "duration": 3.45
    },
    {
      "text": "and overdubbing, all of this.",
      "start": 4264.92,
      "duration": 1.71
    },
    {
      "text": "So, I envision all of\nthose kinds of tasks.",
      "start": 4266.63,
      "duration": 3.54
    },
    {
      "text": "So automating many of the tasks",
      "start": 4270.17,
      "duration": 1.68
    },
    {
      "text": "that don't have to do directly\nwith the editing, so that.",
      "start": 4271.85,
      "duration": 3.57
    },
    {
      "text": "Okay, that's what I was thinking about.",
      "start": 4275.42,
      "duration": 1.59
    },
    {
      "text": "But in terms of coding,",
      "start": 4277.01,
      "duration": 2.52
    },
    {
      "text": "I would be fundamentally\nthinking about bug finding,",
      "start": 4279.53,
      "duration": 4.413
    },
    {
      "text": "many levels of kind of bug finding",
      "start": 4285.35,
      "duration": 2.1
    },
    {
      "text": "and also bug finding like logical bugs,",
      "start": 4287.45,
      "duration": 2.76
    },
    {
      "text": "not logical like spiritual\nbugs or something.",
      "start": 4290.21,
      "duration": 2.361
    },
    {
      "text": "(group chuckling)",
      "start": 4292.571,
      "duration": 1.689
    },
    {
      "text": "Ones like big directions\nof implementation,",
      "start": 4294.26,
      "duration": 3.12
    },
    {
      "text": "that kind of stuff.",
      "start": 4297.38,
      "duration": 1.29
    },
    {
      "text": "- Magical (indistinct) and bug finding.",
      "start": 4298.67,
      "duration": 1.32
    },
    {
      "text": "- Yeah, I mean, it's really interesting",
      "start": 4299.99,
      "duration": 1.95
    },
    {
      "text": "that these models are\nso bad at bug finding",
      "start": 4301.94,
      "duration": 2.89
    },
    {
      "text": "when just naively prompted to find a bug.",
      "start": 4305.75,
      "duration": 3.0
    },
    {
      "text": "They're incredibly poorly calibrated.",
      "start": 4308.75,
      "duration": 2.58
    },
    {
      "text": "- Even the smartest models.\n- Exactly, even o1.",
      "start": 4311.33,
      "duration": 2.623
    },
    {
      "text": "- How do you explain that?",
      "start": 4315.144,
      "duration": 1.346
    },
    {
      "text": "Is there a good intuition?",
      "start": 4316.49,
      "duration": 1.383
    },
    {
      "text": "- I think these models are\nreally strong reflection",
      "start": 4318.74,
      "duration": 3.33
    },
    {
      "text": "of the pre-training distribution,",
      "start": 4322.07,
      "duration": 2.49
    },
    {
      "text": "and I do think they generalize",
      "start": 4324.56,
      "duration": 2.37
    },
    {
      "text": "as the loss gets lower and lower,",
      "start": 4326.93,
      "duration": 1.59
    },
    {
      "text": "but I don't think the loss is low enough",
      "start": 4328.52,
      "duration": 4.47
    },
    {
      "text": "such that they're really\nfully generalizing on code.",
      "start": 4332.99,
      "duration": 3.48
    },
    {
      "text": "The things that we use these things for,",
      "start": 4336.47,
      "duration": 2.22
    },
    {
      "text": "the frontier models that\nthey're quite good at,",
      "start": 4338.69,
      "duration": 2.64
    },
    {
      "text": "are really code generation\nand question answering.",
      "start": 4341.33,
      "duration": 3.75
    },
    {
      "text": "And these things exist in massive\nquantities in pre-training",
      "start": 4345.08,
      "duration": 3.99
    },
    {
      "text": "with all of the code\nin GitHub on the scale",
      "start": 4349.07,
      "duration": 2.22
    },
    {
      "text": "of many, many trillions of\ntokens and questions and answers",
      "start": 4351.29,
      "duration": 4.2
    },
    {
      "text": "on things like stack overflow\nand maybe GitHub issues.",
      "start": 4355.49,
      "duration": 3.69
    },
    {
      "text": "And so, when you try to\npush one of these things",
      "start": 4359.18,
      "duration": 2.613
    },
    {
      "text": "that really don't exist very much online,",
      "start": 4361.793,
      "duration": 4.44
    },
    {
      "text": "for example, the Cursor Tab objective",
      "start": 4367.25,
      "duration": 1.44
    },
    {
      "text": "of predicting the next edit\ngiven the edits done so far,",
      "start": 4368.69,
      "duration": 3.39
    },
    {
      "text": "the brittleness kind of shows.",
      "start": 4372.08,
      "duration": 1.65
    },
    {
      "text": "And then bug detection\nis another great example,",
      "start": 4373.73,
      "duration": 2.19
    },
    {
      "text": "where there aren't\nreally that many examples",
      "start": 4375.92,
      "duration": 2.16
    },
    {
      "text": "of actually detecting real\nbugs and then proposing fixes",
      "start": 4378.08,
      "duration": 2.95
    },
    {
      "text": "and the models just kind\nof really struggle at it.",
      "start": 4382.82,
      "duration": 3.12
    },
    {
      "text": "But I think it's a question\nof transferring the model",
      "start": 4385.94,
      "duration": 2.73
    },
    {
      "text": "in the same way that you\nget this fantastic transfer",
      "start": 4388.67,
      "duration": 3.27
    },
    {
      "text": "from pre-trained models\njust on code in general",
      "start": 4391.94,
      "duration": 2.79
    },
    {
      "text": "to the Cursor Tab objective.",
      "start": 4394.73,
      "duration": 2.31
    },
    {
      "text": "You'll see a very, very similar thing",
      "start": 4397.04,
      "duration": 2.07
    },
    {
      "text": "with generalized models\nthat are really good at code",
      "start": 4399.11,
      "duration": 2.49
    },
    {
      "text": "to bug detection.",
      "start": 4401.6,
      "duration": 0.84
    },
    {
      "text": "It just takes a little bit of kind nudging",
      "start": 4402.44,
      "duration": 1.89
    },
    {
      "text": "in that direction.",
      "start": 4404.33,
      "duration": 0.99
    },
    {
      "text": "- Look, to be clear,",
      "start": 4405.32,
      "duration": 0.833
    },
    {
      "text": "I think, they understand code really well.",
      "start": 4406.153,
      "duration": 2.617
    },
    {
      "text": "While they're being pre-trained,",
      "start": 4408.77,
      "duration": 1.623
    },
    {
      "text": "the representation that's\nbeing built up almost certainly",
      "start": 4411.26,
      "duration": 3.75
    },
    {
      "text": "like somewhere in the\nstream, the model knows",
      "start": 4415.01,
      "duration": 3.93
    },
    {
      "text": "that maybe there's\nsomething sketchy going on.",
      "start": 4418.94,
      "duration": 3.003
    },
    {
      "text": "Part of it is that humans\nare really calibrated",
      "start": 4430.88,
      "duration": 2.1
    },
    {
      "text": "on which bugs are really important.",
      "start": 4432.98,
      "duration": 1.833
    },
    {
      "text": "It's not just actually saying\nthere's something sketchy.",
      "start": 4436.716,
      "duration": 1.364
    },
    {
      "text": "It's like it's this sketchy trivial,",
      "start": 4438.08,
      "duration": 2.43
    },
    {
      "text": "it's this sketchy like you're\ngonna take the server down.",
      "start": 4440.51,
      "duration": 3.51
    },
    {
      "text": "Part of it is maybe the cultural knowledge",
      "start": 4444.02,
      "duration": 1.77
    },
    {
      "text": "of why is a staff engineer is good",
      "start": 4445.79,
      "duration": 3.277
    },
    {
      "text": "because they know that three years ago",
      "start": 4450.8,
      "duration": 2.31
    },
    {
      "text": "someone wrote a really\nsketchy piece of code",
      "start": 4453.11,
      "duration": 2.91
    },
    {
      "text": "that took the server down.",
      "start": 4456.02,
      "duration": 1.863
    },
    {
      "text": "(group chuckling)",
      "start": 4458.784,
      "duration": 2.833
    },
    {
      "text": "This thing is an experiment.",
      "start": 4463.76,
      "duration": 2.25
    },
    {
      "text": "So, a few bugs are fine,",
      "start": 4466.01,
      "duration": 1.65
    },
    {
      "text": "you're just trying to experiment",
      "start": 4467.66,
      "duration": 1.11
    },
    {
      "text": "and get the feel of the thing.",
      "start": 4468.77,
      "duration": 1.59
    },
    {
      "text": "And so if the model gets really annoying",
      "start": 4470.36,
      "duration": 1.65
    },
    {
      "text": "when you're writing an\nexperiment, that's really bad,",
      "start": 4472.01,
      "duration": 2.61
    },
    {
      "text": "but if you're writing\nsomething for super production,",
      "start": 4474.62,
      "duration": 2.4
    },
    {
      "text": "you're writing a database.",
      "start": 4477.02,
      "duration": 1.59
    },
    {
      "text": "You're writing code in\nPostgres or Linux or whatever.",
      "start": 4478.61,
      "duration": 2.34
    },
    {
      "text": "You're Linus Torvalds.",
      "start": 4480.95,
      "duration": 1.77
    },
    {
      "text": "It's sort of unacceptable\nto have even an edge case",
      "start": 4482.72,
      "duration": 2.7
    },
    {
      "text": "and just having the calibration\nof how paranoid is the user.",
      "start": 4485.42,
      "duration": 5.0
    },
    {
      "text": "- But even then if you're\nputting in a maximum paranoia,",
      "start": 4491.66,
      "duration": 3.51
    },
    {
      "text": "it still just doesn't quite get it.",
      "start": 4495.17,
      "duration": 2.55
    },
    {
      "text": "- Yeah, yeah, yeah.",
      "start": 4497.72,
      "duration": 1.14
    },
    {
      "text": "- I mean, but this is hard\nfor humans too to understand",
      "start": 4498.86,
      "duration": 4.08
    },
    {
      "text": "which line of code is\nimportant, which is not.",
      "start": 4502.94,
      "duration": 2.3
    },
    {
      "text": "I think one of your\nprinciples on a website says",
      "start": 4506.12,
      "duration": 2.28
    },
    {
      "text": "if a code can do a lot of damage,",
      "start": 4508.4,
      "duration": 3.12
    },
    {
      "text": "one should add a comment that say,",
      "start": 4511.52,
      "duration": 2.047
    },
    {
      "text": "\"This line of code is dangerous.\"",
      "start": 4513.567,
      "duration": 3.443
    },
    {
      "text": "- And all caps, repeated 10 times.",
      "start": 4517.01,
      "duration": 3.122
    },
    {
      "text": "(group chuckling)",
      "start": 4520.132,
      "duration": 0.833
    },
    {
      "text": "- No, you say for every\nsingle line of code",
      "start": 4520.965,
      "duration": 3.695
    },
    {
      "text": "inside the function you have\nto, and that's quite profound,",
      "start": 4524.66,
      "duration": 3.42
    },
    {
      "text": "that says something about human beings",
      "start": 4528.08,
      "duration": 2.07
    },
    {
      "text": "because the engineers move on,",
      "start": 4530.15,
      "duration": 3.27
    },
    {
      "text": "even the same person might just forget",
      "start": 4533.42,
      "duration": 2.91
    },
    {
      "text": "how it can sink the\nTitanic a single function.",
      "start": 4536.33,
      "duration": 2.91
    },
    {
      "text": "You might not intuit that quite clearly",
      "start": 4539.24,
      "duration": 1.83
    },
    {
      "text": "by looking at the single piece of code.",
      "start": 4541.07,
      "duration": 1.86
    },
    {
      "text": "- Yeah, and I think that\none is partially also",
      "start": 4542.93,
      "duration": 3.15
    },
    {
      "text": "for today's AI models",
      "start": 4546.08,
      "duration": 2.04
    },
    {
      "text": "where if you actually write\ndangerous, dangerous, dangerous",
      "start": 4548.12,
      "duration": 3.56
    },
    {
      "text": "in every single line,",
      "start": 4551.68,
      "duration": 1.123
    },
    {
      "text": "the models will pay more attention to that",
      "start": 4554.54,
      "duration": 3.0
    },
    {
      "text": "and will be more likely to\nfind bugs in that region.",
      "start": 4557.54,
      "duration": 2.94
    },
    {
      "text": "- That's actually just straight\nup a really good practice",
      "start": 4560.48,
      "duration": 3.12
    },
    {
      "text": "of labeling code of how\nmuch damages can do.",
      "start": 4563.6,
      "duration": 4.71
    },
    {
      "text": "- Yeah, I mean, it's controversial.",
      "start": 4568.31,
      "duration": 1.89
    },
    {
      "text": "Some people think it's ugly.",
      "start": 4570.2,
      "duration": 1.533
    },
    {
      "text": "Sualeh does not like it.",
      "start": 4572.775,
      "duration": 1.955
    },
    {
      "text": "- In fact, I actually think\nthis is one of the things",
      "start": 4574.73,
      "duration": 1.98
    },
    {
      "text": "I learned from Arvid.",
      "start": 4576.71,
      "duration": 1.05
    },
    {
      "text": "Aesthetically, I don't like it,",
      "start": 4580.04,
      "duration": 2.07
    },
    {
      "text": "but I think there's certainly something",
      "start": 4582.11,
      "duration": 2.19
    },
    {
      "text": "where it's useful for the models",
      "start": 4584.3,
      "duration": 2.25
    },
    {
      "text": "and humans just forget a lot,",
      "start": 4586.55,
      "duration": 1.68
    },
    {
      "text": "and it's really easy to\nmake a small mistake.",
      "start": 4588.23,
      "duration": 2.463
    },
    {
      "text": "Just bring down the server.",
      "start": 4594.234,
      "duration": 1.35
    },
    {
      "text": "Of course, we test a lot and whatever,",
      "start": 4597.516,
      "duration": 1.874
    },
    {
      "text": "but there's always these things",
      "start": 4599.39,
      "duration": 2.111
    },
    {
      "text": "that you have to be very careful.",
      "start": 4601.501,
      "duration": 1.099
    },
    {
      "text": "- Yeah, like with just normal docstrings,",
      "start": 4602.6,
      "duration": 1.8
    },
    {
      "text": "I think people will often just skim it",
      "start": 4604.4,
      "duration": 1.95
    },
    {
      "text": "when making a change and think,\n\"Oh, I know how to do this,\"",
      "start": 4606.35,
      "duration": 3.327
    },
    {
      "text": "and you really need to\npoint it out to them",
      "start": 4610.97,
      "duration": 2.4
    },
    {
      "text": "so that doesn't slip through.",
      "start": 4613.37,
      "duration": 1.45
    },
    {
      "text": "- Yeah, you have to be reminded",
      "start": 4615.83,
      "duration": 1.2
    },
    {
      "text": "that you could do a lot of damage,",
      "start": 4617.03,
      "duration": 1.7
    },
    {
      "text": "that's like we don't\nreally think about that.",
      "start": 4619.76,
      "duration": 2.34
    },
    {
      "text": "You think about, \"Okay, how\ndo I figure out how this works",
      "start": 4622.1,
      "duration": 2.88
    },
    {
      "text": "so I can improve it?\"",
      "start": 4624.98,
      "duration": 0.84
    },
    {
      "text": "You don't think about the\nother direction that it could-",
      "start": 4625.82,
      "duration": 2.857
    },
    {
      "text": "- Until we have formal\nverification for everything,",
      "start": 4628.677,
      "duration": 4.073
    },
    {
      "text": "then you can do whatever you want",
      "start": 4632.75,
      "duration": 1.5
    },
    {
      "text": "and you know for certain that\nyou have not introduced a bug",
      "start": 4634.25,
      "duration": 3.6
    },
    {
      "text": "if the proof pass.",
      "start": 4637.85,
      "duration": 1.08
    },
    {
      "text": "- Well, concretely, what\ndo you think that future",
      "start": 4638.93,
      "duration": 1.65
    },
    {
      "text": "would look like?",
      "start": 4640.58,
      "duration": 1.41
    },
    {
      "text": "- I think people will just\nnot write to tests anymore.",
      "start": 4641.99,
      "duration": 4.113
    },
    {
      "text": "You write a function, the\nmodel will suggest a spec,",
      "start": 4650.424,
      "duration": 0.833
    },
    {
      "text": "and you review the spec.",
      "start": 4652.97,
      "duration": 1.26
    },
    {
      "text": "And in the meantime, smart\nreasoning model computes a proof",
      "start": 4654.23,
      "duration": 5.0
    },
    {
      "text": "that the implementation follows the spec,",
      "start": 4659.45,
      "duration": 2.7
    },
    {
      "text": "and I think that happens\nfor most functions.",
      "start": 4662.15,
      "duration": 2.34
    },
    {
      "text": "- Do you think this gets at a little bit",
      "start": 4664.49,
      "duration": 1.89
    },
    {
      "text": "some of the stuff you\nwere talking about earlier",
      "start": 4666.38,
      "duration": 1.32
    },
    {
      "text": "with the difficulty of specifying intent",
      "start": 4667.7,
      "duration": 1.71
    },
    {
      "text": "for what you want with software,",
      "start": 4669.41,
      "duration": 2.25
    },
    {
      "text": "where sometimes it might be",
      "start": 4671.66,
      "duration": 1.5
    },
    {
      "text": "because the intent is\nreally hard to specify,",
      "start": 4673.16,
      "duration": 1.65
    },
    {
      "text": "it's also then going to\nbe really hard to prove",
      "start": 4674.81,
      "duration": 1.89
    },
    {
      "text": "that it's actually matching\nwhatever your intent is?",
      "start": 4676.7,
      "duration": 1.327
    },
    {
      "text": "- You think that spec is hard to generate?",
      "start": 4678.027,
      "duration": 2.876
    },
    {
      "text": "- Yeah, or just for a given spec.",
      "start": 4681.8,
      "duration": 5.0
    },
    {
      "text": "I think there is a question of,",
      "start": 4688.071,
      "duration": 1.234
    },
    {
      "text": "can you actually do the\nformal verification?",
      "start": 4689.305,
      "duration": 2.575
    },
    {
      "text": "Is that possible?",
      "start": 4691.88,
      "duration": 1.02
    },
    {
      "text": "I think that there's more to\ndig into there, but then also-",
      "start": 4692.9,
      "duration": 3.0
    },
    {
      "text": "- Even if you have the spec?",
      "start": 4695.9,
      "duration": 1.59
    },
    {
      "text": "- If you have the spec-\n- Even if you have the spec,",
      "start": 4697.49,
      "duration": 1.98
    },
    {
      "text": "is the spec written in natural language?",
      "start": 4699.47,
      "duration": 1.59
    },
    {
      "text": "Or is it-",
      "start": 4701.06,
      "duration": 1.172
    },
    {
      "text": "- No, the spec would be formal.",
      "start": 4702.232,
      "duration": 2.638
    },
    {
      "text": "- But how easier would\nthat be (indistinct).",
      "start": 4704.87,
      "duration": 0.833
    },
    {
      "text": "- Okay, so then I think\nthat you care about things",
      "start": 4705.703,
      "duration": 2.377
    },
    {
      "text": "that are not going to\nbe easily well specified",
      "start": 4708.08,
      "duration": 1.53
    },
    {
      "text": "in the spec language.",
      "start": 4709.61,
      "duration": 1.23
    },
    {
      "text": "- I see, I see, yeah, yeah.",
      "start": 4710.84,
      "duration": 1.452
    },
    {
      "text": "- Would be maybe an argument\nagainst formal verification",
      "start": 4712.292,
      "duration": 2.898
    },
    {
      "text": "is all you need.",
      "start": 4715.19,
      "duration": 1.23
    },
    {
      "text": "- The worry is there's\nthis massive document-",
      "start": 4716.42,
      "duration": 2.4
    },
    {
      "text": "- Replacing something\nlike unit tests, sure.",
      "start": 4718.82,
      "duration": 2.82
    },
    {
      "text": "- Yeah, yeah.",
      "start": 4721.64,
      "duration": 0.833
    },
    {
      "text": "I think you can probably also\nevolve the spec languages",
      "start": 4723.38,
      "duration": 3.66
    },
    {
      "text": "to capture some of the things",
      "start": 4727.04,
      "duration": 1.787
    },
    {
      "text": "that they don't really capture right now.",
      "start": 4728.827,
      "duration": 2.326
    },
    {
      "text": "I don't know, I think it's very exciting.",
      "start": 4732.8,
      "duration": 2.16
    },
    {
      "text": "- And you're speaking not\njust about single functions,",
      "start": 4734.96,
      "duration": 2.97
    },
    {
      "text": "you're speaking about entire code bases.",
      "start": 4737.93,
      "duration": 2.19
    },
    {
      "text": "- I think entire code bases is harder,",
      "start": 4740.12,
      "duration": 1.47
    },
    {
      "text": "but that is what I would love to have",
      "start": 4741.59,
      "duration": 2.08
    },
    {
      "text": "and I think it should be possible.",
      "start": 4743.67,
      "duration": 2.243
    },
    {
      "text": "There's a lot of work recently",
      "start": 4747.68,
      "duration": 1.5
    },
    {
      "text": "where you can prove formally\nverified down to the hardware.",
      "start": 4749.18,
      "duration": 4.623
    },
    {
      "text": "You formally verify the C code,\nand then you formally verify",
      "start": 4755.21,
      "duration": 2.76
    },
    {
      "text": "through the GCC compiler,",
      "start": 4757.97,
      "duration": 1.62
    },
    {
      "text": "and then through the Verilog\ndown to the hardware.",
      "start": 4759.59,
      "duration": 2.703
    },
    {
      "text": "And that's incredibly big\nsystem, but it actually works.",
      "start": 4763.19,
      "duration": 3.54
    },
    {
      "text": "And I think big code bases\nare sort of similar in that",
      "start": 4766.73,
      "duration": 2.127
    },
    {
      "text": "and they're like multi-layered system.",
      "start": 4768.857,
      "duration": 2.253
    },
    {
      "text": "And if you can decompose it\nand formally verify each part,",
      "start": 4771.11,
      "duration": 3.96
    },
    {
      "text": "then I think it should be possible.",
      "start": 4775.07,
      "duration": 1.5
    },
    {
      "text": "I think this specification\nproblem is a real problem.",
      "start": 4776.57,
      "duration": 2.52
    },
    {
      "text": "- How do you handle side\neffects or how do you handle,",
      "start": 4779.09,
      "duration": 3.24
    },
    {
      "text": "I guess, external dependencies\nlike calling the Stripe API?",
      "start": 4782.33,
      "duration": 3.99
    },
    {
      "text": "- Maybe Stripe would write\na spec for their API.",
      "start": 4786.32,
      "duration": 2.16
    },
    {
      "text": "- But you can't do this for everything.",
      "start": 4788.48,
      "duration": 2.07
    },
    {
      "text": "Can you do this for everything you use?",
      "start": 4790.55,
      "duration": 1.95
    },
    {
      "text": "Maybe people will use\nlanguage models as primitives",
      "start": 4795.32,
      "duration": 2.7
    },
    {
      "text": "in the programs they write,",
      "start": 4798.02,
      "duration": 1.44
    },
    {
      "text": "and there's a dependence on it",
      "start": 4799.46,
      "duration": 1.26
    },
    {
      "text": "and how do you now include that?",
      "start": 4800.72,
      "duration": 1.98
    },
    {
      "text": "- I think you might be\nable to prove that still.",
      "start": 4802.7,
      "duration": 3.24
    },
    {
      "text": "- Prove what about language models?",
      "start": 4805.94,
      "duration": 1.68
    },
    {
      "text": "- I think it feels possible\nthat you could actually prove",
      "start": 4807.62,
      "duration": 3.18
    },
    {
      "text": "that a language model\nis aligned, for example,",
      "start": 4810.8,
      "duration": 4.14
    },
    {
      "text": "or you can prove that it\nactually gives the right answer.",
      "start": 4814.94,
      "duration": 3.963
    },
    {
      "text": "- That's the dream.",
      "start": 4820.458,
      "duration": 0.902
    },
    {
      "text": "- Yeah, I mean, if it's possible.",
      "start": 4821.36,
      "duration": 3.24
    },
    {
      "text": "That's your I have a dream speech.",
      "start": 4824.6,
      "duration": 1.44
    },
    {
      "text": "If it's possible, that will certainly help",
      "start": 4826.04,
      "duration": 2.46
    },
    {
      "text": "with making sure your\ncode doesn't have bugs",
      "start": 4828.5,
      "duration": 3.39
    },
    {
      "text": "and making sure AI doesn't\ndestroy all human civilization.",
      "start": 4831.89,
      "duration": 3.18
    },
    {
      "text": "So, the full spectrum of AI\nsafety to just bug finding.",
      "start": 4835.07,
      "duration": 4.233
    },
    {
      "text": "So, you said the models\nstruggle with bug finding.",
      "start": 4840.44,
      "duration": 2.25
    },
    {
      "text": "What's the hope?",
      "start": 4842.69,
      "duration": 1.2
    },
    {
      "text": "- My hope initially is, and I\ncan let Michael chime in too,",
      "start": 4843.89,
      "duration": 4.68
    },
    {
      "text": "but it was like it should first\nhelp with the stupid bugs.",
      "start": 4848.57,
      "duration": 5.0
    },
    {
      "text": "It should query quickly,",
      "start": 4854.72,
      "duration": 1.11
    },
    {
      "text": "catch the stupid bugs off by one error.",
      "start": 4855.83,
      "duration": 3.03
    },
    {
      "text": "Sometimes you write something in a comment",
      "start": 4858.86,
      "duration": 1.5
    },
    {
      "text": "and do the other way.",
      "start": 4860.36,
      "duration": 1.59
    },
    {
      "text": "It's very common.",
      "start": 4861.95,
      "duration": 0.87
    },
    {
      "text": "I do this.",
      "start": 4862.82,
      "duration": 0.833
    },
    {
      "text": "I write less than in a comment",
      "start": 4863.653,
      "duration": 1.297
    },
    {
      "text": "and I maybe write the greater\nthan or something like that.",
      "start": 4864.95,
      "duration": 2.97
    },
    {
      "text": "And the model is like,\n\"Yeah, you looks sketchy.",
      "start": 4867.92,
      "duration": 2.52
    },
    {
      "text": "You sure you wanna do that?\"",
      "start": 4870.44,
      "duration": 1.557
    },
    {
      "text": "But eventually, it should be\nable to catch harder bugs too.",
      "start": 4873.02,
      "duration": 3.15
    },
    {
      "text": "- Yeah, and I think that\nit's also important to note",
      "start": 4876.17,
      "duration": 2.91
    },
    {
      "text": "that having good bug, finding\nmodels feels necessary",
      "start": 4879.08,
      "duration": 4.41
    },
    {
      "text": "to get to the highest reaches",
      "start": 4883.49,
      "duration": 1.17
    },
    {
      "text": "of having AI do more and\nmore programming for you.",
      "start": 4884.66,
      "duration": 2.943
    },
    {
      "text": "If AI is building more and\nmore of the system for you,",
      "start": 4889.22,
      "duration": 1.95
    },
    {
      "text": "you need to not just\ngenerate but also verify.",
      "start": 4891.17,
      "duration": 2.64
    },
    {
      "text": "And without that, some of the problems",
      "start": 4893.81,
      "duration": 1.743
    },
    {
      "text": "that we've talked about\nbefore with programming,",
      "start": 4895.553,
      "duration": 2.457
    },
    {
      "text": "with these models will\njust become untenable.",
      "start": 4898.01,
      "duration": 4.38
    },
    {
      "text": "So it's not just for humans\nlike you write a bug,",
      "start": 4902.39,
      "duration": 3.27
    },
    {
      "text": "I write a bug, find the bug for me,",
      "start": 4905.66,
      "duration": 1.47
    },
    {
      "text": "but it's also being able\nto verify the AI's code",
      "start": 4907.13,
      "duration": 3.03
    },
    {
      "text": "and check it is really important.",
      "start": 4910.16,
      "duration": 2.37
    },
    {
      "text": "- Yeah, and then how do\nyou actually do this?",
      "start": 4912.53,
      "duration": 1.71
    },
    {
      "text": "We have had a lot of\ncontentious dinner discussions",
      "start": 4914.24,
      "duration": 2.07
    },
    {
      "text": "of how do you actually train a bug model,",
      "start": 4916.31,
      "duration": 1.497
    },
    {
      "text": "but one very popular idea\nis it's potentially easy",
      "start": 4917.807,
      "duration": 4.983
    },
    {
      "text": "to introduce a bug than\nactually finding the bug.",
      "start": 4922.79,
      "duration": 2.49
    },
    {
      "text": "And so, you can train a\nmodel to introduce bugs",
      "start": 4925.28,
      "duration": 2.88
    },
    {
      "text": "in existing code,",
      "start": 4928.16,
      "duration": 1.38
    },
    {
      "text": "and then you can train\na reverse bug model then",
      "start": 4929.54,
      "duration": 4.2
    },
    {
      "text": "that can find bugs using\nthis synthetic data.",
      "start": 4933.74,
      "duration": 3.63
    },
    {
      "text": "So that's one example,",
      "start": 4937.37,
      "duration": 1.263
    },
    {
      "text": "but there are lots of ideas\nfor how to (indistinct).",
      "start": 4939.68,
      "duration": 2.6
    },
    {
      "text": "- You can also do a bunch of work",
      "start": 4943.644,
      "duration": 0.926
    },
    {
      "text": "not even at the model level\nof taking the biggest models",
      "start": 4944.57,
      "duration": 2.82
    },
    {
      "text": "and then maybe giving them\naccess to a lot of information",
      "start": 4947.39,
      "duration": 3.36
    },
    {
      "text": "that's not just the code.",
      "start": 4950.75,
      "duration": 1.77
    },
    {
      "text": "It's a hard problem to\nstare at a file and be like,",
      "start": 4952.52,
      "duration": 2.077
    },
    {
      "text": "\"Where's the bug?\"",
      "start": 4954.597,
      "duration": 1.073
    },
    {
      "text": "And that's hard for humans often, right?",
      "start": 4955.67,
      "duration": 2.277
    },
    {
      "text": "And so often, you have to run the code",
      "start": 4957.947,
      "duration": 1.773
    },
    {
      "text": "and being able to see things like traces",
      "start": 4959.72,
      "duration": 1.65
    },
    {
      "text": "and step through a debugger,",
      "start": 4961.37,
      "duration": 1.89
    },
    {
      "text": "there's another whole other direction",
      "start": 4963.26,
      "duration": 1.29
    },
    {
      "text": "where it tends toward that.",
      "start": 4964.55,
      "duration": 1.77
    },
    {
      "text": "- It could also be",
      "start": 4966.32,
      "duration": 0.833
    },
    {
      "text": "that there are two different\nproduct form factors here.",
      "start": 4967.153,
      "duration": 1.317
    },
    {
      "text": "It could be that you have\na really specialty model",
      "start": 4968.47,
      "duration": 2.17
    },
    {
      "text": "that's quite fast that's\nrunning in the background",
      "start": 4970.64,
      "duration": 1.83
    },
    {
      "text": "and trying to spot bugs.",
      "start": 4972.47,
      "duration": 1.41
    },
    {
      "text": "And it might be that sometimes\nto Arvid's earlier example",
      "start": 4973.88,
      "duration": 3.3
    },
    {
      "text": "about some nefarious input box bug.",
      "start": 4977.18,
      "duration": 2.433
    },
    {
      "text": "You know there's a bug,",
      "start": 4981.26,
      "duration": 0.93
    },
    {
      "text": "you're not just checking hypothesis free,",
      "start": 4982.19,
      "duration": 1.89
    },
    {
      "text": "you're like, \"This is a problem,\nI really wanna solve it,\"",
      "start": 4984.08,
      "duration": 2.49
    },
    {
      "text": "and you zap that with tons\nand tons and tons of compute,",
      "start": 4986.57,
      "duration": 2.31
    },
    {
      "text": "and you're willing to put\nin $50 to solve that bug",
      "start": 4988.88,
      "duration": 2.25
    },
    {
      "text": "or something even more.",
      "start": 4991.13,
      "duration": 1.68
    },
    {
      "text": "- Have you thought about integrating money",
      "start": 4992.81,
      "duration": 1.92
    },
    {
      "text": "into this whole thing?",
      "start": 4994.73,
      "duration": 0.87
    },
    {
      "text": "I would pay probably a\nlarge amount of money",
      "start": 4995.6,
      "duration": 2.19
    },
    {
      "text": "if you found a bug or even generated code",
      "start": 4997.79,
      "duration": 2.52
    },
    {
      "text": "that I really appreciated.",
      "start": 5000.31,
      "duration": 0.9
    },
    {
      "text": "I had a moment a few days ago\nwhen I started using Cursor",
      "start": 5001.21,
      "duration": 3.03
    },
    {
      "text": "where it generated perfect three functions",
      "start": 5004.24,
      "duration": 3.63
    },
    {
      "text": "for interacting with the\nYouTube API to update captions",
      "start": 5012.76,
      "duration": 5.0
    },
    {
      "text": "for localization in different languages.",
      "start": 5019.63,
      "duration": 2.76
    },
    {
      "text": "The API documentation is not\nvery good and the code across.",
      "start": 5022.39,
      "duration": 4.17
    },
    {
      "text": "I googled it for a while.",
      "start": 5026.56,
      "duration": 1.74
    },
    {
      "text": "I couldn't find exactly,",
      "start": 5028.3,
      "duration": 1.23
    },
    {
      "text": "there's a lot of confusing information,",
      "start": 5029.53,
      "duration": 1.71
    },
    {
      "text": "and Cursor generated perfectly.",
      "start": 5031.24,
      "duration": 2.52
    },
    {
      "text": "I just sit back, I read\nthe code, I was like,",
      "start": 5033.76,
      "duration": 2.017
    },
    {
      "text": "\"This is correct, I\ntested it, it's correct.\"",
      "start": 5035.777,
      "duration": 2.363
    },
    {
      "text": "I was like, \"I wanna tip.\"",
      "start": 5038.14,
      "duration": 1.347
    },
    {
      "text": "I want a button that goes, \"Here's $5.\"",
      "start": 5040.54,
      "duration": 3.36
    },
    {
      "text": "One that's really good\njust to support the company",
      "start": 5043.9,
      "duration": 2.01
    },
    {
      "text": "and support what the interface is.",
      "start": 5045.91,
      "duration": 2.25
    },
    {
      "text": "And the other is that\nprobably sends a strong signal",
      "start": 5048.16,
      "duration": 2.64
    },
    {
      "text": "like good job.",
      "start": 5050.8,
      "duration": 1.983
    },
    {
      "text": "(all chuckling)",
      "start": 5052.783,
      "duration": 1.317
    },
    {
      "text": "So, there's this much stronger signal",
      "start": 5054.1,
      "duration": 1.47
    },
    {
      "text": "than just accepting the code, right?",
      "start": 5055.57,
      "duration": 1.44
    },
    {
      "text": "You just actually send a strong good job.",
      "start": 5057.01,
      "duration": 3.21
    },
    {
      "text": "That and for bug finding, obviously,",
      "start": 5060.22,
      "duration": 2.313
    },
    {
      "text": "there's a lot of people",
      "start": 5063.4,
      "duration": 1.45
    },
    {
      "text": "that would pay a huge amount of money",
      "start": 5066.28,
      "duration": 1.83
    },
    {
      "text": "for a bug bounty thing, right?",
      "start": 5068.11,
      "duration": 4.29
    },
    {
      "text": "You guys think about that?",
      "start": 5072.4,
      "duration": 1.35
    },
    {
      "text": "- Yeah, it's a controversial\nidea inside the company.",
      "start": 5073.75,
      "duration": 3.27
    },
    {
      "text": "I think it depends",
      "start": 5077.02,
      "duration": 1.2
    },
    {
      "text": "on how much you believe\nin humanity almost.",
      "start": 5078.22,
      "duration": 3.783
    },
    {
      "text": "I think it would be really cool",
      "start": 5083.92,
      "duration": 1.59
    },
    {
      "text": "if you spend nothing to try to find a bug.",
      "start": 5085.51,
      "duration": 3.63
    },
    {
      "text": "And if it doesn't find\na bug, you spend $0.",
      "start": 5089.14,
      "duration": 2.46
    },
    {
      "text": "And then if it does find a\nbug and you click accept,",
      "start": 5091.6,
      "duration": 3.0
    },
    {
      "text": "then it also shows in parentheses like $1.",
      "start": 5094.6,
      "duration": 2.623
    },
    {
      "text": "And so, you spend $1 to accept the bug.",
      "start": 5098.295,
      "duration": 2.185
    },
    {
      "text": "And then, of course, there's a worry like,",
      "start": 5100.48,
      "duration": 1.627
    },
    {
      "text": "\"Okay, we spent a lot of computation,",
      "start": 5102.107,
      "duration": 1.673
    },
    {
      "text": "maybe people will just copy paste.\"",
      "start": 5103.78,
      "duration": 1.797
    },
    {
      "text": "I think that's a worry.",
      "start": 5106.66,
      "duration": 1.15
    },
    {
      "text": "Then there is also the\nworry that introducing money",
      "start": 5108.79,
      "duration": 2.19
    },
    {
      "text": "into the product.",
      "start": 5110.98,
      "duration": 1.533
    },
    {
      "text": "It doesn't feel as fun anymore.",
      "start": 5114.88,
      "duration": 1.5
    },
    {
      "text": "You have to think about money.",
      "start": 5116.38,
      "duration": 1.74
    },
    {
      "text": "And all you want to\nthink about is the code,",
      "start": 5118.12,
      "duration": 2.807
    },
    {
      "text": "and so maybe it actually makes more sense",
      "start": 5120.927,
      "duration": 1.813
    },
    {
      "text": "to separate it out, and you\npay some fee every month,",
      "start": 5122.74,
      "duration": 4.05
    },
    {
      "text": "and then you get all of\nthese things for free.",
      "start": 5126.79,
      "duration": 2.58
    },
    {
      "text": "- But there could be a tipping component",
      "start": 5129.37,
      "duration": 1.44
    },
    {
      "text": "which is not like it cost this-",
      "start": 5130.81,
      "duration": 2.261
    },
    {
      "text": "- Yes, but it still\nhas that dollar symbol.",
      "start": 5133.071,
      "duration": 1.549
    },
    {
      "text": "I think it's fine, but\nI also see the point",
      "start": 5134.62,
      "duration": 2.16
    },
    {
      "text": "where maybe you don't\nwant to introduce it.",
      "start": 5136.78,
      "duration": 3.42
    },
    {
      "text": "- Yeah, I was gonna say the moment",
      "start": 5140.2,
      "duration": 0.833
    },
    {
      "text": "that feels like people do\nthis is when they share it.",
      "start": 5141.033,
      "duration": 2.257
    },
    {
      "text": "When they have this fantastic example,",
      "start": 5143.29,
      "duration": 1.86
    },
    {
      "text": "they just share it with their friends.",
      "start": 5145.15,
      "duration": 1.74
    },
    {
      "text": "- There is also a potential world",
      "start": 5146.89,
      "duration": 1.2
    },
    {
      "text": "where there's a technical solution to this",
      "start": 5148.09,
      "duration": 1.8
    },
    {
      "text": "like honor system problem too,",
      "start": 5149.89,
      "duration": 1.71
    },
    {
      "text": "where if we can get to a place",
      "start": 5151.6,
      "duration": 1.23
    },
    {
      "text": "where we understand the\noutput of the system more,",
      "start": 5152.83,
      "duration": 3.15
    },
    {
      "text": "I mean, to the stuff we were talking about",
      "start": 5155.98,
      "duration": 1.23
    },
    {
      "text": "with error checking with the LSP",
      "start": 5157.21,
      "duration": 2.19
    },
    {
      "text": "and then also running the code.",
      "start": 5159.4,
      "duration": 1.32
    },
    {
      "text": "But if you could get to a place",
      "start": 5160.72,
      "duration": 0.87
    },
    {
      "text": "where you could actually somehow verify,",
      "start": 5161.59,
      "duration": 1.927
    },
    {
      "text": "\"Oh, I have fixed the bug,\"\nmaybe then the bounty system",
      "start": 5163.517,
      "duration": 3.713
    },
    {
      "text": "doesn't need to rely on\nthe honor system too.",
      "start": 5167.23,
      "duration": 1.92
    },
    {
      "text": "- How much interaction is there",
      "start": 5169.15,
      "duration": 1.29
    },
    {
      "text": "between the terminal and the code?",
      "start": 5170.44,
      "duration": 2.1
    },
    {
      "text": "How much information is gained from",
      "start": 5172.54,
      "duration": 2.1
    },
    {
      "text": "if you run the code in the terminal?",
      "start": 5174.64,
      "duration": 1.8
    },
    {
      "text": "Can you do a loop where it runs the code",
      "start": 5178.628,
      "duration": 4.742
    },
    {
      "text": "and suggests how to change the code?",
      "start": 5183.37,
      "duration": 1.56
    },
    {
      "text": "If the code and runtime gets an error?",
      "start": 5184.93,
      "duration": 2.88
    },
    {
      "text": "Is right now there's\nseparate worlds completely?",
      "start": 5187.81,
      "duration": 2.97
    },
    {
      "text": "I know you can do control\nK inside the terminal",
      "start": 5190.78,
      "duration": 3.36
    },
    {
      "text": "to help you write the code.",
      "start": 5194.14,
      "duration": 0.96
    },
    {
      "text": "- You can use terminal context as well",
      "start": 5195.1,
      "duration": 3.03
    },
    {
      "text": "inside of check Command+K\nkind of everything.",
      "start": 5198.13,
      "duration": 2.883
    },
    {
      "text": "We don't have the looping part yet,",
      "start": 5201.85,
      "duration": 2.82
    },
    {
      "text": "so we suspect something like\nthis could make a lot of sense.",
      "start": 5204.67,
      "duration": 2.73
    },
    {
      "text": "There's a question of whether it happens",
      "start": 5207.4,
      "duration": 1.23
    },
    {
      "text": "in the foreground too or if\nit happens in the background",
      "start": 5208.63,
      "duration": 2.82
    },
    {
      "text": "like what we've been discussing.",
      "start": 5211.45,
      "duration": 1.29
    },
    {
      "text": "- Sure, the background's pretty cool.",
      "start": 5212.74,
      "duration": 1.5
    },
    {
      "text": "I could be running the\ncode in different ways.",
      "start": 5214.24,
      "duration": 2.46
    },
    {
      "text": "Plus there's a database side to this,",
      "start": 5216.7,
      "duration": 1.62
    },
    {
      "text": "which how do you protect it\nfrom not modifying the database,",
      "start": 5218.32,
      "duration": 2.85
    },
    {
      "text": "but okay.",
      "start": 5221.17,
      "duration": 1.053
    },
    {
      "text": "(group chuckling)",
      "start": 5222.223,
      "duration": 1.047
    },
    {
      "text": "- I mean, there's certainly\ncool solutions there.",
      "start": 5223.27,
      "duration": 2.82
    },
    {
      "text": "There's this new API\nthat is being developed.",
      "start": 5226.09,
      "duration": 3.063
    },
    {
      "text": "It's not in AWS, but\nit certainly, I think,",
      "start": 5230.41,
      "duration": 5.0
    },
    {
      "text": "it's in PlanetScale.",
      "start": 5235.57,
      "duration": 0.96
    },
    {
      "text": "I don't know if PlanetScale was\nthe first one to you add it.",
      "start": 5236.53,
      "duration": 2.58
    },
    {
      "text": "It's this ability sort of\nadd branches to a database,",
      "start": 5239.11,
      "duration": 3.63
    },
    {
      "text": "which is like if you're\nworking on a feature",
      "start": 5242.74,
      "duration": 2.82
    },
    {
      "text": "and you wanna test against\nthe broad database,",
      "start": 5245.56,
      "duration": 2.13
    },
    {
      "text": "but you don't actually want to test",
      "start": 5247.69,
      "duration": 1.26
    },
    {
      "text": "against the broad database,",
      "start": 5248.95,
      "duration": 0.99
    },
    {
      "text": "you could add a branch to the database.",
      "start": 5249.94,
      "duration": 2.01
    },
    {
      "text": "And the way they do that\nis they add a branch",
      "start": 5251.95,
      "duration": 1.5
    },
    {
      "text": "to the write-ahead log.",
      "start": 5253.45,
      "duration": 1.77
    },
    {
      "text": "And there's obviously a\nlot of technical complexity",
      "start": 5255.22,
      "duration": 2.22
    },
    {
      "text": "in doing it correctly.",
      "start": 5257.44,
      "duration": 1.11
    },
    {
      "text": "I guess database companies\nneed new things to do.",
      "start": 5258.55,
      "duration": 3.511
    },
    {
      "text": "(group chuckling)",
      "start": 5262.061,
      "duration": 3.115
    },
    {
      "text": "They have good databases now.",
      "start": 5265.176,
      "duration": 1.45
    },
    {
      "text": "And I think turbopuffer,",
      "start": 5267.79,
      "duration": 2.37
    },
    {
      "text": "which is one of the databases we use,",
      "start": 5270.16,
      "duration": 2.19
    },
    {
      "text": "is going to add maybe branching\nto the write-ahead log.",
      "start": 5272.35,
      "duration": 4.893
    },
    {
      "text": "So maybe the AI agents will use branching,",
      "start": 5279.01,
      "duration": 4.47
    },
    {
      "text": "they'll test against some branch,",
      "start": 5283.48,
      "duration": 1.92
    },
    {
      "text": "and it's gonna be a\nrequirement for the database",
      "start": 5285.4,
      "duration": 3.36
    },
    {
      "text": "to support branching or something.",
      "start": 5288.76,
      "duration": 2.058
    },
    {
      "text": "- It would be really interesting",
      "start": 5290.818,
      "duration": 0.833
    },
    {
      "text": "if you could branch a file system, right?",
      "start": 5291.651,
      "duration": 2.029
    },
    {
      "text": "- Yeah.",
      "start": 5293.68,
      "duration": 1.033
    },
    {
      "text": "I feel like everything needs branching.",
      "start": 5294.713,
      "duration": 1.457
    },
    {
      "text": "- [Aman] Yeah.",
      "start": 5296.17,
      "duration": 0.87
    },
    {
      "text": "- Yeah.",
      "start": 5297.04,
      "duration": 1.11
    },
    {
      "text": "The problem with the multiverse, right?",
      "start": 5298.15,
      "duration": 1.686
    },
    {
      "text": "(group chuckling)",
      "start": 5299.836,
      "duration": 2.738
    },
    {
      "text": "If you branch on everything\nthat's like a lot.",
      "start": 5302.574,
      "duration": 2.206
    },
    {
      "text": "- There's obviously these\nsuper clever algorithms",
      "start": 5304.78,
      "duration": 2.01
    },
    {
      "text": "to make sure that you don't\nactually use a lot of space",
      "start": 5306.79,
      "duration": 3.33
    },
    {
      "text": "or CPU or whatever.",
      "start": 5310.12,
      "duration": 1.623
    },
    {
      "text": "- Okay, this is a good place\nto ask about infrastructure.",
      "start": 5311.743,
      "duration": 3.177
    },
    {
      "text": "So, you guys mostly use AWS,",
      "start": 5314.92,
      "duration": 1.98
    },
    {
      "text": "what are some interesting details?",
      "start": 5316.9,
      "duration": 1.38
    },
    {
      "text": "What are some interesting challenges?",
      "start": 5318.28,
      "duration": 1.41
    },
    {
      "text": "Why'd you choose AWS?",
      "start": 5319.69,
      "duration": 1.935
    },
    {
      "text": "Why is AWS still winning?",
      "start": 5321.625,
      "duration": 2.475
    },
    {
      "text": "Hashtag.",
      "start": 5324.1,
      "duration": 1.32
    },
    {
      "text": "- AWS is just really, really good.",
      "start": 5325.42,
      "duration": 2.65
    },
    {
      "text": "It is really good.",
      "start": 5328.07,
      "duration": 1.013
    },
    {
      "text": "Whenever you use an AWS product,",
      "start": 5331.18,
      "duration": 3.81
    },
    {
      "text": "you just know that it's going to work.",
      "start": 5334.99,
      "duration": 2.1
    },
    {
      "text": "It might be absolute hell\nto go through the steps",
      "start": 5337.09,
      "duration": 3.42
    },
    {
      "text": "to set it up.",
      "start": 5340.51,
      "duration": 1.62
    },
    {
      "text": "- Why is the interface so horrible?",
      "start": 5342.13,
      "duration": 2.1
    },
    {
      "text": "- Because it's. (chuckles)",
      "start": 5344.23,
      "duration": 1.023
    },
    {
      "text": "- It's just so good.",
      "start": 5345.253,
      "duration": 0.957
    },
    {
      "text": "It doesn't need to-\n- It's the nature of winning.",
      "start": 5346.21,
      "duration": 1.897
    },
    {
      "text": "(group chuckling)",
      "start": 5348.107,
      "duration": 1.066
    },
    {
      "text": "- I think it's exactly, it's\njust nature they're winning.",
      "start": 5349.173,
      "duration": 2.077
    },
    {
      "text": "- Yeah, yeah.",
      "start": 5351.25,
      "duration": 1.23
    },
    {
      "text": "But AWS we can always\ntrust, it will always work.",
      "start": 5352.48,
      "duration": 2.73
    },
    {
      "text": "And if there is a problem,\nit's probably your problem.",
      "start": 5355.21,
      "duration": 4.096
    },
    {
      "text": "(Lex chuckles)\nYeah.",
      "start": 5359.306,
      "duration": 1.634
    },
    {
      "text": "- Okay, is there some\ninteresting challenges,",
      "start": 5360.94,
      "duration": 2.7
    },
    {
      "text": "you guys are pretty\nnew startup to scaling,",
      "start": 5363.64,
      "duration": 3.273
    },
    {
      "text": "to so many people.",
      "start": 5367.93,
      "duration": 1.38
    },
    {
      "text": "- Yeah, I think that it has\nbeen an interesting journey",
      "start": 5369.31,
      "duration": 5.0
    },
    {
      "text": "adding each extra zero to\nthe request per second.",
      "start": 5374.41,
      "duration": 3.14
    },
    {
      "text": "(Lex chuckles)",
      "start": 5377.55,
      "duration": 0.833
    },
    {
      "text": "You run into all of these\nwith the general components",
      "start": 5378.383,
      "duration": 1.787
    },
    {
      "text": "you're using for caching and databases,",
      "start": 5380.17,
      "duration": 2.01
    },
    {
      "text": "run into issues as you make\nthings bigger and bigger,",
      "start": 5382.18,
      "duration": 2.07
    },
    {
      "text": "and now we're at the scale\nwhere we get into overflows",
      "start": 5384.25,
      "duration": 2.19
    },
    {
      "text": "on our tables and things like that.",
      "start": 5386.44,
      "duration": 2.28
    },
    {
      "text": "And then, also there have\nbeen some custom systems",
      "start": 5388.72,
      "duration": 3.12
    },
    {
      "text": "that we've built.",
      "start": 5391.84,
      "duration": 0.9
    },
    {
      "text": "For instance, our retrieval\nsystem for computing,",
      "start": 5392.74,
      "duration": 3.54
    },
    {
      "text": "a semantic index of your code\nbase and answering questions",
      "start": 5396.28,
      "duration": 2.64
    },
    {
      "text": "about a code base that have continually,",
      "start": 5398.92,
      "duration": 2.43
    },
    {
      "text": "I feel like, been one of the\ntrickier things to scale.",
      "start": 5401.35,
      "duration": 3.06
    },
    {
      "text": "- I have a few friends who\nare super senior engineers",
      "start": 5404.41,
      "duration": 3.15
    },
    {
      "text": "and one of their lines is,\nit's very hard to predict",
      "start": 5407.56,
      "duration": 2.88
    },
    {
      "text": "where systems will break\nwhen you scale them.",
      "start": 5410.44,
      "duration": 2.94
    },
    {
      "text": "You can try to predict in advance,",
      "start": 5413.38,
      "duration": 3.66
    },
    {
      "text": "but there's always something\nweird that's gonna happen",
      "start": 5417.04,
      "duration": 2.76
    },
    {
      "text": "when you add these extras here.",
      "start": 5419.8,
      "duration": 2.25
    },
    {
      "text": "You thought through everything,",
      "start": 5422.05,
      "duration": 1.71
    },
    {
      "text": "which you didn't actually\nthink through everything.",
      "start": 5423.76,
      "duration": 2.61
    },
    {
      "text": "But I think for that particular system,",
      "start": 5426.37,
      "duration": 3.033
    },
    {
      "text": "we chunk up all of your code,",
      "start": 5439.431,
      "duration": 1.729
    },
    {
      "text": "and then we send up the code for embedding",
      "start": 5441.16,
      "duration": 3.63
    },
    {
      "text": "and we embed the code.",
      "start": 5444.79,
      "duration": 1.227
    },
    {
      "text": "And then, we store the\nembeddings in a database,",
      "start": 5446.017,
      "duration": 3.303
    },
    {
      "text": "but we don't actually\nstore any of the code.",
      "start": 5449.32,
      "duration": 2.55
    },
    {
      "text": "And then there's reasons\naround making sure",
      "start": 5451.87,
      "duration": 1.77
    },
    {
      "text": "that we don't introduce client bugs",
      "start": 5453.64,
      "duration": 2.73
    },
    {
      "text": "because we're very, very\nparanoid about client bugs.",
      "start": 5456.37,
      "duration": 2.76
    },
    {
      "text": "We store much of the\ndetails on the server.",
      "start": 5459.13,
      "duration": 4.443
    },
    {
      "text": "Everything is encrypted.",
      "start": 5465.31,
      "duration": 1.443
    },
    {
      "text": "So, one of the technical\nchallenges is always making sure",
      "start": 5468.13,
      "duration": 2.97
    },
    {
      "text": "that the local index, the local\ncode base state is the same",
      "start": 5471.1,
      "duration": 4.37
    },
    {
      "text": "as the state that is on the server.",
      "start": 5475.47,
      "duration": 1.75
    },
    {
      "text": "The way, technically, we\nended up doing that is,",
      "start": 5478.57,
      "duration": 3.69
    },
    {
      "text": "for every single file\nyou can keep this hash,",
      "start": 5482.26,
      "duration": 3.57
    },
    {
      "text": "and then for every folder\nyou can keep a hash,",
      "start": 5485.83,
      "duration": 2.85
    },
    {
      "text": "which is the hash of all of its children.",
      "start": 5488.68,
      "duration": 2.58
    },
    {
      "text": "You can recursively do that until the top.",
      "start": 5491.26,
      "duration": 3.03
    },
    {
      "text": "Why do something complicated?",
      "start": 5494.29,
      "duration": 3.39
    },
    {
      "text": "One thing you could do",
      "start": 5497.68,
      "duration": 0.9
    },
    {
      "text": "is you could keep a hash for every file,",
      "start": 5498.58,
      "duration": 2.4
    },
    {
      "text": "and every minute, you could\ntry to download the hashes",
      "start": 5500.98,
      "duration": 3.0
    },
    {
      "text": "that are on the server,\nfigure out what are the files",
      "start": 5503.98,
      "duration": 2.28
    },
    {
      "text": "that don't exist on the server.",
      "start": 5506.26,
      "duration": 1.14
    },
    {
      "text": "Maybe you just created a new file,",
      "start": 5507.4,
      "duration": 1.53
    },
    {
      "text": "maybe you just deleted a file,",
      "start": 5508.93,
      "duration": 1.44
    },
    {
      "text": "maybe you checked out a new branch,",
      "start": 5510.37,
      "duration": 2.07
    },
    {
      "text": "and try to reconcile the state",
      "start": 5512.44,
      "duration": 2.1
    },
    {
      "text": "between the client and the server.",
      "start": 5514.54,
      "duration": 1.7
    },
    {
      "text": "But that introduces absolutely\nginormous network overhead",
      "start": 5517.18,
      "duration": 3.9
    },
    {
      "text": "both on the client side.",
      "start": 5521.08,
      "duration": 2.97
    },
    {
      "text": "Nobody really wants us to\nhammer their WiFi all the time",
      "start": 5524.05,
      "duration": 3.45
    },
    {
      "text": "if you're using Cursor.",
      "start": 5527.5,
      "duration": 1.89
    },
    {
      "text": "But also, it would\nintroduce ginormous overhead",
      "start": 5529.39,
      "duration": 3.33
    },
    {
      "text": "on the database.",
      "start": 5532.72,
      "duration": 0.99
    },
    {
      "text": "It would be reading these\ntens of terabytes database,",
      "start": 5533.71,
      "duration": 5.0
    },
    {
      "text": "approaching 20 terabytes",
      "start": 5540.85,
      "duration": 2.37
    },
    {
      "text": "or something data base every second.",
      "start": 5543.22,
      "duration": 2.46
    },
    {
      "text": "That's just crazy.",
      "start": 5545.68,
      "duration": 2.52
    },
    {
      "text": "You definitely don't wanna do that.",
      "start": 5548.2,
      "duration": 2.34
    },
    {
      "text": "So what you do, you just try\nto reconcile the single hash,",
      "start": 5550.54,
      "duration": 3.78
    },
    {
      "text": "which is at the root of the project.",
      "start": 5554.32,
      "duration": 1.44
    },
    {
      "text": "And then if something\nmismatches, then you go,",
      "start": 5555.76,
      "duration": 2.07
    },
    {
      "text": "you find where all the things disagree.",
      "start": 5557.83,
      "duration": 1.8
    },
    {
      "text": "Maybe you look at the children\nand see if the hashes match.",
      "start": 5559.63,
      "duration": 2.4
    },
    {
      "text": "If the hashes don't match,",
      "start": 5562.03,
      "duration": 1.02
    },
    {
      "text": "go look at their children and so on.",
      "start": 5563.05,
      "duration": 1.407
    },
    {
      "text": "But you only do that in the scenario",
      "start": 5564.457,
      "duration": 1.863
    },
    {
      "text": "where things don't match.",
      "start": 5566.32,
      "duration": 1.05
    },
    {
      "text": "For most people, most of\nthe time, the hashes match.",
      "start": 5567.37,
      "duration": 2.67
    },
    {
      "text": "- So it's like a\nhierarchical reconciliation",
      "start": 5570.04,
      "duration": 3.75
    },
    {
      "text": "of hashes.\n- Yeah, something like that.",
      "start": 5573.79,
      "duration": 1.05
    },
    {
      "text": "- Yeah, it's called a Merkle tree.",
      "start": 5574.84,
      "duration": 1.56
    },
    {
      "text": "- Yeah, Merkle.\n- Yeah.",
      "start": 5576.4,
      "duration": 1.158
    },
    {
      "text": "- Yeah.",
      "start": 5577.558,
      "duration": 1.542
    },
    {
      "text": "This is cool to see",
      "start": 5579.1,
      "duration": 0.84
    },
    {
      "text": "that you have to think\nthrough all these problems.",
      "start": 5579.94,
      "duration": 2.593
    },
    {
      "text": "- The reason it's gotten hard",
      "start": 5582.533,
      "duration": 1.867
    },
    {
      "text": "is just because the\nnumber of people using it",
      "start": 5584.4,
      "duration": 2.71
    },
    {
      "text": "and some of your customers",
      "start": 5587.11,
      "duration": 2.16
    },
    {
      "text": "have really, really large code bases.",
      "start": 5589.27,
      "duration": 3.243
    },
    {
      "text": "We originally reordered dark\ncode base, which is big,",
      "start": 5596.08,
      "duration": 2.58
    },
    {
      "text": "but it's just not the size of some company",
      "start": 5598.66,
      "duration": 2.7
    },
    {
      "text": "that's been there for 20 years",
      "start": 5601.36,
      "duration": 1.26
    },
    {
      "text": "and has a ginormous number of files",
      "start": 5602.62,
      "duration": 2.46
    },
    {
      "text": "and you wanna scale\nthat across programmers.",
      "start": 5605.08,
      "duration": 3.12
    },
    {
      "text": "There's all these details",
      "start": 5608.2,
      "duration": 0.99
    },
    {
      "text": "where building the simple thing is easy,",
      "start": 5609.19,
      "duration": 2.16
    },
    {
      "text": "but scaling it to a lot of people,",
      "start": 5611.35,
      "duration": 2.28
    },
    {
      "text": "a lot of companies is\nobviously a difficult problem,",
      "start": 5613.63,
      "duration": 2.61
    },
    {
      "text": "which is independent of, actually,",
      "start": 5616.24,
      "duration": 2.1
    },
    {
      "text": "so that there's part of this scaling.",
      "start": 5618.34,
      "duration": 1.38
    },
    {
      "text": "Our current solution is also\ncoming up with new ideas",
      "start": 5619.72,
      "duration": 3.27
    },
    {
      "text": "that, obviously, we're working on,",
      "start": 5622.99,
      "duration": 2.52
    },
    {
      "text": "but then scaling all of that\nin the last few weeks, months.",
      "start": 5625.51,
      "duration": 2.91
    },
    {
      "text": "- Yeah.",
      "start": 5628.42,
      "duration": 0.833
    },
    {
      "text": "And there are a lot of clever things,",
      "start": 5629.253,
      "duration": 1.597
    },
    {
      "text": "additional things that go\ninto this indexing system.",
      "start": 5630.85,
      "duration": 2.793
    },
    {
      "text": "For example, the bottleneck\nin terms of costs",
      "start": 5634.54,
      "duration": 2.61
    },
    {
      "text": "is not soaring things\nin the vector database",
      "start": 5637.15,
      "duration": 1.83
    },
    {
      "text": "or the database, it's\nactually embedding the code.",
      "start": 5638.98,
      "duration": 2.22
    },
    {
      "text": "You don't wanna re-embed the code base",
      "start": 5641.2,
      "duration": 1.47
    },
    {
      "text": "for every single person in a company",
      "start": 5642.67,
      "duration": 2.07
    },
    {
      "text": "that is using the same exact code",
      "start": 5644.74,
      "duration": 2.67
    },
    {
      "text": "except for maybe they're\na different branch",
      "start": 5647.41,
      "duration": 1.56
    },
    {
      "text": "with a few different files",
      "start": 5648.97,
      "duration": 0.93
    },
    {
      "text": "or they've made a few local changes.",
      "start": 5649.9,
      "duration": 2.043
    },
    {
      "text": "Because again, embeddings\nare the bottleneck,",
      "start": 5653.02,
      "duration": 1.62
    },
    {
      "text": "you can do this one clever trick",
      "start": 5654.64,
      "duration": 1.47
    },
    {
      "text": "and not have to worry about the complexity",
      "start": 5656.11,
      "duration": 2.22
    },
    {
      "text": "of dealing with branches\nand the other databases",
      "start": 5658.33,
      "duration": 2.28
    },
    {
      "text": "where you just have some cash\non the actual vectors computed",
      "start": 5660.61,
      "duration": 5.0
    },
    {
      "text": "from the hash of a given chunk.",
      "start": 5667.63,
      "duration": 1.92
    },
    {
      "text": "- Mm-hmm.",
      "start": 5669.55,
      "duration": 1.59
    },
    {
      "text": "- So this means that when the\nnth person at a company goes",
      "start": 5671.14,
      "duration": 2.82
    },
    {
      "text": "and embed their code base,\nit's really, really fast.",
      "start": 5673.96,
      "duration": 2.94
    },
    {
      "text": "You do all this without\nactually storing any code",
      "start": 5676.9,
      "duration": 2.37
    },
    {
      "text": "on our servers at all.",
      "start": 5679.27,
      "duration": 0.9
    },
    {
      "text": "No code data is stored.",
      "start": 5680.17,
      "duration": 1.53
    },
    {
      "text": "We just store the vectors\nin the vector database",
      "start": 5681.7,
      "duration": 1.77
    },
    {
      "text": "and the vector cache.",
      "start": 5683.47,
      "duration": 2.04
    },
    {
      "text": "- What's the biggest\ngains at this time you get",
      "start": 5685.51,
      "duration": 4.41
    },
    {
      "text": "from indexing the code base?",
      "start": 5689.92,
      "duration": 1.83
    },
    {
      "text": "Just out of curiosity,\nwhat benefit do users have?",
      "start": 5691.75,
      "duration": 4.29
    },
    {
      "text": "It seems like longer term,",
      "start": 5696.04,
      "duration": 1.41
    },
    {
      "text": "there'll be more and more\nbenefit, but in the short term,",
      "start": 5697.45,
      "duration": 2.22
    },
    {
      "text": "just asking questions of the code base,",
      "start": 5699.67,
      "duration": 2.073
    },
    {
      "text": "what's the usefulness of that?",
      "start": 5702.94,
      "duration": 3.15
    },
    {
      "text": "- I think the most obvious one\nis just, you want to find out",
      "start": 5706.09,
      "duration": 5.0
    },
    {
      "text": "where something is happening\nin your large code base,",
      "start": 5711.76,
      "duration": 2.91
    },
    {
      "text": "and you have a fuzzy memory of,",
      "start": 5714.67,
      "duration": 2.257
    },
    {
      "text": "\"Okay, I want to find\nthe place where we do X,\"",
      "start": 5716.927,
      "duration": 3.263
    },
    {
      "text": "but you don't exactly\nknow what to search for",
      "start": 5720.19,
      "duration": 2.1
    },
    {
      "text": "in a normal text search.",
      "start": 5722.29,
      "duration": 1.32
    },
    {
      "text": "So you ask a chat, you hit Command+Enter",
      "start": 5723.61,
      "duration": 2.52
    },
    {
      "text": "to ask with the code base chat.",
      "start": 5726.13,
      "duration": 1.83
    },
    {
      "text": "And then very often, it\nfinds the right place",
      "start": 5727.96,
      "duration": 3.21
    },
    {
      "text": "that you were thinking of.",
      "start": 5731.17,
      "duration": 1.3
    },
    {
      "text": "- Like you mentioned, in the future,",
      "start": 5733.78,
      "duration": 1.47
    },
    {
      "text": "I think there's only going to\nget more and more powerful,",
      "start": 5735.25,
      "duration": 3.12
    },
    {
      "text": "where we're working a lot\non improving the quality",
      "start": 5738.37,
      "duration": 2.58
    },
    {
      "text": "of our retrieval.",
      "start": 5740.95,
      "duration": 1.59
    },
    {
      "text": "I think the ceiling for that\nis really, really much higher",
      "start": 5742.54,
      "duration": 1.83
    },
    {
      "text": "than people give the credit for.",
      "start": 5744.37,
      "duration": 1.65
    },
    {
      "text": "- One question that's good to\nask here, have you considered",
      "start": 5746.02,
      "duration": 2.82
    },
    {
      "text": "and why haven't you much done local stuff,",
      "start": 5748.84,
      "duration": 3.153
    },
    {
      "text": "it seems like everything\nwas just discussed",
      "start": 5754.0,
      "duration": 1.74
    },
    {
      "text": "as exceptionally difficult to do.",
      "start": 5755.74,
      "duration": 2.07
    },
    {
      "text": "To go to the cloud, you have\nto think about all these things",
      "start": 5757.81,
      "duration": 2.28
    },
    {
      "text": "with the caching and the large code base",
      "start": 5760.09,
      "duration": 5.0
    },
    {
      "text": "where a large number of\nprogrammers are using",
      "start": 5765.28,
      "duration": 1.59
    },
    {
      "text": "the same code base.",
      "start": 5766.87,
      "duration": 0.833
    },
    {
      "text": "You have to figure out the puzzle of that.",
      "start": 5767.703,
      "duration": 1.747
    },
    {
      "text": "A lot of it, most software",
      "start": 5769.45,
      "duration": 2.94
    },
    {
      "text": "just does this heavy\ncomputational stuff locally.",
      "start": 5772.39,
      "duration": 3.807
    },
    {
      "text": "So, have you considered\ndoing embeddings locally?",
      "start": 5776.197,
      "duration": 2.673
    },
    {
      "text": "- Yeah, we thought about it,",
      "start": 5778.87,
      "duration": 1.05
    },
    {
      "text": "and I think it would be\ncool to do it locally.",
      "start": 5779.92,
      "duration": 2.79
    },
    {
      "text": "I think it's just really hard.",
      "start": 5782.71,
      "duration": 2.37
    },
    {
      "text": "One thing to keep in mind",
      "start": 5785.08,
      "duration": 1.02
    },
    {
      "text": "is that some of our users\nuse the latest MacBook Pro,",
      "start": 5786.1,
      "duration": 4.773
    },
    {
      "text": "but most of our users,\nmore than 80% of our users",
      "start": 5792.04,
      "duration": 2.64
    },
    {
      "text": "are in Windows machines,",
      "start": 5794.68,
      "duration": 1.59
    },
    {
      "text": "which many of them are not very powerful.",
      "start": 5796.27,
      "duration": 4.02
    },
    {
      "text": "So, local models really only\nworks on the latest computers,",
      "start": 5800.29,
      "duration": 3.99
    },
    {
      "text": "and it's also a big\noverhead to build that in.",
      "start": 5804.28,
      "duration": 4.23
    },
    {
      "text": "So even if we would like to do that,",
      "start": 5808.51,
      "duration": 1.923
    },
    {
      "text": "it's currently not something\nthat we are able to focus on.",
      "start": 5811.72,
      "duration": 2.82
    },
    {
      "text": "I think there are some\npeople that do that,",
      "start": 5814.54,
      "duration": 3.06
    },
    {
      "text": "and I think that's great,",
      "start": 5817.6,
      "duration": 1.263
    },
    {
      "text": "but especially as models\nget bigger and bigger",
      "start": 5819.97,
      "duration": 2.64
    },
    {
      "text": "and you want to do fancier\nthings with bigger models,",
      "start": 5822.61,
      "duration": 3.27
    },
    {
      "text": "it becomes even harder to do it locally.",
      "start": 5825.88,
      "duration": 1.98
    },
    {
      "text": "- Yeah, it's not a problem\nof weaker computers.",
      "start": 5827.86,
      "duration": 3.72
    },
    {
      "text": "It's just that for example,\nif you're some big company,",
      "start": 5831.58,
      "duration": 3.423
    },
    {
      "text": "you have big company code base.",
      "start": 5835.967,
      "duration": 1.643
    },
    {
      "text": "It's just really hard to\nprocess big company code base",
      "start": 5837.61,
      "duration": 2.61
    },
    {
      "text": "even on the beefiest MacBook Pros.",
      "start": 5840.22,
      "duration": 2.58
    },
    {
      "text": "It's not even a matter of\nif you're just a student",
      "start": 5842.8,
      "duration": 4.47
    },
    {
      "text": "or something.",
      "start": 5847.27,
      "duration": 0.833
    },
    {
      "text": "I think, if you're the best\nprogrammer at a big company,",
      "start": 5848.103,
      "duration": 3.637
    },
    {
      "text": "you're still gonna have\na horrible experience.",
      "start": 5851.74,
      "duration": 2.64
    },
    {
      "text": "If you do everything locally\nwhere you could do it",
      "start": 5854.38,
      "duration": 3.03
    },
    {
      "text": "and scrape by, but again,\nit wouldn't be fun anymore.",
      "start": 5857.41,
      "duration": 3.39
    },
    {
      "text": "- Yeah, like at approximate\nnearest neighbors",
      "start": 5860.8,
      "duration": 1.59
    },
    {
      "text": "and this massive code base is\ngonna just eat up your memory",
      "start": 5862.39,
      "duration": 2.58
    },
    {
      "text": "and your CPU, and it's based off of that.",
      "start": 5864.97,
      "duration": 2.073
    },
    {
      "text": "That's just that.",
      "start": 5869.306,
      "duration": 1.064
    },
    {
      "text": "Let's talk about also\nthe modeling side where,",
      "start": 5870.37,
      "duration": 2.97
    },
    {
      "text": "as Arvid said, there are\nthese massive headwinds",
      "start": 5873.34,
      "duration": 2.43
    },
    {
      "text": "against local models where one,",
      "start": 5875.77,
      "duration": 3.81
    },
    {
      "text": "things that seem to move\ntowards MOEs, which one benefit",
      "start": 5879.58,
      "duration": 3.33
    },
    {
      "text": "is maybe their more\nmemory bandwidth bound,",
      "start": 5882.91,
      "duration": 2.43
    },
    {
      "text": "which plays in favor of\nlocal versus using GPUs",
      "start": 5885.34,
      "duration": 4.24
    },
    {
      "text": "or using Nvidia GPUs.",
      "start": 5890.62,
      "duration": 1.71
    },
    {
      "text": "But the downside is, these\nmodels are just bigger in total,",
      "start": 5892.33,
      "duration": 4.17
    },
    {
      "text": "and they're gonna need to fit,",
      "start": 5896.5,
      "duration": 2.13
    },
    {
      "text": "often not even on a single\nnode but multiple nodes.",
      "start": 5898.63,
      "duration": 2.55
    },
    {
      "text": "There's no way that's gonna fit inside",
      "start": 5902.14,
      "duration": 2.1
    },
    {
      "text": "of even really good MacBooks.",
      "start": 5904.24,
      "duration": 2.58
    },
    {
      "text": "I think especially for coding,",
      "start": 5906.82,
      "duration": 2.04
    },
    {
      "text": "it's not a question as much of,",
      "start": 5908.86,
      "duration": 2.61
    },
    {
      "text": "does it clear some bar of\nthe model's good enough",
      "start": 5911.47,
      "duration": 3.39
    },
    {
      "text": "to do these things and\nthen we're satisfied?",
      "start": 5914.86,
      "duration": 2.46
    },
    {
      "text": "Which may be the case for other problems",
      "start": 5917.32,
      "duration": 2.37
    },
    {
      "text": "and maybe where local models shine,",
      "start": 5919.69,
      "duration": 1.95
    },
    {
      "text": "but people are always gonna want the best,",
      "start": 5921.64,
      "duration": 1.83
    },
    {
      "text": "the most intelligent,\nthe most capable things,",
      "start": 5923.47,
      "duration": 2.73
    },
    {
      "text": "and that's gonna be\nreally, really hard to run",
      "start": 5926.2,
      "duration": 2.28
    },
    {
      "text": "for almost all people, locally.",
      "start": 5928.48,
      "duration": 2.82
    },
    {
      "text": "- Don't you want the most capable model?",
      "start": 5931.3,
      "duration": 2.583
    },
    {
      "text": "You want Sonnet too?",
      "start": 5934.75,
      "duration": 1.38
    },
    {
      "text": "- And also o1-",
      "start": 5936.13,
      "duration": 1.235
    },
    {
      "text": "(Lex chuckling)",
      "start": 5937.365,
      "duration": 0.833
    },
    {
      "text": "- I like how you're pitching me.",
      "start": 5938.198,
      "duration": 1.364
    },
    {
      "text": "(group chuckling)",
      "start": 5939.562,
      "duration": 0.922
    },
    {
      "text": "- O1 is another-",
      "start": 5940.484,
      "duration": 0.833
    },
    {
      "text": "- Would you be satisfied\nwith an inferior model?",
      "start": 5941.317,
      "duration": 1.893
    },
    {
      "text": "Listen, yes, I'm one of those,",
      "start": 5943.21,
      "duration": 2.31
    },
    {
      "text": "but there's some people that\nlike to do stuff locally,",
      "start": 5945.52,
      "duration": 2.7
    },
    {
      "text": "really, there's a whole\nobviously open source movement",
      "start": 5949.99,
      "duration": 2.37
    },
    {
      "text": "that resists.",
      "start": 5952.36,
      "duration": 1.74
    },
    {
      "text": "It's good that they exist actually",
      "start": 5954.1,
      "duration": 1.41
    },
    {
      "text": "because you wanna resist the power centers",
      "start": 5955.51,
      "duration": 3.39
    },
    {
      "text": "that are growing our-",
      "start": 5958.9,
      "duration": 1.2
    },
    {
      "text": "- There's actually an\nalternative to local models",
      "start": 5960.1,
      "duration": 2.85
    },
    {
      "text": "that I am particularly fond of.",
      "start": 5962.95,
      "duration": 2.22
    },
    {
      "text": "I think it's still very\nmuch in the research stage,",
      "start": 5965.17,
      "duration": 3.3
    },
    {
      "text": "but you could imagine to\ndo homomorphic encryption",
      "start": 5968.47,
      "duration": 4.08
    },
    {
      "text": "for language model inference.",
      "start": 5972.55,
      "duration": 1.8
    },
    {
      "text": "So you encrypt your input\non your local machine,",
      "start": 5974.35,
      "duration": 2.535
    },
    {
      "text": "then you send that up,",
      "start": 5976.885,
      "duration": 1.005
    },
    {
      "text": "and then the server can\nuse loss of computation.",
      "start": 5977.89,
      "duration": 5.0
    },
    {
      "text": "They can run models that\nyou cannot run locally",
      "start": 5983.02,
      "duration": 1.95
    },
    {
      "text": "on this encrypted data,",
      "start": 5984.97,
      "duration": 1.92
    },
    {
      "text": "but they cannot see what the data is,",
      "start": 5986.89,
      "duration": 1.59
    },
    {
      "text": "and then they send back the answer",
      "start": 5988.48,
      "duration": 1.23
    },
    {
      "text": "and you decrypt the answer and\nonly you can see the answer.",
      "start": 5989.71,
      "duration": 2.76
    },
    {
      "text": "So I think that's still very much research",
      "start": 5992.47,
      "duration": 3.36
    },
    {
      "text": "and all of it is about trying\nto make the overhead lower",
      "start": 5995.83,
      "duration": 4.86
    },
    {
      "text": "because right now, the\noverhead is really big,",
      "start": 6000.69,
      "duration": 2.07
    },
    {
      "text": "but if you can make that happen,",
      "start": 6002.76,
      "duration": 1.68
    },
    {
      "text": "I think that would be really, really cool,",
      "start": 6004.44,
      "duration": 2.76
    },
    {
      "text": "and I think it would be\nreally, really impactful",
      "start": 6007.2,
      "duration": 2.82
    },
    {
      "text": "because I think one thing that's\nactually worrisome is that,",
      "start": 6010.02,
      "duration": 2.64
    },
    {
      "text": "as these models get better and better,",
      "start": 6012.66,
      "duration": 2.13
    },
    {
      "text": "they're going to become more\nand more economically useful.",
      "start": 6014.79,
      "duration": 3.06
    },
    {
      "text": "And so, more and more of the\nworld's information and data",
      "start": 6017.85,
      "duration": 3.15
    },
    {
      "text": "will flow through one or\ntwo centralized actors.",
      "start": 6021.0,
      "duration": 5.0
    },
    {
      "text": "And then there are worries about,",
      "start": 6026.31,
      "duration": 3.15
    },
    {
      "text": "there can be traditional hacker attempts,",
      "start": 6029.46,
      "duration": 1.92
    },
    {
      "text": "but it also creates this scary part",
      "start": 6031.38,
      "duration": 3.66
    },
    {
      "text": "where if all of the world's\ninformation is flowing",
      "start": 6035.04,
      "duration": 3.15
    },
    {
      "text": "through one node in plaintext,",
      "start": 6038.19,
      "duration": 1.593
    },
    {
      "text": "you can have surveillance\nin very bad ways.",
      "start": 6040.8,
      "duration": 3.303
    },
    {
      "text": "Initially, will be good reasons.",
      "start": 6048.18,
      "duration": 1.83
    },
    {
      "text": "People will want to try to protect",
      "start": 6050.01,
      "duration": 2.46
    },
    {
      "text": "against bad actors using\nAI models in bad ways,",
      "start": 6052.47,
      "duration": 3.18
    },
    {
      "text": "and then you will add in\nsome surveillance code.",
      "start": 6055.65,
      "duration": 1.83
    },
    {
      "text": "And then, someone else will come in",
      "start": 6057.48,
      "duration": 1.62
    },
    {
      "text": "and you're on a slippery slope,",
      "start": 6059.1,
      "duration": 1.53
    },
    {
      "text": "and then you start doing bad things",
      "start": 6060.63,
      "duration": 3.84
    },
    {
      "text": "with a lot of the world's data.",
      "start": 6064.47,
      "duration": 2.55
    },
    {
      "text": "So, I am very hopeful",
      "start": 6067.02,
      "duration": 1.59
    },
    {
      "text": "that we can solve homomorphic encryption",
      "start": 6068.61,
      "duration": 3.03
    },
    {
      "text": "for language model inference.",
      "start": 6071.64,
      "duration": 1.43
    },
    {
      "text": "- Yeah, and doing privacy,\npreserving machine learning.",
      "start": 6073.07,
      "duration": 1.27
    },
    {
      "text": "But I would say, that's\nthe challenge we have",
      "start": 6074.34,
      "duration": 1.92
    },
    {
      "text": "with all software these days.",
      "start": 6076.26,
      "duration": 2.46
    },
    {
      "text": "It's like, there's so many\nfeatures that can be provided",
      "start": 6078.72,
      "duration": 3.54
    },
    {
      "text": "from the cloud and all us\nincreasingly rely on it",
      "start": 6082.26,
      "duration": 2.94
    },
    {
      "text": "and make our life awesome.",
      "start": 6085.2,
      "duration": 1.38
    },
    {
      "text": "But there's downsides,",
      "start": 6086.58,
      "duration": 1.17
    },
    {
      "text": "and that's why you rely\non really good security",
      "start": 6087.75,
      "duration": 1.86
    },
    {
      "text": "to protect from basic attacks.",
      "start": 6089.61,
      "duration": 1.98
    },
    {
      "text": "But there's also only a\nsmall set of companies",
      "start": 6091.59,
      "duration": 3.78
    },
    {
      "text": "that are controlling that data,",
      "start": 6095.37,
      "duration": 1.55
    },
    {
      "text": "and they obviously have leverage",
      "start": 6097.83,
      "duration": 2.25
    },
    {
      "text": "and they could be infiltrated\nin all kinds of ways.",
      "start": 6100.08,
      "duration": 1.92
    },
    {
      "text": "That's the world we live in.",
      "start": 6102.0,
      "duration": 1.65
    },
    {
      "text": "- Yeah, the thing I'm just\nactually quite worried about",
      "start": 6103.65,
      "duration": 3.27
    },
    {
      "text": "is Anthropic has this\nresponsible scaling policy",
      "start": 6106.92,
      "duration": 4.27
    },
    {
      "text": "where we're the low ASLs,",
      "start": 6112.47,
      "duration": 2.67
    },
    {
      "text": "which is the Anthropic\nsecurity level or whatever",
      "start": 6115.14,
      "duration": 2.1
    },
    {
      "text": "of the models.",
      "start": 6117.24,
      "duration": 1.71
    },
    {
      "text": "But as we get to, quote,\nunquote, \"ASL-3, ASL-4,\"",
      "start": 6118.95,
      "duration": 3.42
    },
    {
      "text": "whatever models which are very powerful.",
      "start": 6122.37,
      "duration": 4.08
    },
    {
      "text": "But for mostly reasonable\nsecurity reasons,",
      "start": 6126.45,
      "duration": 4.68
    },
    {
      "text": "you would wanna monitor all the prompts.",
      "start": 6131.13,
      "duration": 2.46
    },
    {
      "text": "But I think that's\nreasonable and understandable",
      "start": 6133.59,
      "duration": 3.57
    },
    {
      "text": "where everyone is coming from.",
      "start": 6137.16,
      "duration": 1.41
    },
    {
      "text": "But man, it'd be really horrible",
      "start": 6138.57,
      "duration": 2.43
    },
    {
      "text": "if all the world's information\nis monitored that heavily,",
      "start": 6141.0,
      "duration": 3.78
    },
    {
      "text": "it's way too centralized.",
      "start": 6144.78,
      "duration": 2.22
    },
    {
      "text": "It's like this really\nfine line you're walking",
      "start": 6147.0,
      "duration": 3.6
    },
    {
      "text": "where on the one side,",
      "start": 6150.6,
      "duration": 2.73
    },
    {
      "text": "you don't want the models to go rogue.",
      "start": 6153.33,
      "duration": 1.83
    },
    {
      "text": "On the other side, humans like,",
      "start": 6155.16,
      "duration": 3.0
    },
    {
      "text": "I don't know if I trust\nall the world's information",
      "start": 6158.16,
      "duration": 2.91
    },
    {
      "text": "to pass through three model providers.",
      "start": 6161.07,
      "duration": 2.34
    },
    {
      "text": "- Yeah.",
      "start": 6163.41,
      "duration": 1.26
    },
    {
      "text": "- Why do you think it's\ndifferent than cloud providers?",
      "start": 6164.67,
      "duration": 2.97
    },
    {
      "text": "- Because I think a lot of\nthis data would never have gone",
      "start": 6167.64,
      "duration": 5.0
    },
    {
      "text": "to the cloud providers in the first place.",
      "start": 6174.12,
      "duration": 2.403
    },
    {
      "text": "You want to give more\ndata to the AI models,",
      "start": 6180.57,
      "duration": 1.92
    },
    {
      "text": "you want to give personal data",
      "start": 6182.49,
      "duration": 2.01
    },
    {
      "text": "that you would never have\nput online in the first place",
      "start": 6184.5,
      "duration": 2.94
    },
    {
      "text": "to these companies or to these models.",
      "start": 6187.44,
      "duration": 2.763
    },
    {
      "text": "It also centralizes control\nwhere right now, for cloud,",
      "start": 6191.55,
      "duration": 5.0
    },
    {
      "text": "you can often use your\nown encryption keys,",
      "start": 6199.08,
      "duration": 2.07
    },
    {
      "text": "and AWS can't really do much.",
      "start": 6201.15,
      "duration": 3.273
    },
    {
      "text": "But here, it's just centralized actors",
      "start": 6206.1,
      "duration": 3.18
    },
    {
      "text": "that see the exact plain\ntext of everything.",
      "start": 6209.28,
      "duration": 2.373
    },
    {
      "text": "- On the topic of a context,",
      "start": 6212.7,
      "duration": 1.71
    },
    {
      "text": "that's actually been a friction for me.",
      "start": 6214.41,
      "duration": 1.71
    },
    {
      "text": "When I'm writing code in Python,",
      "start": 6216.12,
      "duration": 1.95
    },
    {
      "text": "there's a bunch of stuff imported.",
      "start": 6218.07,
      "duration": 1.7
    },
    {
      "text": "You could probably\nintuit the kind of stuff",
      "start": 6221.37,
      "duration": 1.98
    },
    {
      "text": "I would like to include in the context.",
      "start": 6223.35,
      "duration": 1.95
    },
    {
      "text": "How hard is it to auto\nfigure out the context?",
      "start": 6227.34,
      "duration": 3.75
    },
    {
      "text": "- It's tricky.",
      "start": 6231.09,
      "duration": 0.903
    },
    {
      "text": "I think we can do a lot better",
      "start": 6232.86,
      "duration": 1.84
    },
    {
      "text": "at computing the context\nautomatically in the future.",
      "start": 6235.62,
      "duration": 3.12
    },
    {
      "text": "One thing that's important to note is,",
      "start": 6238.74,
      "duration": 1.41
    },
    {
      "text": "there are trade-offs with\nincluding automatic context.",
      "start": 6240.15,
      "duration": 3.51
    },
    {
      "text": "So, the more context you\ninclude for these models,",
      "start": 6243.66,
      "duration": 3.12
    },
    {
      "text": "first of all, the slower they are",
      "start": 6246.78,
      "duration": 2.91
    },
    {
      "text": "and the more expensive those requests are,",
      "start": 6249.69,
      "duration": 2.55
    },
    {
      "text": "which means you can\nthen do less model calls",
      "start": 6252.24,
      "duration": 1.68
    },
    {
      "text": "and do less fancy stuff in the background.",
      "start": 6253.92,
      "duration": 2.16
    },
    {
      "text": "Also, for a lot of these\nmodels, they get confused",
      "start": 6256.08,
      "duration": 2.22
    },
    {
      "text": "if you have a lot of\ninformation in the prompt.",
      "start": 6258.3,
      "duration": 1.89
    },
    {
      "text": "So the bar for accuracy and for relevance",
      "start": 6260.19,
      "duration": 3.6
    },
    {
      "text": "of the context you include\nshould be quite high.",
      "start": 6263.79,
      "duration": 2.4
    },
    {
      "text": "Already, we do some automatic context",
      "start": 6268.95,
      "duration": 2.57
    },
    {
      "text": "in some places within the product.",
      "start": 6271.52,
      "duration": 1.54
    },
    {
      "text": "It's definitely something we\nwanna get a lot better at.",
      "start": 6273.06,
      "duration": 2.363
    },
    {
      "text": "I think that there are a lot\nof cool ideas to try there,",
      "start": 6277.02,
      "duration": 3.213
    },
    {
      "text": "both on the learning\nbetter retrieval systems,",
      "start": 6281.43,
      "duration": 4.29
    },
    {
      "text": "like better embedding\nmodels, better rerankers.",
      "start": 6285.72,
      "duration": 2.7
    },
    {
      "text": "I think that there are\nalso cool academic ideas,",
      "start": 6288.42,
      "duration": 3.75
    },
    {
      "text": "stuff we've tried out internally,",
      "start": 6292.17,
      "duration": 1.17
    },
    {
      "text": "but also the field is grappling\nwith writ large about,",
      "start": 6293.34,
      "duration": 3.27
    },
    {
      "text": "can you get language models to a place",
      "start": 6296.61,
      "duration": 1.62
    },
    {
      "text": "where you can actually\njust have the model itself",
      "start": 6298.23,
      "duration": 2.22
    },
    {
      "text": "understand a new corpus of information?",
      "start": 6300.45,
      "duration": 2.7
    },
    {
      "text": "The most popular talked\nabout version of this is",
      "start": 6303.15,
      "duration": 2.55
    },
    {
      "text": "can you make the context windows infinite?",
      "start": 6305.7,
      "duration": 1.86
    },
    {
      "text": "Then if you make the\ncontext windows infinite,",
      "start": 6307.56,
      "duration": 1.38
    },
    {
      "text": "can you make the model\nactually pay attention",
      "start": 6308.94,
      "duration": 1.59
    },
    {
      "text": "to the infinite context?",
      "start": 6310.53,
      "duration": 1.2
    },
    {
      "text": "And then, after you can\nmake it pay attention",
      "start": 6311.73,
      "duration": 1.44
    },
    {
      "text": "to the infinite context to\nmake it somewhat feasible",
      "start": 6313.17,
      "duration": 2.58
    },
    {
      "text": "to actually do it, can you then do caching",
      "start": 6315.75,
      "duration": 2.1
    },
    {
      "text": "for that infinite context?",
      "start": 6317.85,
      "duration": 0.99
    },
    {
      "text": "You don't have to recompute\nthat all the time.",
      "start": 6318.84,
      "duration": 2.13
    },
    {
      "text": "But there are other cool\nideas that are being tried,",
      "start": 6320.97,
      "duration": 2.52
    },
    {
      "text": "that are a little bit more\nanalogous to fine-tuning",
      "start": 6323.49,
      "duration": 2.34
    },
    {
      "text": "of actually learning this information",
      "start": 6325.83,
      "duration": 1.35
    },
    {
      "text": "in the weights of the model.",
      "start": 6327.18,
      "duration": 1.4
    },
    {
      "text": "It might be that you\nactually get a qualitative",
      "start": 6329.43,
      "duration": 2.73
    },
    {
      "text": "lead different type of understanding",
      "start": 6332.16,
      "duration": 2.64
    },
    {
      "text": "if you do it more at the weight level",
      "start": 6334.8,
      "duration": 1.26
    },
    {
      "text": "than if you do it at the\nin-context learning level.",
      "start": 6336.06,
      "duration": 1.74
    },
    {
      "text": "I think the jury's still a little bit out",
      "start": 6337.8,
      "duration": 2.91
    },
    {
      "text": "on how this is all gonna work in the end?",
      "start": 6340.71,
      "duration": 2.37
    },
    {
      "text": "But in the interim, us as a company,",
      "start": 6343.08,
      "duration": 1.59
    },
    {
      "text": "we are really excited about\nbetter retrieval systems",
      "start": 6344.67,
      "duration": 2.58
    },
    {
      "text": "and picking the parts of the code base",
      "start": 6347.25,
      "duration": 2.01
    },
    {
      "text": "that are most relevant\nto what you're doing,",
      "start": 6349.26,
      "duration": 1.65
    },
    {
      "text": "and we could do that a lot better.",
      "start": 6350.91,
      "duration": 1.83
    },
    {
      "text": "- One interesting proof of concept",
      "start": 6352.74,
      "duration": 1.77
    },
    {
      "text": "for the learning this knowledge\ndirectly in the weights",
      "start": 6354.51,
      "duration": 3.84
    },
    {
      "text": "is with VS Code.",
      "start": 6358.35,
      "duration": 2.1
    },
    {
      "text": "So, we're in a VS Code fork and VS Code.",
      "start": 6360.45,
      "duration": 3.0
    },
    {
      "text": "The code is all public.",
      "start": 6363.45,
      "duration": 1.47
    },
    {
      "text": "So these models in pre-training\nhave seen all the code.",
      "start": 6364.92,
      "duration": 2.913
    },
    {
      "text": "They've probably also seen\nquestions and answers about it.",
      "start": 6368.67,
      "duration": 2.4
    },
    {
      "text": "And then, they've been\nfine-tuned and RLHFed",
      "start": 6371.07,
      "duration": 2.7
    },
    {
      "text": "to be able to answer questions\nabout code in general.",
      "start": 6373.77,
      "duration": 2.28
    },
    {
      "text": "So when you ask it a\nquestion about VS Code,",
      "start": 6376.05,
      "duration": 2.82
    },
    {
      "text": "sometimes it'll hallucinate,",
      "start": 6378.87,
      "duration": 1.2
    },
    {
      "text": "but sometimes it actually\ndoes a pretty good job",
      "start": 6380.07,
      "duration": 2.91
    },
    {
      "text": "at answering the question.",
      "start": 6382.98,
      "duration": 1.623
    },
    {
      "text": "It happens to be okay,",
      "start": 6387.42,
      "duration": 2.13
    },
    {
      "text": "but what if you could\nactually specifically train",
      "start": 6389.55,
      "duration": 2.55
    },
    {
      "text": "or post-train a model such\nthat it really was built",
      "start": 6392.1,
      "duration": 3.6
    },
    {
      "text": "to understand this code base?",
      "start": 6395.7,
      "duration": 1.833
    },
    {
      "text": "It's an open research question,",
      "start": 6398.76,
      "duration": 1.32
    },
    {
      "text": "one that we're quite interested in.",
      "start": 6400.08,
      "duration": 1.32
    },
    {
      "text": "And then there's also uncertainty of,",
      "start": 6401.4,
      "duration": 1.59
    },
    {
      "text": "do you want the model to be the thing",
      "start": 6402.99,
      "duration": 1.74
    },
    {
      "text": "that end-to-end is doing everything,",
      "start": 6404.73,
      "duration": 2.1
    },
    {
      "text": "i.e., it's doing the\nretrieval in its internals,",
      "start": 6406.83,
      "duration": 2.85
    },
    {
      "text": "and then answering a\nquestion, creating the code,",
      "start": 6409.68,
      "duration": 2.22
    },
    {
      "text": "or do you want to separate the retrieval",
      "start": 6411.9,
      "duration": 3.36
    },
    {
      "text": "from the frontier model,",
      "start": 6415.26,
      "duration": 1.95
    },
    {
      "text": "where maybe you'll get\nsome really capable models",
      "start": 6417.21,
      "duration": 2.37
    },
    {
      "text": "that are much better than\nthe best open source ones",
      "start": 6419.58,
      "duration": 2.43
    },
    {
      "text": "in a handful of months?",
      "start": 6422.01,
      "duration": 1.323
    },
    {
      "text": "And then, you'll want to separately train",
      "start": 6424.5,
      "duration": 2.67
    },
    {
      "text": "a really good open source\nmodel to be the retriever,",
      "start": 6427.17,
      "duration": 2.25
    },
    {
      "text": "to be the thing that feeds in the context",
      "start": 6429.42,
      "duration": 2.91
    },
    {
      "text": "to these larger models.",
      "start": 6432.33,
      "duration": 2.01
    },
    {
      "text": "- Can you speak a little\nmore to post-training a model",
      "start": 6434.34,
      "duration": 2.58
    },
    {
      "text": "to understand the code base?",
      "start": 6436.92,
      "duration": 1.4
    },
    {
      "text": "What do you mean by that?",
      "start": 6439.26,
      "duration": 0.89
    },
    {
      "text": "Is this a synthetic data direction?",
      "start": 6440.15,
      "duration": 2.26
    },
    {
      "text": "Is this-",
      "start": 6442.41,
      "duration": 0.986
    },
    {
      "text": "- Yeah, there are many possible\nways you could try doing it.",
      "start": 6443.396,
      "duration": 3.424
    },
    {
      "text": "There's certainly no shortage of ideas.",
      "start": 6446.82,
      "duration": 3.42
    },
    {
      "text": "It's just a question of going\nin and trying all of them",
      "start": 6450.24,
      "duration": 2.22
    },
    {
      "text": "and being empirical about\nwhich one works best.",
      "start": 6452.46,
      "duration": 2.35
    },
    {
      "text": "One very naive thing is to\ntry to replicate what's done",
      "start": 6456.219,
      "duration": 3.981
    },
    {
      "text": "with VS Code and these frontier models.",
      "start": 6460.2,
      "duration": 2.88
    },
    {
      "text": "So, let's continue pre-training.",
      "start": 6463.08,
      "duration": 2.76
    },
    {
      "text": "Some kind of continued pre-training",
      "start": 6465.84,
      "duration": 1.02
    },
    {
      "text": "that includes general code data",
      "start": 6466.86,
      "duration": 1.2
    },
    {
      "text": "but also throws in of the data\nof some particular repository",
      "start": 6468.06,
      "duration": 3.96
    },
    {
      "text": "that you care about.",
      "start": 6472.02,
      "duration": 1.08
    },
    {
      "text": "And then in post-training, meaning,",
      "start": 6473.1,
      "duration": 3.3
    },
    {
      "text": "let's just start with\ninstruction fine-tuning.",
      "start": 6476.4,
      "duration": 1.95
    },
    {
      "text": "You have a normal instruction\nfine-tuning data set",
      "start": 6478.35,
      "duration": 2.1
    },
    {
      "text": "about code.",
      "start": 6480.45,
      "duration": 0.833
    },
    {
      "text": "Then you throw in a lot\nof questions about code",
      "start": 6481.283,
      "duration": 3.607
    },
    {
      "text": "in that repository.",
      "start": 6484.89,
      "duration": 2.16
    },
    {
      "text": "So, you could either\nget ground truth ones,",
      "start": 6487.05,
      "duration": 2.64
    },
    {
      "text": "which might be difficult or\nyou could do what you hinted at",
      "start": 6489.69,
      "duration": 2.52
    },
    {
      "text": "or suggested using synthetic data,",
      "start": 6492.21,
      "duration": 2.61
    },
    {
      "text": "i.e., having the model ask questions",
      "start": 6494.82,
      "duration": 5.0
    },
    {
      "text": "about various recent pieces of the code.",
      "start": 6499.89,
      "duration": 3.0
    },
    {
      "text": "So you take the pieces of the code,",
      "start": 6502.89,
      "duration": 1.56
    },
    {
      "text": "then prompt the model or have\na model propose a question",
      "start": 6504.45,
      "duration": 3.39
    },
    {
      "text": "for that piece of code,",
      "start": 6507.84,
      "duration": 1.14
    },
    {
      "text": "and then add those as instruction\nfine-tuning data points.",
      "start": 6508.98,
      "duration": 3.6
    },
    {
      "text": "And then in theory, this might\nunlock the model's ability",
      "start": 6512.58,
      "duration": 3.63
    },
    {
      "text": "to answer questions about that code base.",
      "start": 6516.21,
      "duration": 3.21
    },
    {
      "text": "- Let me ask you about OpenAI o1.",
      "start": 6519.42,
      "duration": 3.06
    },
    {
      "text": "What do you think is the role",
      "start": 6522.48,
      "duration": 0.9
    },
    {
      "text": "of that kind of test time\ncompute system in programming?",
      "start": 6523.38,
      "duration": 3.93
    },
    {
      "text": "- I think test time compute\nis really, really interesting.",
      "start": 6527.31,
      "duration": 3.51
    },
    {
      "text": "So, there's been the pre-training regime",
      "start": 6530.82,
      "duration": 2.0
    },
    {
      "text": "as you scale up the amount of data",
      "start": 6535.08,
      "duration": 1.98
    },
    {
      "text": "and the size of your model,",
      "start": 6537.06,
      "duration": 0.99
    },
    {
      "text": "get you better and better\nperformance both on loss,",
      "start": 6538.05,
      "duration": 2.28
    },
    {
      "text": "and then on downstream benchmarks",
      "start": 6540.33,
      "duration": 2.61
    },
    {
      "text": "and just general performance,",
      "start": 6542.94,
      "duration": 1.11
    },
    {
      "text": "so we use it for coding or other tasks.",
      "start": 6544.05,
      "duration": 2.973
    },
    {
      "text": "We're starting to hit\na bit of a data wall,",
      "start": 6549.0,
      "duration": 3.63
    },
    {
      "text": "meaning, it's going to be hard to continue",
      "start": 6552.63,
      "duration": 1.83
    },
    {
      "text": "scaling up this regime.",
      "start": 6554.46,
      "duration": 1.83
    },
    {
      "text": "So, scaling up test time\ncompute is an interesting way,",
      "start": 6556.29,
      "duration": 2.91
    },
    {
      "text": "if now increasing the number\nof inference time flops.",
      "start": 6559.2,
      "duration": 4.563
    },
    {
      "text": "Yeah, as you increase the number",
      "start": 6567.214,
      "duration": 1.016
    },
    {
      "text": "of flops you use inference\ntime getting corresponding",
      "start": 6568.23,
      "duration": 2.85
    },
    {
      "text": "improvements in the\nperformance of these models.",
      "start": 6571.08,
      "duration": 2.34
    },
    {
      "text": "Traditionally, we just had to\nliterally train a bigger model",
      "start": 6573.42,
      "duration": 2.29
    },
    {
      "text": "that always used that many more flops,",
      "start": 6575.71,
      "duration": 3.17
    },
    {
      "text": "but now, we could perhaps\nuse the same size model",
      "start": 6578.88,
      "duration": 2.64
    },
    {
      "text": "and run it for longer to\nbe able to get an answer",
      "start": 6581.52,
      "duration": 3.93
    },
    {
      "text": "at the quality of a much larger model.",
      "start": 6585.45,
      "duration": 1.35
    },
    {
      "text": "And so, the really interesting\nthing I like about this",
      "start": 6586.8,
      "duration": 2.7
    },
    {
      "text": "is there are some problems\nthat perhaps require",
      "start": 6589.5,
      "duration": 2.71
    },
    {
      "text": "100 trillion parameter\nmodel intelligence trained",
      "start": 6593.25,
      "duration": 2.16
    },
    {
      "text": "on 100 trillion tokens.",
      "start": 6595.41,
      "duration": 1.353
    },
    {
      "text": "But that's maybe 1%,\nmaybe .1% of all queries.",
      "start": 6597.96,
      "duration": 4.98
    },
    {
      "text": "So are you going to\nspend all of this effort,",
      "start": 6602.94,
      "duration": 2.64
    },
    {
      "text": "all of this compute training\na model that costs that much",
      "start": 6605.58,
      "duration": 3.96
    },
    {
      "text": "and then run it so infrequently?",
      "start": 6609.54,
      "duration": 2.253
    },
    {
      "text": "You train the model that is capable",
      "start": 6616.715,
      "duration": 1.075
    },
    {
      "text": "of doing the 99.9% of queries,",
      "start": 6617.79,
      "duration": 2.49
    },
    {
      "text": "then you have a way of\ninference time running it longer",
      "start": 6620.28,
      "duration": 3.3
    },
    {
      "text": "for those few people",
      "start": 6623.58,
      "duration": 1.02
    },
    {
      "text": "that really, really want max intelligence.",
      "start": 6624.6,
      "duration": 2.343
    },
    {
      "text": "- How do you figure out\nwhich problem requires",
      "start": 6628.02,
      "duration": 3.09
    },
    {
      "text": "what level of intelligence?",
      "start": 6631.11,
      "duration": 2.22
    },
    {
      "text": "Is that possible to dynamically figure out",
      "start": 6633.33,
      "duration": 1.83
    },
    {
      "text": "when to use GPT-4, when\nto use a small model",
      "start": 6635.16,
      "duration": 3.87
    },
    {
      "text": "and when you need the o1?",
      "start": 6639.03,
      "duration": 2.203
    },
    {
      "text": "(group chuckles)",
      "start": 6642.856,
      "duration": 1.754
    },
    {
      "text": "- Yeah, that's an open\nresearch problem, certainly.",
      "start": 6644.61,
      "duration": 2.67
    },
    {
      "text": "I don't think anyone's actually cracked",
      "start": 6647.28,
      "duration": 1.56
    },
    {
      "text": "this model routing problem quite well.",
      "start": 6648.84,
      "duration": 2.442
    },
    {
      "text": "We have initial implementations of this",
      "start": 6651.282,
      "duration": 3.318
    },
    {
      "text": "for something like Cursor Tab,",
      "start": 6654.6,
      "duration": 2.343
    },
    {
      "text": "but at the level of going\nbetween 4o Sonnet to o1,",
      "start": 6658.11,
      "duration": 4.563
    },
    {
      "text": "it's a bit trickier.",
      "start": 6663.57,
      "duration": 1.35
    },
    {
      "text": "There's also a question like,",
      "start": 6664.92,
      "duration": 0.96
    },
    {
      "text": "what level of intelligence\ndo you need to determine",
      "start": 6665.88,
      "duration": 2.58
    },
    {
      "text": "if the thing is too hard\nfor the four level model?",
      "start": 6668.46,
      "duration": 5.0
    },
    {
      "text": "Maybe you need the o1 level model.",
      "start": 6673.86,
      "duration": 3.87
    },
    {
      "text": "It's really unclear.",
      "start": 6677.73,
      "duration": 1.98
    },
    {
      "text": "- But you mentioned this.",
      "start": 6679.71,
      "duration": 0.833
    },
    {
      "text": "So, there's a pre-training process",
      "start": 6680.543,
      "duration": 3.067
    },
    {
      "text": "then there's post-training,",
      "start": 6683.61,
      "duration": 1.62
    },
    {
      "text": "and then there's test time compute.",
      "start": 6685.23,
      "duration": 1.76
    },
    {
      "text": "Is that fair to separate?",
      "start": 6686.99,
      "duration": 1.75
    },
    {
      "text": "Where's the biggest gains?",
      "start": 6688.74,
      "duration": 1.413
    },
    {
      "text": "- Well, it's weird\nbecause test time compute,",
      "start": 6691.08,
      "duration": 2.61
    },
    {
      "text": "there's a whole training strategy needed",
      "start": 6693.69,
      "duration": 2.49
    },
    {
      "text": "to get test time compute to work.",
      "start": 6696.18,
      "duration": 1.923
    },
    {
      "text": "The other really weird thing about this",
      "start": 6699.03,
      "duration": 2.07
    },
    {
      "text": "is outside of the big labs\nand maybe even just OpenAI,",
      "start": 6701.1,
      "duration": 4.41
    },
    {
      "text": "no one really knows how it works.",
      "start": 6705.51,
      "duration": 2.43
    },
    {
      "text": "There've been some\nreally interesting papers",
      "start": 6707.94,
      "duration": 1.98
    },
    {
      "text": "that show hints of what\nthey might be doing.",
      "start": 6709.92,
      "duration": 4.17
    },
    {
      "text": "So, perhaps they're doing something",
      "start": 6714.09,
      "duration": 2.64
    },
    {
      "text": "with tree search using\nprocess reward models.",
      "start": 6716.73,
      "duration": 3.42
    },
    {
      "text": "But yeah, I think the issue",
      "start": 6720.15,
      "duration": 1.68
    },
    {
      "text": "is we don't quite know\nexactly what it looks like,",
      "start": 6721.83,
      "duration": 3.09
    },
    {
      "text": "so it would be hard to\ncomment on where it fits in.",
      "start": 6724.92,
      "duration": 3.12
    },
    {
      "text": "I would put it in post-training,",
      "start": 6728.04,
      "duration": 1.44
    },
    {
      "text": "but maybe the compute spent",
      "start": 6729.48,
      "duration": 1.77
    },
    {
      "text": "for this forgetting test time\ncompute to work for a model",
      "start": 6731.25,
      "duration": 3.51
    },
    {
      "text": "is going to dwarf pre-training eventually.",
      "start": 6734.76,
      "duration": 2.883
    },
    {
      "text": "- So we don't even know if o1\nis using just chain of thought",
      "start": 6738.81,
      "duration": 5.0
    },
    {
      "text": "or we don't know how\nthey're using any of these?",
      "start": 6743.91,
      "duration": 2.22
    },
    {
      "text": "We don't know anything?",
      "start": 6746.13,
      "duration": 1.23
    },
    {
      "text": "- It's fun to speculate.",
      "start": 6747.36,
      "duration": 1.316
    },
    {
      "text": "(group chuckling)",
      "start": 6748.676,
      "duration": 2.134
    },
    {
      "text": "- If you were to build a competing\nmodel, what would you do?",
      "start": 6750.81,
      "duration": 3.423
    },
    {
      "text": "- Yeah, so one thing to do would be,",
      "start": 6755.1,
      "duration": 3.24
    },
    {
      "text": "I think you probably need to\ntrain a process reward model.",
      "start": 6758.34,
      "duration": 3.57
    },
    {
      "text": "So maybe we can get into reward models",
      "start": 6761.91,
      "duration": 1.647
    },
    {
      "text": "and outcome reward models\nversus process reward models.",
      "start": 6763.557,
      "duration": 2.853
    },
    {
      "text": "Outcome reward models are\nthe traditional reward models",
      "start": 6766.41,
      "duration": 3.21
    },
    {
      "text": "that people are trained\nfor language modeling,",
      "start": 6769.62,
      "duration": 4.38
    },
    {
      "text": "and it's just looking at the final thing.",
      "start": 6774.0,
      "duration": 1.44
    },
    {
      "text": "So if you're doing some math problem,",
      "start": 6775.44,
      "duration": 1.17
    },
    {
      "text": "let's look at that final thing.",
      "start": 6776.61,
      "duration": 1.2
    },
    {
      "text": "You've done everything and\nlet's assign a grade to it,",
      "start": 6777.81,
      "duration": 4.35
    },
    {
      "text": "how likely we think.",
      "start": 6782.16,
      "duration": 1.56
    },
    {
      "text": "What's the reward for this outcome?",
      "start": 6783.72,
      "duration": 2.13
    },
    {
      "text": "Process reward models",
      "start": 6785.85,
      "duration": 0.96
    },
    {
      "text": "instead try to grade the chain of thought.",
      "start": 6786.81,
      "duration": 2.46
    },
    {
      "text": "And so OpenAI had preliminary\npaper on this, I think,",
      "start": 6789.27,
      "duration": 3.54
    },
    {
      "text": "last summer where they use human labelers",
      "start": 6792.81,
      "duration": 4.38
    },
    {
      "text": "to get this pretty large several\nhundred thousand data set",
      "start": 6797.19,
      "duration": 3.15
    },
    {
      "text": "of creating chains of thought.",
      "start": 6800.34,
      "duration": 1.683
    },
    {
      "text": "Ultimately, it feels like",
      "start": 6803.85,
      "duration": 1.08
    },
    {
      "text": "I haven't seen anything\ninteresting in the ways",
      "start": 6804.93,
      "duration": 2.43
    },
    {
      "text": "that people use process reward models",
      "start": 6807.36,
      "duration": 2.01
    },
    {
      "text": "outside of just using it\nas a means of affecting",
      "start": 6809.37,
      "duration": 5.0
    },
    {
      "text": "how we choose between a bunch of samples.",
      "start": 6814.59,
      "duration": 1.89
    },
    {
      "text": "So, what people do in all these papers",
      "start": 6816.48,
      "duration": 2.58
    },
    {
      "text": "is they sample a bunch of\noutputs from the language model,",
      "start": 6819.06,
      "duration": 3.15
    },
    {
      "text": "and then use the process reward models",
      "start": 6822.21,
      "duration": 2.28
    },
    {
      "text": "to grade all those generations",
      "start": 6824.49,
      "duration": 2.73
    },
    {
      "text": "alongside maybe some other heuristics,",
      "start": 6827.22,
      "duration": 1.92
    },
    {
      "text": "and then use that to\nchoose the best answer.",
      "start": 6829.14,
      "duration": 2.58
    },
    {
      "text": "The really interesting thing\nthat people think might work",
      "start": 6831.72,
      "duration": 3.33
    },
    {
      "text": "and people want to work is tree search",
      "start": 6835.05,
      "duration": 2.67
    },
    {
      "text": "with these process reward models.",
      "start": 6837.72,
      "duration": 1.11
    },
    {
      "text": "Because if you really can\ngrade every single step",
      "start": 6838.83,
      "duration": 3.51
    },
    {
      "text": "of the chain of thought,\nthen you can branch out",
      "start": 6842.34,
      "duration": 3.36
    },
    {
      "text": "and explore multiple paths\nof this chain of thought",
      "start": 6845.7,
      "duration": 3.24
    },
    {
      "text": "and then use these process\nreward models to evaluate",
      "start": 6848.94,
      "duration": 2.07
    },
    {
      "text": "how good is this branch\nthat you're taking.",
      "start": 6851.01,
      "duration": 2.15
    },
    {
      "text": "- Yeah, when the quality of the branch",
      "start": 6854.01,
      "duration": 2.61
    },
    {
      "text": "is somehow strongly correlated",
      "start": 6856.62,
      "duration": 1.62
    },
    {
      "text": "with the quality of the\noutcome at the very end,",
      "start": 6858.24,
      "duration": 2.22
    },
    {
      "text": "so you have a good model of\nknowing which branch to take.",
      "start": 6860.46,
      "duration": 3.0
    },
    {
      "text": "So not just in the short\nterm, in the long term?",
      "start": 6863.46,
      "duration": 2.49
    },
    {
      "text": "- Yeah.",
      "start": 6865.95,
      "duration": 0.833
    },
    {
      "text": "The interesting work that\nI think has been done",
      "start": 6866.783,
      "duration": 1.447
    },
    {
      "text": "is figuring out how to\nproperly train the process,",
      "start": 6868.23,
      "duration": 2.64
    },
    {
      "text": "or the interesting work\nthat has been open sourced",
      "start": 6870.87,
      "duration": 2.73
    },
    {
      "text": "and people I think talk about is",
      "start": 6873.6,
      "duration": 2.76
    },
    {
      "text": "how to train the process reward models,",
      "start": 6876.36,
      "duration": 2.52
    },
    {
      "text": "maybe in a more automated way.",
      "start": 6878.88,
      "duration": 2.1
    },
    {
      "text": "I could be wrong here, could\nnot be mentioning some papers.",
      "start": 6880.98,
      "duration": 2.49
    },
    {
      "text": "I haven't seen anything super\nthat seems to work really well",
      "start": 6883.47,
      "duration": 3.99
    },
    {
      "text": "for using the process\nreward models creatively",
      "start": 6887.46,
      "duration": 3.33
    },
    {
      "text": "to do tree search and code.",
      "start": 6890.79,
      "duration": 1.95
    },
    {
      "text": "- This is an AI safety,",
      "start": 6892.74,
      "duration": 1.44
    },
    {
      "text": "maybe a bit of a philosophy question.",
      "start": 6894.18,
      "duration": 1.65
    },
    {
      "text": "So OpenAI says that they're\nhiding the chain of thought",
      "start": 6895.83,
      "duration": 2.73
    },
    {
      "text": "from the user,",
      "start": 6898.56,
      "duration": 1.38
    },
    {
      "text": "and they've said that that was\na difficult decision to make.",
      "start": 6899.94,
      "duration": 3.0
    },
    {
      "text": "Instead of showing the chain of thought,",
      "start": 6903.78,
      "duration": 2.28
    },
    {
      "text": "they're asking the model to\nsummarize the chain of thought.",
      "start": 6906.06,
      "duration": 3.24
    },
    {
      "text": "They're also in the background saying",
      "start": 6909.3,
      "duration": 1.26
    },
    {
      "text": "they're going to monitor\nthe chain of thought",
      "start": 6910.56,
      "duration": 2.46
    },
    {
      "text": "to make sure the model is not\ntrying to manipulate the user,",
      "start": 6913.02,
      "duration": 2.82
    },
    {
      "text": "which is a fascinating possibility.",
      "start": 6915.84,
      "duration": 1.92
    },
    {
      "text": "But anyway, what do you think",
      "start": 6917.76,
      "duration": 1.59
    },
    {
      "text": "about hiding the chain of thought?",
      "start": 6919.35,
      "duration": 1.83
    },
    {
      "text": "- One consideration for OpenAI,",
      "start": 6921.18,
      "duration": 1.287
    },
    {
      "text": "and this is completely speculative,",
      "start": 6922.467,
      "duration": 2.073
    },
    {
      "text": "could be that they wanna\nmake it hard for people",
      "start": 6924.54,
      "duration": 2.37
    },
    {
      "text": "to distill these capabilities\nout of their model.",
      "start": 6926.91,
      "duration": 2.82
    },
    {
      "text": "It might actually be easier",
      "start": 6929.73,
      "duration": 1.38
    },
    {
      "text": "if you had access to that\nhidden chain of thought",
      "start": 6931.11,
      "duration": 2.49
    },
    {
      "text": "to replicate the technology,\nbecause pretty important data,",
      "start": 6933.6,
      "duration": 3.54
    },
    {
      "text": "like seeing the steps that the model took",
      "start": 6937.14,
      "duration": 1.68
    },
    {
      "text": "to get to the final results.",
      "start": 6938.82,
      "duration": 1.11
    },
    {
      "text": "- So, you could probably\ntrain on that also?",
      "start": 6939.93,
      "duration": 2.43
    },
    {
      "text": "- And there was a mirror\nsituation with this,",
      "start": 6942.36,
      "duration": 2.88
    },
    {
      "text": "with some of the large\nlanguage model providers,",
      "start": 6945.24,
      "duration": 1.8
    },
    {
      "text": "and also this is speculation,",
      "start": 6947.04,
      "duration": 1.71
    },
    {
      "text": "but some of these APIs\nused to offer easy access",
      "start": 6948.75,
      "duration": 5.0
    },
    {
      "text": "to log probabilities for all the tokens",
      "start": 6954.33,
      "duration": 2.01
    },
    {
      "text": "that they're generating",
      "start": 6956.34,
      "duration": 1.32
    },
    {
      "text": "and also log probabilities\nover the prompt tokens.",
      "start": 6957.66,
      "duration": 2.31
    },
    {
      "text": "And then some of these\nAPIs took those away.",
      "start": 6959.97,
      "duration": 2.67
    },
    {
      "text": "Again, complete speculation,\nbut one of the thoughts",
      "start": 6962.64,
      "duration": 3.99
    },
    {
      "text": "is that the reason those were taken away",
      "start": 6966.63,
      "duration": 1.71
    },
    {
      "text": "is if you have access to log probabilities",
      "start": 6968.34,
      "duration": 2.76
    },
    {
      "text": "similar to this hidden chain of thought,",
      "start": 6971.1,
      "duration": 1.44
    },
    {
      "text": "that can give you even more information",
      "start": 6972.54,
      "duration": 1.32
    },
    {
      "text": "to try and distill these\ncapabilities out of the APIs,",
      "start": 6973.86,
      "duration": 3.06
    },
    {
      "text": "out of these biggest models\nand to models you control.",
      "start": 6976.92,
      "duration": 3.12
    },
    {
      "text": "As an asterisk on also\nthe previous discussion",
      "start": 6980.04,
      "duration": 3.15
    },
    {
      "text": "about us integrating o1,",
      "start": 6983.19,
      "duration": 2.94
    },
    {
      "text": "I think that we're still\nlearning how to use this model.",
      "start": 6986.13,
      "duration": 3.21
    },
    {
      "text": "So, we made o1 available in Cursor",
      "start": 6989.34,
      "duration": 1.8
    },
    {
      "text": "because when we got the model,",
      "start": 6991.14,
      "duration": 2.76
    },
    {
      "text": "we were really interested\nin trying it out.",
      "start": 6993.9,
      "duration": 1.95
    },
    {
      "text": "I think a lot of programmers\nare gonna be interested",
      "start": 6995.85,
      "duration": 2.19
    },
    {
      "text": "in trying it out.",
      "start": 6998.04,
      "duration": 0.85
    },
    {
      "text": "O1 is not part of the\ndefault Cursor experience",
      "start": 7000.58,
      "duration": 2.98
    },
    {
      "text": "in any way up,",
      "start": 7003.56,
      "duration": 0.833
    },
    {
      "text": "and we still haven't found\na way to yet integrate it",
      "start": 7005.54,
      "duration": 5.0
    },
    {
      "text": "into the editor in a way\nthat we reach for every hour,",
      "start": 7010.7,
      "duration": 4.2
    },
    {
      "text": "maybe even every day.",
      "start": 7014.9,
      "duration": 1.47
    },
    {
      "text": "So, I think that the jury's still out",
      "start": 7016.37,
      "duration": 2.19
    },
    {
      "text": "on how to use the model,",
      "start": 7018.56,
      "duration": 1.533
    },
    {
      "text": "and we haven't seen examples\nyet of people releasing things",
      "start": 7021.05,
      "duration": 4.56
    },
    {
      "text": "where it seems really clear like,",
      "start": 7025.61,
      "duration": 2.013
    },
    {
      "text": "\"Oh, that's now the use case.\"",
      "start": 7027.623,
      "duration": 2.157
    },
    {
      "text": "The obvious one to turn to",
      "start": 7029.78,
      "duration": 1.44
    },
    {
      "text": "is maybe this can make it easier",
      "start": 7031.22,
      "duration": 1.68
    },
    {
      "text": "for you to have these\nbackground things running,",
      "start": 7032.9,
      "duration": 2.31
    },
    {
      "text": "to have these models and loops,",
      "start": 7035.21,
      "duration": 0.93
    },
    {
      "text": "to have these models be agentic.",
      "start": 7036.14,
      "duration": 1.683
    },
    {
      "text": "But we're still discovering.",
      "start": 7039.26,
      "duration": 3.27
    },
    {
      "text": "- To be clear, we have ideas.",
      "start": 7042.53,
      "duration": 1.5
    },
    {
      "text": "We just need to try and get\nsomething incredibly useful",
      "start": 7044.03,
      "duration": 4.11
    },
    {
      "text": "before we put it out there.",
      "start": 7048.14,
      "duration": 1.47
    },
    {
      "text": "- But it has these\nsignificant limitations.",
      "start": 7049.61,
      "duration": 2.85
    },
    {
      "text": "Even barring capabilities,\nit does not stream.",
      "start": 7052.46,
      "duration": 4.773
    },
    {
      "text": "That means it's really, really\npainful to use for things",
      "start": 7058.1,
      "duration": 2.97
    },
    {
      "text": "where you want to supervise the output.",
      "start": 7061.07,
      "duration": 2.55
    },
    {
      "text": "Instead, you're just waiting\nfor the wall text to show up.",
      "start": 7063.62,
      "duration": 3.69
    },
    {
      "text": "Also, it does feel like the\nearly innings of test time,",
      "start": 7067.31,
      "duration": 2.82
    },
    {
      "text": "compute and search where it's\njust a very, very much a v0,",
      "start": 7070.13,
      "duration": 3.163
    },
    {
      "text": "and there's so many things\nthat don't feel quite right.",
      "start": 7074.66,
      "duration": 4.17
    },
    {
      "text": "I suspect in parallel to\npeople increasing the amount",
      "start": 7078.83,
      "duration": 5.0
    },
    {
      "text": "of pre-training data",
      "start": 7084.74,
      "duration": 1.396
    },
    {
      "text": "and the size of the\nmodels and pre-training",
      "start": 7086.136,
      "duration": 0.944
    },
    {
      "text": "and finding tricks there, you'll\nnow have this other thread",
      "start": 7087.08,
      "duration": 2.85
    },
    {
      "text": "of getting search to\nwork better and better.",
      "start": 7089.93,
      "duration": 2.703
    },
    {
      "text": "- So, let me ask you about\nstrawberry tomorrow eyes.",
      "start": 7093.8,
      "duration": 5.0
    },
    {
      "text": "(group chuckles)",
      "start": 7098.808,
      "duration": 1.052
    },
    {
      "text": "So, it looks like GitHub\nCopilot might be integrating o1",
      "start": 7099.86,
      "duration": 5.0
    },
    {
      "text": "in some kind of way,",
      "start": 7106.67,
      "duration": 1.65
    },
    {
      "text": "and I think some of the\ncomments are saying,",
      "start": 7108.32,
      "duration": 1.62
    },
    {
      "text": "does this mean Cursor is done?",
      "start": 7109.94,
      "duration": 1.99
    },
    {
      "text": "(group chuckles)",
      "start": 7111.93,
      "duration": 1.25
    },
    {
      "text": "I think I saw one comment saying that.",
      "start": 7113.18,
      "duration": 1.757
    },
    {
      "text": "- It's a time to shut down Cursor, yeah.",
      "start": 7114.937,
      "duration": 2.026
    },
    {
      "text": "- Time to shut down Cursor, thank you.",
      "start": 7116.963,
      "duration": 1.947
    },
    {
      "text": "(group chuckling)",
      "start": 7118.91,
      "duration": 0.833
    },
    {
      "text": "So, is it time to shut down Cursor?",
      "start": 7119.743,
      "duration": 1.717
    },
    {
      "text": "- I think this space is\na little bit different",
      "start": 7121.46,
      "duration": 1.68
    },
    {
      "text": "from past software spaces over the 2010s,",
      "start": 7123.14,
      "duration": 3.753
    },
    {
      "text": "where I think that the ceiling here",
      "start": 7127.97,
      "duration": 1.26
    },
    {
      "text": "is really, really, really incredibly high.",
      "start": 7129.23,
      "duration": 2.34
    },
    {
      "text": "So, I think that the best\nproduct in three to four years",
      "start": 7131.57,
      "duration": 2.85
    },
    {
      "text": "will just be soon much more useful",
      "start": 7134.42,
      "duration": 1.32
    },
    {
      "text": "than the best product today.",
      "start": 7135.74,
      "duration": 1.4
    },
    {
      "text": "You can wax poetic about\nmoats this and brand that",
      "start": 7138.02,
      "duration": 4.86
    },
    {
      "text": "and this is our advantage,\nbut I think in the end,",
      "start": 7142.88,
      "duration": 3.18
    },
    {
      "text": "just if you stop innovating\non the product, you will lose.",
      "start": 7146.06,
      "duration": 5.0
    },
    {
      "text": "That's also great for startups,",
      "start": 7151.31,
      "duration": 2.13
    },
    {
      "text": "that's great for people\ntrying to enter this market",
      "start": 7153.44,
      "duration": 2.64
    },
    {
      "text": "because it means you have an opportunity",
      "start": 7156.08,
      "duration": 1.95
    },
    {
      "text": "to win against people who\nhave lots of users already",
      "start": 7158.03,
      "duration": 3.63
    },
    {
      "text": "by just building something better.",
      "start": 7161.66,
      "duration": 1.7
    },
    {
      "text": "And so, I think over the next few years,",
      "start": 7164.48,
      "duration": 1.71
    },
    {
      "text": "it's just about building the best product,",
      "start": 7166.19,
      "duration": 2.67
    },
    {
      "text": "building the best system,\nand that both comes down",
      "start": 7168.86,
      "duration": 2.43
    },
    {
      "text": "to the modeling engine side of things,",
      "start": 7171.29,
      "duration": 3.27
    },
    {
      "text": "and it also comes down to\nthe editing experience.",
      "start": 7174.56,
      "duration": 2.94
    },
    {
      "text": "- Yeah, I think most of the\nadditional value from Cursor",
      "start": 7177.5,
      "duration": 3.33
    },
    {
      "text": "versus everything else out there",
      "start": 7180.83,
      "duration": 1.77
    },
    {
      "text": "is not just integrating\nthe new model fast like o1.",
      "start": 7182.6,
      "duration": 3.63
    },
    {
      "text": "It comes from all of the depth",
      "start": 7186.23,
      "duration": 3.27
    },
    {
      "text": "that goes into these custom models",
      "start": 7189.5,
      "duration": 2.1
    },
    {
      "text": "that you don't realize are working for you",
      "start": 7191.6,
      "duration": 1.92
    },
    {
      "text": "in every facet of the product,",
      "start": 7193.52,
      "duration": 1.98
    },
    {
      "text": "as well as the really thoughtful UX",
      "start": 7195.5,
      "duration": 3.93
    },
    {
      "text": "with every single feature.",
      "start": 7199.43,
      "duration": 1.3
    },
    {
      "text": "- All right, from that profound answer,",
      "start": 7201.77,
      "duration": 2.07
    },
    {
      "text": "let's descend back down to the technical.",
      "start": 7203.84,
      "duration": 1.77
    },
    {
      "text": "You mentioned you have a\ntaxonomy of synthetic data.",
      "start": 7205.61,
      "duration": 2.797
    },
    {
      "text": "- (chuckles) Oh, yeah.",
      "start": 7208.407,
      "duration": 1.343
    },
    {
      "text": "- Can you please explain?",
      "start": 7209.75,
      "duration": 0.84
    },
    {
      "text": "- Yeah, I think there are three main kinds",
      "start": 7210.59,
      "duration": 3.12
    },
    {
      "text": "of synthetic data.",
      "start": 7213.71,
      "duration": 0.993
    },
    {
      "text": "So what is synthetic data, first?",
      "start": 7216.59,
      "duration": 1.65
    },
    {
      "text": "So there's normal data,\nlike non-synthetic data,",
      "start": 7218.24,
      "duration": 2.22
    },
    {
      "text": "which is just data\nthat's naturally created,",
      "start": 7220.46,
      "duration": 3.36
    },
    {
      "text": "i.e., usually it'll be from\nhumans having done things.",
      "start": 7223.82,
      "duration": 3.36
    },
    {
      "text": "So, from some human\nprocess you get this data.",
      "start": 7227.18,
      "duration": 3.36
    },
    {
      "text": "Synthetic data, the first\none would be distillation.",
      "start": 7230.54,
      "duration": 4.2
    },
    {
      "text": "So having a language model,",
      "start": 7234.74,
      "duration": 2.07
    },
    {
      "text": "output tokens or probability\ndistributions over tokens,",
      "start": 7236.81,
      "duration": 4.023
    },
    {
      "text": "and then you can train some\nless capable model on this.",
      "start": 7241.76,
      "duration": 3.87
    },
    {
      "text": "This approach is not gonna\nget you a more capable model",
      "start": 7245.63,
      "duration": 3.42
    },
    {
      "text": "than the original one that\nhas produced the tokens,",
      "start": 7249.05,
      "duration": 2.55
    },
    {
      "text": "but it's really useful",
      "start": 7252.71,
      "duration": 0.93
    },
    {
      "text": "if there's some capability\nyou wanna elicit",
      "start": 7253.64,
      "duration": 2.67
    },
    {
      "text": "from some really expensive\nhigh-latency model.",
      "start": 7256.31,
      "duration": 2.607
    },
    {
      "text": "You can then distill that down",
      "start": 7258.917,
      "duration": 1.953
    },
    {
      "text": "into some smaller task-specific model.",
      "start": 7260.87,
      "duration": 2.553
    },
    {
      "text": "The second kind is when one\ndirection of the problem",
      "start": 7264.95,
      "duration": 4.32
    },
    {
      "text": "is easier than the reverse.",
      "start": 7269.27,
      "duration": 2.163
    },
    {
      "text": "So, a great example of\nthis is bug detection,",
      "start": 7272.9,
      "duration": 3.21
    },
    {
      "text": "like we mentioned earlier,",
      "start": 7276.11,
      "duration": 1.38
    },
    {
      "text": "where it's a lot easier to\nintroduce reasonable-looking bugs",
      "start": 7277.49,
      "duration": 5.0
    },
    {
      "text": "than it is to actually detect them.",
      "start": 7282.59,
      "duration": 2.4
    },
    {
      "text": "And this is probably\nthe case for humans too.",
      "start": 7284.99,
      "duration": 2.25
    },
    {
      "text": "And so what you can do,\nis you can get a model",
      "start": 7288.53,
      "duration": 2.91
    },
    {
      "text": "that's not trained in that much\ndata, that's not that smart,",
      "start": 7291.44,
      "duration": 2.91
    },
    {
      "text": "to introduce a bunch of bugs and code.",
      "start": 7294.35,
      "duration": 0.967
    },
    {
      "text": "And then, you can use that to then train.",
      "start": 7295.317,
      "duration": 3.023
    },
    {
      "text": "Use the synthetic data to train a model",
      "start": 7298.34,
      "duration": 1.05
    },
    {
      "text": "that can be really good at detecting bugs.",
      "start": 7299.39,
      "duration": 2.85
    },
    {
      "text": "The last category I think\nis, I guess the main one",
      "start": 7302.24,
      "duration": 3.0
    },
    {
      "text": "that it feels like the big labs are doing",
      "start": 7305.24,
      "duration": 3.12
    },
    {
      "text": "for synthetic data,\nwhich is producing text",
      "start": 7308.36,
      "duration": 4.44
    },
    {
      "text": "with language models that\ncan then be verified easily.",
      "start": 7312.8,
      "duration": 4.563
    },
    {
      "text": "So, extreme example of this",
      "start": 7318.23,
      "duration": 1.822
    },
    {
      "text": "is if you have a verification\nsystem that can detect",
      "start": 7320.052,
      "duration": 2.798
    },
    {
      "text": "if language is Shakespeare level,",
      "start": 7322.85,
      "duration": 3.164
    },
    {
      "text": "and then you have a bunch of\nmonkeys typing and typewriters.",
      "start": 7326.014,
      "duration": 2.746
    },
    {
      "text": "You can eventually get\nenough training data",
      "start": 7328.76,
      "duration": 1.89
    },
    {
      "text": "to train a Shakespeare-level\nlanguage model.",
      "start": 7330.65,
      "duration": 2.04
    },
    {
      "text": "And I mean this is very\nmuch the case for math",
      "start": 7332.69,
      "duration": 3.66
    },
    {
      "text": "where verification is\nactually really, really easy",
      "start": 7336.35,
      "duration": 2.79
    },
    {
      "text": "for formal languages.",
      "start": 7339.14,
      "duration": 3.57
    },
    {
      "text": "And then what you can do, is\nyou can have an okay model,",
      "start": 7342.71,
      "duration": 3.51
    },
    {
      "text": "generate a ton of rollouts,\nand then choose the ones",
      "start": 7346.22,
      "duration": 3.39
    },
    {
      "text": "that you know have actually proved",
      "start": 7349.61,
      "duration": 2.22
    },
    {
      "text": "the ground truth theorems,\nand train that further.",
      "start": 7351.83,
      "duration": 2.85
    },
    {
      "text": "There's similar things you can do for code",
      "start": 7354.68,
      "duration": 1.71
    },
    {
      "text": "with lead code like problems,",
      "start": 7356.39,
      "duration": 1.863
    },
    {
      "text": "where if you have some set of tests",
      "start": 7359.3,
      "duration": 1.86
    },
    {
      "text": "that you know correspond to if\nsomething passes these tests,",
      "start": 7361.16,
      "duration": 2.61
    },
    {
      "text": "it actually solved problem.\nYou could do the same thing",
      "start": 7363.77,
      "duration": 2.693
    },
    {
      "text": "where you verify that it's passed the test",
      "start": 7366.463,
      "duration": 1.177
    },
    {
      "text": "and then train the model in the outputs",
      "start": 7367.64,
      "duration": 1.56
    },
    {
      "text": "that have passed the tests.",
      "start": 7369.2,
      "duration": 1.563
    },
    {
      "text": "I think it's gonna be a little\ntricky getting this to work",
      "start": 7371.72,
      "duration": 2.61
    },
    {
      "text": "in all domains, or just in general.",
      "start": 7374.33,
      "duration": 3.63
    },
    {
      "text": "Having the perfect verifier\nfeels really, really hard to do",
      "start": 7377.96,
      "duration": 3.45
    },
    {
      "text": "with just open-ended miscellaneous tasks.",
      "start": 7381.41,
      "duration": 3.39
    },
    {
      "text": "You give the model or\nmore long horizon tasks,",
      "start": 7384.8,
      "duration": 3.03
    },
    {
      "text": "even in coding.",
      "start": 7387.83,
      "duration": 1.23
    },
    {
      "text": "- [Lex] That's 'cause you're\nnot as optimistic as Arvid.",
      "start": 7389.06,
      "duration": 1.98
    },
    {
      "text": "But yeah, so yeah,",
      "start": 7391.04,
      "duration": 1.207
    },
    {
      "text": "(Aman chuckles)",
      "start": 7392.247,
      "duration": 0.833
    },
    {
      "text": "that third category\nrequires having a verifier.",
      "start": 7393.08,
      "duration": 3.48
    },
    {
      "text": "- Yeah.",
      "start": 7396.56,
      "duration": 0.833
    },
    {
      "text": "Verification, it feels like\nit's best when you know",
      "start": 7397.393,
      "duration": 2.137
    },
    {
      "text": "for a fact that it's correct.",
      "start": 7399.53,
      "duration": 2.049
    },
    {
      "text": "And then it wouldn't be\nlike using a language model",
      "start": 7401.579,
      "duration": 2.151
    },
    {
      "text": "to verify, it would be using\ntests or formal systems.",
      "start": 7403.73,
      "duration": 4.77
    },
    {
      "text": "- Or running the thing too.",
      "start": 7408.5,
      "duration": 2.16
    },
    {
      "text": "Doing the human form of verification,",
      "start": 7410.66,
      "duration": 1.8
    },
    {
      "text": "where you just do manual quality control.",
      "start": 7412.46,
      "duration": 1.86
    },
    {
      "text": "- Yeah.\n- Yeah.",
      "start": 7414.32,
      "duration": 1.08
    },
    {
      "text": "- But the language model version of that,",
      "start": 7415.4,
      "duration": 1.65
    },
    {
      "text": "where it's running the thing",
      "start": 7417.05,
      "duration": 0.833
    },
    {
      "text": "and it actually understands the output.",
      "start": 7417.883,
      "duration": 1.927
    },
    {
      "text": "- Yeah, no, that's-",
      "start": 7419.81,
      "duration": 1.131
    },
    {
      "text": "- I'm sure it's somewhere in between.",
      "start": 7420.941,
      "duration": 0.833
    },
    {
      "text": "- Yeah.",
      "start": 7421.774,
      "duration": 1.225
    },
    {
      "text": "I think that's the category\nthat is most likely to result",
      "start": 7422.999,
      "duration": 3.681
    },
    {
      "text": "in massive gains.",
      "start": 7426.68,
      "duration": 1.65
    },
    {
      "text": "- What about RL with feedback\nside RLHF versus RLAIF?",
      "start": 7428.33,
      "duration": 4.053
    },
    {
      "text": "What's the role of that",
      "start": 7433.79,
      "duration": 1.29
    },
    {
      "text": "in getting better\nperformance on the models?",
      "start": 7435.08,
      "duration": 3.783
    },
    {
      "text": "- Yeah.",
      "start": 7440.09,
      "duration": 0.833
    },
    {
      "text": "So, RLHF is when the reward\nmodel you use is trained",
      "start": 7440.923,
      "duration": 4.497
    },
    {
      "text": "from some labels you've collected",
      "start": 7447.05,
      "duration": 2.82
    },
    {
      "text": "from humans giving feedback.",
      "start": 7449.87,
      "duration": 1.503
    },
    {
      "text": "I think this works if you have the ability",
      "start": 7452.93,
      "duration": 2.43
    },
    {
      "text": "to get a ton of human feedback",
      "start": 7455.36,
      "duration": 2.91
    },
    {
      "text": "for this kind of task that you care about.",
      "start": 7458.27,
      "duration": 2.133
    },
    {
      "text": "RLAIF is interesting",
      "start": 7461.768,
      "duration": 1.942
    },
    {
      "text": "because it's depending on the\nconstraint that verification",
      "start": 7470.688,
      "duration": 2.522
    },
    {
      "text": "is actually a decent bit\neasier than generation.",
      "start": 7473.21,
      "duration": 3.66
    },
    {
      "text": "Because it feels like,\nokay, what are you doing?",
      "start": 7476.87,
      "duration": 2.07
    },
    {
      "text": "Are you using this language model",
      "start": 7478.94,
      "duration": 1.14
    },
    {
      "text": "to look at the language model outputs",
      "start": 7480.08,
      "duration": 1.23
    },
    {
      "text": "and then prove the language model?",
      "start": 7481.31,
      "duration": 1.35
    },
    {
      "text": "But no, it actually may work",
      "start": 7482.66,
      "duration": 1.98
    },
    {
      "text": "if the language model has a\nmuch easier time verifying",
      "start": 7484.64,
      "duration": 3.93
    },
    {
      "text": "some solution than it does generating it.",
      "start": 7488.57,
      "duration": 2.31
    },
    {
      "text": "Then you actually could perhaps\nget this recursive loop.",
      "start": 7490.88,
      "duration": 3.33
    },
    {
      "text": "But I don't think it's gonna\nlook exactly like that.",
      "start": 7494.21,
      "duration": 2.61
    },
    {
      "text": "The other thing you could\ndo, that we kind of do,",
      "start": 7496.82,
      "duration": 4.26
    },
    {
      "text": "is a little bit of a\nmix of RLAIF and RLHF,",
      "start": 7501.08,
      "duration": 4.35
    },
    {
      "text": "where usually the model\nis actually quite correct",
      "start": 7505.43,
      "duration": 2.22
    },
    {
      "text": "and this is the case of\nprecursor tap picking",
      "start": 7507.65,
      "duration": 3.48
    },
    {
      "text": "between two possible generations\nof what is the better one.",
      "start": 7511.13,
      "duration": 4.11
    },
    {
      "text": "And then, it just needs a\nlittle bit of human nudging",
      "start": 7515.24,
      "duration": 3.63
    },
    {
      "text": "with only on the order 50, 100 examples",
      "start": 7518.87,
      "duration": 4.0
    },
    {
      "text": "to align that prior the model has",
      "start": 7524.09,
      "duration": 3.18
    },
    {
      "text": "with exactly with what you want.",
      "start": 7527.27,
      "duration": 1.95
    },
    {
      "text": "It looks different than\nI think normal RLHF",
      "start": 7529.22,
      "duration": 2.262
    },
    {
      "text": "where you're usually\ntraining these reward models",
      "start": 7531.482,
      "duration": 1.638
    },
    {
      "text": "in tons of examples.",
      "start": 7533.12,
      "duration": 1.383
    },
    {
      "text": "- What's your intuition\nwhen you compare generation",
      "start": 7535.94,
      "duration": 3.39
    },
    {
      "text": "and verification or\ngeneration and ranking?",
      "start": 7539.33,
      "duration": 2.15
    },
    {
      "text": "Is ranking way easier than generation?",
      "start": 7542.33,
      "duration": 3.51
    },
    {
      "text": "- My intuition would just\nsay, yeah, it should be.",
      "start": 7545.84,
      "duration": 2.943
    },
    {
      "text": "Like, if you believe P does not equal NP,",
      "start": 7554.093,
      "duration": 2.517
    },
    {
      "text": "then there's this\nmassive class of problems",
      "start": 7556.61,
      "duration": 2.94
    },
    {
      "text": "that are much, much easier\nto verify given proof,",
      "start": 7559.55,
      "duration": 2.7
    },
    {
      "text": "than actually proving it.",
      "start": 7562.25,
      "duration": 1.65
    },
    {
      "text": "- I wonder if the same thing\nwill prove P not equal to NP",
      "start": 7563.9,
      "duration": 3.33
    },
    {
      "text": "or P equal to NP.",
      "start": 7567.23,
      "duration": 1.624
    },
    {
      "text": "- (chuckles) That would be really cool.",
      "start": 7568.854,
      "duration": 2.786
    },
    {
      "text": "- That'd be a whatever Field's Medal",
      "start": 7571.64,
      "duration": 2.607
    },
    {
      "text": "(group giggling)",
      "start": 7574.247,
      "duration": 1.323
    },
    {
      "text": "by AI.",
      "start": 7575.57,
      "duration": 0.833
    },
    {
      "text": "Who gets the credit?",
      "start": 7576.403,
      "duration": 1.387
    },
    {
      "text": "Another the open philosophical question.",
      "start": 7577.79,
      "duration": 2.135
    },
    {
      "text": "(group chuckling)",
      "start": 7579.925,
      "duration": 2.3
    },
    {
      "text": "- Whoever prompted it.",
      "start": 7582.225,
      "duration": 0.833
    },
    {
      "text": "(group chuckling)",
      "start": 7583.058,
      "duration": 1.122
    },
    {
      "text": "- I'm actually surprisingly curious",
      "start": 7584.18,
      "duration": 2.326
    },
    {
      "text": "what a good bet for one AI",
      "start": 7586.506,
      "duration": 3.254
    },
    {
      "text": "will get the Field's Medal will be.",
      "start": 7589.76,
      "duration": 1.363
    },
    {
      "text": "I actually don't have-\n- Isn't this",
      "start": 7591.123,
      "duration": 1.182
    },
    {
      "text": "Aman's specialty?",
      "start": 7592.305,
      "duration": 0.833
    },
    {
      "text": "- I don't know what Aman's bet here is.",
      "start": 7593.138,
      "duration": 2.262
    },
    {
      "text": "- Oh, sorry, Nobel Prize\nor Field's Medal first?",
      "start": 7595.4,
      "duration": 2.13
    },
    {
      "text": "- Field's Medal-\n- Oh, Field's Medal level?",
      "start": 7597.53,
      "duration": 2.19
    },
    {
      "text": "- Field's Medal comes first, I think.",
      "start": 7599.72,
      "duration": 1.53
    },
    {
      "text": "- Field's Medal comes first.",
      "start": 7601.25,
      "duration": 1.26
    },
    {
      "text": "Well, you would say that, of course.",
      "start": 7602.51,
      "duration": 1.743
    },
    {
      "text": "(group chuckling)",
      "start": 7604.253,
      "duration": 0.833
    },
    {
      "text": "- But it's also this\nisolated system you verify.",
      "start": 7605.086,
      "duration": 2.794
    },
    {
      "text": "- Sure.\n- Yeah.",
      "start": 7607.88,
      "duration": 1.05
    },
    {
      "text": "- I don't even know if I-",
      "start": 7608.93,
      "duration": 0.833
    },
    {
      "text": "- You don't need to do (indistinct).",
      "start": 7609.763,
      "duration": 0.833
    },
    {
      "text": "- I feel like I have\nmuch more to do there.",
      "start": 7610.596,
      "duration": 1.154
    },
    {
      "text": "It felt like the path",
      "start": 7611.75,
      "duration": 0.96
    },
    {
      "text": "to get to IMO was a little bit more clear.",
      "start": 7612.71,
      "duration": 2.43
    },
    {
      "text": "Because it already could\nget a few IMO problems",
      "start": 7615.14,
      "duration": 2.56
    },
    {
      "text": "and there was a bunch\nof low-hanging fruit,",
      "start": 7619.255,
      "duration": 1.105
    },
    {
      "text": "given the literature at the time,",
      "start": 7620.36,
      "duration": 1.23
    },
    {
      "text": "of what tactics people could take.",
      "start": 7621.59,
      "duration": 2.4
    },
    {
      "text": "I think I'm, one, much less versed",
      "start": 7623.99,
      "duration": 2.1
    },
    {
      "text": "in the space of theorem proving now.",
      "start": 7626.09,
      "duration": 1.53
    },
    {
      "text": "And two, less intuition about\nhow close we are to solving",
      "start": 7627.62,
      "duration": 4.5
    },
    {
      "text": "these really, really hard open problems.",
      "start": 7632.12,
      "duration": 3.45
    },
    {
      "text": "- So you think you'll\nbe Field's Medal first?",
      "start": 7635.57,
      "duration": 1.68
    },
    {
      "text": "It won't be in physics or in-",
      "start": 7637.25,
      "duration": 3.12
    },
    {
      "text": "- Oh, 100%, I think that's\nprobably more likely.",
      "start": 7640.37,
      "duration": 3.44
    },
    {
      "text": "It is probably much more\nlikely that it'll get in.",
      "start": 7643.81,
      "duration": 3.081
    },
    {
      "text": "Yeah, yeah, yeah.",
      "start": 7646.891,
      "duration": 0.833
    },
    {
      "text": "Well, I think it both\nto, I don't know, BSD,",
      "start": 7647.724,
      "duration": 2.456
    },
    {
      "text": "which is a Birch and\nSwinnerton-Dyer conjecture,",
      "start": 7650.18,
      "duration": 2.101
    },
    {
      "text": "or (indistinct) iPods,",
      "start": 7652.281,
      "duration": 1.349
    },
    {
      "text": "or any one of these hard math problems",
      "start": 7653.63,
      "duration": 3.03
    },
    {
      "text": "are just actually really hard.",
      "start": 7656.66,
      "duration": 1.83
    },
    {
      "text": "It's unclear what the path",
      "start": 7658.49,
      "duration": 1.71
    },
    {
      "text": "to get even a solution looks like.",
      "start": 7660.2,
      "duration": 2.94
    },
    {
      "text": "We don't even know what a path looks like,",
      "start": 7663.14,
      "duration": 1.44
    },
    {
      "text": "let alone (indistinct).",
      "start": 7664.58,
      "duration": 1.15
    },
    {
      "text": "- And you don't buy the idea",
      "start": 7667.138,
      "duration": 0.833
    },
    {
      "text": "this is just like an isolated system",
      "start": 7667.971,
      "duration": 1.079
    },
    {
      "text": "and you can actually have\na good reward system,",
      "start": 7669.05,
      "duration": 2.25
    },
    {
      "text": "and it feels like it's\neasier to train for that.",
      "start": 7671.3,
      "duration": 4.71
    },
    {
      "text": "- I think we might get\nField's Medal before AGI.",
      "start": 7676.01,
      "duration": 2.973
    },
    {
      "text": "- I mean, I'd be very happy.",
      "start": 7679.82,
      "duration": 2.313
    },
    {
      "text": "I'd be very happy.",
      "start": 7683.45,
      "duration": 1.05
    },
    {
      "text": "But I don't know if I think 2028, 2030.",
      "start": 7684.5,
      "duration": 4.381
    },
    {
      "text": "(Aman chuckles)",
      "start": 7688.881,
      "duration": 0.833
    },
    {
      "text": "- For Field's Medal?",
      "start": 7689.714,
      "duration": 1.176
    },
    {
      "text": "- Field's Medal.\n- All right.",
      "start": 7690.89,
      "duration": 2.07
    },
    {
      "text": "It feels like forever from now,",
      "start": 7692.96,
      "duration": 2.07
    },
    {
      "text": "given how fast things have been going.",
      "start": 7695.03,
      "duration": 2.52
    },
    {
      "text": "Speaking of how fast\nthings have been going,",
      "start": 7697.55,
      "duration": 1.59
    },
    {
      "text": "let's talk about scaling laws.",
      "start": 7699.14,
      "duration": 2.31
    },
    {
      "text": "So, for people who don't know,",
      "start": 7701.45,
      "duration": 1.563
    },
    {
      "text": "maybe it's good to talk\nabout this whole idea",
      "start": 7703.91,
      "duration": 4.087
    },
    {
      "text": "of scaling laws.",
      "start": 7709.19,
      "duration": 0.84
    },
    {
      "text": "What are they, where'd you think stand,",
      "start": 7710.03,
      "duration": 2.13
    },
    {
      "text": "and where do you think things are going?",
      "start": 7712.16,
      "duration": 2.447
    },
    {
      "text": "- I think it was interesting.",
      "start": 7714.607,
      "duration": 0.833
    },
    {
      "text": "The original scaling laws paper",
      "start": 7715.44,
      "duration": 1.1
    },
    {
      "text": "by OpenAI was slightly wrong.",
      "start": 7716.54,
      "duration": 1.47
    },
    {
      "text": "'Cause I think of some issues they did",
      "start": 7718.01,
      "duration": 2.49
    },
    {
      "text": "with learning right schedules.",
      "start": 7720.5,
      "duration": 2.91
    },
    {
      "text": "And then, Chinchilla showed\na more correct version.",
      "start": 7723.41,
      "duration": 3.15
    },
    {
      "text": "And then, from then\npeople have again deviated",
      "start": 7726.56,
      "duration": 2.61
    },
    {
      "text": "from doing the compute optimal thing.",
      "start": 7729.17,
      "duration": 1.23
    },
    {
      "text": "'Cause people start now optimizing more so",
      "start": 7730.4,
      "duration": 2.97
    },
    {
      "text": "for making the thing work really well",
      "start": 7733.37,
      "duration": 3.208
    },
    {
      "text": "given an inference budget.",
      "start": 7736.578,
      "duration": 2.375
    },
    {
      "text": "And I think there are a lot\nmore dimensions to these curves",
      "start": 7739.79,
      "duration": 3.51
    },
    {
      "text": "than what we originally used,",
      "start": 7743.3,
      "duration": 1.62
    },
    {
      "text": "of just compute number\nof parameters and data.",
      "start": 7744.92,
      "duration": 4.743
    },
    {
      "text": "Like inference compute is the obvious one.",
      "start": 7750.56,
      "duration": 2.07
    },
    {
      "text": "I think context length\nis another obvious one.",
      "start": 7752.63,
      "duration": 2.88
    },
    {
      "text": "Let's say you care about the two things",
      "start": 7755.51,
      "duration": 1.35
    },
    {
      "text": "of inference compute\nand then context window,",
      "start": 7756.86,
      "duration": 4.44
    },
    {
      "text": "maybe the thing you wanna\ntrain is some kind of SSM.",
      "start": 7761.3,
      "duration": 3.42
    },
    {
      "text": "Because they're much,\nmuch cheaper and faster",
      "start": 7764.72,
      "duration": 2.82
    },
    {
      "text": "at super, super long context.",
      "start": 7767.54,
      "duration": 1.47
    },
    {
      "text": "And even if, maybe it was\n10 X more scaling properties",
      "start": 7769.01,
      "duration": 2.7
    },
    {
      "text": "during training, meaning,\nyou spend 10 X more compute",
      "start": 7771.71,
      "duration": 2.88
    },
    {
      "text": "to train the thing to get the\nsame level of capabilities,",
      "start": 7774.59,
      "duration": 3.3
    },
    {
      "text": "it's worth it",
      "start": 7777.89,
      "duration": 0.96
    },
    {
      "text": "because you care most\nabout that inference budget",
      "start": 7778.85,
      "duration": 2.55
    },
    {
      "text": "for really long context windows.",
      "start": 7781.4,
      "duration": 2.1
    },
    {
      "text": "So, it'll be interesting to see",
      "start": 7783.5,
      "duration": 1.02
    },
    {
      "text": "how people play with all these dimensions.",
      "start": 7784.52,
      "duration": 3.06
    },
    {
      "text": "- So, yeah, I mean, you speak",
      "start": 7787.58,
      "duration": 0.9
    },
    {
      "text": "to the multiple dimensions, obviously.",
      "start": 7788.48,
      "duration": 1.47
    },
    {
      "text": "The original conception was\njust looking at the variables",
      "start": 7789.95,
      "duration": 2.55
    },
    {
      "text": "of the size of the model\nas measured by parameters,",
      "start": 7792.5,
      "duration": 3.03
    },
    {
      "text": "and the size of the data",
      "start": 7795.53,
      "duration": 1.08
    },
    {
      "text": "as measured by the number of tokens,",
      "start": 7796.61,
      "duration": 1.68
    },
    {
      "text": "and looking at the ratio of the two.",
      "start": 7798.29,
      "duration": 1.53
    },
    {
      "text": "- Yeah.",
      "start": 7799.82,
      "duration": 0.9
    },
    {
      "text": "- And it's kind of a compelling notion",
      "start": 7800.72,
      "duration": 1.83
    },
    {
      "text": "that there is a number,\nor at least a minimum.",
      "start": 7802.55,
      "duration": 3.84
    },
    {
      "text": "And it seems like one was emerging.",
      "start": 7806.39,
      "duration": 1.833
    },
    {
      "text": "Do you still believe",
      "start": 7810.44,
      "duration": 0.87
    },
    {
      "text": "that there is a kind of bigger is better?",
      "start": 7811.31,
      "duration": 2.943
    },
    {
      "text": "- I mean, I think bigger\nis certainly better",
      "start": 7815.72,
      "duration": 3.36
    },
    {
      "text": "for just raw performance.",
      "start": 7819.08,
      "duration": 2.46
    },
    {
      "text": "- And raw intelligence.",
      "start": 7821.54,
      "duration": 0.93
    },
    {
      "text": "- And raw intelligence.",
      "start": 7822.47,
      "duration": 1.11
    },
    {
      "text": "I think the path that people might take,",
      "start": 7823.58,
      "duration": 2.07
    },
    {
      "text": "I'm particularly bullish on distillation.",
      "start": 7825.65,
      "duration": 2.67
    },
    {
      "text": "And how many knobs can you turn to,",
      "start": 7828.32,
      "duration": 2.85
    },
    {
      "text": "if we spend a ton, ton\nof money on training,",
      "start": 7831.17,
      "duration": 3.87
    },
    {
      "text": "get the most capable cheap model.",
      "start": 7835.04,
      "duration": 3.48
    },
    {
      "text": "Really, really caring as much as you can.",
      "start": 7838.52,
      "duration": 1.86
    },
    {
      "text": "'Cause the naive version of\ncaring as much as you can",
      "start": 7840.38,
      "duration": 2.61
    },
    {
      "text": "about inference time compute,",
      "start": 7842.99,
      "duration": 0.833
    },
    {
      "text": "is what people have already\ndone with the Llama models.",
      "start": 7843.823,
      "duration": 2.197
    },
    {
      "text": "Or just over-training\nthe shit out of 7B models",
      "start": 7846.02,
      "duration": 4.47
    },
    {
      "text": "on way, way, way more tokens\nthan is essential optimal.",
      "start": 7850.49,
      "duration": 3.69
    },
    {
      "text": "But if you really care about it,",
      "start": 7854.18,
      "duration": 0.96
    },
    {
      "text": "maybe the thing to do is what Gamma did,",
      "start": 7855.14,
      "duration": 1.26
    },
    {
      "text": "which is let's not just train on tokens,",
      "start": 7856.4,
      "duration": 2.64
    },
    {
      "text": "let's literally train on\nminimizing the KL divergence",
      "start": 7859.04,
      "duration": 5.0
    },
    {
      "text": "with the distribution of gemma 27B, right?",
      "start": 7864.47,
      "duration": 4.02
    },
    {
      "text": "So knowledge distillation there.",
      "start": 7868.49,
      "duration": 1.6
    },
    {
      "text": "And you're spending the compute",
      "start": 7870.98,
      "duration": 1.74
    },
    {
      "text": "of literally training this\n27 billion parameter model",
      "start": 7872.72,
      "duration": 3.87
    },
    {
      "text": "on all these tokens, just to get out this,",
      "start": 7876.59,
      "duration": 2.589
    },
    {
      "text": "I don't know, smaller model.",
      "start": 7879.179,
      "duration": 1.131
    },
    {
      "text": "- And the distillation gives\nyou just a faster model,",
      "start": 7880.31,
      "duration": 2.16
    },
    {
      "text": "smaller means faster.",
      "start": 7882.47,
      "duration": 1.38
    },
    {
      "text": "- Yeah, distillation in theory is,",
      "start": 7883.85,
      "duration": 1.833
    },
    {
      "text": "I think, getting out more signal",
      "start": 7887.48,
      "duration": 1.59
    },
    {
      "text": "from the data that you're training on.",
      "start": 7889.07,
      "duration": 2.612
    },
    {
      "text": "And it's perhaps another\nway of getting over,",
      "start": 7891.682,
      "duration": 2.098
    },
    {
      "text": "not completely over,",
      "start": 7893.78,
      "duration": 1.23
    },
    {
      "text": "but partially helping with the data wall.",
      "start": 7895.01,
      "duration": 2.64
    },
    {
      "text": "Where you only have so\nmuch data to train on,",
      "start": 7897.65,
      "duration": 1.74
    },
    {
      "text": "let's train this really, really big model",
      "start": 7899.39,
      "duration": 1.77
    },
    {
      "text": "on all these tokens",
      "start": 7901.16,
      "duration": 1.14
    },
    {
      "text": "and we'll distill it\ninto this smaller one.",
      "start": 7902.3,
      "duration": 1.5
    },
    {
      "text": "And maybe we can get more signal per token",
      "start": 7903.8,
      "duration": 3.76
    },
    {
      "text": "for this much smaller model",
      "start": 7908.433,
      "duration": 1.607
    },
    {
      "text": "than we would've originally\nif we trained it.",
      "start": 7910.04,
      "duration": 1.56
    },
    {
      "text": "- So if I gave you $10 trillion,\nhow would you spend it?",
      "start": 7911.6,
      "duration": 4.143
    },
    {
      "text": "(Aman chuckles)",
      "start": 7915.743,
      "duration": 0.833
    },
    {
      "text": "I mean, you can't buy\nan island or whatever.",
      "start": 7916.576,
      "duration": 2.044
    },
    {
      "text": "How would you allocate it",
      "start": 7918.62,
      "duration": 1.74
    },
    {
      "text": "in terms of improving the big model",
      "start": 7920.36,
      "duration": 3.24
    },
    {
      "text": "versus maybe paying for HF in the RLHF?",
      "start": 7923.6,
      "duration": 5.0
    },
    {
      "text": "- Yeah, yeah.",
      "start": 7929.656,
      "duration": 1.294
    },
    {
      "text": "I think, there's a lot of\nthese secrets and details",
      "start": 7930.95,
      "duration": 4.29
    },
    {
      "text": "about training these large\nmodels that I just don't know,",
      "start": 7935.24,
      "duration": 3.15
    },
    {
      "text": "and are only privy to the large labs.",
      "start": 7938.39,
      "duration": 1.56
    },
    {
      "text": "And the issue is, I would\nwaste a lot of that money",
      "start": 7939.95,
      "duration": 2.67
    },
    {
      "text": "if I even attempted this,",
      "start": 7942.62,
      "duration": 1.41
    },
    {
      "text": "because I wouldn't know those things.",
      "start": 7944.03,
      "duration": 2.31
    },
    {
      "text": "Suspending a lot of disbelief",
      "start": 7946.34,
      "duration": 1.86
    },
    {
      "text": "and assuming you had the know-how,",
      "start": 7948.2,
      "duration": 2.493
    },
    {
      "text": "or if you're saying you have to operate",
      "start": 7952.539,
      "duration": 2.681
    },
    {
      "text": "with the limited information you have now.",
      "start": 7955.22,
      "duration": 2.58
    },
    {
      "text": "- No, no, no, actually, I would say,",
      "start": 7957.8,
      "duration": 2.22
    },
    {
      "text": "you swoop in and you\nget all the information,",
      "start": 7960.02,
      "duration": 2.01
    },
    {
      "text": "all the little heuristics,\nall the little parameters,",
      "start": 7962.03,
      "duration": 2.52
    },
    {
      "text": "all the parameters that define\nhow the thing is trained.",
      "start": 7964.55,
      "duration": 4.71
    },
    {
      "text": "- Mm-hmm.",
      "start": 7969.26,
      "duration": 1.35
    },
    {
      "text": "- If we look in how to invest\nmoney for the next five years",
      "start": 7970.61,
      "duration": 3.72
    },
    {
      "text": "in terms of maximizing what\nyou called raw intelligence.",
      "start": 7974.33,
      "duration": 3.15
    },
    {
      "text": "- I mean, isn't the answer really simple?",
      "start": 7977.48,
      "duration": 2.37
    },
    {
      "text": "You just try to get as\nmuch compute as possible.",
      "start": 7979.85,
      "duration": 3.145
    },
    {
      "text": "At the end of the day, all\nyou need to buy is the GPUs.",
      "start": 7982.995,
      "duration": 2.75
    },
    {
      "text": "You can tune whether you\nwant to pre-train a big model",
      "start": 7990.2,
      "duration": 3.27
    },
    {
      "text": "or a small model.",
      "start": 7993.47,
      "duration": 1.74
    },
    {
      "text": "- Well, this gets into the question",
      "start": 7995.21,
      "duration": 0.947
    },
    {
      "text": "of are you really limited\nby compute and money,",
      "start": 7996.157,
      "duration": 2.743
    },
    {
      "text": "or are you limited by these other things?",
      "start": 7998.9,
      "duration": 2.103
    },
    {
      "text": "- I'm more privy to Arvid's\nbelief that we're idea-limited,",
      "start": 8002.23,
      "duration": 3.6
    },
    {
      "text": "but there's always that like-",
      "start": 8005.83,
      "duration": 1.95
    },
    {
      "text": "- But if you have a lot of compute,",
      "start": 8007.78,
      "duration": 2.88
    },
    {
      "text": "you can run a lot of experiments.",
      "start": 8010.66,
      "duration": 2.13
    },
    {
      "text": "- So you would run a lot of experiments",
      "start": 8012.79,
      "duration": 2.16
    },
    {
      "text": "versus use that compute\nto trend a gigantic model?",
      "start": 8014.95,
      "duration": 3.63
    },
    {
      "text": "- I would, but I do\nbelieve that we are limited",
      "start": 8018.58,
      "duration": 3.99
    },
    {
      "text": "in terms of ideas that we have.",
      "start": 8022.57,
      "duration": 2.07
    },
    {
      "text": "- I think yeah, 'cause\neven with all this compute,",
      "start": 8024.64,
      "duration": 3.36
    },
    {
      "text": "and all the data you could\ncollect in the world,",
      "start": 8028.0,
      "duration": 3.03
    },
    {
      "text": "I think you really are ultimately\nlimited by not even ideas,",
      "start": 8031.03,
      "duration": 3.75
    },
    {
      "text": "but just really good engineering.",
      "start": 8034.78,
      "duration": 3.573
    },
    {
      "text": "There aren't that many people in the world",
      "start": 8043.63,
      "duration": 1.95
    },
    {
      "text": "who really can make the difference here.",
      "start": 8045.58,
      "duration": 2.46
    },
    {
      "text": "And there's so much work\nthat goes into research",
      "start": 8048.04,
      "duration": 3.66
    },
    {
      "text": "that is just pure, really,\nreally hard engineering work.",
      "start": 8051.7,
      "duration": 4.11
    },
    {
      "text": "As a very hand-wavy example,",
      "start": 8055.81,
      "duration": 2.94
    },
    {
      "text": "if you look at the\noriginal Transformer paper,",
      "start": 8058.75,
      "duration": 2.16
    },
    {
      "text": "how much work was joining together a lot",
      "start": 8060.91,
      "duration": 2.22
    },
    {
      "text": "of these really interesting\nconcepts embedded",
      "start": 8063.13,
      "duration": 2.67
    },
    {
      "text": "in the literature, versus then going in",
      "start": 8065.8,
      "duration": 2.97
    },
    {
      "text": "and writing all the codes,\nmaybe the CUDA kernels,",
      "start": 8068.77,
      "duration": 2.4
    },
    {
      "text": "maybe whatever else.",
      "start": 8071.17,
      "duration": 0.833
    },
    {
      "text": "I don't know if it ran them GPUs or TPUs.",
      "start": 8072.003,
      "duration": 1.327
    },
    {
      "text": "Originally, such that\nit actually saturated",
      "start": 8073.33,
      "duration": 2.52
    },
    {
      "text": "the GPU performance.",
      "start": 8075.85,
      "duration": 2.58
    },
    {
      "text": "Getting GNOME Azure to go\nin and do all this code.",
      "start": 8078.43,
      "duration": 2.79
    },
    {
      "text": "And GNOME is probably\none of the best engineers",
      "start": 8081.22,
      "duration": 1.74
    },
    {
      "text": "in the world.",
      "start": 8082.96,
      "duration": 0.833
    },
    {
      "text": "Or maybe going a step further,",
      "start": 8083.793,
      "duration": 1.387
    },
    {
      "text": "like the next generation of\nmodels, having these things.",
      "start": 8085.18,
      "duration": 2.58
    },
    {
      "text": "Like getting model parallelism to work,",
      "start": 8087.76,
      "duration": 1.74
    },
    {
      "text": "and scaling it on thousands of,",
      "start": 8089.5,
      "duration": 2.06
    },
    {
      "text": "or maybe tens of thousands of V100s,",
      "start": 8091.56,
      "duration": 2.77
    },
    {
      "text": "which I think GBDE-III may have been.",
      "start": 8094.33,
      "duration": 2.85
    },
    {
      "text": "There's just so much engineering effort",
      "start": 8097.18,
      "duration": 1.59
    },
    {
      "text": "that has to go into all of\nthese things to make it work.",
      "start": 8098.77,
      "duration": 3.033
    },
    {
      "text": "If you really brought that\ncost down to maybe not zero,",
      "start": 8103.18,
      "duration": 5.0
    },
    {
      "text": "but just made it 10 X easier,\nmade it super easy for someone",
      "start": 8108.37,
      "duration": 3.27
    },
    {
      "text": "with really fantastic ideas,",
      "start": 8111.64,
      "duration": 1.98
    },
    {
      "text": "to immediately get to the version",
      "start": 8113.62,
      "duration": 2.16
    },
    {
      "text": "of the new architecture they dreamed up,",
      "start": 8115.78,
      "duration": 1.77
    },
    {
      "text": "that is getting 50, 40%\nutilization on their GPUs,",
      "start": 8117.55,
      "duration": 5.0
    },
    {
      "text": "I think that would just\nspeed up research by a ton.",
      "start": 8122.89,
      "duration": 4.83
    },
    {
      "text": "- I mean, I think if you see\na clear path to improvement,",
      "start": 8127.72,
      "duration": 3.0
    },
    {
      "text": "you should always take the\nlow-hanging fruit first, right?",
      "start": 8130.72,
      "duration": 2.94
    },
    {
      "text": "I think probably OpenAI\nand all the other labs",
      "start": 8133.66,
      "duration": 3.12
    },
    {
      "text": "that did the right thing to\npick off the low-hanging fruit.",
      "start": 8136.78,
      "duration": 2.55
    },
    {
      "text": "Where the low-hanging fruit is like,",
      "start": 8139.33,
      "duration": 1.8
    },
    {
      "text": "you could scale up to a GPT-4.25 scale",
      "start": 8143.2,
      "duration": 4.45
    },
    {
      "text": "and you just keep scaling,",
      "start": 8149.62,
      "duration": 1.44
    },
    {
      "text": "and things keep getting better.",
      "start": 8151.06,
      "duration": 2.403
    },
    {
      "text": "There's no point of\nexperimenting with new ideas",
      "start": 8155.83,
      "duration": 1.68
    },
    {
      "text": "when everything is working.",
      "start": 8157.51,
      "duration": 2.07
    },
    {
      "text": "And you should bang on",
      "start": 8159.58,
      "duration": 2.037
    },
    {
      "text": "and to try to get as much as\nmuch juice out of the possible.",
      "start": 8161.617,
      "duration": 3.0
    },
    {
      "text": "I think if you're spending $10 trillion,",
      "start": 8167.41,
      "duration": 1.317
    },
    {
      "text": "you probably wanna spend some,",
      "start": 8168.727,
      "duration": 2.853
    },
    {
      "text": "then actually reevaluate your ideas,",
      "start": 8171.58,
      "duration": 1.98
    },
    {
      "text": "probably your idea a\nlittle bit at that point.",
      "start": 8173.56,
      "duration": 2.01
    },
    {
      "text": "- I think all of us believe\nnew ideas are probably needed",
      "start": 8175.57,
      "duration": 3.63
    },
    {
      "text": "to get all the way there to AGI.",
      "start": 8179.2,
      "duration": 3.69
    },
    {
      "text": "And all of us also probably believe",
      "start": 8182.89,
      "duration": 4.38
    },
    {
      "text": "there exist ways of\ntesting out those ideas",
      "start": 8187.27,
      "duration": 3.03
    },
    {
      "text": "at smaller scales, and\nbeing fairly confident",
      "start": 8190.3,
      "duration": 3.87
    },
    {
      "text": "that they'll play out.",
      "start": 8194.17,
      "duration": 1.62
    },
    {
      "text": "It's just quite difficult for the labs",
      "start": 8195.79,
      "duration": 3.39
    },
    {
      "text": "in their current position",
      "start": 8199.18,
      "duration": 1.56
    },
    {
      "text": "to dedicate their very limited research",
      "start": 8200.74,
      "duration": 2.88
    },
    {
      "text": "and engineering talent to\nexploring all these other ideas,",
      "start": 8203.62,
      "duration": 3.75
    },
    {
      "text": "when there's this core thing",
      "start": 8207.37,
      "duration": 1.32
    },
    {
      "text": "that will probably improve performance",
      "start": 8208.69,
      "duration": 4.05
    },
    {
      "text": "for some decent amount of time.",
      "start": 8212.74,
      "duration": 1.923
    },
    {
      "text": "- Yeah, but also, these\nbig labs like winning.",
      "start": 8216.16,
      "duration": 2.3
    },
    {
      "text": "(Lex chuckles)",
      "start": 8219.44,
      "duration": 1.344
    },
    {
      "text": "So, they're just going wild.",
      "start": 8220.784,
      "duration": 1.706
    },
    {
      "text": "Okay.",
      "start": 8222.49,
      "duration": 1.059
    },
    {
      "text": "(all chuckling)",
      "start": 8223.549,
      "duration": 1.221
    },
    {
      "text": "So, big question, looking\nout into the future.",
      "start": 8224.77,
      "duration": 3.66
    },
    {
      "text": "You're now at the center\nof the programming world.",
      "start": 8228.43,
      "duration": 3.63
    },
    {
      "text": "How do you think programming,",
      "start": 8232.06,
      "duration": 1.26
    },
    {
      "text": "the nature of programming\nchanges in the next few months,",
      "start": 8233.32,
      "duration": 3.63
    },
    {
      "text": "in the next year, in the next two years",
      "start": 8236.95,
      "duration": 1.69
    },
    {
      "text": "and the next five years, 10 years?",
      "start": 8238.64,
      "duration": 2.24
    },
    {
      "text": "- I think we're really\nexcited about a future",
      "start": 8240.88,
      "duration": 2.97
    },
    {
      "text": "where the programmer\nis in the driver's seat",
      "start": 8243.85,
      "duration": 2.52
    },
    {
      "text": "for a long time.",
      "start": 8246.37,
      "duration": 1.65
    },
    {
      "text": "And you've heard us talk\nabout this a little bit,",
      "start": 8248.02,
      "duration": 2.37
    },
    {
      "text": "but one that emphasizes speed",
      "start": 8250.39,
      "duration": 3.12
    },
    {
      "text": "and agency for the programmer and control.",
      "start": 8253.51,
      "duration": 2.73
    },
    {
      "text": "The ability to modify\nanything you wanna modify,",
      "start": 8256.24,
      "duration": 2.46
    },
    {
      "text": "the ability to iterate really\nfast on what you're building.",
      "start": 8258.7,
      "duration": 3.06
    },
    {
      "text": "And this is a little different,",
      "start": 8261.76,
      "duration": 3.3
    },
    {
      "text": "I think, than where some people\nare jumping to in the space,",
      "start": 8265.06,
      "duration": 5.0
    },
    {
      "text": "where I think one idea\nthat's captivated people,",
      "start": 8270.55,
      "duration": 3.69
    },
    {
      "text": "is can you talk to your computer?",
      "start": 8274.24,
      "duration": 4.08
    },
    {
      "text": "Can you have it build software for you?",
      "start": 8278.32,
      "duration": 1.32
    },
    {
      "text": "As if you're talking to\nan engineering department",
      "start": 8279.64,
      "duration": 1.8
    },
    {
      "text": "or an engineer over Slack.",
      "start": 8281.44,
      "duration": 1.32
    },
    {
      "text": "And can it just be this\nsort of isolated text box?",
      "start": 8282.76,
      "duration": 2.94
    },
    {
      "text": "And part of the reason we're\nnot excited about that,",
      "start": 8285.7,
      "duration": 5.0
    },
    {
      "text": "is some of the stuff we've\ntalked about with latency,",
      "start": 8290.77,
      "duration": 2.07
    },
    {
      "text": "but then a big piece, a reason\nwe're not excited about that,",
      "start": 8292.84,
      "duration": 3.24
    },
    {
      "text": "is because that comes with\ngiving up a lot of control.",
      "start": 8296.08,
      "duration": 3.06
    },
    {
      "text": "It's much harder to be really specific",
      "start": 8299.14,
      "duration": 1.53
    },
    {
      "text": "when you're talking in the text box.",
      "start": 8300.67,
      "duration": 1.74
    },
    {
      "text": "And if you're necessarily\njust going to communicate",
      "start": 8302.41,
      "duration": 3.39
    },
    {
      "text": "with a thing like you\nwould be communicating",
      "start": 8305.8,
      "duration": 1.47
    },
    {
      "text": "with an engineering department,",
      "start": 8307.27,
      "duration": 0.84
    },
    {
      "text": "you're actually advocating tons",
      "start": 8308.11,
      "duration": 1.08
    },
    {
      "text": "of really important decisions to this bot.",
      "start": 8309.19,
      "duration": 3.363
    },
    {
      "text": "And this kind of gets at, fundamentally,",
      "start": 8313.63,
      "duration": 2.19
    },
    {
      "text": "what engineering is.",
      "start": 8315.82,
      "duration": 2.85
    },
    {
      "text": "I think that some people",
      "start": 8318.67,
      "duration": 1.83
    },
    {
      "text": "who are a little bit more\nremoved from engineering",
      "start": 8320.5,
      "duration": 1.41
    },
    {
      "text": "might think of it as the spec\nis completely written out",
      "start": 8321.91,
      "duration": 3.06
    },
    {
      "text": "and then the engineers just\ncome and they just implement.",
      "start": 8324.97,
      "duration": 2.97
    },
    {
      "text": "And it's just about making\nthe thing happen in code",
      "start": 8327.94,
      "duration": 2.07
    },
    {
      "text": "and making the thing exist.",
      "start": 8330.01,
      "duration": 2.043
    },
    {
      "text": "But I think a lot of the best engineering,",
      "start": 8333.13,
      "duration": 2.07
    },
    {
      "text": "the engineering we enjoy,",
      "start": 8335.2,
      "duration": 1.25
    },
    {
      "text": "involves tons of tiny micro decisions",
      "start": 8337.63,
      "duration": 1.95
    },
    {
      "text": "about what exactly you're building,",
      "start": 8339.58,
      "duration": 1.71
    },
    {
      "text": "and about really hard trade-offs\nbetween speed and cost",
      "start": 8341.29,
      "duration": 3.84
    },
    {
      "text": "and just all the other\nthings involved in a system.",
      "start": 8345.13,
      "duration": 2.853
    },
    {
      "text": "As long as humans are actually",
      "start": 8350.95,
      "duration": 2.52
    },
    {
      "text": "the ones designing the software",
      "start": 8353.47,
      "duration": 2.07
    },
    {
      "text": "and the ones specifying\nwhat they want to be built,",
      "start": 8355.54,
      "duration": 2.91
    },
    {
      "text": "and it's not just like\ncompany run by all AIs,",
      "start": 8358.45,
      "duration": 2.37
    },
    {
      "text": "we think you'll really want the human",
      "start": 8360.82,
      "duration": 1.85
    },
    {
      "text": "in a driver's seat\ndictating these decisions.",
      "start": 8362.67,
      "duration": 3.67
    },
    {
      "text": "And so the jury's still out\non what that looks like.",
      "start": 8366.34,
      "duration": 4.35
    },
    {
      "text": "I think that one weird idea\nfor what that could look like,",
      "start": 8370.69,
      "duration": 3.57
    },
    {
      "text": "is it could look like you can control",
      "start": 8374.26,
      "duration": 3.0
    },
    {
      "text": "the level of abstraction\nyou view a code base at.",
      "start": 8377.26,
      "duration": 2.55
    },
    {
      "text": "And you can point at specific\nparts of a code base,",
      "start": 8379.81,
      "duration": 3.603
    },
    {
      "text": "like, maybe you digest a\ncode base by looking at it",
      "start": 8384.921,
      "duration": 2.779
    },
    {
      "text": "in the form of pseudocode.",
      "start": 8387.7,
      "duration": 1.47
    },
    {
      "text": "And you can actually\nedit that pseudocode too,",
      "start": 8389.17,
      "duration": 3.45
    },
    {
      "text": "and then have changes get made down",
      "start": 8392.62,
      "duration": 1.77
    },
    {
      "text": "at the formal programming level.",
      "start": 8394.39,
      "duration": 2.19
    },
    {
      "text": "And you can gesture at any piece of logic",
      "start": 8396.58,
      "duration": 5.0
    },
    {
      "text": "in your software component of programming.",
      "start": 8401.86,
      "duration": 2.31
    },
    {
      "text": "You keep the inflow text editing\ncomponent of programming,",
      "start": 8404.17,
      "duration": 3.0
    },
    {
      "text": "you keep the control of, you\ncan even go down into the code,",
      "start": 8407.17,
      "duration": 2.94
    },
    {
      "text": "you can go at higher\nlevels of abstraction,",
      "start": 8410.11,
      "duration": 2.25
    },
    {
      "text": "while also giving you these\nbig productivity gains.",
      "start": 8412.36,
      "duration": 2.34
    },
    {
      "text": "- It'd be nice",
      "start": 8414.7,
      "duration": 0.833
    },
    {
      "text": "if you can go up and down\nthe abstraction stack.",
      "start": 8415.533,
      "duration": 2.797
    },
    {
      "text": "- Yeah.",
      "start": 8418.33,
      "duration": 0.833
    },
    {
      "text": "And there are a lot of\ndetails to figure out there",
      "start": 8419.163,
      "duration": 1.087
    },
    {
      "text": "that's sort of like a fuzzy idea.",
      "start": 8420.25,
      "duration": 1.59
    },
    {
      "text": "Time will tell if it actually works.",
      "start": 8421.84,
      "duration": 1.41
    },
    {
      "text": "But these principles of control and speed",
      "start": 8423.25,
      "duration": 2.087
    },
    {
      "text": "in the human in the driver's seat,",
      "start": 8425.337,
      "duration": 1.333
    },
    {
      "text": "we think are really important.",
      "start": 8426.67,
      "duration": 2.07
    },
    {
      "text": "We think for some things\nlike Arvid mentioned before,",
      "start": 8428.74,
      "duration": 2.4
    },
    {
      "text": "for some styles of programming,",
      "start": 8431.14,
      "duration": 1.26
    },
    {
      "text": "you can hand it off chatbot-style.",
      "start": 8432.4,
      "duration": 2.43
    },
    {
      "text": "If you have a bug that's\nreally well specified.",
      "start": 8434.83,
      "duration": 2.01
    },
    {
      "text": "But that's not most of programming,",
      "start": 8436.84,
      "duration": 2.46
    },
    {
      "text": "and that's also not\nmost of the programming",
      "start": 8439.3,
      "duration": 2.52
    },
    {
      "text": "we think a lot of people value.",
      "start": 8441.82,
      "duration": 1.68
    },
    {
      "text": "- What about the fundamental\nskill of programming?",
      "start": 8443.5,
      "duration": 2.61
    },
    {
      "text": "There's a lot of people, like\nyoung people right now scared,",
      "start": 8446.11,
      "duration": 4.487
    },
    {
      "text": "'cause they love programming,\nbut they're scared about,",
      "start": 8454.162,
      "duration": 2.095
    },
    {
      "text": "\"Will I be able to have a future",
      "start": 8456.257,
      "duration": 1.403
    },
    {
      "text": "if I pursue this career path?\"",
      "start": 8457.66,
      "duration": 2.16
    },
    {
      "text": "Do you think the very skill of programming",
      "start": 8459.82,
      "duration": 2.07
    },
    {
      "text": "will change fundamentally?",
      "start": 8461.89,
      "duration": 2.16
    },
    {
      "text": "- I actually think this is a\nreally, really exciting time",
      "start": 8464.05,
      "duration": 2.58
    },
    {
      "text": "to be building software.",
      "start": 8466.63,
      "duration": 1.77
    },
    {
      "text": "We remember what programming was like",
      "start": 8468.4,
      "duration": 3.21
    },
    {
      "text": "in 2013, 2012, whatever it was.",
      "start": 8471.61,
      "duration": 3.153
    },
    {
      "text": "And there was just so much\nmore cruft and boilerplate",
      "start": 8475.93,
      "duration": 4.5
    },
    {
      "text": "and looking up something really gnarly.",
      "start": 8480.43,
      "duration": 4.95
    },
    {
      "text": "And that stuff still exists,\nit's definitely not at zero.",
      "start": 8485.38,
      "duration": 3.3
    },
    {
      "text": "But programming today is\nway more fun than back then.",
      "start": 8488.68,
      "duration": 3.9
    },
    {
      "text": "It's like we're really getting down",
      "start": 8492.58,
      "duration": 2.06
    },
    {
      "text": "to the delight concentration.",
      "start": 8494.64,
      "duration": 2.11
    },
    {
      "text": "And all the things that really\ndraw people to programming,",
      "start": 8496.75,
      "duration": 3.12
    },
    {
      "text": "for instance, this element of being able",
      "start": 8499.87,
      "duration": 1.38
    },
    {
      "text": "to build things really fast and speed,",
      "start": 8501.25,
      "duration": 2.94
    },
    {
      "text": "and also individual control,",
      "start": 8504.19,
      "duration": 1.89
    },
    {
      "text": "all those are just being turned up a ton.",
      "start": 8506.08,
      "duration": 2.283
    },
    {
      "text": "And so I think it's gonna\nbe a really, really fun time",
      "start": 8509.26,
      "duration": 2.52
    },
    {
      "text": "for people who build software.",
      "start": 8511.78,
      "duration": 1.98
    },
    {
      "text": "I think that the skills\nwill probably change too.",
      "start": 8513.76,
      "duration": 2.4
    },
    {
      "text": "I think that people's\ntaste and creative ideas",
      "start": 8516.16,
      "duration": 2.49
    },
    {
      "text": "will be magnified.",
      "start": 8518.65,
      "duration": 1.2
    },
    {
      "text": "And it will be maybe less, a little bit,",
      "start": 8519.85,
      "duration": 3.3
    },
    {
      "text": "about boilerplate text editing.",
      "start": 8523.15,
      "duration": 2.01
    },
    {
      "text": "Maybe even a little bit\nless about carefulness,",
      "start": 8525.16,
      "duration": 2.67
    },
    {
      "text": "which I think is really important today",
      "start": 8527.83,
      "duration": 2.94
    },
    {
      "text": "if you're a programmer.",
      "start": 8530.77,
      "duration": 0.99
    },
    {
      "text": "I think it'll be a lot more fun.",
      "start": 8531.76,
      "duration": 1.65
    },
    {
      "text": "- What do you guys think?",
      "start": 8533.41,
      "duration": 1.77
    },
    {
      "text": "- I agree.",
      "start": 8535.18,
      "duration": 0.93
    },
    {
      "text": "I'm very excited to be able to change.",
      "start": 8536.11,
      "duration": 2.103
    },
    {
      "text": "One thing that happened recently,",
      "start": 8540.4,
      "duration": 2.4
    },
    {
      "text": "was we wanted to do a\nrelatively big migration",
      "start": 8542.8,
      "duration": 2.97
    },
    {
      "text": "to our code base.",
      "start": 8545.77,
      "duration": 0.833
    },
    {
      "text": "We were using\nAsyncLocalStorage in Node.js,",
      "start": 8546.603,
      "duration": 3.517
    },
    {
      "text": "which is known to be not very performant,",
      "start": 8550.12,
      "duration": 1.83
    },
    {
      "text": "and we wanted to migrate\nto a context object.",
      "start": 8551.95,
      "duration": 1.8
    },
    {
      "text": "And this is a big migration",
      "start": 8553.75,
      "duration": 1.68
    },
    {
      "text": "and affects the entire code base.",
      "start": 8555.43,
      "duration": 1.637
    },
    {
      "text": "Sualeh and I spent, I don't know,",
      "start": 8557.067,
      "duration": 2.803
    },
    {
      "text": "five days working through this,\neven with today's AI tools.",
      "start": 8559.87,
      "duration": 3.78
    },
    {
      "text": "And I am really excited for a future",
      "start": 8563.65,
      "duration": 3.42
    },
    {
      "text": "where I can just show a couple of examples",
      "start": 8567.07,
      "duration": 3.45
    },
    {
      "text": "and then the AI applies that\nto all of the locations.",
      "start": 8570.52,
      "duration": 3.6
    },
    {
      "text": "And then it highlights,",
      "start": 8574.12,
      "duration": 1.057
    },
    {
      "text": "\"Oh, this is a new\nexample, what should I do?\"",
      "start": 8575.177,
      "duration": 2.483
    },
    {
      "text": "And then, I show exactly what to do there.",
      "start": 8577.66,
      "duration": 1.8
    },
    {
      "text": "And then, that can be done in 10 minutes.",
      "start": 8579.46,
      "duration": 3.06
    },
    {
      "text": "And then, you can iterate\nmuch, much faster.",
      "start": 8582.52,
      "duration": 2.553
    },
    {
      "text": "Then, you don't have to\nthink as much upfront",
      "start": 8586.504,
      "duration": 1.776
    },
    {
      "text": "and stand at the blackboard and think,",
      "start": 8588.28,
      "duration": 2.377
    },
    {
      "text": "\"Exactly, how are we gonna do this,",
      "start": 8590.657,
      "duration": 1.703
    },
    {
      "text": "because the cost is so high?\"",
      "start": 8592.36,
      "duration": 1.41
    },
    {
      "text": "But you can just try something\nfirst and you realize,",
      "start": 8593.77,
      "duration": 2.647
    },
    {
      "text": "\"Oh, this is not actually\nexactly what I want.\"",
      "start": 8596.417,
      "duration": 2.212
    },
    {
      "text": "And then, you can change\nit instantly again after.",
      "start": 8598.629,
      "duration": 2.191
    },
    {
      "text": "And so, yeah, I think being\na programmer in the future",
      "start": 8600.82,
      "duration": 4.14
    },
    {
      "text": "is going to be a lot of fun.",
      "start": 8604.96,
      "duration": 1.62
    },
    {
      "text": "- Yeah, I really like that point.",
      "start": 8606.58,
      "duration": 2.223
    },
    {
      "text": "It feels like a lot of\nthe time with programming,",
      "start": 8609.85,
      "duration": 1.44
    },
    {
      "text": "there are two ways you can go about it.",
      "start": 8611.29,
      "duration": 2.28
    },
    {
      "text": "One is you think really\nhard, carefully upfront",
      "start": 8613.57,
      "duration": 3.69
    },
    {
      "text": "about the best possible way to do it,",
      "start": 8617.26,
      "duration": 2.49
    },
    {
      "text": "and then you spend your\nlimited time of engineering",
      "start": 8619.75,
      "duration": 2.46
    },
    {
      "text": "to actually implement it.",
      "start": 8622.21,
      "duration": 1.263
    },
    {
      "text": "But I must refer just getting in the code",
      "start": 8624.43,
      "duration": 1.65
    },
    {
      "text": "and taking a crack at\nseeing how it lays out",
      "start": 8626.08,
      "duration": 3.81
    },
    {
      "text": "and then iterating really quickly on that.",
      "start": 8629.89,
      "duration": 2.82
    },
    {
      "text": "That feels more fun.",
      "start": 8632.71,
      "duration": 1.113
    },
    {
      "text": "- Yeah, just speaking to generate\nthe boilerplate, is great.",
      "start": 8635.89,
      "duration": 3.42
    },
    {
      "text": "So you just focus on the nuanced,",
      "start": 8639.31,
      "duration": 3.42
    },
    {
      "text": "difficult design decisions.",
      "start": 8642.73,
      "duration": 1.59
    },
    {
      "text": "Migration, I feel like this is a cool one.",
      "start": 8644.32,
      "duration": 3.81
    },
    {
      "text": "It seems like a larger\nlanguage models is able",
      "start": 8648.13,
      "duration": 1.68
    },
    {
      "text": "to basically translate for one\nprogram language to another.",
      "start": 8649.81,
      "duration": 2.73
    },
    {
      "text": "Or translate, migrate in the general sense",
      "start": 8652.54,
      "duration": 3.69
    },
    {
      "text": "of what migrate is.",
      "start": 8656.23,
      "duration": 1.023
    },
    {
      "text": "But that's in the current moment.",
      "start": 8658.66,
      "duration": 2.04
    },
    {
      "text": "So mean the fear has to do with,",
      "start": 8660.7,
      "duration": 1.95
    },
    {
      "text": "okay, as these models\nget better and better,",
      "start": 8662.65,
      "duration": 2.25
    },
    {
      "text": "then you're doing less and\nless creative decisions.",
      "start": 8664.9,
      "duration": 2.25
    },
    {
      "text": "And is it going to kind of move to a place",
      "start": 8667.15,
      "duration": 1.74
    },
    {
      "text": "where you're operating",
      "start": 8668.89,
      "duration": 3.15
    },
    {
      "text": "in the design space of natural language",
      "start": 8672.04,
      "duration": 1.98
    },
    {
      "text": "where natural language is the\nmain programming language?",
      "start": 8674.02,
      "duration": 3.3
    },
    {
      "text": "And, I guess, I could ask\nthat by way of advice.",
      "start": 8677.32,
      "duration": 2.13
    },
    {
      "text": "If somebody's interested\nin programming now,",
      "start": 8679.45,
      "duration": 2.04
    },
    {
      "text": "what do you think they should learn?",
      "start": 8681.49,
      "duration": 1.733
    },
    {
      "text": "You guys started in some Java.",
      "start": 8685.33,
      "duration": 2.653
    },
    {
      "text": "(group chuckling)",
      "start": 8687.983,
      "duration": 1.541
    },
    {
      "text": "And I forget, oh, some PHP.",
      "start": 8689.524,
      "duration": 3.104
    },
    {
      "text": "- PHP.",
      "start": 8692.628,
      "duration": 0.833
    },
    {
      "text": "- Objective-C.",
      "start": 8693.461,
      "duration": 0.833
    },
    {
      "text": "- Objective-C, there you go.",
      "start": 8694.294,
      "duration": 2.016
    },
    {
      "text": "I mean in the end, we all know\nJavaScript was going to win",
      "start": 8696.31,
      "duration": 2.701
    },
    {
      "text": "(group chuckling)",
      "start": 8699.011,
      "duration": 2.549
    },
    {
      "text": "and not TypeScript.",
      "start": 8701.56,
      "duration": 1.2
    },
    {
      "text": "It's going to be like vanilla JavaScript.",
      "start": 8702.76,
      "duration": 1.92
    },
    {
      "text": "It's just going to eat the\nworld and maybe live with PHP.",
      "start": 8704.68,
      "duration": 4.009
    },
    {
      "text": "And I mean, it also\nbrings up the question of,",
      "start": 8708.689,
      "duration": 2.111
    },
    {
      "text": "I think Don Knuth has this idea",
      "start": 8710.8,
      "duration": 3.27
    },
    {
      "text": "that some percent of\nthe population is geeks,",
      "start": 8714.07,
      "duration": 2.61
    },
    {
      "text": "and there's a particular\nkind of psychology",
      "start": 8716.68,
      "duration": 2.61
    },
    {
      "text": "in mind required for programming.",
      "start": 8719.29,
      "duration": 2.91
    },
    {
      "text": "And it feels like more\nand more that expands",
      "start": 8722.2,
      "duration": 3.6
    },
    {
      "text": "the kind of person that should be able to,",
      "start": 8725.8,
      "duration": 1.89
    },
    {
      "text": "can do great programming might expand.",
      "start": 8727.69,
      "duration": 3.243
    },
    {
      "text": "- I think different people do programming",
      "start": 8732.64,
      "duration": 2.28
    },
    {
      "text": "for different reasons.",
      "start": 8734.92,
      "duration": 1.65
    },
    {
      "text": "But I think the true,\nmaybe the best programmers",
      "start": 8736.57,
      "duration": 3.19
    },
    {
      "text": "are the ones that really love,",
      "start": 8741.52,
      "duration": 1.923
    },
    {
      "text": "just absolutely love programming.",
      "start": 8744.67,
      "duration": 1.92
    },
    {
      "text": "For example, there are folks on our team",
      "start": 8746.59,
      "duration": 1.77
    },
    {
      "text": "who literally when they\nget back from work,",
      "start": 8748.36,
      "duration": 5.0
    },
    {
      "text": "they go and then they boot up Cursor,",
      "start": 8755.59,
      "duration": 2.85
    },
    {
      "text": "and then they start coding\non their side projects",
      "start": 8758.44,
      "duration": 2.16
    },
    {
      "text": "for the entire night,",
      "start": 8760.6,
      "duration": 0.833
    },
    {
      "text": "and they stay up until 3:00 am doing that.",
      "start": 8761.433,
      "duration": 2.017
    },
    {
      "text": "And when they're sad, they said,",
      "start": 8764.68,
      "duration": 2.043
    },
    {
      "text": "\"I just really need to code.\"",
      "start": 8767.987,
      "duration": 1.905
    },
    {
      "text": "(group chuckling)",
      "start": 8769.892,
      "duration": 1.644
    },
    {
      "text": "And I think there's\nthat level of programmer",
      "start": 8771.536,
      "duration": 3.944
    },
    {
      "text": "where this obsession\nand love of programming,",
      "start": 8775.48,
      "duration": 2.553
    },
    {
      "text": "I think makes, really,\nthe best programmers.",
      "start": 8779.38,
      "duration": 3.51
    },
    {
      "text": "And I think these types of people",
      "start": 8782.89,
      "duration": 1.485
    },
    {
      "text": "will really get into the\ndetails of how things work.",
      "start": 8784.375,
      "duration": 5.0
    },
    {
      "text": "- I guess the question I'm\nasking, that exact programmer,",
      "start": 8789.43,
      "duration": 2.426
    },
    {
      "text": "let's think about that person.",
      "start": 8791.856,
      "duration": 1.717
    },
    {
      "text": "When the super Tab,",
      "start": 8794.74,
      "duration": 1.56
    },
    {
      "text": "the super awesome praise\nbe the Tab succeeds,",
      "start": 8796.3,
      "duration": 4.11
    },
    {
      "text": "and you keep pressing Tab.",
      "start": 8800.41,
      "duration": 1.95
    },
    {
      "text": "- That person in the team loves Cursor Tab",
      "start": 8802.36,
      "duration": 2.19
    },
    {
      "text": "more than anybody else, right?",
      "start": 8804.55,
      "duration": 1.11
    },
    {
      "text": "- Yeah.",
      "start": 8805.66,
      "duration": 0.833
    },
    {
      "text": "Pressing Tab is just pressing Tab.",
      "start": 8808.24,
      "duration": 2.34
    },
    {
      "text": "That's the easy way to\nsay it in the catchphrase.",
      "start": 8810.58,
      "duration": 2.763
    },
    {
      "text": "But what you're actually doing\nwhen you're pressing Tab,",
      "start": 8814.39,
      "duration": 2.16
    },
    {
      "text": "is that you're injecting\nintent all the time",
      "start": 8816.55,
      "duration": 3.96
    },
    {
      "text": "while you're doing it.",
      "start": 8820.51,
      "duration": 1.8
    },
    {
      "text": "Sometimes you're rejecting it,",
      "start": 8822.31,
      "duration": 1.11
    },
    {
      "text": "sometimes you're typing\na few more characters.",
      "start": 8823.42,
      "duration": 2.49
    },
    {
      "text": "And that's the way that\nyou're shaping the things",
      "start": 8825.91,
      "duration": 5.0
    },
    {
      "text": "that's being created.",
      "start": 8831.13,
      "duration": 1.05
    },
    {
      "text": "And I think programming\nwill change a lot to just,",
      "start": 8832.18,
      "duration": 3.937
    },
    {
      "text": "\"What is it that you want to make?\"",
      "start": 8836.117,
      "duration": 1.613
    },
    {
      "text": "- It's sort of higher bandwidth.",
      "start": 8837.73,
      "duration": 1.17
    },
    {
      "text": "The communication to the computer",
      "start": 8838.9,
      "duration": 1.29
    },
    {
      "text": "just becomes higher and higher bandwidth",
      "start": 8840.19,
      "duration": 1.56
    },
    {
      "text": "as opposed to just typing\nas much lower bandwidth",
      "start": 8841.75,
      "duration": 4.112
    },
    {
      "text": "than communicating intent.",
      "start": 8845.862,
      "duration": 1.918
    },
    {
      "text": "- I mean, this goes to your manifesto",
      "start": 8847.78,
      "duration": 3.48
    },
    {
      "text": "titled Engineering Genius.",
      "start": 8851.26,
      "duration": 2.497
    },
    {
      "text": "\"We are an applied research lab building",
      "start": 8853.757,
      "duration": 2.393
    },
    {
      "text": "extraordinary productive\nhuman AI systems.\"",
      "start": 8856.15,
      "duration": 2.31
    },
    {
      "text": "So, speaking to this hybrid element.",
      "start": 8858.46,
      "duration": 4.027
    },
    {
      "text": "\"To start, we're building\nthe engineer of the future,",
      "start": 8862.487,
      "duration": 2.423
    },
    {
      "text": "a human AI programmer that's an order",
      "start": 8864.91,
      "duration": 2.82
    },
    {
      "text": "of magnitude more effective\nthan any one engineer.",
      "start": 8867.73,
      "duration": 2.97
    },
    {
      "text": "This hybrid engineer will\nhave effortless control",
      "start": 8870.7,
      "duration": 2.55
    },
    {
      "text": "over their code base and\nno low entropy keystrokes.",
      "start": 8873.25,
      "duration": 3.69
    },
    {
      "text": "They will iterate at the\nspeed of their judgment,",
      "start": 8876.94,
      "duration": 2.94
    },
    {
      "text": "even in the most complex systems.",
      "start": 8879.88,
      "duration": 2.25
    },
    {
      "text": "Using a combination of\nAI and human ingenuity,",
      "start": 8882.13,
      "duration": 3.15
    },
    {
      "text": "they will out-smart and out-engineer",
      "start": 8885.28,
      "duration": 2.16
    },
    {
      "text": "the best pure AI systems.",
      "start": 8887.44,
      "duration": 2.19
    },
    {
      "text": "We are a group of\nresearchers and engineers.",
      "start": 8889.63,
      "duration": 2.46
    },
    {
      "text": "We build software and models to invent",
      "start": 8892.09,
      "duration": 2.37
    },
    {
      "text": "at the edge of what's\nuseful and what's possible.",
      "start": 8894.46,
      "duration": 1.77
    },
    {
      "text": "Our work has already improved the lives",
      "start": 8896.23,
      "duration": 2.31
    },
    {
      "text": "of hundreds of thousands of programmers.\"",
      "start": 8898.54,
      "duration": 2.73
    },
    {
      "text": "And on the way to that,",
      "start": 8901.27,
      "duration": 1.62
    },
    {
      "text": "we'll at least make programming more fun.",
      "start": 8902.89,
      "duration": 1.98
    },
    {
      "text": "So, thank you for talking today.",
      "start": 8904.87,
      "duration": 1.83
    },
    {
      "text": "- Thank you.\n- Thanks for having us.",
      "start": 8906.7,
      "duration": 1.443
    },
    {
      "text": "- Thank you.\n- Thank you.",
      "start": 8908.143,
      "duration": 2.032
    },
    {
      "text": "- Thanks for listening\nto this conversation",
      "start": 8910.175,
      "duration": 1.175
    },
    {
      "text": "with Michael, Sualeh, Arvid and Aman.",
      "start": 8911.35,
      "duration": 3.3
    },
    {
      "text": "To support this podcast,",
      "start": 8914.65,
      "duration": 0.99
    },
    {
      "text": "please check out our\nsponsors in the description.",
      "start": 8915.64,
      "duration": 2.61
    },
    {
      "text": "And now, let me leave\nyou with a random, funny,",
      "start": 8918.25,
      "duration": 4.35
    },
    {
      "text": "and perhaps profound programming\ncode I saw on Reddit.",
      "start": 8922.6,
      "duration": 2.973
    },
    {
      "text": "Nothing is as permanent as a\ntemporary solution that works.",
      "start": 8926.53,
      "duration": 4.593
    },
    {
      "text": "Thank you for listening and\nhope to see you next time.",
      "start": 8932.32,
      "duration": 3.033
    }
  ],
  "full_text": "- The following is a conversation with the founding members\nof the Cursor Team, Michael Truell, Sualeh\nAsif, Arvid Lunnemark, and Aman Sanger. Cursor is a code editor based on VS Code that adds a lot of powerful\nfeatures for AI-assisted coding. It has captivated the\nattention and excitement of the programming and AI communities. So I thought, this is\nan excellent opportunity to dive deep into the\nrole of AI in programming. This is a super technical conversation that is bigger than just\nabout one code editor. It's about the future of programming, and in general, the future\nof human AI collaboration in designing and engineering complicated and powerful systems. This is the \"Lex Fridman Podcast.\" To support it, please check out our\nsponsors in the description. And now, dear friends, here's Michael, Sualeh, Arvid and Aman. All right, this is awesome. We have Michael, Aman, Sualeh, Arvid here from the Cursor Team. First up, big ridiculous question. What's the point of a code editor? - So the code editor is largely the place where you build software. And today or for a long\ntime, that's meant the place where you text edit a\nformal programming language. And for people who aren't programmers, the way to think of a code editor is a really souped up word\nprocessor for programmers, where the reason it's souped up is code has a lot of structure. And so the, quote,\nunquote, \"word processor,\" the code editor can\nactually do a lot for you that word processors in the writing space haven't been able to do for\npeople editing texts there. And so that's everything from giving you visual differentiation of the actual tokens in the\ncode so you can scan it quickly, to letting you navigate\naround the code base, like you're navigating around\nthe internet with hyperlinks, you're going to definitions\nof things you're using to error checking to\ncatch rudimentary bugs. And so traditionally, that's\nwhat a code editor has meant. And I think that what a code editor is, is going to change a lot\nover the next 10 years as what it means to build software maybe starts to look a bit different. - I think also a code\neditor should just be fun. - Yes, that is very important,\nthat is very important. And it's actually an underrated aspect of how we decide what to build. A lot of the things that we\nbuild and then we try them out, we do an experiment and then\nwe actually throw them out because they're not fun. And so, a big part of being fun is being fast a lot of the time. Fast is fun. - Yeah, fast is. (chuckles) Yeah, that should be a T-shirt. (group chuckling) - Fundamentally, I think one of the things that draws a lot of people to\nbuilding stuff on computers is this insane iteration speed, where in other disciplines\nyou might be gate capped by resources or the ability. Even the ability to get\na large group together and coding is this amazing thing where it's you and the\ncomputer and that alone, you can build really cool\nstuff really quickly. - So for people who don't know, Cursor is this super cool new editor that's a fork of VS Code. It would be interesting\nto get your explanation of your own journey of editors. I think all of you were big\nfans of VS Code with Copilot. How did you arrive to VS Code and how did that lead to\nyour journey with Cursor? - Yeah, so I think a lot of us, well, all of us were originally Vim users. - Pure Vim.\n- Pure Vim, yeah. No Neovim, just pure Vim and a terminal. And at least for myself, it was around the time\nthat Copilot came out, so 2021 that I really wanted to try it. So, I went into VS Code, the only code editor in\nwhich it was available, and even though I really\nenjoyed using Vim, just the experience of\nCopilot with VS Code was more than good enough\nto convince me to switch. And so that kind of was the default until we started working on Cursor. - And maybe we should\nexplain what Copilot does. It's a really nice auto complete. As you start writing a thing, it suggests one or two or three lines how to complete the thing. And there's a fun experience in that. You know like when you\nhave a close friendship and your friend completes your sentences? (group chuckles) When it's done well,\nthere's an intimate feeling. There's probably a better\nword than intimate, but there's a cool feeling of\nlike, \"Holy shit, it gets me.\" (all chuckles) And then, there's an unpleasant feeling when it doesn't get you. And so, there's that kind of friction. But I would say for a lot of people, the feeling that it gets me\noverpowers that it doesn't. - And, I think, actually one\nof the underrated aspects of Github Copilot is that\neven when it's wrong, it's a little bit annoying,\nbut it's not that bad because you just type another character, and then maybe then it gets you, or you type another character\nand then it gets you. So even when it's wrong,\nit's not that bad. - Yeah, you can iterate and fix it. I mean, the other underrated\npart of Copilot for me was just the first real AI product. So the first language\nmodel consumer product. - So, Copilot was like\nthe first killer app for LLMs.\n- Yeah. - Yeah, and the beta was out in 2021. - Right, okay. So, what's the origin story of Cursor? - So around 2020, the scaling loss papers\ncame out from OpenAI, and that was a moment where this looked like\nclear predictable progress for the field where even if\nwe didn't have any more ideas, it looked like you could make\nthese models a lot better if you had more compute and more data. - By the way, we'll probably\ntalk for three to four hours on the topic of scaling loss. (group chuckling) But just to summarize, it's a paper in a set of\npapers in a set of ideas that say bigger might be better for model size and data size in the realm of machine learning. - It's bigger and better,\nbut predictably better. - Okay, that's another\ntopic of conversation. - Yes.\n- Yeah. - So around that time for some of us, there were a lot of\nconceptual conversations about what's this gonna look like? What's the story gonna be for all these different\nknowledge worker fields about how they're gonna be made better by this technology getting better? And then, I think, there\nwere a couple of moments where the theoretical gains predicted in that paper started\nto feel really concrete, and it started to feel like a moment where you could actually\ngo and not do a PhD if you wanted to do useful work in AI. It actually felt like now\nthere was this whole set of systems one could build\nthat were really useful. And I think that the first moment we already talked about a little bit, which was playing with\nthe early beta of Copilot, that was awesome and magical. I think that the next big moment where everything kind of clicked together was actually getting\nearly access to GPT-4. So, it was sort of end of 2022 was when we were\ntinkering with that model, and the step-upping\ncapabilities felt enormous. And previous to that, we had been working on a\ncouple of different projects. Because of Copilot,\nbecause of scaling odds, because of our prior\ninterest in the technology, we had been tinkering around\nwith tools for programmers, but things that are very specific. So, we were building tools\nfor financial professionals who have to work within a Jupyter Notebook or playing around with can you do static analysis\nwith these models? And then, the step-up in GPT-4 felt like, look, that really made\nconcrete the theoretical gains that we had predicted before. It felt like you could build\na lot more just immediately at that point in time. And also, if we were being\nconsistent, it really felt like this wasn't just gonna be\na point solution thing. This was gonna be all of\nprogramming was gonna flow through these models. And it felt like that\ndemanded a different type of programming environment, a\ndifferent type of programming. And so, we set off to build\nthat larger vision around then. - There's one that I distinctly remember. So, my roommate is an IMO Gold winner and there's a competition\nin the US called the PUTNAM, which is the IMO for college people, and it's this math competition. It's exceptionally good. So, Shengtong and Aman I\nremember, sort of June of 2022, had this bet on whether\nthe 2024 June or July, you were going to win a gold\nmedal in the IMO with models. - IMO is the International Math Olympiad. - Yeah, IMO is\nInternational Math Olympiad. And so, Arvid and I are\nboth also competing in it. So, it was sort of personal. (group chuckling) And I remember thinking, \"Matt,\nthis is not gonna happen.\" Even though I sort of\nbelieved in progress, I thought IMO Gold, Aman is delusional. And to be honest, I mean, I\nwas, to be clear, very wrong. But that was maybe the most\nprescient bet in the group. - So the new results from DeepMind, it turned out that you were correct. (group chattering) - [Arvid] Technically not. - Technically incorrect\nbut one point away. - Aman was very enthusiastic\nabout this stuff back then. And before, Aman had\nthis scaling loss T-shirt that he would wear around where it had the charts\nand the formulas on it. - So, you felt the AGI or\nyou felt the scaling loss. - Yeah, I distinctly remember there was this one\nconversation I had with Michael before I hadn't thought super deeply and critically about scaling laws. And he kind of posed the question, why isn't scaling all you need, or why isn't scaling gonna result in massive gains in progress? And I think I went through\nthe stages of grief. There is anger, denial,\nand then finally at the end just thinking about it, acceptance. And I think I've been quite hopeful and optimistic about progress since. I think one thing I'll caveat\nis, I think, it also depends on which domains you're\ngonna see progress. Math is a great domain\nespecially formal theorem proving because you get this fantastic\nsignal of actually verifying if the thing was correct. And so this means something like RL can\nwork really, really well, and I think you could have systems that are perhaps very superhuman in math and still not technically have AGI. - Okay, so can we take\nit all the way to Cursor. And what is Cursor? It's a fork of VS Code, and VS Code is one of\nthe most popular editors for a long time. Everybody fell in love with it. Everybody left Vim, I left DMAX for it. Sorry. (all laughing) So, unified in some fundamental\nway the developer community. And then, you look at the space of things, you look at the scaling\nlaws, AI is becoming amazing. And you decided, okay, it's not enough to just write an extension via VS Code because there's a lot\nof limitations to that. If AI is gonna keep getting\nbetter and better and better, we need to really rethink\nhow the AI is gonna be part of the editing process. And so, you decided to fork VS Code, and start to build a lot\nof the amazing features we'll be able to talk about. But what was that decision like? Because there's a lot of\nextensions, including Copilot, of VS Code that are doing\nsort of AI type stuff. What was the decision\nlike to just fork VS Code? - So the decision to do an\neditor seemed self-evident to us for at least what we\nwanted to do and achieve, because when we started\nworking on the editor, the idea was these models\nare gonna get much better, their capabilities are gonna improve, and it's gonna entirely\nchange how you build software, both in a you will have\nbig productivity gains but also radical and now\nthe active building software is gonna change a lot. And so, you're very limited in the control you have over a code editor if you're a plugin to an\nexisting coding environment, and we didn't wanna get locked\nin by those limitations. We wanted to be able to just\nbuild the most useful stuff. - Okay. Well then, the natural question is, VS Code is kind of with\nCopilot a competitor, so how do you win? Is it basically just the\nspeed and the quality of the features? - Yeah, I mean, I think this is a space that is quite interesting,\nperhaps quite unique where if you look at previous tech waves, maybe there's kind of one\nmajor thing that happened and it unlocked a new wave of companies, but every single year, every\nsingle model capability or jump you get in model capabilities, you now unlock this new wave of features, things that are possible,\nespecially in programming. And so, I think, in AI programming, being even just a few months\nahead, let alone a year ahead, makes your product much,\nmuch, much more useful. I think the Cursor a year from now will need to make the Cursor\nof today look obsolete. And I think Microsoft has done\na number of fantastic things, but I don't think they're in a great place to really keep innovating and pushing on this in the\nway that a startup can. - Just rapidly implementing features. - Yeah, and doing the research\nexperimentation necessary to really push the ceiling. - I don't know if I think\nof it in terms of features as I think of it in terms of\ncapabilities for programmers. As the new o1 model came out, and I'm sure there are\ngonna be more models of different types, like longer\ncontext and maybe faster, there's all these crazy\nideas that you can try, and hopefully 10% of the crazy ideas will make it into something\nkind of cool and useful and we want people to have that sooner. To rephrase, an underrated fact is we're making it for ourself. When we started Cursor, you really felt this\nfrustration that models, you could see models getting better, but the Copilot experience\nhad not changed. It was like, \"Man, the\nceiling is getting higher, why are they not making new things? They should be making new things. Where's all the alpha features? There were no alpha features.\" I'm sure it was selling well. I'm sure it was a great business, I'm one of these people that really want to\ntry and use new things, and there was no new thing\nfor a very long while. - Yeah, it's interesting. I don't know how you put that into words, but when you compare\na Cursor with Copilot, Copilot pretty quickly\nstarted to feel stale for some reason. - Yeah, I think one thing\nthat I think helps us is that we're doing it all in one where we're developing the UX and the way you interact with the model at the same time as we're developing how we actually make the\nmodel give better answers. So, how you build up the prompt or how do you find the\ncontext and for a Cursor Tab, how do you train the model? So, I think that helps\nus to have all of it the same people working on the\nentire experience end to end. - Yeah, it's like the person making the UI and the person training the\nmodel sit like 18 feet away. - [Aman] Often the same person even. - Yeah, often even the same person. You can create things that\nare sort of not possible if you're not talking,\nyou're not experimenting. - And you're using, like you\nsaid, Cursor to write Cursor? - Of course. - Oh, yeah.\n- Yeah. - Well, let's talk about\nsome of these features. Let's talk about the all-knowing, the all-powerful praise be to the Tab, (group chuckles) auto complete on steroids basically. So how does Tab work? What is Tab? - To highlight and\nsummarize at a high level, I'd say that there are two things that Cursor is pretty good at right now. There are other things that it does, but two things that it\nhelps programmers with. One is this idea of\nlooking over your shoulder, and being a really fast colleague who can kind of jump\nahead of you, and type, and figure out what you're gonna do next. That was the kernel of the idea\nbehind a good auto complete was predicting what you're gonna do next, but you can make that\nconcept even more ambitious by not just predicting the\ncharacters after your Cursor but actually predicting\nthe next entire change you're gonna make, the next diff, next place you're gonna jump to. And the second thing Cursor is\npretty good at right now too is helping you sometimes\njump ahead of the AI and tell it what to do and\ngo from instructions to code. And on both of those,\nwe've done a lot of work on making the editing experience\nfor those things ergonomic and also making those\nthings smart and fast. - One of the things we really wanted, was we wanted the model to\nbe able to edit code for us. That was kind of a wish and\nwe had multiple attempts at it before we had a good model\nthat could edit code for you. Then after we had a good model, I think there've been a lot of effort to make the inference fast\nfor having a good experience, and we've been starting to incorporate, I mean, Michael sort of\nmentioned this ability to jump to different places, and that jump to different\nplaces I think came from a feeling of once you accept an edit, it's like, \"Man, it should\nbe just really obvious where to go next.\" It's like, \"I'd made this change,\nthe model should just know that the next place to\ngo to is 18 lines down.\" If you're a WIM user, you\ncould press 18JJ or whatever, but why am I doing this? The model should just know it. So the idea was you just press Tab, it would go 18 lines down, and then show you the next\nedit and you would press Tab, so as long as you could keep pressing Tab. And so the internal competition was, how many Tabs can we make someone press? Once you have the idea, more abstractly, the thing to think about is\nhow are the edits zero entropy? There's no new bits of information\nto finish your thought, but you still have to type some characters to make the computer understand what you're actually thinking, then maybe the model\nshould just read your mind and all the zero entropy bits\nshould just be tabbed away. That was sort of the abstract version. - There's this interesting thing where if you look at language model loss on different domains, I\nbelieve the bits per byte, which is a kind of\ncharacter normalize loss for code is lower than language, which means in general, there\nare a lot of tokens in code that are super predictable, a lot of characters that\nare super predictable. And this is I think even magnified when you're not just trying\nto auto complete code, but predicting what the\nuser's going to do next in their editing of existing code. And so, the goal of Cursor\nTab is let's eliminate all the low entropy actions\nyou take inside of the editor. When the intent is effectively determined, let's just jump you forward\nin time, skip you forward. - Well, what's the intuition and what's the technical details of how to do next Cursor prediction? That jump, that's not so\nintuitive I think to people. - Yeah. I think I can speak to\na few of the details on how to make these things work. They're incredibly low latency, so you need to train\nsmall models on this task. In particular, they're\nincredibly pre-fill token hungry. What that means is they have these really, really long prompts where\nthey see a lot of your code and they're not actually\ngenerating that many tokens. And so, the perfect fit for\nthat is using a sparse model, meaning an MOE model. So that was one breakthrough we made that substantially\nimproved its performance at longer context. The other being a variant\nof speculative decoding that we built out called\nspeculative edits. These are two, I think, important pieces of what make it quite high\nquality and very fast. - Okay, so MoE, Mixture of Experts, the input is huge, the output is small. - Yeah.\n- Okay. Does caching play a role- - Oh, caching plays a huge role. Because you're dealing with\nthis many input tokens, if every single keystroke that\nyou're typing in a given line you had to rerun the model on\nall of those tokens passed in, you're just going to one,\nsignificantly degrade latency, two, you're gonna kill\nyour GPUs with load. So, you need to design\nthe actual prompts you use for the model such that\nthey're caching aware. And then yeah, you need\nto reuse the KV cache across requests just so that you're spending\nless work, less compute. - Again, what are the\nthings that Tab is supposed to be able to do in the near\nterm, just to linger on that? Generate code, fill empty space, also edit code across multiple lines, and then jump to different\nlocations inside the same file, and then-\n- Hopefully, jump to different files also. So if you make an edit in one file, and maybe you have to go to another file to finish your thought, it should go to the second file also. - The full generalization\nis next action prediction. Sometimes, you need to run\na command in the terminal and it should be able to\nsuggest the command based on the code that you wrote too. It suggests something, but it's hard for you\nto know if it's correct because you actually need some\nmore information to learn. You need to know the\ntype to be able to verify that it's correct. And so maybe it should\nactually take you to a place that's the definition of something, and then take you back so that you have all\nthe requisite knowledge to be able to accept the next completion. - So providing the human the knowledge. - [Arvid] Yes. - Right.\n- Mm-hmm, yeah. - I just gotten to know\na guy named Primeagen. You can order coffee via SSH. - (chuckles) Oh, yeah. - We did that.\n- We did that. - So, can also the model do that and provide you with caffeine? Okay, so that's the general framework. - Yeah. - Programming is this weird discipline where sometimes the next\nfive minutes, not always, but sometimes the next five\nminutes of what you're gonna do is actually predictable from\nthe stuff you've done recently. And so, can you get to a world\nwhere that next five minutes either happens by you disengaging\nand it taking you through? Or maybe a little bit more\nof just you seeing next step what it's gonna do and you're like, \"Okay, that's good, that's\ngood, that's good, that's good,\" and you can just tap, tap\nthrough these big changes. - As we're talking about this, I should mention one of the really cool and noticeable things about Cursor is that there's this whole diff\ninterface situation going on. So, the model suggests\nwith the red and the green of here's how we're gonna modify the code, and in the chat window you can apply and it shows you the diff\nand you can accept the diff. So, maybe can you speak to\nwhatever direction of that? - We'll probably have four or\nfive different kinds of diffs. So we have optimized the\ndiff for the auto complete, so that has a different diff interface than when you're reviewing\nlarger blocks of code. And then we're trying to\noptimize another diff thing for when you're doing\nmultiple different files. And at a high level, the difference is for when you're doing auto-complete, it should be really, really fast to read. Actually, it should be really fast to read in all situations, but in auto-complete your\neyes are focused in one area. The humans can't look in\ntoo many different places. - So, you're talking about\non the interface side? - On the interface side. So it currently has this box on this side. So we have the current box,\nand it you tries to delete code in some place and tries to add other code, it tries to show you a box on the side. - You can maybe show it if\nwe pull it up in cursor.com. This is what we're talking. - So that box-\n- Exactly here. - It was like three or\nfour different attempts at trying to make this thing work where first the attempt was\nthis blue crossed out line. So before it was a box on the side, it used to show you the code to delete by showing you Google Docs style, you would see a line through it and then you would see the new code. That was super distracting. There was deletions, there\nwas trying the red highlight. Then the next iteration of\nit, which is sort of funny, you would hold the, on\nMac, the Option button. So, it would highlight a region of code to show you that there\nmight be something coming. So, maybe in this example, the input and the value\nwould all get blue. And the blue was to highlight that the AI had a suggestion for you. So instead of directly\nshowing you the thing, it would just hint that\nthe AI had a suggestion and if you really wanted to see it, you would hold the Option button, and then you would see the new suggestion. And if you release the Option button, you would then see your original code. - Mm-hmm, by the way, that's pretty nice, but you have to know to\nhold the Option button. - Yeah. - And by the way, I'm not a\nMac user, but I got it, Option. It's a button I guess you people have. - Again, it's just not intuitive. I think that's the key thing. - And there's a chance this is also not the final version of it. - I am personally very excited for making a lot of\nimprovements in this area. We often talk about it as\nthe verification problem where these diffs are\ngreat for small edits. For large edits or when it's\nmultiple files or something, it's actually a little bit prohibitive to review these diffs. So, there are a couple\nof different ideas here. One idea that we have is, okay, parts of the diffs are important. They have a lot of information. And then parts of the diff\nare just very low entropy. They're the same thing\nover and over again. And so maybe you can\nhighlight the important pieces and then gray out the\nnot so important pieces. Or maybe you can have a\nmodel that looks at the diff and sees, oh, there's a likely bug here. I will mark this with a\nlittle red squiggly and say, \"You should probably review\nthis part of the diff.\" Ideas in that vein I think are exciting. - Yeah, that's a really fascinating space of UX design engineering. So you're, basically, trying\nto guide the human programmer through all the things they need to read and nothing more, optimally. - Yeah, and you want an\nintelligent model to do it. Currently, diff algorithms, they're just like normal algorithms. There's no intelligence. There's intelligence that went\ninto designing the algorithm, but then you don't care\nif it's about this thing or this thing as you want\nthe model to do this. - So, I think the\ngeneral question is like, man, these models are\ngoing to get much smarter. As the models get much smarter, changes they will be able\nto propose are much bigger. So as the changes gets\nbigger and bigger and bigger, the humans have to do more and more and more verification work. You need to help them out. I don't wanna spend all\nmy time reviewing code. - Can you say a little more\nacross multiple files diff? - Yeah, I mean, so GitHub\ntries to solve this, right, with code review. When you're doing code review, you're reviewing multiple\ndiffs across multiple files. But like Arvid said earlier, I think you can do much\nbetter than code review. Code review kind of sucks. You spend a lot of time\ntrying to grok this code that's often quite unfamiliar to you and it often doesn't even\nactually catch that many bugs. And I think you can significantly improve that review experience using\nlanguage models, for example, using the kinds of tricks\nthat Arvid had described of maybe pointing you towards the regions that actually matter. I think also if the code is produced by these language models and it's not produced by someone else. The code review experience is\ndesign for both the reviewer and the person that produced the code. In the case where the person that produced the code\nis a language model, you don't have to care that\nmuch about their experience and you can design the entire thing around the reviewer such that\nthe reviewer's job is as fun, as easy, as productive as possible. I think that feels like the\nissue with just naively trying to make these things\nlook like code review. I think you can be a lot more creative and push the boundary on what's possible. - And just one idea there\nis, I think ordering matters. Generally, when you review a\nPR, you have this list of files and you're reviewing\nthem from top to bottom, but you actually wanna\nunderstand this part first because that came logically first, and then you want to\nunderstand the next part. And you don't want to have\nto figure out that yourself. You want a model to guide\nyou through the thing. - And is the step of\ncreation going to be more and more natural language, is the goal versus with\nactual writing the book? - I think sometimes. I don't\nthink it's going to be the case that all of programming\nwill be natural language, and the reason for that is if I'm pair programming with Sualeh, and Sualeh is at the\ncomputer and the keyboard, and sometimes if I'm driving,\nI want to say to Sualeh, \"Hey, implement this\nfunction,\" and that works. And then sometimes it's just so annoying to explain to Sualeh\nwhat I want him to do, and so I actually take over\nthe keyboard and I show him. I write part of the example\nand then it makes sense and that's the easiest way to communicate. And so, I think that's\nalso the case for AI. Sometimes the easiest way\nto communicate with the AI will be to show an example and then it goes and does\nthe thing everywhere else. Or sometimes if you're making\na website, for example, the easiest way to show\nto the AI what you want is not to tell it what to do but drag things around or draw things, and maybe eventually, we will get to brain machine\ninterfaces or whatever and you can understand\nwhat you're thinking. And so, I think natural\nlanguage will have a place. I think it will definitely not be the way most people program most of the time. - I'm really feeling the\nAGI with this editor. (group chuckling) It\nfeels like there's a lot of machine learning going on underneath. Tell me about some of the ML\nstuff that makes it all work? - Where Cursor really works via this ensemble of custom models that we've trained alongside\nthe frontier models that are fantastic at the\nreasoning intense things. And so Cursor Tab, for\nexample, is a great example of where you can specialize\nthis model to be, even better than even frontier models if you look at evals on\nthe task we set it at. The other domain, which it's surprising that it requires custom models but it's necessary and works\nquite well, is in Apply. The frontier models are quite\ngood at sketching out plans for code and generating\nrough sketches of the change, but actually, creating diffs is quite hard for frontier models, for\nyour training models. You try to do this with Sonnet,\nwith o1, any frontier model and it really messes up stupid things like counting line numbers, especially in super, super large files. And so what we've done to alleviate this is we let the model sketch\nout this rough code block that indicates what the change will be and we train a model to then\nApply that change to the file. - And we should say that\nApply is the model looks at your code, it gives you a\nreally damn good suggestion of what new things to do. And the seemingly for humans trivial step of combining the two, you're\nsaying is not so trivial. - Contrary to popular perception, it is not a deterministic algorithm. - Yeah, I think you see shallow\ncopies of Apply elsewhere and it just breaks most of the time because you think you can try to do some deterministic matching, and then it fails at least 40% of the time and that just results in a\nterrible product experience. I think in general, this regime of you are going to get smarter\nand smarter models. So one other thing that Apply lets you do is it lets you use fewer tokens with the most intelligent models. This is both expensive in terms of latency for generating all these tokens and cost. So, you can give this\nvery, very rough sketch and then have your model\nmodels go and implement it because it's a much\neasier task to implement, this very, very sketched out code. And I think that this regime will continue where you can use smarter\nand smarter models to do the planning and then\nmaybe the implementation details can be handled by the\nless intelligent ones. Perhaps you'll have maybe o1, maybe it'll be even more capable models given an even higher level plan that is recursively applied by Sauna and then the Apply model. - Maybe we should talk\nabout how to make it fast if you like. Fast is always an interesting detail. - [Arvid] Fast is good. - Yeah, how do you make it fast? - Yeah, so one big component of making it fast is speculative edits. So, speculative edits are a\nvariant of speculative decoding, and maybe it'd be helpful to briefly describe speculative decoding. With speculative decoding, what you do is you can\ntake advantage of the fact that most of the time,\nand I'll add the caveat that it would be when you're memory bound in language model generation, if you process multiple tokens at once, it is faster than generating\none token at a time. So this is the same reason why if you look at tokens per\nsecond with prompt tokens versus generated tokens, it's much much faster for prompt tokens. So what we do is instead of using what speculative decoding normally does, which is using a really small model to predict these draft\ntokens that your larger model will then go in and verify, with code edits, we\nhave a very strong prior of what the existing code will look like, and that prior is literally\nthe same exact code. So you can do is you can just feed chunks of the original code back into the model, and then the model will\njust pretty much agree most of the time that, \"Okay, I'm just gonna\nspit this code back out.\" And so, you can process all\nof those lines in parallel and you just do this with\nsufficiently many chunks. And then eventually, you'll\nreach a point of disagreement where the model will now\npredict text that is different from the ground truth original code. It'll generate those tokens\nand then we will decide after enough tokens\nmatch the original code to re-start speculating in chunks of code. What this actually ends up looking like is just a much faster version\nof normal editing code. So, it looks like a much faster version of the model rewriting all the code. So, we can use the same exact interface that we use for diffs, but it will just stream down a lot faster. - And then the advantage is\nthat while it's streaming, you can just also start reviewing the code before it's done so there's\nno big loading screen. Maybe that is part of the advantage. - So, the human can start\nreading before the thing is done. - I think the interesting\nriff here is something like, I feel like speculation is a\nfairly common idea nowadays. It's not only in language models. There's obviously speculation in CPUs and there's speculation for databases and there's speculation\nall over the place. - Well, let me ask the ridiculous question of which LLM is better at coding? GPT, Claude, who wins in\nthe context of programming? And I'm sure the answer\nis much more nuanced because it sounds like every single part of this involves a different model. - Yeah, I think there's no model that Pareto dominates others, meaning, it is better in all categories that we think matter, the\ncategories being speed, ability to edit code, ability\nto process lots of code, long context, a couple of other things and coding capabilities. The one that I'd say right now\nis just net best is Sonnet. I think this is a consensus opinion. O1's really interesting and\nit's really good at reasoning. So if you give it really hard programming interview style\nproblems or lead code problems, it can do quite well on them, but it doesn't feel like it\nunderstands your rough intent as well as Sonnet does. If you look at a lot of\nthe other frontier models, one qualm I have is it feels like they're not necessarily over, I'm not saying they train on benchmarks, but they perform really\nwell in benchmarks relative to everything that's in the middle. So if you tried on all these benchmarks and things that are in the distribution of the benchmarks they're evaluated on, they'll do really well. But when you push them a\nlittle bit outside of that, Sonnet is I think the one that does best at maintaining that same capability. You have the same\ncapability in the benchmark as when you try to instruct\nit to do anything with coding. - Another ridiculous\nquestion is the difference between the normal programming experience versus what benchmarks represent? Where do benchmarks fall\nshort, do you think, when we're evaluating these models? - By the way, that's\na really, really hard, critically important detail of how different benchmarks\nare versus real coding, where real coding, it's\nnot interview style coding. Humans are saying\nhalf-broken English sometimes and sometimes you're saying,\n\"Oh, do what I did before.\" Sometimes you're saying, \"Go add this thing and then\ndo this other thing for me and then make this UI element.\" And then, it's just a lot of\nthings are context dependent. You really want to understand the human and then do what the human\nwants, as opposed to this, maybe the way to put it abstractly is the interview problems\nare very well specified. They lean a lot on specification while the human stuff is less specified. - Yeah. I think that this benchmark\nquestion is both complicated by what Sualeh just mentioned, and then also what Aman was getting into, there's this problem of the skew between what can you actually model in a benchmark versus real programming, and that can be sometimes\nhard to encapsulate because it's real programming's very messy and sometimes things\naren't super well specified what's correct or what isn't. But then it's also doubly hard because of this public benchmark problem. And that's both because public benchmarks are sometimes hill climbed on, then it's really, really\nhard to also get the data from the public benchmarks\nout of the models. And so for instance, one of the most popular\nagent benchmarks, SWE-Bench, is really, really contaminated\nin the training data of these foundation models. And so if you ask these foundation models to do a SWE-Bench problem, but you actually don't give\nthem the context of a code base, they can hallucinate the right file pass, they can hallucinate the\nright function names. And so, it's also just the public aspect of these things is tricky. - Yeah, in that case, it could be trained on the literal issues or\npull requests themselves, and maybe the labs will\nstart to do a better job or they've already done a good job at decontaminating those things, but they're not going to\nomit the actual training data of the repository itself. These are all some of the most\npopular Python repositories. SimPy is one example. I don't think they're going\nto handicap their models on SimPy and all these\npopular Python repositories in order to get true evaluation\nscores in these benchmarks. - I think that given\nthe dirts in benchmarks, there have been a few\ninteresting crutches that places that build systems with these models or build these models actually use to get a sense of are they going\nthe right direction or not. And in a lot of places, people will actually just have\nhumans play with the things and give qualitative feedback on these. One or two of the\nfoundation model companies, they have people that's\na big part of their role. And internally, we also\nqualitatively assess these models and actually lean on that a lot in addition to private\nemails that we have. - [Arvid] It's like the vibe. - The vibe, yeah, the vibe.\n- It's like the vibe. - The vibe benchmark, human\nbenchmark, the humans. You pull in the humans to do a vibe check. - Yeah. (chuckles) - Okay. That's what I do, just\nreading online forums and Reddit and X. Well, I don't know how to\nproperly load in people's opinions 'cause they'll say things like, \"I feel like Claude or GPT has\ngotten dumber,\" or something. They'll say, \"I feel like.\" And then I sometimes feel like that too, but I wonder if it's the\nmodel's problem or mine. - With Claude, there's an\ninteresting take I heard where I think AWS has different chips, and I suspect they have\nslightly different numerics than Nvidia GPUs, and someone speculated that\nClaude's degraded performance had to do with maybe using\nthe quantized version that existed on AWS Bedrock versus whatever was\nrunning on Anthropics GPUs. - I interview a bunch of people that have conspiracy theories, so I'm glad you spoke to this conspiracy. - Well, it's not like conspiracy\ntheory as much as humans. Humans are humans and\nthere's these details. - [Lex] Yes. - And you're doing this\nqueasy amount of flops and chips are messy and\nman, you can just have bugs. It's hard to overstate how\nhard bugs are to avoid. - What's the role of a\ngood prompt in all of this? We mentioned that benchmarks\nhave really structured, well-formulated prompts. What should a human be\ndoing to maximize success and what's the importance\nof what the humans, you wrote a blog post, you\ncalled it Prompt Design. - Yeah, I think it depends\non which model you're using, and all of them are slightly different and they respond differently\nto different prompts, but I think the original GPT-4 and the original (indistinct)\nmodels last year, they were quite sensitive to the prompts, and they also had a very\nsmall context window. And so, we have all of\nthese pieces of information around the code base that would maybe be\nrelevant in the prompt. You have the docs, you have\nthe files that you add, you have the conversation history, and then there's a problem\nlike how do you decide what you actually put in the prompt and when you have a limited space? And even for today's models, even when you have long context, filling out the entire\ncontext window means that it's slower. It means that sometimes the\nmodel actually gets confused and some models get more\nconfused than others. And we have this one system\ninternally that we call Preempt, which helps us with that a little bit. And I think it was\nbuilt for the era before where we had 8,000 token contact windows. And it's a little bit similar to when you're making a website. You want it to work on mobile, you want it to work on a desktop screen, and you have this dynamic\ninformation which you don't have. For example, if you're\ndesigning a print magazine, you know exactly where you can put stuff. But when you have a website\nor when you have a prompt, you have these inputs and\nthen you need to format them to always work, even if\nthe input is really big, then you might have to cut something down. And so the idea was, okay,\nlet's take some inspiration. What's the best way to design websites? Well, the thing that\nwe really like is React and the declarative approach where you use JSX in JavaScript,\nand then you declare, \"This is what I want and I\nthink this has higher priority or this has higher Z index\nthan something else.\" And then, you have this\nrendering engine in web design. It's like Chrome, and in our\ncase it's a preempt renderer, which then fits everything onto the page. And as you declare, decide what you want and then it figures out what you want. And so, we have found\nthat to be quite helpful and I think the role of\nit has shifted over time where initially it was to fit to these small context windows. Now, it's really useful because it helps us with\nsplitting up the data that goes into the prompt and\nthe actual rendering of it. And so, it's easier to debug because you can change the\nrendering of the prompt and then try it on old prompts because you have the raw data\nthat went into the prompt, and then you can see, \"Did\nmy change actually improve it for this entire eval set?\" - So, do you literally prompt with JSX? - Yes, yes.\n- Yeah. - So it looks like React,\nthere are components. We have one component\nthat's a file component and it takes in the Cursor. Usually, there's one line where\nthe Cursor is in your file and that's probably\nthe most important line because that's the one you're looking at. And so, then you can give priorities, so that line has the highest priority, and then you subtract one for every line that is farther away. And then eventually, when it's rendered, it figures out how many\nlines can actually fit and it centers around that thing. - That's amazing.\n- Yeah. - And you can do other fancy things where if you have lots of code blocks from the entire code base,\nyou could use retrieval and things like embedding\nand re-ranking scores to add priorities for you\nthrough these components. - So should humans when\nthey ask questions, also try to use something like that? Would it be beneficial to\nwrite JSX in the problem or the whole idea is this\nshould be loose and messy? - I think our goal is that\nyou should just do whatever is the most natural thing for you, and then our job is to figure out how do we actually retrieve\nthe relative event things so that your thinking\nactually makes sense? - Well, this is the discussion I had with Aravind of Perplexity\nis his whole idea is you should let the person\nbe as lazy as he wants. - Yeah.\n- Mm-hmm. - Yeah, that's a beautiful thing, but I feel like you're allowed to ask more of programmers, right? - Yes. - So if you say, \"Just do what you want,\" I mean, humans are lazy. There's a tension between just being lazy versus provide more as be prompted, almost like the system pressuring you or inspiring you to be articulate. Not in terms of the\ngrammar of the sentences, but in terms of the depth of thoughts that you convey inside the prompts. - I think even as a system gets closer to some level of perfection, often when you ask the\nmodel for something, not enough intent is\nconveyed to know what to do. And there are a few ways\nto resolve that intent. One is the simple thing of\nhaving the model just ask you, \"I'm not sure how to do these\nparts based on your query. Could you clarify that?\" I think the other could be maybe if there are five or six\npossible generations, \"Given the uncertainty\npresent in your query so far, why don't we just actually\nshow you all of those and let you pick them?\" - How hard is it for the\nmodel to choose to talk back? It's hard, how deal with the uncertainty. Do I choose to ask for more information to reduce the ambiguity? - So, I mean, one of the things we do, it's like a recent addition, is try to suggest files that you can add. And while you're typing, one can guess what the uncertainty is and maybe suggest that maybe\nyou're writing your API and we can guess using the commits that you've made\npreviously in the same file that the client and the\nserver is super useful and there's a hard technical problem of how do you resolve\nit across all commits? Which files are the most important given your current prompt? And we're still initial\nversion is ruled out and I'm sure we can make\nit much more accurate. It's very experimental, but\nthen the idea is we show you, do you just want to add this\nfile, this file, this file also to tell the model to\nedit those files for you? Because if maybe you're making the API, you should also edit the\nclient and the server that is using the API and the\nother one resolving the API. So that would be cool as\nboth there's the phase where you're writing a prompt. Before you even click, \"Enter,\" maybe we can help resolve\nsome of the uncertainty. - To what degree do you\nuse agentic approaches? How useful are agents? - We think agents are really, really cool. - [Lex] (chuckles) Okay. - I think agents, it's like\nresembles like a human. You can feel that you're\ngetting closer to AGI because you see a demo where\nit acts as a human would and it's really, really cool. I think agents are not yet\nsuper useful for many things. I think we're getting close to where they will actually be useful. And so, I think there are\ncertain types of tasks where having an agent\nwould be really nice. I would love to have an agent.\nFor example, if we have a bug where you sometimes can't\nCommand+C and Command+V inside our chat input box, and that's a task that's\nsuper well specified. I just want to say in two sentences, \"This does not work, please fix it.\" And then I would love to have an agent that just goes off, does it, and then a day later, I come\nback and I review the thing. - You mean it goes, finds the right file? - Yeah, it finds the right files, it tries to reproduce the bug, it fixes the bug and then it\nverifies that it's correct. And this could be a process\nthat takes a long time. And so, I think I would love to have that. And then I think a lot of programming, there is often this belief that agents will take\nover all of programming. I don't think we think\nthat that's the case because a lot of programming, a lot of the value is in iterating, or you don't actually want\nto specify something upfront because you don't really\nknow what you want until you have seen an initial version and then you want to iterate on that, and then you provide more information. And so, for a lot of programming, I think you actually want\na system that's instant, that gives you an initial\nversion instantly back and then you can iterate\nsuper, super quickly. - What about something like\nthat recently came out, replica agent, that does also setting up the development environment\nand solving software packages, configuring everything,\nconfiguring the databases and actually deploying the app. Is that also in the set\nof things you dream about? - I think so. I think that would be really cool. For certain types of programming,\nit would be really cool. - Is that within scope of Cursor? - Yeah, we aren't actively\nworking on it right now. We want to make the\nprogrammer's life easier and more fun and some things\nare just really tedious and you need to go\nthrough a bunch of steps and you want to delegate that to an agent. And then some things you\ncan actually have an agent in the background while you're working. Let's say you have a PR that's\nboth backend and frontend, and you're working in the frontend, and then you can have a background agent that doesn't work and figure\nout what you're doing. And then, when you get to\nthe backend part of your PR, then you have some initial piece of code that you can iterate on. And so that would also be really cool. - One of the things we\nalready talked about is speed, but I wonder if we can just\nlinger on that some more in the various places that\nthe technical details involved in making this thing really fast. So every single aspect of Cursor, most aspects of Cursor feel really fast. Like I mentioned, the Apply\nis probably the slowest thing. I'm sorry, the pain on\nArvid's face as I say that. - I know. It's a pain, it's a\npain that we're feeling and we're working on fixing it. (Arvid and Lex chuckling) - Yeah, it says something that feels, I don't know what it is, like\none second or two seconds, that feels slow. That means that actually shows that everything else is\njust really, really fast. So, is there some technical details about how to make some of these models,\nhow to make the chat fast, how to make the diffs fast? Is there something that\njust jumps to mind? - Yeah. So, we can go over a lot of\nthe strategies that we use. One interesting thing is cache warming. You're probably going to\nuse some piece of context and you can know that before\nthe user's done typing. So as we discussed before, reusing the KV cache\nresults in lower latency, lower costs, cross requests. So as the user starts typing, you can immediately warm the cache with let's say the current file contents, and then when they press Enter, there's very few tokens it\nactually has to pre-fill and compute before\nstarting the generation. This will significantly lower TTFT. - Can you explain how KV cache works? - [Aman] Yeah, so the\nway transformers work. (group chuckling) - I like it. (group chuckling) - The mechanism that allows transformers to not just independently\nlook at each token, but see previous tokens are the keys and values to attention. And generally, the way attention works is you have at your\ncurrent token some query, and then you've all the keys and values of all your previous tokens, which are some kind of representation that the model stores internally\nof all the previous tokens in the prompt. And by default, when you're\ndoing a chat, the model has to, for every single token, do this forward pass\nthrough the entire model. That's a lot of matrix\nmultiplies that happen, and that is really, really slow. Instead, if you have already done that and you stored the keys and values and you keep that in the GPU, let's say I have to sort\nit for the last N tokens. If I now wanna compute the output token for the N+1nth token, I don't need to pass those first N tokens through the entire model because I already have\nall those keys and values. And so, you just need\nto do the forward pass through that last token. And then when you're doing attention, you're reusing those keys and values that have been computed, which is the only kind of sequential part or sequentially dependent\npart of the transformer. - Is there higher level caching\nof caching of the prompts or that kind of stuff that could help?\n- I see. Yeah, there's other types\nof caching you can do. One interesting thing that\nyou can do for Cursor Tab is you can basically predict ahead as if the user would've\naccepted the suggestion and then trigger another request. And so then you've cached,\nyou've done the speculative. It's a mix of speculation\nand caching, right? Because speculating what would\nhappen if they accepted it. And then you have this value\nthat is cached this suggestion. And then when they press Tab, the next one would be\nwaiting for them immediately. It's a clever heuristic/trick that uses a higher level caching. It feels fast despite there\nnot actually being any changes in the model. - And if you can make\nthe KV cache smaller, one of the advantages you get is like maybe you can speculate even more. Maybe you can guess, \"Here's the 10 things\nthat could be useful, predict the next 10,\" and then it's possible the\nuser hits the one of the 10. It's much higher chance than\nthe user hits the exact one that you showed them. Maybe they type in other character and hit something else in the cache. The general phenomena here is, I think it's also super useful for RL is maybe a single sample from\nthe model isn't very good, but if you predict 10 different things, turns out that one of the 10 that's right is the\nprobability is much higher. There's these passive K\ncurves and part of RL, what RL does is you can exploit\nthis passive K phenomena to make many different predictions. And one way to think about this, the model knows internally\nhas some uncertainty over which of the key things is correct or which of the key things\ndoes the human wants? When we RL our Cursor Tab model, one of the things we're\ndoing is we're predicting which of the 100 different\nsuggestions the model produces is more amenable for humans? Which of them do humans\nmore like than other things? Maybe there's something where the model can predict very far ahead versus a little bit, maybe\nsomewhere in the middle. And then you can give\na reward to the things that humans would like more and punish the things that it would like, and then train the model\nto output the suggestions that humans would like more. You have these RL loops\nthat are very useful that exploit these passive K curves. Aman, maybe can go into even more detail. - Yeah, it is a little\ndifferent than speed, but technically, you tie it back in because you can get away\nwith the smaller model if you RL your smaller model and it gets the same\nperformance as the bigger one. So while I was mentioning stuff about KV, about reducing the size of your KV cache, there are other techniques there as well that are really helpful for speed. So, kind of back in the day,\nall the way two years ago, people mainly use multi-head attention, and I think there's been a migration towards more efficient attention\nschemes like group query or multi-query attention, and this is really helpful for\nthen with larger batch sizes being able to generate\nthe tokens much faster. The interesting thing here\nis this now has no effect on that time to first\ntoken pre-fill speed. The thing this matters for\nis now generating tokens. And why is that? 'Cause when you're generating tokens, instead of being bottlenecked by doing these super\nparallelizable matrix multiplies across all your tokens,\nyou're bottlenecked, for a long context with large batch sizes, by how quickly you can read\nthose cache, keys, and values. And so then that's memory bandwidth, and how can we make this faster? We can try to compress the\nsize of these keys and values. So multi-query attention is\nthe most aggressive of these. Where normally with multi-head attention, you have some number of, quote,\nunquote, \"attention heads,\" and some number of query heads. Multi-query just\npreserves the query heads, gets rid of all the key value heads. So there's only one\nkind of key value head, and there's all the remaining query heads. With group query, you instead\npreserve all the query heads. There are fewer heads\nfor the keys and values, but you're not reducing it to just one. But anyways, the whole point here is you're just reducing\nthe size of your KV cache. - And then there is MLA. - Yeah, multi-latent. That's a little more complicated. And the way that this works is\nit kind of turns the entirety of your keys and values\nacross all your heads into this one latent vector that has then kind of\nexpanded in for its time. - But MLA is from this\ncompany called DeepSeek. It's quite an interesting algorithm. Maybe the key idea is in\nboth MQA and in other places, what you're doing is you're\nreducing the number of KV heads. And the advantage you get from\nthat is there's less of them. You want each of the keys and values to actually be different. So, one way to reduce the size is you keep one big shared vector for all the keys and values, and then you have smaller\nvectors for every single token. So that you can store the\nonly the smaller thing as some sort of low-rank reduction. At the end of the time, when you eventually wanna\ncompute the final thing, remember that your memory band, which means that you still\nhave some compute left that you can use for these things. And if you can expand the\nlatent vector back out and somehow this is far more efficient because you're reducing, for example, maybe you're reducing vec 32 or something like the size of the\nvector that you're keeping. - Yeah, there's perhaps some richness in having a separate\nset of keys and values and query that kind of pairwise match up versus compressing that all into one in that interaction at least. - Okay, and all of that is\ndealing with being memory bound. - Yeah. - I mean, ultimately, how does that map to the user experience? Trying to get the- - Yeah, the two things that it maps to is you can now make\nyour cache a lot larger because you've less space\nallocated for the KV cache. You can maybe cache a\nlot more aggressively in a lot more things, so\nyou get more cache hits, which are helpful for reducing\nthe time to first token for the reasons that were\nkind of described earlier. And then the second being, when you start doing inference\nwith more and more requests and larger and larger batch sizes, you don't see much of a slowdown as it's generating the\ntokens at the speed of that. - Well, it also allows you\nto make your prompt bigger for certain-\n- Yeah, yeah. So, the size of your KV cache is both the size of all your prompts, multiplied by the number of prompts being processed in parallel. So you could increase either\nthose dimensions, right? The batch size or the size of your prompts without degrading the\nlatency of generating tokens. - Arvid, you wrote a blog post, \"Shadow Workspace: Iterating\non Code in the Background.\" So, what's going on? - So, to be clear, we want there to be a lot of stuff\nhappening in the background, and we're experimenting\nwith a lot of things. Right now, we don't have\nmuch stuff happening other than the cache warming or figuring out the right context that goes into your command\nkey prompts, for example. But the idea is if you can\nactually spend computation in the background, then\nyou can help the user maybe at a slightly longer time horizon than just predicting the next few lines that you're gonna make. But actually in the next 10 minutes, what are you going to make? And by doing it in background, you can spend more computation doing that. And so the idea of the Shadow\nWorkspace that we implemented, and we use it internally for experiments is that to actually get advantage of doing stuff in the background, you want some kind of feedback signal to give back to the model because otherwise, you\ncan get higher performance by just letting the\nmodel think for longer, and so o1 is a good example of that. But another way you\ncan improve performance is by letting the model\niterate and get feedback. And so, one very important\npiece of feedback when you're a programmer\nis the language server, which is this thing, it exists\nfor most different languages, and there's a separate\nlanguage server per language. And it can tell you, \"You're\nusing the wrong type here,\" and then gives you an error, or it can allow you to go to definition and understands the\nstructure of your code. There is a TypeScript\nlanguage server developed by the TypeScript people, a Rust language server\ndeveloped by the Rust people, and then they all interface over the language server\nprotocol to VS Code. So that VS Code doesn't need to have all of the different languages\nbuilt into VS Code but rather you can use the\nexisting compiler infrastructure. - For linting purposes, what- - It's for linting. It's for going to definition, and for seeing the right\ntypes that you're using. - So it's doing type checking also? - Yes, type checking\nand going to references. And that's like when you're\nworking in a big project, you kind of need that. If you don't have that, it's really hard to code in a big project. - Can you say, again, how\nthat's being used inside Cursor, the language server protocol\ncommunication thing? - So it's being used in Cursor\nto show to the programmer just like in VS Code, but then the idea is you want to show that same\ninformation to the models, the IM models, and you\nwant to do that in a way that doesn't affect the user because you want to do it in background. And so the idea behind\nthe Shadow Workspace was, okay, one way we can do this\nis we spawn a separate window of Cursor that's hidden, and\nso you can set this flag in it and like turn it's hidden. There is a window but you\ndon't actually see it. And inside of this window,\nthe AI agents can modify code however they want as long\nas they don't save it because it's still the same folder, and then can get feedback from the linters and go to definition and\niterate on their code. - So literally run\neverything in the background, right, maybe even run the code. - So that's the eventual version\nand that's what you want. And a lot of the blog\npost is actually about how do you make that happen because it's a little bit tricky. You want it to be on the user's machine so that it exactly mirrors\nthe user's environment. And then on Linux, you\ncan do this cool thing where you can actually\nmirror the file system and have the AI make changes to the files, and it thinks that it's\noperating on the file level, but actually, that's stored in memory and you can create this\nkernel-like extension to make it work. Whereas on Mac and Windows, it's a little bit more difficult, but it's a fun technical\nproblem, so that's why. - One may be hacky but interesting idea that I like is holding a lock on saving. And so basically, you can\nthen have the language model kind of hold the lock on saving to disk, and then instead of you operating in the ground truth version of the files that are saved to disk,\nyou actually are operating what was the Shadow Workspace before and these unsaved things\nthat only exist in memory that you still get linter\nerrors for, and you can code in. And then when you try to maybe run code, it's just like there's a small\nwarning that there's a lock, and then you kind of\nwill take back the lock from the language server if you're trying to do things concurrently or from the Shadow Workspace if you're trying to do\nthings concurrently. - That's such an exciting\nfuture by the way. It's a bit of a tangent, but to allow a model to change files, it's scary for people\nbut it's really cool, to be able to just let the\nagent do a set of tasks and you come back the next\nday and kind of observe, like it's a colleague\nor something like that. - And I think there may be\ndifferent versions of runability for the simple things\nwhere you're doing things in the span of a few minutes\non behalf of the user as they're programming, it makes sense to make something work\nlocally in their machine. I think for the more aggressive things where you're making larger changes that take longer periods of time, you'll probably wanna do this in some sandbox remote environment and that's another\nincredibly tricky problem of how do you exactly reproduce or mostly reproduce to the point of it being effectively\nequivalent for running code the user's environment\nwith this remote sandbox. - I'm curious what kind of\nagents you want for coding? Do you want them to find bugs? Do you want them to\nimplement new features? What agents do you want? - So by the way, when\nI think about agents, I don't think just about coding. I think so for this particular podcast, there's video editing\nand if you look in Adobe, there's code behind. It's very poorly documented code, but you can interact with\nPremiere, for example, using code, and basically all the uploading,\neverything I do on YouTube, everything as you could probably imagine, I do all of that through code\nand including translation and overdubbing, all of this. So, I envision all of\nthose kinds of tasks. So automating many of the tasks that don't have to do directly\nwith the editing, so that. Okay, that's what I was thinking about. But in terms of coding, I would be fundamentally\nthinking about bug finding, many levels of kind of bug finding and also bug finding like logical bugs, not logical like spiritual\nbugs or something. (group chuckling) Ones like big directions\nof implementation, that kind of stuff. - Magical (indistinct) and bug finding. - Yeah, I mean, it's really interesting that these models are\nso bad at bug finding when just naively prompted to find a bug. They're incredibly poorly calibrated. - Even the smartest models.\n- Exactly, even o1. - How do you explain that? Is there a good intuition? - I think these models are\nreally strong reflection of the pre-training distribution, and I do think they generalize as the loss gets lower and lower, but I don't think the loss is low enough such that they're really\nfully generalizing on code. The things that we use these things for, the frontier models that\nthey're quite good at, are really code generation\nand question answering. And these things exist in massive\nquantities in pre-training with all of the code\nin GitHub on the scale of many, many trillions of\ntokens and questions and answers on things like stack overflow\nand maybe GitHub issues. And so, when you try to\npush one of these things that really don't exist very much online, for example, the Cursor Tab objective of predicting the next edit\ngiven the edits done so far, the brittleness kind of shows. And then bug detection\nis another great example, where there aren't\nreally that many examples of actually detecting real\nbugs and then proposing fixes and the models just kind\nof really struggle at it. But I think it's a question\nof transferring the model in the same way that you\nget this fantastic transfer from pre-trained models\njust on code in general to the Cursor Tab objective. You'll see a very, very similar thing with generalized models\nthat are really good at code to bug detection. It just takes a little bit of kind nudging in that direction. - Look, to be clear, I think, they understand code really well. While they're being pre-trained, the representation that's\nbeing built up almost certainly like somewhere in the\nstream, the model knows that maybe there's\nsomething sketchy going on. Part of it is that humans\nare really calibrated on which bugs are really important. It's not just actually saying\nthere's something sketchy. It's like it's this sketchy trivial, it's this sketchy like you're\ngonna take the server down. Part of it is maybe the cultural knowledge of why is a staff engineer is good because they know that three years ago someone wrote a really\nsketchy piece of code that took the server down. (group chuckling) This thing is an experiment. So, a few bugs are fine, you're just trying to experiment and get the feel of the thing. And so if the model gets really annoying when you're writing an\nexperiment, that's really bad, but if you're writing\nsomething for super production, you're writing a database. You're writing code in\nPostgres or Linux or whatever. You're Linus Torvalds. It's sort of unacceptable\nto have even an edge case and just having the calibration\nof how paranoid is the user. - But even then if you're\nputting in a maximum paranoia, it still just doesn't quite get it. - Yeah, yeah, yeah. - I mean, but this is hard\nfor humans too to understand which line of code is\nimportant, which is not. I think one of your\nprinciples on a website says if a code can do a lot of damage, one should add a comment that say, \"This line of code is dangerous.\" - And all caps, repeated 10 times. (group chuckling) - No, you say for every\nsingle line of code inside the function you have\nto, and that's quite profound, that says something about human beings because the engineers move on, even the same person might just forget how it can sink the\nTitanic a single function. You might not intuit that quite clearly by looking at the single piece of code. - Yeah, and I think that\none is partially also for today's AI models where if you actually write\ndangerous, dangerous, dangerous in every single line, the models will pay more attention to that and will be more likely to\nfind bugs in that region. - That's actually just straight\nup a really good practice of labeling code of how\nmuch damages can do. - Yeah, I mean, it's controversial. Some people think it's ugly. Sualeh does not like it. - In fact, I actually think\nthis is one of the things I learned from Arvid. Aesthetically, I don't like it, but I think there's certainly something where it's useful for the models and humans just forget a lot, and it's really easy to\nmake a small mistake. Just bring down the server. Of course, we test a lot and whatever, but there's always these things that you have to be very careful. - Yeah, like with just normal docstrings, I think people will often just skim it when making a change and think,\n\"Oh, I know how to do this,\" and you really need to\npoint it out to them so that doesn't slip through. - Yeah, you have to be reminded that you could do a lot of damage, that's like we don't\nreally think about that. You think about, \"Okay, how\ndo I figure out how this works so I can improve it?\" You don't think about the\nother direction that it could- - Until we have formal\nverification for everything, then you can do whatever you want and you know for certain that\nyou have not introduced a bug if the proof pass. - Well, concretely, what\ndo you think that future would look like? - I think people will just\nnot write to tests anymore. You write a function, the\nmodel will suggest a spec, and you review the spec. And in the meantime, smart\nreasoning model computes a proof that the implementation follows the spec, and I think that happens\nfor most functions. - Do you think this gets at a little bit some of the stuff you\nwere talking about earlier with the difficulty of specifying intent for what you want with software, where sometimes it might be because the intent is\nreally hard to specify, it's also then going to\nbe really hard to prove that it's actually matching\nwhatever your intent is? - You think that spec is hard to generate? - Yeah, or just for a given spec. I think there is a question of, can you actually do the\nformal verification? Is that possible? I think that there's more to\ndig into there, but then also- - Even if you have the spec? - If you have the spec-\n- Even if you have the spec, is the spec written in natural language? Or is it- - No, the spec would be formal. - But how easier would\nthat be (indistinct). - Okay, so then I think\nthat you care about things that are not going to\nbe easily well specified in the spec language. - I see, I see, yeah, yeah. - Would be maybe an argument\nagainst formal verification is all you need. - The worry is there's\nthis massive document- - Replacing something\nlike unit tests, sure. - Yeah, yeah. I think you can probably also\nevolve the spec languages to capture some of the things that they don't really capture right now. I don't know, I think it's very exciting. - And you're speaking not\njust about single functions, you're speaking about entire code bases. - I think entire code bases is harder, but that is what I would love to have and I think it should be possible. There's a lot of work recently where you can prove formally\nverified down to the hardware. You formally verify the C code,\nand then you formally verify through the GCC compiler, and then through the Verilog\ndown to the hardware. And that's incredibly big\nsystem, but it actually works. And I think big code bases\nare sort of similar in that and they're like multi-layered system. And if you can decompose it\nand formally verify each part, then I think it should be possible. I think this specification\nproblem is a real problem. - How do you handle side\neffects or how do you handle, I guess, external dependencies\nlike calling the Stripe API? - Maybe Stripe would write\na spec for their API. - But you can't do this for everything. Can you do this for everything you use? Maybe people will use\nlanguage models as primitives in the programs they write, and there's a dependence on it and how do you now include that? - I think you might be\nable to prove that still. - Prove what about language models? - I think it feels possible\nthat you could actually prove that a language model\nis aligned, for example, or you can prove that it\nactually gives the right answer. - That's the dream. - Yeah, I mean, if it's possible. That's your I have a dream speech. If it's possible, that will certainly help with making sure your\ncode doesn't have bugs and making sure AI doesn't\ndestroy all human civilization. So, the full spectrum of AI\nsafety to just bug finding. So, you said the models\nstruggle with bug finding. What's the hope? - My hope initially is, and I\ncan let Michael chime in too, but it was like it should first\nhelp with the stupid bugs. It should query quickly, catch the stupid bugs off by one error. Sometimes you write something in a comment and do the other way. It's very common. I do this. I write less than in a comment and I maybe write the greater\nthan or something like that. And the model is like,\n\"Yeah, you looks sketchy. You sure you wanna do that?\" But eventually, it should be\nable to catch harder bugs too. - Yeah, and I think that\nit's also important to note that having good bug, finding\nmodels feels necessary to get to the highest reaches of having AI do more and\nmore programming for you. If AI is building more and\nmore of the system for you, you need to not just\ngenerate but also verify. And without that, some of the problems that we've talked about\nbefore with programming, with these models will\njust become untenable. So it's not just for humans\nlike you write a bug, I write a bug, find the bug for me, but it's also being able\nto verify the AI's code and check it is really important. - Yeah, and then how do\nyou actually do this? We have had a lot of\ncontentious dinner discussions of how do you actually train a bug model, but one very popular idea\nis it's potentially easy to introduce a bug than\nactually finding the bug. And so, you can train a\nmodel to introduce bugs in existing code, and then you can train\na reverse bug model then that can find bugs using\nthis synthetic data. So that's one example, but there are lots of ideas\nfor how to (indistinct). - You can also do a bunch of work not even at the model level\nof taking the biggest models and then maybe giving them\naccess to a lot of information that's not just the code. It's a hard problem to\nstare at a file and be like, \"Where's the bug?\" And that's hard for humans often, right? And so often, you have to run the code and being able to see things like traces and step through a debugger, there's another whole other direction where it tends toward that. - It could also be that there are two different\nproduct form factors here. It could be that you have\na really specialty model that's quite fast that's\nrunning in the background and trying to spot bugs. And it might be that sometimes\nto Arvid's earlier example about some nefarious input box bug. You know there's a bug, you're not just checking hypothesis free, you're like, \"This is a problem,\nI really wanna solve it,\" and you zap that with tons\nand tons and tons of compute, and you're willing to put\nin $50 to solve that bug or something even more. - Have you thought about integrating money into this whole thing? I would pay probably a\nlarge amount of money if you found a bug or even generated code that I really appreciated. I had a moment a few days ago\nwhen I started using Cursor where it generated perfect three functions for interacting with the\nYouTube API to update captions for localization in different languages. The API documentation is not\nvery good and the code across. I googled it for a while. I couldn't find exactly, there's a lot of confusing information, and Cursor generated perfectly. I just sit back, I read\nthe code, I was like, \"This is correct, I\ntested it, it's correct.\" I was like, \"I wanna tip.\" I want a button that goes, \"Here's $5.\" One that's really good\njust to support the company and support what the interface is. And the other is that\nprobably sends a strong signal like good job. (all chuckling) So, there's this much stronger signal than just accepting the code, right? You just actually send a strong good job. That and for bug finding, obviously, there's a lot of people that would pay a huge amount of money for a bug bounty thing, right? You guys think about that? - Yeah, it's a controversial\nidea inside the company. I think it depends on how much you believe\nin humanity almost. I think it would be really cool if you spend nothing to try to find a bug. And if it doesn't find\na bug, you spend $0. And then if it does find a\nbug and you click accept, then it also shows in parentheses like $1. And so, you spend $1 to accept the bug. And then, of course, there's a worry like, \"Okay, we spent a lot of computation, maybe people will just copy paste.\" I think that's a worry. Then there is also the\nworry that introducing money into the product. It doesn't feel as fun anymore. You have to think about money. And all you want to\nthink about is the code, and so maybe it actually makes more sense to separate it out, and you\npay some fee every month, and then you get all of\nthese things for free. - But there could be a tipping component which is not like it cost this- - Yes, but it still\nhas that dollar symbol. I think it's fine, but\nI also see the point where maybe you don't\nwant to introduce it. - Yeah, I was gonna say the moment that feels like people do\nthis is when they share it. When they have this fantastic example, they just share it with their friends. - There is also a potential world where there's a technical solution to this like honor system problem too, where if we can get to a place where we understand the\noutput of the system more, I mean, to the stuff we were talking about with error checking with the LSP and then also running the code. But if you could get to a place where you could actually somehow verify, \"Oh, I have fixed the bug,\"\nmaybe then the bounty system doesn't need to rely on\nthe honor system too. - How much interaction is there between the terminal and the code? How much information is gained from if you run the code in the terminal? Can you do a loop where it runs the code and suggests how to change the code? If the code and runtime gets an error? Is right now there's\nseparate worlds completely? I know you can do control\nK inside the terminal to help you write the code. - You can use terminal context as well inside of check Command+K\nkind of everything. We don't have the looping part yet, so we suspect something like\nthis could make a lot of sense. There's a question of whether it happens in the foreground too or if\nit happens in the background like what we've been discussing. - Sure, the background's pretty cool. I could be running the\ncode in different ways. Plus there's a database side to this, which how do you protect it\nfrom not modifying the database, but okay. (group chuckling) - I mean, there's certainly\ncool solutions there. There's this new API\nthat is being developed. It's not in AWS, but\nit certainly, I think, it's in PlanetScale. I don't know if PlanetScale was\nthe first one to you add it. It's this ability sort of\nadd branches to a database, which is like if you're\nworking on a feature and you wanna test against\nthe broad database, but you don't actually want to test against the broad database, you could add a branch to the database. And the way they do that\nis they add a branch to the write-ahead log. And there's obviously a\nlot of technical complexity in doing it correctly. I guess database companies\nneed new things to do. (group chuckling) They have good databases now. And I think turbopuffer, which is one of the databases we use, is going to add maybe branching\nto the write-ahead log. So maybe the AI agents will use branching, they'll test against some branch, and it's gonna be a\nrequirement for the database to support branching or something. - It would be really interesting if you could branch a file system, right? - Yeah. I feel like everything needs branching. - [Aman] Yeah. - Yeah. The problem with the multiverse, right? (group chuckling) If you branch on everything\nthat's like a lot. - There's obviously these\nsuper clever algorithms to make sure that you don't\nactually use a lot of space or CPU or whatever. - Okay, this is a good place\nto ask about infrastructure. So, you guys mostly use AWS, what are some interesting details? What are some interesting challenges? Why'd you choose AWS? Why is AWS still winning? Hashtag. - AWS is just really, really good. It is really good. Whenever you use an AWS product, you just know that it's going to work. It might be absolute hell\nto go through the steps to set it up. - Why is the interface so horrible? - Because it's. (chuckles) - It's just so good. It doesn't need to-\n- It's the nature of winning. (group chuckling) - I think it's exactly, it's\njust nature they're winning. - Yeah, yeah. But AWS we can always\ntrust, it will always work. And if there is a problem,\nit's probably your problem. (Lex chuckles)\nYeah. - Okay, is there some\ninteresting challenges, you guys are pretty\nnew startup to scaling, to so many people. - Yeah, I think that it has\nbeen an interesting journey adding each extra zero to\nthe request per second. (Lex chuckles) You run into all of these\nwith the general components you're using for caching and databases, run into issues as you make\nthings bigger and bigger, and now we're at the scale\nwhere we get into overflows on our tables and things like that. And then, also there have\nbeen some custom systems that we've built. For instance, our retrieval\nsystem for computing, a semantic index of your code\nbase and answering questions about a code base that have continually, I feel like, been one of the\ntrickier things to scale. - I have a few friends who\nare super senior engineers and one of their lines is,\nit's very hard to predict where systems will break\nwhen you scale them. You can try to predict in advance, but there's always something\nweird that's gonna happen when you add these extras here. You thought through everything, which you didn't actually\nthink through everything. But I think for that particular system, we chunk up all of your code, and then we send up the code for embedding and we embed the code. And then, we store the\nembeddings in a database, but we don't actually\nstore any of the code. And then there's reasons\naround making sure that we don't introduce client bugs because we're very, very\nparanoid about client bugs. We store much of the\ndetails on the server. Everything is encrypted. So, one of the technical\nchallenges is always making sure that the local index, the local\ncode base state is the same as the state that is on the server. The way, technically, we\nended up doing that is, for every single file\nyou can keep this hash, and then for every folder\nyou can keep a hash, which is the hash of all of its children. You can recursively do that until the top. Why do something complicated? One thing you could do is you could keep a hash for every file, and every minute, you could\ntry to download the hashes that are on the server,\nfigure out what are the files that don't exist on the server. Maybe you just created a new file, maybe you just deleted a file, maybe you checked out a new branch, and try to reconcile the state between the client and the server. But that introduces absolutely\nginormous network overhead both on the client side. Nobody really wants us to\nhammer their WiFi all the time if you're using Cursor. But also, it would\nintroduce ginormous overhead on the database. It would be reading these\ntens of terabytes database, approaching 20 terabytes or something data base every second. That's just crazy. You definitely don't wanna do that. So what you do, you just try\nto reconcile the single hash, which is at the root of the project. And then if something\nmismatches, then you go, you find where all the things disagree. Maybe you look at the children\nand see if the hashes match. If the hashes don't match, go look at their children and so on. But you only do that in the scenario where things don't match. For most people, most of\nthe time, the hashes match. - So it's like a\nhierarchical reconciliation of hashes.\n- Yeah, something like that. - Yeah, it's called a Merkle tree. - Yeah, Merkle.\n- Yeah. - Yeah. This is cool to see that you have to think\nthrough all these problems. - The reason it's gotten hard is just because the\nnumber of people using it and some of your customers have really, really large code bases. We originally reordered dark\ncode base, which is big, but it's just not the size of some company that's been there for 20 years and has a ginormous number of files and you wanna scale\nthat across programmers. There's all these details where building the simple thing is easy, but scaling it to a lot of people, a lot of companies is\nobviously a difficult problem, which is independent of, actually, so that there's part of this scaling. Our current solution is also\ncoming up with new ideas that, obviously, we're working on, but then scaling all of that\nin the last few weeks, months. - Yeah. And there are a lot of clever things, additional things that go\ninto this indexing system. For example, the bottleneck\nin terms of costs is not soaring things\nin the vector database or the database, it's\nactually embedding the code. You don't wanna re-embed the code base for every single person in a company that is using the same exact code except for maybe they're\na different branch with a few different files or they've made a few local changes. Because again, embeddings\nare the bottleneck, you can do this one clever trick and not have to worry about the complexity of dealing with branches\nand the other databases where you just have some cash\non the actual vectors computed from the hash of a given chunk. - Mm-hmm. - So this means that when the\nnth person at a company goes and embed their code base,\nit's really, really fast. You do all this without\nactually storing any code on our servers at all. No code data is stored. We just store the vectors\nin the vector database and the vector cache. - What's the biggest\ngains at this time you get from indexing the code base? Just out of curiosity,\nwhat benefit do users have? It seems like longer term, there'll be more and more\nbenefit, but in the short term, just asking questions of the code base, what's the usefulness of that? - I think the most obvious one\nis just, you want to find out where something is happening\nin your large code base, and you have a fuzzy memory of, \"Okay, I want to find\nthe place where we do X,\" but you don't exactly\nknow what to search for in a normal text search. So you ask a chat, you hit Command+Enter to ask with the code base chat. And then very often, it\nfinds the right place that you were thinking of. - Like you mentioned, in the future, I think there's only going to\nget more and more powerful, where we're working a lot\non improving the quality of our retrieval. I think the ceiling for that\nis really, really much higher than people give the credit for. - One question that's good to\nask here, have you considered and why haven't you much done local stuff, it seems like everything\nwas just discussed as exceptionally difficult to do. To go to the cloud, you have\nto think about all these things with the caching and the large code base where a large number of\nprogrammers are using the same code base. You have to figure out the puzzle of that. A lot of it, most software just does this heavy\ncomputational stuff locally. So, have you considered\ndoing embeddings locally? - Yeah, we thought about it, and I think it would be\ncool to do it locally. I think it's just really hard. One thing to keep in mind is that some of our users\nuse the latest MacBook Pro, but most of our users,\nmore than 80% of our users are in Windows machines, which many of them are not very powerful. So, local models really only\nworks on the latest computers, and it's also a big\noverhead to build that in. So even if we would like to do that, it's currently not something\nthat we are able to focus on. I think there are some\npeople that do that, and I think that's great, but especially as models\nget bigger and bigger and you want to do fancier\nthings with bigger models, it becomes even harder to do it locally. - Yeah, it's not a problem\nof weaker computers. It's just that for example,\nif you're some big company, you have big company code base. It's just really hard to\nprocess big company code base even on the beefiest MacBook Pros. It's not even a matter of\nif you're just a student or something. I think, if you're the best\nprogrammer at a big company, you're still gonna have\na horrible experience. If you do everything locally\nwhere you could do it and scrape by, but again,\nit wouldn't be fun anymore. - Yeah, like at approximate\nnearest neighbors and this massive code base is\ngonna just eat up your memory and your CPU, and it's based off of that. That's just that. Let's talk about also\nthe modeling side where, as Arvid said, there are\nthese massive headwinds against local models where one, things that seem to move\ntowards MOEs, which one benefit is maybe their more\nmemory bandwidth bound, which plays in favor of\nlocal versus using GPUs or using Nvidia GPUs. But the downside is, these\nmodels are just bigger in total, and they're gonna need to fit, often not even on a single\nnode but multiple nodes. There's no way that's gonna fit inside of even really good MacBooks. I think especially for coding, it's not a question as much of, does it clear some bar of\nthe model's good enough to do these things and\nthen we're satisfied? Which may be the case for other problems and maybe where local models shine, but people are always gonna want the best, the most intelligent,\nthe most capable things, and that's gonna be\nreally, really hard to run for almost all people, locally. - Don't you want the most capable model? You want Sonnet too? - And also o1- (Lex chuckling) - I like how you're pitching me. (group chuckling) - O1 is another- - Would you be satisfied\nwith an inferior model? Listen, yes, I'm one of those, but there's some people that\nlike to do stuff locally, really, there's a whole\nobviously open source movement that resists. It's good that they exist actually because you wanna resist the power centers that are growing our- - There's actually an\nalternative to local models that I am particularly fond of. I think it's still very\nmuch in the research stage, but you could imagine to\ndo homomorphic encryption for language model inference. So you encrypt your input\non your local machine, then you send that up, and then the server can\nuse loss of computation. They can run models that\nyou cannot run locally on this encrypted data, but they cannot see what the data is, and then they send back the answer and you decrypt the answer and\nonly you can see the answer. So I think that's still very much research and all of it is about trying\nto make the overhead lower because right now, the\noverhead is really big, but if you can make that happen, I think that would be really, really cool, and I think it would be\nreally, really impactful because I think one thing that's\nactually worrisome is that, as these models get better and better, they're going to become more\nand more economically useful. And so, more and more of the\nworld's information and data will flow through one or\ntwo centralized actors. And then there are worries about, there can be traditional hacker attempts, but it also creates this scary part where if all of the world's\ninformation is flowing through one node in plaintext, you can have surveillance\nin very bad ways. Initially, will be good reasons. People will want to try to protect against bad actors using\nAI models in bad ways, and then you will add in\nsome surveillance code. And then, someone else will come in and you're on a slippery slope, and then you start doing bad things with a lot of the world's data. So, I am very hopeful that we can solve homomorphic encryption for language model inference. - Yeah, and doing privacy,\npreserving machine learning. But I would say, that's\nthe challenge we have with all software these days. It's like, there's so many\nfeatures that can be provided from the cloud and all us\nincreasingly rely on it and make our life awesome. But there's downsides, and that's why you rely\non really good security to protect from basic attacks. But there's also only a\nsmall set of companies that are controlling that data, and they obviously have leverage and they could be infiltrated\nin all kinds of ways. That's the world we live in. - Yeah, the thing I'm just\nactually quite worried about is Anthropic has this\nresponsible scaling policy where we're the low ASLs, which is the Anthropic\nsecurity level or whatever of the models. But as we get to, quote,\nunquote, \"ASL-3, ASL-4,\" whatever models which are very powerful. But for mostly reasonable\nsecurity reasons, you would wanna monitor all the prompts. But I think that's\nreasonable and understandable where everyone is coming from. But man, it'd be really horrible if all the world's information\nis monitored that heavily, it's way too centralized. It's like this really\nfine line you're walking where on the one side, you don't want the models to go rogue. On the other side, humans like, I don't know if I trust\nall the world's information to pass through three model providers. - Yeah. - Why do you think it's\ndifferent than cloud providers? - Because I think a lot of\nthis data would never have gone to the cloud providers in the first place. You want to give more\ndata to the AI models, you want to give personal data that you would never have\nput online in the first place to these companies or to these models. It also centralizes control\nwhere right now, for cloud, you can often use your\nown encryption keys, and AWS can't really do much. But here, it's just centralized actors that see the exact plain\ntext of everything. - On the topic of a context, that's actually been a friction for me. When I'm writing code in Python, there's a bunch of stuff imported. You could probably\nintuit the kind of stuff I would like to include in the context. How hard is it to auto\nfigure out the context? - It's tricky. I think we can do a lot better at computing the context\nautomatically in the future. One thing that's important to note is, there are trade-offs with\nincluding automatic context. So, the more context you\ninclude for these models, first of all, the slower they are and the more expensive those requests are, which means you can\nthen do less model calls and do less fancy stuff in the background. Also, for a lot of these\nmodels, they get confused if you have a lot of\ninformation in the prompt. So the bar for accuracy and for relevance of the context you include\nshould be quite high. Already, we do some automatic context in some places within the product. It's definitely something we\nwanna get a lot better at. I think that there are a lot\nof cool ideas to try there, both on the learning\nbetter retrieval systems, like better embedding\nmodels, better rerankers. I think that there are\nalso cool academic ideas, stuff we've tried out internally, but also the field is grappling\nwith writ large about, can you get language models to a place where you can actually\njust have the model itself understand a new corpus of information? The most popular talked\nabout version of this is can you make the context windows infinite? Then if you make the\ncontext windows infinite, can you make the model\nactually pay attention to the infinite context? And then, after you can\nmake it pay attention to the infinite context to\nmake it somewhat feasible to actually do it, can you then do caching for that infinite context? You don't have to recompute\nthat all the time. But there are other cool\nideas that are being tried, that are a little bit more\nanalogous to fine-tuning of actually learning this information in the weights of the model. It might be that you\nactually get a qualitative lead different type of understanding if you do it more at the weight level than if you do it at the\nin-context learning level. I think the jury's still a little bit out on how this is all gonna work in the end? But in the interim, us as a company, we are really excited about\nbetter retrieval systems and picking the parts of the code base that are most relevant\nto what you're doing, and we could do that a lot better. - One interesting proof of concept for the learning this knowledge\ndirectly in the weights is with VS Code. So, we're in a VS Code fork and VS Code. The code is all public. So these models in pre-training\nhave seen all the code. They've probably also seen\nquestions and answers about it. And then, they've been\nfine-tuned and RLHFed to be able to answer questions\nabout code in general. So when you ask it a\nquestion about VS Code, sometimes it'll hallucinate, but sometimes it actually\ndoes a pretty good job at answering the question. It happens to be okay, but what if you could\nactually specifically train or post-train a model such\nthat it really was built to understand this code base? It's an open research question, one that we're quite interested in. And then there's also uncertainty of, do you want the model to be the thing that end-to-end is doing everything, i.e., it's doing the\nretrieval in its internals, and then answering a\nquestion, creating the code, or do you want to separate the retrieval from the frontier model, where maybe you'll get\nsome really capable models that are much better than\nthe best open source ones in a handful of months? And then, you'll want to separately train a really good open source\nmodel to be the retriever, to be the thing that feeds in the context to these larger models. - Can you speak a little\nmore to post-training a model to understand the code base? What do you mean by that? Is this a synthetic data direction? Is this- - Yeah, there are many possible\nways you could try doing it. There's certainly no shortage of ideas. It's just a question of going\nin and trying all of them and being empirical about\nwhich one works best. One very naive thing is to\ntry to replicate what's done with VS Code and these frontier models. So, let's continue pre-training. Some kind of continued pre-training that includes general code data but also throws in of the data\nof some particular repository that you care about. And then in post-training, meaning, let's just start with\ninstruction fine-tuning. You have a normal instruction\nfine-tuning data set about code. Then you throw in a lot\nof questions about code in that repository. So, you could either\nget ground truth ones, which might be difficult or\nyou could do what you hinted at or suggested using synthetic data, i.e., having the model ask questions about various recent pieces of the code. So you take the pieces of the code, then prompt the model or have\na model propose a question for that piece of code, and then add those as instruction\nfine-tuning data points. And then in theory, this might\nunlock the model's ability to answer questions about that code base. - Let me ask you about OpenAI o1. What do you think is the role of that kind of test time\ncompute system in programming? - I think test time compute\nis really, really interesting. So, there's been the pre-training regime as you scale up the amount of data and the size of your model, get you better and better\nperformance both on loss, and then on downstream benchmarks and just general performance, so we use it for coding or other tasks. We're starting to hit\na bit of a data wall, meaning, it's going to be hard to continue scaling up this regime. So, scaling up test time\ncompute is an interesting way, if now increasing the number\nof inference time flops. Yeah, as you increase the number of flops you use inference\ntime getting corresponding improvements in the\nperformance of these models. Traditionally, we just had to\nliterally train a bigger model that always used that many more flops, but now, we could perhaps\nuse the same size model and run it for longer to\nbe able to get an answer at the quality of a much larger model. And so, the really interesting\nthing I like about this is there are some problems\nthat perhaps require 100 trillion parameter\nmodel intelligence trained on 100 trillion tokens. But that's maybe 1%,\nmaybe .1% of all queries. So are you going to\nspend all of this effort, all of this compute training\na model that costs that much and then run it so infrequently? You train the model that is capable of doing the 99.9% of queries, then you have a way of\ninference time running it longer for those few people that really, really want max intelligence. - How do you figure out\nwhich problem requires what level of intelligence? Is that possible to dynamically figure out when to use GPT-4, when\nto use a small model and when you need the o1? (group chuckles) - Yeah, that's an open\nresearch problem, certainly. I don't think anyone's actually cracked this model routing problem quite well. We have initial implementations of this for something like Cursor Tab, but at the level of going\nbetween 4o Sonnet to o1, it's a bit trickier. There's also a question like, what level of intelligence\ndo you need to determine if the thing is too hard\nfor the four level model? Maybe you need the o1 level model. It's really unclear. - But you mentioned this. So, there's a pre-training process then there's post-training, and then there's test time compute. Is that fair to separate? Where's the biggest gains? - Well, it's weird\nbecause test time compute, there's a whole training strategy needed to get test time compute to work. The other really weird thing about this is outside of the big labs\nand maybe even just OpenAI, no one really knows how it works. There've been some\nreally interesting papers that show hints of what\nthey might be doing. So, perhaps they're doing something with tree search using\nprocess reward models. But yeah, I think the issue is we don't quite know\nexactly what it looks like, so it would be hard to\ncomment on where it fits in. I would put it in post-training, but maybe the compute spent for this forgetting test time\ncompute to work for a model is going to dwarf pre-training eventually. - So we don't even know if o1\nis using just chain of thought or we don't know how\nthey're using any of these? We don't know anything? - It's fun to speculate. (group chuckling) - If you were to build a competing\nmodel, what would you do? - Yeah, so one thing to do would be, I think you probably need to\ntrain a process reward model. So maybe we can get into reward models and outcome reward models\nversus process reward models. Outcome reward models are\nthe traditional reward models that people are trained\nfor language modeling, and it's just looking at the final thing. So if you're doing some math problem, let's look at that final thing. You've done everything and\nlet's assign a grade to it, how likely we think. What's the reward for this outcome? Process reward models instead try to grade the chain of thought. And so OpenAI had preliminary\npaper on this, I think, last summer where they use human labelers to get this pretty large several\nhundred thousand data set of creating chains of thought. Ultimately, it feels like I haven't seen anything\ninteresting in the ways that people use process reward models outside of just using it\nas a means of affecting how we choose between a bunch of samples. So, what people do in all these papers is they sample a bunch of\noutputs from the language model, and then use the process reward models to grade all those generations alongside maybe some other heuristics, and then use that to\nchoose the best answer. The really interesting thing\nthat people think might work and people want to work is tree search with these process reward models. Because if you really can\ngrade every single step of the chain of thought,\nthen you can branch out and explore multiple paths\nof this chain of thought and then use these process\nreward models to evaluate how good is this branch\nthat you're taking. - Yeah, when the quality of the branch is somehow strongly correlated with the quality of the\noutcome at the very end, so you have a good model of\nknowing which branch to take. So not just in the short\nterm, in the long term? - Yeah. The interesting work that\nI think has been done is figuring out how to\nproperly train the process, or the interesting work\nthat has been open sourced and people I think talk about is how to train the process reward models, maybe in a more automated way. I could be wrong here, could\nnot be mentioning some papers. I haven't seen anything super\nthat seems to work really well for using the process\nreward models creatively to do tree search and code. - This is an AI safety, maybe a bit of a philosophy question. So OpenAI says that they're\nhiding the chain of thought from the user, and they've said that that was\na difficult decision to make. Instead of showing the chain of thought, they're asking the model to\nsummarize the chain of thought. They're also in the background saying they're going to monitor\nthe chain of thought to make sure the model is not\ntrying to manipulate the user, which is a fascinating possibility. But anyway, what do you think about hiding the chain of thought? - One consideration for OpenAI, and this is completely speculative, could be that they wanna\nmake it hard for people to distill these capabilities\nout of their model. It might actually be easier if you had access to that\nhidden chain of thought to replicate the technology,\nbecause pretty important data, like seeing the steps that the model took to get to the final results. - So, you could probably\ntrain on that also? - And there was a mirror\nsituation with this, with some of the large\nlanguage model providers, and also this is speculation, but some of these APIs\nused to offer easy access to log probabilities for all the tokens that they're generating and also log probabilities\nover the prompt tokens. And then some of these\nAPIs took those away. Again, complete speculation,\nbut one of the thoughts is that the reason those were taken away is if you have access to log probabilities similar to this hidden chain of thought, that can give you even more information to try and distill these\ncapabilities out of the APIs, out of these biggest models\nand to models you control. As an asterisk on also\nthe previous discussion about us integrating o1, I think that we're still\nlearning how to use this model. So, we made o1 available in Cursor because when we got the model, we were really interested\nin trying it out. I think a lot of programmers\nare gonna be interested in trying it out. O1 is not part of the\ndefault Cursor experience in any way up, and we still haven't found\na way to yet integrate it into the editor in a way\nthat we reach for every hour, maybe even every day. So, I think that the jury's still out on how to use the model, and we haven't seen examples\nyet of people releasing things where it seems really clear like, \"Oh, that's now the use case.\" The obvious one to turn to is maybe this can make it easier for you to have these\nbackground things running, to have these models and loops, to have these models be agentic. But we're still discovering. - To be clear, we have ideas. We just need to try and get\nsomething incredibly useful before we put it out there. - But it has these\nsignificant limitations. Even barring capabilities,\nit does not stream. That means it's really, really\npainful to use for things where you want to supervise the output. Instead, you're just waiting\nfor the wall text to show up. Also, it does feel like the\nearly innings of test time, compute and search where it's\njust a very, very much a v0, and there's so many things\nthat don't feel quite right. I suspect in parallel to\npeople increasing the amount of pre-training data and the size of the\nmodels and pre-training and finding tricks there, you'll\nnow have this other thread of getting search to\nwork better and better. - So, let me ask you about\nstrawberry tomorrow eyes. (group chuckles) So, it looks like GitHub\nCopilot might be integrating o1 in some kind of way, and I think some of the\ncomments are saying, does this mean Cursor is done? (group chuckles) I think I saw one comment saying that. - It's a time to shut down Cursor, yeah. - Time to shut down Cursor, thank you. (group chuckling) So, is it time to shut down Cursor? - I think this space is\na little bit different from past software spaces over the 2010s, where I think that the ceiling here is really, really, really incredibly high. So, I think that the best\nproduct in three to four years will just be soon much more useful than the best product today. You can wax poetic about\nmoats this and brand that and this is our advantage,\nbut I think in the end, just if you stop innovating\non the product, you will lose. That's also great for startups, that's great for people\ntrying to enter this market because it means you have an opportunity to win against people who\nhave lots of users already by just building something better. And so, I think over the next few years, it's just about building the best product, building the best system,\nand that both comes down to the modeling engine side of things, and it also comes down to\nthe editing experience. - Yeah, I think most of the\nadditional value from Cursor versus everything else out there is not just integrating\nthe new model fast like o1. It comes from all of the depth that goes into these custom models that you don't realize are working for you in every facet of the product, as well as the really thoughtful UX with every single feature. - All right, from that profound answer, let's descend back down to the technical. You mentioned you have a\ntaxonomy of synthetic data. - (chuckles) Oh, yeah. - Can you please explain? - Yeah, I think there are three main kinds of synthetic data. So what is synthetic data, first? So there's normal data,\nlike non-synthetic data, which is just data\nthat's naturally created, i.e., usually it'll be from\nhumans having done things. So, from some human\nprocess you get this data. Synthetic data, the first\none would be distillation. So having a language model, output tokens or probability\ndistributions over tokens, and then you can train some\nless capable model on this. This approach is not gonna\nget you a more capable model than the original one that\nhas produced the tokens, but it's really useful if there's some capability\nyou wanna elicit from some really expensive\nhigh-latency model. You can then distill that down into some smaller task-specific model. The second kind is when one\ndirection of the problem is easier than the reverse. So, a great example of\nthis is bug detection, like we mentioned earlier, where it's a lot easier to\nintroduce reasonable-looking bugs than it is to actually detect them. And this is probably\nthe case for humans too. And so what you can do,\nis you can get a model that's not trained in that much\ndata, that's not that smart, to introduce a bunch of bugs and code. And then, you can use that to then train. Use the synthetic data to train a model that can be really good at detecting bugs. The last category I think\nis, I guess the main one that it feels like the big labs are doing for synthetic data,\nwhich is producing text with language models that\ncan then be verified easily. So, extreme example of this is if you have a verification\nsystem that can detect if language is Shakespeare level, and then you have a bunch of\nmonkeys typing and typewriters. You can eventually get\nenough training data to train a Shakespeare-level\nlanguage model. And I mean this is very\nmuch the case for math where verification is\nactually really, really easy for formal languages. And then what you can do, is\nyou can have an okay model, generate a ton of rollouts,\nand then choose the ones that you know have actually proved the ground truth theorems,\nand train that further. There's similar things you can do for code with lead code like problems, where if you have some set of tests that you know correspond to if\nsomething passes these tests, it actually solved problem.\nYou could do the same thing where you verify that it's passed the test and then train the model in the outputs that have passed the tests. I think it's gonna be a little\ntricky getting this to work in all domains, or just in general. Having the perfect verifier\nfeels really, really hard to do with just open-ended miscellaneous tasks. You give the model or\nmore long horizon tasks, even in coding. - [Lex] That's 'cause you're\nnot as optimistic as Arvid. But yeah, so yeah, (Aman chuckles) that third category\nrequires having a verifier. - Yeah. Verification, it feels like\nit's best when you know for a fact that it's correct. And then it wouldn't be\nlike using a language model to verify, it would be using\ntests or formal systems. - Or running the thing too. Doing the human form of verification, where you just do manual quality control. - Yeah.\n- Yeah. - But the language model version of that, where it's running the thing and it actually understands the output. - Yeah, no, that's- - I'm sure it's somewhere in between. - Yeah. I think that's the category\nthat is most likely to result in massive gains. - What about RL with feedback\nside RLHF versus RLAIF? What's the role of that in getting better\nperformance on the models? - Yeah. So, RLHF is when the reward\nmodel you use is trained from some labels you've collected from humans giving feedback. I think this works if you have the ability to get a ton of human feedback for this kind of task that you care about. RLAIF is interesting because it's depending on the\nconstraint that verification is actually a decent bit\neasier than generation. Because it feels like,\nokay, what are you doing? Are you using this language model to look at the language model outputs and then prove the language model? But no, it actually may work if the language model has a\nmuch easier time verifying some solution than it does generating it. Then you actually could perhaps\nget this recursive loop. But I don't think it's gonna\nlook exactly like that. The other thing you could\ndo, that we kind of do, is a little bit of a\nmix of RLAIF and RLHF, where usually the model\nis actually quite correct and this is the case of\nprecursor tap picking between two possible generations\nof what is the better one. And then, it just needs a\nlittle bit of human nudging with only on the order 50, 100 examples to align that prior the model has with exactly with what you want. It looks different than\nI think normal RLHF where you're usually\ntraining these reward models in tons of examples. - What's your intuition\nwhen you compare generation and verification or\ngeneration and ranking? Is ranking way easier than generation? - My intuition would just\nsay, yeah, it should be. Like, if you believe P does not equal NP, then there's this\nmassive class of problems that are much, much easier\nto verify given proof, than actually proving it. - I wonder if the same thing\nwill prove P not equal to NP or P equal to NP. - (chuckles) That would be really cool. - That'd be a whatever Field's Medal (group giggling) by AI. Who gets the credit? Another the open philosophical question. (group chuckling) - Whoever prompted it. (group chuckling) - I'm actually surprisingly curious what a good bet for one AI will get the Field's Medal will be. I actually don't have-\n- Isn't this Aman's specialty? - I don't know what Aman's bet here is. - Oh, sorry, Nobel Prize\nor Field's Medal first? - Field's Medal-\n- Oh, Field's Medal level? - Field's Medal comes first, I think. - Field's Medal comes first. Well, you would say that, of course. (group chuckling) - But it's also this\nisolated system you verify. - Sure.\n- Yeah. - I don't even know if I- - You don't need to do (indistinct). - I feel like I have\nmuch more to do there. It felt like the path to get to IMO was a little bit more clear. Because it already could\nget a few IMO problems and there was a bunch\nof low-hanging fruit, given the literature at the time, of what tactics people could take. I think I'm, one, much less versed in the space of theorem proving now. And two, less intuition about\nhow close we are to solving these really, really hard open problems. - So you think you'll\nbe Field's Medal first? It won't be in physics or in- - Oh, 100%, I think that's\nprobably more likely. It is probably much more\nlikely that it'll get in. Yeah, yeah, yeah. Well, I think it both\nto, I don't know, BSD, which is a Birch and\nSwinnerton-Dyer conjecture, or (indistinct) iPods, or any one of these hard math problems are just actually really hard. It's unclear what the path to get even a solution looks like. We don't even know what a path looks like, let alone (indistinct). - And you don't buy the idea this is just like an isolated system and you can actually have\na good reward system, and it feels like it's\neasier to train for that. - I think we might get\nField's Medal before AGI. - I mean, I'd be very happy. I'd be very happy. But I don't know if I think 2028, 2030. (Aman chuckles) - For Field's Medal? - Field's Medal.\n- All right. It feels like forever from now, given how fast things have been going. Speaking of how fast\nthings have been going, let's talk about scaling laws. So, for people who don't know, maybe it's good to talk\nabout this whole idea of scaling laws. What are they, where'd you think stand, and where do you think things are going? - I think it was interesting. The original scaling laws paper by OpenAI was slightly wrong. 'Cause I think of some issues they did with learning right schedules. And then, Chinchilla showed\na more correct version. And then, from then\npeople have again deviated from doing the compute optimal thing. 'Cause people start now optimizing more so for making the thing work really well given an inference budget. And I think there are a lot\nmore dimensions to these curves than what we originally used, of just compute number\nof parameters and data. Like inference compute is the obvious one. I think context length\nis another obvious one. Let's say you care about the two things of inference compute\nand then context window, maybe the thing you wanna\ntrain is some kind of SSM. Because they're much,\nmuch cheaper and faster at super, super long context. And even if, maybe it was\n10 X more scaling properties during training, meaning,\nyou spend 10 X more compute to train the thing to get the\nsame level of capabilities, it's worth it because you care most\nabout that inference budget for really long context windows. So, it'll be interesting to see how people play with all these dimensions. - So, yeah, I mean, you speak to the multiple dimensions, obviously. The original conception was\njust looking at the variables of the size of the model\nas measured by parameters, and the size of the data as measured by the number of tokens, and looking at the ratio of the two. - Yeah. - And it's kind of a compelling notion that there is a number,\nor at least a minimum. And it seems like one was emerging. Do you still believe that there is a kind of bigger is better? - I mean, I think bigger\nis certainly better for just raw performance. - And raw intelligence. - And raw intelligence. I think the path that people might take, I'm particularly bullish on distillation. And how many knobs can you turn to, if we spend a ton, ton\nof money on training, get the most capable cheap model. Really, really caring as much as you can. 'Cause the naive version of\ncaring as much as you can about inference time compute, is what people have already\ndone with the Llama models. Or just over-training\nthe shit out of 7B models on way, way, way more tokens\nthan is essential optimal. But if you really care about it, maybe the thing to do is what Gamma did, which is let's not just train on tokens, let's literally train on\nminimizing the KL divergence with the distribution of gemma 27B, right? So knowledge distillation there. And you're spending the compute of literally training this\n27 billion parameter model on all these tokens, just to get out this, I don't know, smaller model. - And the distillation gives\nyou just a faster model, smaller means faster. - Yeah, distillation in theory is, I think, getting out more signal from the data that you're training on. And it's perhaps another\nway of getting over, not completely over, but partially helping with the data wall. Where you only have so\nmuch data to train on, let's train this really, really big model on all these tokens and we'll distill it\ninto this smaller one. And maybe we can get more signal per token for this much smaller model than we would've originally\nif we trained it. - So if I gave you $10 trillion,\nhow would you spend it? (Aman chuckles) I mean, you can't buy\nan island or whatever. How would you allocate it in terms of improving the big model versus maybe paying for HF in the RLHF? - Yeah, yeah. I think, there's a lot of\nthese secrets and details about training these large\nmodels that I just don't know, and are only privy to the large labs. And the issue is, I would\nwaste a lot of that money if I even attempted this, because I wouldn't know those things. Suspending a lot of disbelief and assuming you had the know-how, or if you're saying you have to operate with the limited information you have now. - No, no, no, actually, I would say, you swoop in and you\nget all the information, all the little heuristics,\nall the little parameters, all the parameters that define\nhow the thing is trained. - Mm-hmm. - If we look in how to invest\nmoney for the next five years in terms of maximizing what\nyou called raw intelligence. - I mean, isn't the answer really simple? You just try to get as\nmuch compute as possible. At the end of the day, all\nyou need to buy is the GPUs. You can tune whether you\nwant to pre-train a big model or a small model. - Well, this gets into the question of are you really limited\nby compute and money, or are you limited by these other things? - I'm more privy to Arvid's\nbelief that we're idea-limited, but there's always that like- - But if you have a lot of compute, you can run a lot of experiments. - So you would run a lot of experiments versus use that compute\nto trend a gigantic model? - I would, but I do\nbelieve that we are limited in terms of ideas that we have. - I think yeah, 'cause\neven with all this compute, and all the data you could\ncollect in the world, I think you really are ultimately\nlimited by not even ideas, but just really good engineering. There aren't that many people in the world who really can make the difference here. And there's so much work\nthat goes into research that is just pure, really,\nreally hard engineering work. As a very hand-wavy example, if you look at the\noriginal Transformer paper, how much work was joining together a lot of these really interesting\nconcepts embedded in the literature, versus then going in and writing all the codes,\nmaybe the CUDA kernels, maybe whatever else. I don't know if it ran them GPUs or TPUs. Originally, such that\nit actually saturated the GPU performance. Getting GNOME Azure to go\nin and do all this code. And GNOME is probably\none of the best engineers in the world. Or maybe going a step further, like the next generation of\nmodels, having these things. Like getting model parallelism to work, and scaling it on thousands of, or maybe tens of thousands of V100s, which I think GBDE-III may have been. There's just so much engineering effort that has to go into all of\nthese things to make it work. If you really brought that\ncost down to maybe not zero, but just made it 10 X easier,\nmade it super easy for someone with really fantastic ideas, to immediately get to the version of the new architecture they dreamed up, that is getting 50, 40%\nutilization on their GPUs, I think that would just\nspeed up research by a ton. - I mean, I think if you see\na clear path to improvement, you should always take the\nlow-hanging fruit first, right? I think probably OpenAI\nand all the other labs that did the right thing to\npick off the low-hanging fruit. Where the low-hanging fruit is like, you could scale up to a GPT-4.25 scale and you just keep scaling, and things keep getting better. There's no point of\nexperimenting with new ideas when everything is working. And you should bang on and to try to get as much as\nmuch juice out of the possible. I think if you're spending $10 trillion, you probably wanna spend some, then actually reevaluate your ideas, probably your idea a\nlittle bit at that point. - I think all of us believe\nnew ideas are probably needed to get all the way there to AGI. And all of us also probably believe there exist ways of\ntesting out those ideas at smaller scales, and\nbeing fairly confident that they'll play out. It's just quite difficult for the labs in their current position to dedicate their very limited research and engineering talent to\nexploring all these other ideas, when there's this core thing that will probably improve performance for some decent amount of time. - Yeah, but also, these\nbig labs like winning. (Lex chuckles) So, they're just going wild. Okay. (all chuckling) So, big question, looking\nout into the future. You're now at the center\nof the programming world. How do you think programming, the nature of programming\nchanges in the next few months, in the next year, in the next two years and the next five years, 10 years? - I think we're really\nexcited about a future where the programmer\nis in the driver's seat for a long time. And you've heard us talk\nabout this a little bit, but one that emphasizes speed and agency for the programmer and control. The ability to modify\nanything you wanna modify, the ability to iterate really\nfast on what you're building. And this is a little different, I think, than where some people\nare jumping to in the space, where I think one idea\nthat's captivated people, is can you talk to your computer? Can you have it build software for you? As if you're talking to\nan engineering department or an engineer over Slack. And can it just be this\nsort of isolated text box? And part of the reason we're\nnot excited about that, is some of the stuff we've\ntalked about with latency, but then a big piece, a reason\nwe're not excited about that, is because that comes with\ngiving up a lot of control. It's much harder to be really specific when you're talking in the text box. And if you're necessarily\njust going to communicate with a thing like you\nwould be communicating with an engineering department, you're actually advocating tons of really important decisions to this bot. And this kind of gets at, fundamentally, what engineering is. I think that some people who are a little bit more\nremoved from engineering might think of it as the spec\nis completely written out and then the engineers just\ncome and they just implement. And it's just about making\nthe thing happen in code and making the thing exist. But I think a lot of the best engineering, the engineering we enjoy, involves tons of tiny micro decisions about what exactly you're building, and about really hard trade-offs\nbetween speed and cost and just all the other\nthings involved in a system. As long as humans are actually the ones designing the software and the ones specifying\nwhat they want to be built, and it's not just like\ncompany run by all AIs, we think you'll really want the human in a driver's seat\ndictating these decisions. And so the jury's still out\non what that looks like. I think that one weird idea\nfor what that could look like, is it could look like you can control the level of abstraction\nyou view a code base at. And you can point at specific\nparts of a code base, like, maybe you digest a\ncode base by looking at it in the form of pseudocode. And you can actually\nedit that pseudocode too, and then have changes get made down at the formal programming level. And you can gesture at any piece of logic in your software component of programming. You keep the inflow text editing\ncomponent of programming, you keep the control of, you\ncan even go down into the code, you can go at higher\nlevels of abstraction, while also giving you these\nbig productivity gains. - It'd be nice if you can go up and down\nthe abstraction stack. - Yeah. And there are a lot of\ndetails to figure out there that's sort of like a fuzzy idea. Time will tell if it actually works. But these principles of control and speed in the human in the driver's seat, we think are really important. We think for some things\nlike Arvid mentioned before, for some styles of programming, you can hand it off chatbot-style. If you have a bug that's\nreally well specified. But that's not most of programming, and that's also not\nmost of the programming we think a lot of people value. - What about the fundamental\nskill of programming? There's a lot of people, like\nyoung people right now scared, 'cause they love programming,\nbut they're scared about, \"Will I be able to have a future if I pursue this career path?\" Do you think the very skill of programming will change fundamentally? - I actually think this is a\nreally, really exciting time to be building software. We remember what programming was like in 2013, 2012, whatever it was. And there was just so much\nmore cruft and boilerplate and looking up something really gnarly. And that stuff still exists,\nit's definitely not at zero. But programming today is\nway more fun than back then. It's like we're really getting down to the delight concentration. And all the things that really\ndraw people to programming, for instance, this element of being able to build things really fast and speed, and also individual control, all those are just being turned up a ton. And so I think it's gonna\nbe a really, really fun time for people who build software. I think that the skills\nwill probably change too. I think that people's\ntaste and creative ideas will be magnified. And it will be maybe less, a little bit, about boilerplate text editing. Maybe even a little bit\nless about carefulness, which I think is really important today if you're a programmer. I think it'll be a lot more fun. - What do you guys think? - I agree. I'm very excited to be able to change. One thing that happened recently, was we wanted to do a\nrelatively big migration to our code base. We were using\nAsyncLocalStorage in Node.js, which is known to be not very performant, and we wanted to migrate\nto a context object. And this is a big migration and affects the entire code base. Sualeh and I spent, I don't know, five days working through this,\neven with today's AI tools. And I am really excited for a future where I can just show a couple of examples and then the AI applies that\nto all of the locations. And then it highlights, \"Oh, this is a new\nexample, what should I do?\" And then, I show exactly what to do there. And then, that can be done in 10 minutes. And then, you can iterate\nmuch, much faster. Then, you don't have to\nthink as much upfront and stand at the blackboard and think, \"Exactly, how are we gonna do this, because the cost is so high?\" But you can just try something\nfirst and you realize, \"Oh, this is not actually\nexactly what I want.\" And then, you can change\nit instantly again after. And so, yeah, I think being\na programmer in the future is going to be a lot of fun. - Yeah, I really like that point. It feels like a lot of\nthe time with programming, there are two ways you can go about it. One is you think really\nhard, carefully upfront about the best possible way to do it, and then you spend your\nlimited time of engineering to actually implement it. But I must refer just getting in the code and taking a crack at\nseeing how it lays out and then iterating really quickly on that. That feels more fun. - Yeah, just speaking to generate\nthe boilerplate, is great. So you just focus on the nuanced, difficult design decisions. Migration, I feel like this is a cool one. It seems like a larger\nlanguage models is able to basically translate for one\nprogram language to another. Or translate, migrate in the general sense of what migrate is. But that's in the current moment. So mean the fear has to do with, okay, as these models\nget better and better, then you're doing less and\nless creative decisions. And is it going to kind of move to a place where you're operating in the design space of natural language where natural language is the\nmain programming language? And, I guess, I could ask\nthat by way of advice. If somebody's interested\nin programming now, what do you think they should learn? You guys started in some Java. (group chuckling) And I forget, oh, some PHP. - PHP. - Objective-C. - Objective-C, there you go. I mean in the end, we all know\nJavaScript was going to win (group chuckling) and not TypeScript. It's going to be like vanilla JavaScript. It's just going to eat the\nworld and maybe live with PHP. And I mean, it also\nbrings up the question of, I think Don Knuth has this idea that some percent of\nthe population is geeks, and there's a particular\nkind of psychology in mind required for programming. And it feels like more\nand more that expands the kind of person that should be able to, can do great programming might expand. - I think different people do programming for different reasons. But I think the true,\nmaybe the best programmers are the ones that really love, just absolutely love programming. For example, there are folks on our team who literally when they\nget back from work, they go and then they boot up Cursor, and then they start coding\non their side projects for the entire night, and they stay up until 3:00 am doing that. And when they're sad, they said, \"I just really need to code.\" (group chuckling) And I think there's\nthat level of programmer where this obsession\nand love of programming, I think makes, really,\nthe best programmers. And I think these types of people will really get into the\ndetails of how things work. - I guess the question I'm\nasking, that exact programmer, let's think about that person. When the super Tab, the super awesome praise\nbe the Tab succeeds, and you keep pressing Tab. - That person in the team loves Cursor Tab more than anybody else, right? - Yeah. Pressing Tab is just pressing Tab. That's the easy way to\nsay it in the catchphrase. But what you're actually doing\nwhen you're pressing Tab, is that you're injecting\nintent all the time while you're doing it. Sometimes you're rejecting it, sometimes you're typing\na few more characters. And that's the way that\nyou're shaping the things that's being created. And I think programming\nwill change a lot to just, \"What is it that you want to make?\" - It's sort of higher bandwidth. The communication to the computer just becomes higher and higher bandwidth as opposed to just typing\nas much lower bandwidth than communicating intent. - I mean, this goes to your manifesto titled Engineering Genius. \"We are an applied research lab building extraordinary productive\nhuman AI systems.\" So, speaking to this hybrid element. \"To start, we're building\nthe engineer of the future, a human AI programmer that's an order of magnitude more effective\nthan any one engineer. This hybrid engineer will\nhave effortless control over their code base and\nno low entropy keystrokes. They will iterate at the\nspeed of their judgment, even in the most complex systems. Using a combination of\nAI and human ingenuity, they will out-smart and out-engineer the best pure AI systems. We are a group of\nresearchers and engineers. We build software and models to invent at the edge of what's\nuseful and what's possible. Our work has already improved the lives of hundreds of thousands of programmers.\" And on the way to that, we'll at least make programming more fun. So, thank you for talking today. - Thank you.\n- Thanks for having us. - Thank you.\n- Thank you. - Thanks for listening\nto this conversation with Michael, Sualeh, Arvid and Aman. To support this podcast, please check out our\nsponsors in the description. And now, let me leave\nyou with a random, funny, and perhaps profound programming\ncode I saw on Reddit. Nothing is as permanent as a\ntemporary solution that works. Thank you for listening and\nhope to see you next time."
}