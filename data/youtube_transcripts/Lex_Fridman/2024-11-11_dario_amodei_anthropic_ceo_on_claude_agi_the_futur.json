{
  "metadata": {
    "video_id": "ugvHCXCOmm4",
    "channel_name": "Lex Fridman",
    "video_title": "Dario Amodei: Anthropic CEO on Claude, AGI & the Future of AI & Humanity | Lex Fridman Podcast #452",
    "published_at": "2024-11-11",
    "view_count": "1,312,360",
    "like_count": "14,183",
    "comment_count": "1,347"
  },
  "transcript": [
    {
      "text": "- If you extrapolate the curves",
      "start": 0.09,
      "duration": 1.86
    },
    {
      "text": "that we've had so far, right?",
      "start": 1.95,
      "duration": 1.35
    },
    {
      "text": "If you say, well, I don't know,",
      "start": 3.3,
      "duration": 1.95
    },
    {
      "text": "we're starting to get to like PhD level,",
      "start": 5.25,
      "duration": 2.19
    },
    {
      "text": "and last year we were\nat undergraduate level,",
      "start": 7.44,
      "duration": 2.367
    },
    {
      "text": "and the year before we\nwere at like the level",
      "start": 9.807,
      "duration": 1.926
    },
    {
      "text": "of a high school student.",
      "start": 11.733,
      "duration": 1.647
    },
    {
      "text": "Again, you can quibble with at what tasks",
      "start": 13.38,
      "duration": 2.73
    },
    {
      "text": "and for what, we're\nstill missing modalities,",
      "start": 16.11,
      "duration": 2.46
    },
    {
      "text": "but those are being added,\nlike computer use was added,",
      "start": 18.57,
      "duration": 2.43
    },
    {
      "text": "like image generation has been added.",
      "start": 21.0,
      "duration": 1.86
    },
    {
      "text": "If you just kind of like eyeball the rate",
      "start": 22.86,
      "duration": 2.19
    },
    {
      "text": "at which these capabilities\nare increasing,",
      "start": 25.05,
      "duration": 2.67
    },
    {
      "text": "it does make you think",
      "start": 27.72,
      "duration": 1.44
    },
    {
      "text": "that we'll get there by 2026 or 2027.",
      "start": 29.16,
      "duration": 2.76
    },
    {
      "text": "I think there are still worlds",
      "start": 31.92,
      "duration": 1.14
    },
    {
      "text": "where it doesn't happen in 100 years.",
      "start": 33.06,
      "duration": 2.31
    },
    {
      "text": "Those world, the number",
      "start": 35.37,
      "duration": 1.17
    },
    {
      "text": "of those worlds is rapidly decreasing.",
      "start": 36.54,
      "duration": 2.22
    },
    {
      "text": "We are rapidly running out\nof truly convincing blockers,",
      "start": 38.76,
      "duration": 3.9
    },
    {
      "text": "truly compelling reasons why",
      "start": 42.66,
      "duration": 1.5
    },
    {
      "text": "this will not happen\nin the next few years.",
      "start": 44.16,
      "duration": 1.89
    },
    {
      "text": "The scale up is very quick.",
      "start": 46.05,
      "duration": 1.53
    },
    {
      "text": "Like we do this today, we make a model,",
      "start": 47.58,
      "duration": 2.34
    },
    {
      "text": "and then we deploy thousands,",
      "start": 49.92,
      "duration": 1.62
    },
    {
      "text": "maybe tens of thousands\nof instances of it.",
      "start": 51.54,
      "duration": 2.52
    },
    {
      "text": "I think by the time, you know,",
      "start": 54.06,
      "duration": 1.95
    },
    {
      "text": "certainly within two to three years,",
      "start": 56.01,
      "duration": 1.38
    },
    {
      "text": "whether we have these\nsuper powerful AIs or not,",
      "start": 57.39,
      "duration": 2.46
    },
    {
      "text": "clusters are gonna get to the size",
      "start": 59.85,
      "duration": 1.65
    },
    {
      "text": "where you'll be able to\ndeploy millions of these.",
      "start": 61.5,
      "duration": 2.1
    },
    {
      "text": "I am optimistic about meaning.",
      "start": 63.6,
      "duration": 2.46
    },
    {
      "text": "I worry about economics and\nthe concentration of power.",
      "start": 66.06,
      "duration": 4.44
    },
    {
      "text": "That's actually what I worry about more,",
      "start": 70.5,
      "duration": 1.74
    },
    {
      "text": "the abuse of power.",
      "start": 72.24,
      "duration": 1.86
    },
    {
      "text": "- And AI increases the\namount of power in the world,",
      "start": 74.1,
      "duration": 4.23
    },
    {
      "text": "and if you concentrate that power",
      "start": 78.33,
      "duration": 1.77
    },
    {
      "text": "and abuse that power, it\ncan do immeasurable damage.",
      "start": 80.1,
      "duration": 2.7
    },
    {
      "text": "- Yes, it's very frightening.",
      "start": 82.8,
      "duration": 1.53
    },
    {
      "text": "It's very frightening.",
      "start": 84.33,
      "duration": 1.383
    },
    {
      "text": "- The following is a\nconversation with Dario Amodei,",
      "start": 87.93,
      "duration": 3.03
    },
    {
      "text": "CEO of Anthropic, the\ncompany that created Claude",
      "start": 90.96,
      "duration": 4.35
    },
    {
      "text": "that is currently and often at the top",
      "start": 95.31,
      "duration": 2.01
    },
    {
      "text": "of most LLM benchmark leaderboards.",
      "start": 97.32,
      "duration": 2.88
    },
    {
      "text": "On top of that, Dario\nand the Anthropic team",
      "start": 100.2,
      "duration": 3.06
    },
    {
      "text": "have been outspoken advocates",
      "start": 103.26,
      "duration": 1.56
    },
    {
      "text": "for taking the topic of\nAI safety very seriously,",
      "start": 104.82,
      "duration": 3.9
    },
    {
      "text": "and they have continued to publish",
      "start": 108.72,
      "duration": 1.83
    },
    {
      "text": "a lot of fascinating AI research\non this and other topics.",
      "start": 110.55,
      "duration": 5.0
    },
    {
      "text": "I'm also joined afterwards",
      "start": 115.83,
      "duration": 2.01
    },
    {
      "text": "by two other brilliant\npeople from Anthropic.",
      "start": 117.84,
      "duration": 2.94
    },
    {
      "text": "First Amanda Askell, who is a researcher",
      "start": 120.78,
      "duration": 3.45
    },
    {
      "text": "working on alignment and\nfine tuning of Claude,",
      "start": 124.23,
      "duration": 3.63
    },
    {
      "text": "including the design of Claude's\ncharacter and personality.",
      "start": 127.86,
      "duration": 4.02
    },
    {
      "text": "A few folks told me\nshe has probably talked",
      "start": 131.88,
      "duration": 2.88
    },
    {
      "text": "with Claude more than\nany human at Anthropic.",
      "start": 134.76,
      "duration": 3.6
    },
    {
      "text": "So she was definitely a fascinating person",
      "start": 138.36,
      "duration": 2.88
    },
    {
      "text": "to talk to about prompt engineering",
      "start": 141.24,
      "duration": 2.28
    },
    {
      "text": "and practical advice on how\nto get the best out of Claude.",
      "start": 143.52,
      "duration": 3.87
    },
    {
      "text": "After that, Chris Olah\nstopped by for a chat.",
      "start": 147.39,
      "duration": 3.6
    },
    {
      "text": "He's one of the pioneers of the field",
      "start": 150.99,
      "duration": 2.31
    },
    {
      "text": "of mechanistic interpretability,",
      "start": 153.3,
      "duration": 2.25
    },
    {
      "text": "which is an exciting set of efforts",
      "start": 155.55,
      "duration": 2.04
    },
    {
      "text": "that aims to reverse\nengineer neural networks",
      "start": 157.59,
      "duration": 3.36
    },
    {
      "text": "to figure out what's going on inside,",
      "start": 160.95,
      "duration": 2.67
    },
    {
      "text": "inferring behaviors from\nneural activation patterns",
      "start": 163.62,
      "duration": 3.48
    },
    {
      "text": "inside the network.",
      "start": 167.1,
      "duration": 1.74
    },
    {
      "text": "This is a very promising approach",
      "start": 168.84,
      "duration": 2.64
    },
    {
      "text": "for keeping future super\nintelligent AI systems safe.",
      "start": 171.48,
      "duration": 3.93
    },
    {
      "text": "For example, by detecting\nfrom the activations",
      "start": 175.41,
      "duration": 3.27
    },
    {
      "text": "when the model is trying to deceive",
      "start": 178.68,
      "duration": 1.59
    },
    {
      "text": "the human it is talking to.",
      "start": 180.27,
      "duration": 2.343
    },
    {
      "text": "This is the \"Lex Fridman Podcast.\"",
      "start": 183.72,
      "duration": 1.89
    },
    {
      "text": "To support it, please check out",
      "start": 185.61,
      "duration": 1.32
    },
    {
      "text": "our sponsors in the description.",
      "start": 186.93,
      "duration": 2.19
    },
    {
      "text": "And now, dear friends,\nhere's Dario Amodei.",
      "start": 189.12,
      "duration": 4.083
    },
    {
      "text": "Let's start with the\nbig idea of scaling laws",
      "start": 194.19,
      "duration": 2.16
    },
    {
      "text": "and the Scaling Hypothesis.",
      "start": 196.35,
      "duration": 1.53
    },
    {
      "text": "What is it?",
      "start": 197.88,
      "duration": 0.93
    },
    {
      "text": "What is its history?",
      "start": 198.81,
      "duration": 1.5
    },
    {
      "text": "And where do we stand today?",
      "start": 200.31,
      "duration": 1.68
    },
    {
      "text": "- So I can only describe it as, you know,",
      "start": 201.99,
      "duration": 2.59
    },
    {
      "text": "as it relates to kind\nof my own experience.",
      "start": 204.58,
      "duration": 2.15
    },
    {
      "text": "But I've been in the AI\nfield for about 10 years",
      "start": 206.73,
      "duration": 3.51
    },
    {
      "text": "and it was something I\nnoticed very early on.",
      "start": 210.24,
      "duration": 2.04
    },
    {
      "text": "So I first joined the AI world",
      "start": 212.28,
      "duration": 2.1
    },
    {
      "text": "when I was working at Baidu\nwith Andrew Ng in late 2014,",
      "start": 214.38,
      "duration": 3.93
    },
    {
      "text": "which is almost exactly 10 years ago now.",
      "start": 218.31,
      "duration": 2.79
    },
    {
      "text": "And the first thing we worked on",
      "start": 221.1,
      "duration": 2.01
    },
    {
      "text": "was speech recognition systems.",
      "start": 223.11,
      "duration": 2.01
    },
    {
      "text": "And in those days I think\ndeep learning was a new thing,",
      "start": 225.12,
      "duration": 2.85
    },
    {
      "text": "it had made lots of progress,",
      "start": 227.97,
      "duration": 1.71
    },
    {
      "text": "but everyone was always saying,",
      "start": 229.68,
      "duration": 1.65
    },
    {
      "text": "we don't have the algorithms\nwe need to succeed.",
      "start": 231.33,
      "duration": 2.16
    },
    {
      "text": "You know, we're not,",
      "start": 233.49,
      "duration": 1.83
    },
    {
      "text": "we're only matching a tiny, tiny fraction.",
      "start": 235.32,
      "duration": 2.79
    },
    {
      "text": "There's so much we need to kind\nof discover algorithmically.",
      "start": 238.11,
      "duration": 3.18
    },
    {
      "text": "We haven't found the picture",
      "start": 241.29,
      "duration": 1.23
    },
    {
      "text": "of how to match the human brain.",
      "start": 242.52,
      "duration": 1.743
    },
    {
      "text": "And when, you know, in\nsome ways I was fortunate,",
      "start": 245.4,
      "duration": 3.27
    },
    {
      "text": "I was kind of, you know,",
      "start": 248.67,
      "duration": 0.833
    },
    {
      "text": "you can have almost\nbeginner's luck, right?",
      "start": 249.503,
      "duration": 1.37
    },
    {
      "text": "I was like a newcomer to the field",
      "start": 250.873,
      "duration": 2.597
    },
    {
      "text": "and, you know, I looked at the neural net",
      "start": 253.47,
      "duration": 1.65
    },
    {
      "text": "that we were using for speech,",
      "start": 255.12,
      "duration": 1.38
    },
    {
      "text": "the recurrent neural networks,",
      "start": 256.5,
      "duration": 1.53
    },
    {
      "text": "and I said, I don't know,",
      "start": 258.03,
      "duration": 0.96
    },
    {
      "text": "what if you make them bigger\nand give them more layers?",
      "start": 258.99,
      "duration": 2.46
    },
    {
      "text": "And what if you scale up the\ndata along with this, right?",
      "start": 261.45,
      "duration": 2.22
    },
    {
      "text": "I just saw these as like",
      "start": 263.67,
      "duration": 1.74
    },
    {
      "text": "independent dials that you could turn.",
      "start": 265.41,
      "duration": 2.25
    },
    {
      "text": "And I noticed that the model started",
      "start": 267.66,
      "duration": 1.53
    },
    {
      "text": "to do better and better as\nyou gave them more data,",
      "start": 269.19,
      "duration": 3.18
    },
    {
      "text": "as you made the models larger,",
      "start": 272.37,
      "duration": 2.19
    },
    {
      "text": "as you trained them for longer.",
      "start": 274.56,
      "duration": 1.623
    },
    {
      "text": "And I didn't measure things\nprecisely in those days,",
      "start": 277.02,
      "duration": 3.36
    },
    {
      "text": "but along with colleagues,",
      "start": 280.38,
      "duration": 2.1
    },
    {
      "text": "we very much got the informal sense",
      "start": 282.48,
      "duration": 1.92
    },
    {
      "text": "that the more data and the more compute",
      "start": 284.4,
      "duration": 3.03
    },
    {
      "text": "and the more training you\nput into these models,",
      "start": 287.43,
      "duration": 2.52
    },
    {
      "text": "the better they perform.",
      "start": 289.95,
      "duration": 1.95
    },
    {
      "text": "And so initially my thinking was,",
      "start": 291.9,
      "duration": 1.47
    },
    {
      "text": "hey, maybe that is just true",
      "start": 293.37,
      "duration": 1.98
    },
    {
      "text": "for speech recognition systems, right?",
      "start": 295.35,
      "duration": 2.1
    },
    {
      "text": "Maybe that's just one particular quirk,",
      "start": 297.45,
      "duration": 2.28
    },
    {
      "text": "one particular area.",
      "start": 299.73,
      "duration": 1.47
    },
    {
      "text": "I think it wasn't until 2017\nwhen I first saw the results",
      "start": 301.2,
      "duration": 3.84
    },
    {
      "text": "from GPT-1 that it clicked for me",
      "start": 305.04,
      "duration": 3.45
    },
    {
      "text": "that language is probably the area",
      "start": 308.49,
      "duration": 2.04
    },
    {
      "text": "in which we can do this.",
      "start": 310.53,
      "duration": 1.11
    },
    {
      "text": "We can get trillions of\nwords of language data,",
      "start": 311.64,
      "duration": 3.75
    },
    {
      "text": "we can train on them.",
      "start": 315.39,
      "duration": 1.53
    },
    {
      "text": "And the models we were\ntraining those days were tiny.",
      "start": 316.92,
      "duration": 2.49
    },
    {
      "text": "You could train them on\none to eight GPUs whereas,",
      "start": 319.41,
      "duration": 3.18
    },
    {
      "text": "you know, now we train\njobs on tens of thousands,",
      "start": 322.59,
      "duration": 2.52
    },
    {
      "text": "soon going to hundreds\nof thousands of GPUs.",
      "start": 325.11,
      "duration": 2.97
    },
    {
      "text": "And so when I saw those\ntwo things together",
      "start": 328.08,
      "duration": 3.03
    },
    {
      "text": "and, you know, there were a few people",
      "start": 331.11,
      "duration": 1.44
    },
    {
      "text": "like Ilya Sutskever,\nwho you've interviewed,",
      "start": 332.55,
      "duration": 2.28
    },
    {
      "text": "who had somewhat similar views, right?",
      "start": 334.83,
      "duration": 2.07
    },
    {
      "text": "He might have been the first one,",
      "start": 336.9,
      "duration": 1.2
    },
    {
      "text": "although I think a few\npeople came to similar views",
      "start": 338.1,
      "duration": 3.45
    },
    {
      "text": "around the same time, right?",
      "start": 341.55,
      "duration": 1.05
    },
    {
      "text": "There was, you know, Rich\nSutton's Bitter Lesson,",
      "start": 342.6,
      "duration": 2.4
    },
    {
      "text": "there was Gwern wrote about\nthe Scaling Hypothesis.",
      "start": 345.0,
      "duration": 3.3
    },
    {
      "text": "But I think somewhere\nbetween 2014 and 2017",
      "start": 348.3,
      "duration": 3.78
    },
    {
      "text": "was when it really clicked for me,",
      "start": 352.08,
      "duration": 1.98
    },
    {
      "text": "when I really got conviction that,",
      "start": 354.06,
      "duration": 1.53
    },
    {
      "text": "hey, we're gonna be able to do",
      "start": 355.59,
      "duration": 1.53
    },
    {
      "text": "these incredibly wide cognitive tasks",
      "start": 357.12,
      "duration": 3.25
    },
    {
      "text": "if we just scale up the models.",
      "start": 361.478,
      "duration": 1.882
    },
    {
      "text": "And at every stage of scaling,",
      "start": 363.36,
      "duration": 2.37
    },
    {
      "text": "there are always arguments.",
      "start": 365.73,
      "duration": 1.35
    },
    {
      "text": "And you know, when I first\nheard them, honestly,",
      "start": 367.08,
      "duration": 1.92
    },
    {
      "text": "I thought probably I'm\nthe one who's wrong,",
      "start": 369.0,
      "duration": 2.07
    },
    {
      "text": "and, you know, all these\nexperts in the field are right.",
      "start": 371.07,
      "duration": 2.34
    },
    {
      "text": "They know the situation\nbetter than I do, right?",
      "start": 373.41,
      "duration": 2.61
    },
    {
      "text": "There's, you know, the\nChomsky argument about like,",
      "start": 376.02,
      "duration": 2.49
    },
    {
      "text": "you can get syntactics, but\nyou can't get semantics.",
      "start": 378.51,
      "duration": 3.03
    },
    {
      "text": "There was this idea, oh, you\ncan make a sentence make sense,",
      "start": 381.54,
      "duration": 2.19
    },
    {
      "text": "but you can't make a paragraph make sense.",
      "start": 383.73,
      "duration": 2.13
    },
    {
      "text": "The latest one we have today is, you know,",
      "start": 385.86,
      "duration": 3.45
    },
    {
      "text": "we're gonna run out of data,",
      "start": 389.31,
      "duration": 1.11
    },
    {
      "text": "or the data isn't high quality enough,",
      "start": 390.42,
      "duration": 1.86
    },
    {
      "text": "or models can't reason.",
      "start": 392.28,
      "duration": 1.86
    },
    {
      "text": "And each time, every time we manage",
      "start": 394.14,
      "duration": 2.23
    },
    {
      "text": "to either find a way around",
      "start": 397.219,
      "duration": 1.571
    },
    {
      "text": "or scaling just is the way around.",
      "start": 398.79,
      "duration": 2.37
    },
    {
      "text": "Sometimes it's one,\nsometimes it's the other.",
      "start": 401.16,
      "duration": 2.76
    },
    {
      "text": "And so I'm now at this\npoint, I still think,",
      "start": 403.92,
      "duration": 3.24
    },
    {
      "text": "you know, it's always quite uncertain.",
      "start": 407.16,
      "duration": 2.1
    },
    {
      "text": "We have nothing but inductive\ninference to tell us",
      "start": 409.26,
      "duration": 3.12
    },
    {
      "text": "that the next two years are gonna be",
      "start": 412.38,
      "duration": 1.44
    },
    {
      "text": "like the last 10 years.",
      "start": 413.82,
      "duration": 2.37
    },
    {
      "text": "But I've seen the movie enough times,",
      "start": 416.19,
      "duration": 2.58
    },
    {
      "text": "I've seen the story\nhappen for enough times",
      "start": 418.77,
      "duration": 3.09
    },
    {
      "text": "to really believe that\nprobably the scaling",
      "start": 421.86,
      "duration": 2.88
    },
    {
      "text": "is going to continue and\nthat there's some magic to it",
      "start": 424.74,
      "duration": 2.91
    },
    {
      "text": "that we haven't really explained\non a theoretical basis yet.",
      "start": 427.65,
      "duration": 2.97
    },
    {
      "text": "- And of course the scaling\nhere is bigger networks,",
      "start": 430.62,
      "duration": 3.75
    },
    {
      "text": "bigger data, bigger compute.",
      "start": 434.37,
      "duration": 2.34
    },
    {
      "text": "- Yes.\n- All of those.",
      "start": 436.71,
      "duration": 1.11
    },
    {
      "text": "- In particular, linear scaling up",
      "start": 437.82,
      "duration": 2.61
    },
    {
      "text": "of bigger networks, bigger training times",
      "start": 440.43,
      "duration": 4.02
    },
    {
      "text": "and more and more data.",
      "start": 444.45,
      "duration": 3.18
    },
    {
      "text": "So all of these things, almost\nlike a chemical reaction,",
      "start": 447.63,
      "duration": 2.4
    },
    {
      "text": "you know, you have three ingredients",
      "start": 450.03,
      "duration": 1.26
    },
    {
      "text": "in the chemical reaction,",
      "start": 451.29,
      "duration": 1.14
    },
    {
      "text": "and you need to linearly scale\nup the three ingredients.",
      "start": 452.43,
      "duration": 2.58
    },
    {
      "text": "If you scale up one, not the others,",
      "start": 455.01,
      "duration": 1.89
    },
    {
      "text": "you run out of the other reagents",
      "start": 456.9,
      "duration": 1.92
    },
    {
      "text": "and the reaction stops.",
      "start": 458.82,
      "duration": 1.41
    },
    {
      "text": "But if you scale up everything in series,",
      "start": 460.23,
      "duration": 3.24
    },
    {
      "text": "then the reaction can proceed.",
      "start": 463.47,
      "duration": 2.01
    },
    {
      "text": "- And of course, now that you have this",
      "start": 465.48,
      "duration": 1.35
    },
    {
      "text": "kind of empirical science/art,",
      "start": 466.83,
      "duration": 2.79
    },
    {
      "text": "you can apply to other more nuanced things",
      "start": 469.62,
      "duration": 4.08
    },
    {
      "text": "like scaling laws applied\nto interpretability,",
      "start": 473.7,
      "duration": 2.91
    },
    {
      "text": "or scaling laws applied to post-training,",
      "start": 476.61,
      "duration": 2.25
    },
    {
      "text": "or just seeing how does this thing scale.",
      "start": 478.86,
      "duration": 2.97
    },
    {
      "text": "But the big scaling law,",
      "start": 481.83,
      "duration": 1.38
    },
    {
      "text": "I guess the underlying Scaling Hypothesis",
      "start": 483.21,
      "duration": 2.28
    },
    {
      "text": "has to do with big networks,\nbig data leads to intelligence.",
      "start": 485.49,
      "duration": 4.23
    },
    {
      "text": "- Yeah, we've documented scaling laws",
      "start": 489.72,
      "duration": 2.82
    },
    {
      "text": "in lots of domains other\nthan language, right?",
      "start": 492.54,
      "duration": 2.76
    },
    {
      "text": "So initially, the paper we did",
      "start": 495.3,
      "duration": 2.73
    },
    {
      "text": "that first showed it was in early 2020",
      "start": 498.03,
      "duration": 2.37
    },
    {
      "text": "where we first showed it for language.",
      "start": 500.4,
      "duration": 1.89
    },
    {
      "text": "There was then some work late in 2020",
      "start": 502.29,
      "duration": 2.7
    },
    {
      "text": "where we showed the same\nthing for other modalities,",
      "start": 504.99,
      "duration": 2.79
    },
    {
      "text": "like images, video, text-to-image,",
      "start": 507.78,
      "duration": 3.51
    },
    {
      "text": "image-to-text, math.",
      "start": 511.29,
      "duration": 2.19
    },
    {
      "text": "They all had the same pattern.",
      "start": 513.48,
      "duration": 1.38
    },
    {
      "text": "And you're right, now\nthere are other stages",
      "start": 514.86,
      "duration": 2.01
    },
    {
      "text": "like post-training or there are new types",
      "start": 516.87,
      "duration": 2.4
    },
    {
      "text": "of reasoning models.",
      "start": 519.27,
      "duration": 1.62
    },
    {
      "text": "And in all of those cases\nthat we've measured,",
      "start": 520.89,
      "duration": 3.51
    },
    {
      "text": "we see similar types of scaling laws.",
      "start": 524.4,
      "duration": 3.57
    },
    {
      "text": "- A bit of a philosophical question,",
      "start": 527.97,
      "duration": 1.5
    },
    {
      "text": "but what's your intuition\nabout why bigger is better",
      "start": 529.47,
      "duration": 3.9
    },
    {
      "text": "in terms of network size and data size?",
      "start": 533.37,
      "duration": 2.94
    },
    {
      "text": "Why does it lead to\nmore intelligent models?",
      "start": 536.31,
      "duration": 3.93
    },
    {
      "text": "- So in my previous\ncareer as a biophysicist,",
      "start": 540.24,
      "duration": 3.0
    },
    {
      "text": "so I did physics undergrad",
      "start": 543.24,
      "duration": 1.29
    },
    {
      "text": "and then biophysics in grad school.",
      "start": 544.53,
      "duration": 2.79
    },
    {
      "text": "So I think back to what\nI know as a physicist,",
      "start": 547.32,
      "duration": 1.86
    },
    {
      "text": "which is actually much less",
      "start": 549.18,
      "duration": 1.5
    },
    {
      "text": "than what some of my colleagues",
      "start": 550.68,
      "duration": 1.02
    },
    {
      "text": "at Anthropic have in terms\nof expertise in physics.",
      "start": 551.7,
      "duration": 4.413
    },
    {
      "text": "There's this concept called the 1/f noise",
      "start": 557.79,
      "duration": 2.82
    },
    {
      "text": "and 1/x distributions\nwhere often, you know,",
      "start": 560.61,
      "duration": 3.963
    },
    {
      "text": "just like if you add up a bunch",
      "start": 566.13,
      "duration": 1.68
    },
    {
      "text": "of natural processes, you get a Gaussian.",
      "start": 567.81,
      "duration": 2.55
    },
    {
      "text": "If you add up a bunch",
      "start": 570.36,
      "duration": 1.23
    },
    {
      "text": "of kind of differently\ndistributed natural processes,",
      "start": 571.59,
      "duration": 4.382
    },
    {
      "text": "if you like take a probe",
      "start": 575.972,
      "duration": 2.458
    },
    {
      "text": "and hook it up to a resistor,",
      "start": 578.43,
      "duration": 2.01
    },
    {
      "text": "the distribution of the thermal noise",
      "start": 580.44,
      "duration": 2.7
    },
    {
      "text": "in the resistor goes as\none over the frequency.",
      "start": 583.14,
      "duration": 2.49
    },
    {
      "text": "It's some kind of natural\nconvergent distribution.",
      "start": 585.63,
      "duration": 2.703
    },
    {
      "text": "And I think what it amounts to is that",
      "start": 590.67,
      "duration": 2.31
    },
    {
      "text": "if you look at a lot of things",
      "start": 592.98,
      "duration": 2.4
    },
    {
      "text": "that are produced by some natural process",
      "start": 595.38,
      "duration": 2.25
    },
    {
      "text": "that has a lot of different scales, right?",
      "start": 597.63,
      "duration": 2.04
    },
    {
      "text": "Not a Gaussian, which is\nkind of narrowly distributed,",
      "start": 599.67,
      "duration": 2.97
    },
    {
      "text": "but you know, if I look at",
      "start": 602.64,
      "duration": 0.87
    },
    {
      "text": "kind of like large and small fluctuations",
      "start": 603.51,
      "duration": 3.36
    },
    {
      "text": "that lead to electrical noise,",
      "start": 606.87,
      "duration": 3.03
    },
    {
      "text": "they have this decaying 1/x distribution.",
      "start": 609.9,
      "duration": 3.33
    },
    {
      "text": "And so now I think of like patterns",
      "start": 613.23,
      "duration": 2.43
    },
    {
      "text": "in the physical world, right?",
      "start": 615.66,
      "duration": 1.83
    },
    {
      "text": "Or in language.",
      "start": 617.49,
      "duration": 1.08
    },
    {
      "text": "If I think about the patterns in language,",
      "start": 618.57,
      "duration": 2.22
    },
    {
      "text": "there are some really simple patterns.",
      "start": 620.79,
      "duration": 1.89
    },
    {
      "text": "Some words are much more\ncommon than others like \"the,\"",
      "start": 622.68,
      "duration": 2.85
    },
    {
      "text": "then there's basic noun verb structure,",
      "start": 625.53,
      "duration": 2.55
    },
    {
      "text": "then there's the fact that, you know,",
      "start": 628.08,
      "duration": 1.44
    },
    {
      "text": "nouns and verbs have to agree,\nthey have to coordinate.",
      "start": 629.52,
      "duration": 2.88
    },
    {
      "text": "And there's the higher\nlevel sentence structure,",
      "start": 632.4,
      "duration": 1.86
    },
    {
      "text": "then there's the thematic\nstructure of paragraphs.",
      "start": 634.26,
      "duration": 2.64
    },
    {
      "text": "And so the fact that there's\nthis regressing structure,",
      "start": 636.9,
      "duration": 2.97
    },
    {
      "text": "you can imagine that as you\nmake the networks larger,",
      "start": 639.87,
      "duration": 2.97
    },
    {
      "text": "first they capture the\nreally simple correlations,",
      "start": 642.84,
      "duration": 3.03
    },
    {
      "text": "the really simple patterns,",
      "start": 645.87,
      "duration": 1.38
    },
    {
      "text": "and there's this long\ntail of other patterns.",
      "start": 647.25,
      "duration": 2.55
    },
    {
      "text": "And if that long tail of other\npatterns is really smooth",
      "start": 649.8,
      "duration": 3.63
    },
    {
      "text": "like it is with the 1/f noise in,",
      "start": 653.43,
      "duration": 2.43
    },
    {
      "text": "you know, physical\nprocesses like resistors,",
      "start": 655.86,
      "duration": 3.24
    },
    {
      "text": "then you can imagine as you\nmake the network larger,",
      "start": 659.1,
      "duration": 2.73
    },
    {
      "text": "it's kind of capturing more\nand more of that distribution,",
      "start": 661.83,
      "duration": 2.97
    },
    {
      "text": "and so that smoothness gets reflected",
      "start": 664.8,
      "duration": 2.64
    },
    {
      "text": "in how well the models are at predicting",
      "start": 667.44,
      "duration": 1.83
    },
    {
      "text": "and how well they perform.",
      "start": 669.27,
      "duration": 1.44
    },
    {
      "text": "Language is an evolved process, right?",
      "start": 670.71,
      "duration": 3.27
    },
    {
      "text": "We've developed language,",
      "start": 673.98,
      "duration": 1.44
    },
    {
      "text": "we have common words\nand less common words.",
      "start": 675.42,
      "duration": 2.85
    },
    {
      "text": "We have common expressions\nand less common expressions.",
      "start": 678.27,
      "duration": 2.97
    },
    {
      "text": "We have ideas, cliches that\nare expressed frequently,",
      "start": 681.24,
      "duration": 3.36
    },
    {
      "text": "and we have novel ideas.",
      "start": 684.6,
      "duration": 1.62
    },
    {
      "text": "And that process has developed,",
      "start": 686.22,
      "duration": 2.16
    },
    {
      "text": "has evolved with humans\nover millions of years.",
      "start": 688.38,
      "duration": 2.997
    },
    {
      "text": "And so the guess,",
      "start": 691.377,
      "duration": 1.653
    },
    {
      "text": "and this is pure speculation would be",
      "start": 693.03,
      "duration": 2.16
    },
    {
      "text": "that there's some kind\nof long tail distribution",
      "start": 695.19,
      "duration": 4.11
    },
    {
      "text": "of the distribution of these ideas.",
      "start": 699.3,
      "duration": 2.49
    },
    {
      "text": "- So there's the long tail,",
      "start": 701.79,
      "duration": 1.32
    },
    {
      "text": "but also there's the\nheight of the hierarchy",
      "start": 703.11,
      "duration": 2.37
    },
    {
      "text": "of concepts that you're building up.",
      "start": 705.48,
      "duration": 1.92
    },
    {
      "text": "So the bigger the network,",
      "start": 707.4,
      "duration": 1.23
    },
    {
      "text": "presumably you have a higher capacity to-",
      "start": 708.63,
      "duration": 2.16
    },
    {
      "text": "- Exactly, if you have a small network,",
      "start": 710.79,
      "duration": 1.59
    },
    {
      "text": "you only get the common stuff, right?",
      "start": 712.38,
      "duration": 2.04
    },
    {
      "text": "if I take a tiny neural network,",
      "start": 714.42,
      "duration": 2.19
    },
    {
      "text": "it's very good at\nunderstanding that, you know,",
      "start": 716.61,
      "duration": 1.29
    },
    {
      "text": "a sentence has to have, you know,",
      "start": 717.9,
      "duration": 2.25
    },
    {
      "text": "verb, adjective, noun, right?",
      "start": 720.15,
      "duration": 1.74
    },
    {
      "text": "But it's terrible at deciding",
      "start": 721.89,
      "duration": 2.31
    },
    {
      "text": "what those verb, adjective\nand noun should be",
      "start": 724.2,
      "duration": 1.8
    },
    {
      "text": "and whether they should make sense.",
      "start": 726.0,
      "duration": 1.44
    },
    {
      "text": "If I make it just a little\nbigger, it gets good at that,",
      "start": 727.44,
      "duration": 2.73
    },
    {
      "text": "then suddenly it's good at the sentences,",
      "start": 730.17,
      "duration": 1.68
    },
    {
      "text": "but it's not good at the paragraphs.",
      "start": 731.85,
      "duration": 1.38
    },
    {
      "text": "And so these rarer",
      "start": 733.23,
      "duration": 2.31
    },
    {
      "text": "and more complex patterns get picked up",
      "start": 735.54,
      "duration": 2.757
    },
    {
      "text": "as I add more capacity to the network.",
      "start": 738.297,
      "duration": 1.983
    },
    {
      "text": "- Well, the natural question then is,",
      "start": 740.28,
      "duration": 1.8
    },
    {
      "text": "what's the ceiling of this?",
      "start": 742.08,
      "duration": 1.92
    },
    {
      "text": "- Yeah.\n- Like how complicated",
      "start": 744.0,
      "duration": 1.59
    },
    {
      "text": "and complex is the real world?",
      "start": 745.59,
      "duration": 2.79
    },
    {
      "text": "How much stuff is there to learn?",
      "start": 748.38,
      "duration": 2.04
    },
    {
      "text": "- I don't think any of us knows\nthe answer to that question.",
      "start": 750.42,
      "duration": 3.21
    },
    {
      "text": "My strong instinct would be that",
      "start": 753.63,
      "duration": 2.1
    },
    {
      "text": "there's no ceiling below\nthe level of humans, right?",
      "start": 755.73,
      "duration": 2.43
    },
    {
      "text": "We humans are able to understand\nthese various patterns,",
      "start": 758.16,
      "duration": 3.0
    },
    {
      "text": "and so that makes me think\nthat if we continue to,",
      "start": 761.16,
      "duration": 3.397
    },
    {
      "text": "you know, scale up these models",
      "start": 764.557,
      "duration": 3.383
    },
    {
      "text": "to kind of develop new\nmethods for training them",
      "start": 767.94,
      "duration": 2.547
    },
    {
      "text": "and scaling them up, that\nwill at least get to the level",
      "start": 770.487,
      "duration": 3.393
    },
    {
      "text": "that we've gotten to with humans.",
      "start": 773.88,
      "duration": 1.86
    },
    {
      "text": "There's then a question of, you know,",
      "start": 775.74,
      "duration": 1.44
    },
    {
      "text": "how much more is it possible\nto understand than humans do?",
      "start": 777.18,
      "duration": 4.135
    },
    {
      "text": "How much is it possible to be smarter",
      "start": 781.315,
      "duration": 1.865
    },
    {
      "text": "and more perceptive than humans?",
      "start": 783.18,
      "duration": 1.89
    },
    {
      "text": "I would guess the answer has\ngot to be domain dependent.",
      "start": 785.07,
      "duration": 4.74
    },
    {
      "text": "If I look at an area like biology,",
      "start": 789.81,
      "duration": 2.52
    },
    {
      "text": "and, you know, I wrote this essay,",
      "start": 792.33,
      "duration": 1.267
    },
    {
      "text": "\"Machines of Loving Grace.\"",
      "start": 793.597,
      "duration": 1.673
    },
    {
      "text": "It seems to me that humans are struggling",
      "start": 795.27,
      "duration": 3.39
    },
    {
      "text": "to understand the complexity\nof biology, right?",
      "start": 798.66,
      "duration": 2.1
    },
    {
      "text": "If you go to Stanford or to Harvard",
      "start": 800.76,
      "duration": 2.61
    },
    {
      "text": "or to Berkeley, you have whole\ndepartments of, you know,",
      "start": 803.37,
      "duration": 3.57
    },
    {
      "text": "folks trying to study, you know,",
      "start": 806.94,
      "duration": 1.74
    },
    {
      "text": "like the immune system\nor metabolic pathways,",
      "start": 808.68,
      "duration": 3.09
    },
    {
      "text": "and each person understands\nonly a tiny bit,",
      "start": 811.77,
      "duration": 3.42
    },
    {
      "text": "part of it, specializes,",
      "start": 815.19,
      "duration": 1.53
    },
    {
      "text": "and they're struggling to\ncombine their knowledge",
      "start": 816.72,
      "duration": 2.82
    },
    {
      "text": "with that of other humans.",
      "start": 819.54,
      "duration": 1.187
    },
    {
      "text": "And so I have an instinct that",
      "start": 820.727,
      "duration": 1.903
    },
    {
      "text": "there's a lot of room at the\ntop for AIs to get smarter.",
      "start": 822.63,
      "duration": 3.54
    },
    {
      "text": "If I think of something like materials",
      "start": 826.17,
      "duration": 3.02
    },
    {
      "text": "in the physical world",
      "start": 829.19,
      "duration": 1.36
    },
    {
      "text": "or you know, like addressing, you know,",
      "start": 830.55,
      "duration": 3.06
    },
    {
      "text": "conflicts between humans\nor something like that.",
      "start": 833.61,
      "duration": 2.4
    },
    {
      "text": "I mean, you know, it may be there's only,",
      "start": 836.01,
      "duration": 1.92
    },
    {
      "text": "some of these problems are not\nintractable, but much harder.",
      "start": 837.93,
      "duration": 3.36
    },
    {
      "text": "And it may be that there's only so well",
      "start": 841.29,
      "duration": 3.48
    },
    {
      "text": "you can do at some of these things, right?",
      "start": 844.77,
      "duration": 1.53
    },
    {
      "text": "Just like with speech recognition,",
      "start": 846.3,
      "duration": 1.26
    },
    {
      "text": "there's only so clear\nI can hear your speech.",
      "start": 847.56,
      "duration": 2.49
    },
    {
      "text": "So I think in some areas\nthere may be ceilings,",
      "start": 850.05,
      "duration": 3.99
    },
    {
      "text": "you know, that are very close\nto what humans have done.",
      "start": 854.04,
      "duration": 2.58
    },
    {
      "text": "in other areas, those\nceilings may be very far away.",
      "start": 856.62,
      "duration": 2.427
    },
    {
      "text": "And I think we'll only find out\nwhen we build these systems.",
      "start": 859.047,
      "duration": 3.693
    },
    {
      "text": "It's very hard to know in advance.",
      "start": 862.74,
      "duration": 1.44
    },
    {
      "text": "We can speculate, but we can't be sure.",
      "start": 864.18,
      "duration": 1.86
    },
    {
      "text": "- And in some domains, the ceiling",
      "start": 866.04,
      "duration": 2.19
    },
    {
      "text": "might have to do with human bureaucracies",
      "start": 868.23,
      "duration": 1.77
    },
    {
      "text": "and things like this, as you write about.",
      "start": 870.0,
      "duration": 1.463
    },
    {
      "text": "- Yes.\n- So humans fundamentally",
      "start": 871.463,
      "duration": 1.597
    },
    {
      "text": "have to be part of the loop.",
      "start": 873.06,
      "duration": 1.74
    },
    {
      "text": "That's the cause of the ceiling,",
      "start": 874.8,
      "duration": 1.62
    },
    {
      "text": "not maybe the limits of the intelligence.",
      "start": 876.42,
      "duration": 1.89
    },
    {
      "text": "- Yeah, I think in many\ncases, you know, in theory,",
      "start": 878.31,
      "duration": 4.26
    },
    {
      "text": "technology could change very fast,",
      "start": 882.57,
      "duration": 2.4
    },
    {
      "text": "for example, all the\nthings that we might invent",
      "start": 884.97,
      "duration": 2.73
    },
    {
      "text": "with respect to biology.",
      "start": 887.7,
      "duration": 2.16
    },
    {
      "text": "But remember there's a, you know,",
      "start": 889.86,
      "duration": 1.313
    },
    {
      "text": "there's a clinical trial system\nthat we have to go through",
      "start": 891.173,
      "duration": 3.457
    },
    {
      "text": "to actually administer\nthese things to humans.",
      "start": 894.63,
      "duration": 2.64
    },
    {
      "text": "I think that's a mixture of things",
      "start": 897.27,
      "duration": 1.59
    },
    {
      "text": "that are unnecessary and bureaucratic",
      "start": 898.86,
      "duration": 2.25
    },
    {
      "text": "and things that kind of protect\nthe integrity of society.",
      "start": 901.11,
      "duration": 3.15
    },
    {
      "text": "And the whole challenge is that",
      "start": 904.26,
      "duration": 2.285
    },
    {
      "text": "it's hard to tell what's going on.",
      "start": 906.545,
      "duration": 1.375
    },
    {
      "text": "It's hard to tell which is which, right?",
      "start": 907.92,
      "duration": 1.44
    },
    {
      "text": "My view is definitely, I think\nin terms of drug development,",
      "start": 909.36,
      "duration": 4.32
    },
    {
      "text": "my view is that we're too slow\nand we're too conservative.",
      "start": 913.68,
      "duration": 3.45
    },
    {
      "text": "But certainly if you get\nthese things wrong, you know,",
      "start": 917.13,
      "duration": 2.37
    },
    {
      "text": "it's possible to risk people's\nlives by being too reckless.",
      "start": 919.5,
      "duration": 5.0
    },
    {
      "text": "And so at least some of\nthese human institutions",
      "start": 924.9,
      "duration": 3.09
    },
    {
      "text": "are in fact protecting people.",
      "start": 927.99,
      "duration": 2.16
    },
    {
      "text": "So it's all about finding the balance.",
      "start": 930.15,
      "duration": 2.43
    },
    {
      "text": "I strongly suspect that balance\nis kind of more on the side",
      "start": 932.58,
      "duration": 2.94
    },
    {
      "text": "of pushing to make things happen faster,",
      "start": 935.52,
      "duration": 2.22
    },
    {
      "text": "but there is a balance.",
      "start": 937.74,
      "duration": 1.26
    },
    {
      "text": "- If we do hit a limit,",
      "start": 939.0,
      "duration": 1.413
    },
    {
      "text": "if we do hit a slow down\nin the scaling laws,",
      "start": 941.37,
      "duration": 3.57
    },
    {
      "text": "what do you think would be the reason?",
      "start": 944.94,
      "duration": 1.14
    },
    {
      "text": "Is it compute limited, data limited?",
      "start": 946.08,
      "duration": 2.67
    },
    {
      "text": "Is it something else, idea limited?",
      "start": 948.75,
      "duration": 2.31
    },
    {
      "text": "- So, a few things.",
      "start": 951.06,
      "duration": 1.38
    },
    {
      "text": "Now we're talking about hitting the limit",
      "start": 952.44,
      "duration": 1.71
    },
    {
      "text": "before we get to the level of humans",
      "start": 954.15,
      "duration": 2.49
    },
    {
      "text": "and the skill of humans.",
      "start": 956.64,
      "duration": 1.203
    },
    {
      "text": "So, I think one that's, you know,",
      "start": 958.77,
      "duration": 1.71
    },
    {
      "text": "one that's popular today\nand I think, you know,",
      "start": 960.48,
      "duration": 1.92
    },
    {
      "text": "could be a limit that we run into.",
      "start": 962.4,
      "duration": 2.22
    },
    {
      "text": "Like most of the limits,\nI would bet against it,",
      "start": 964.62,
      "duration": 1.86
    },
    {
      "text": "but it's definitely possible\nis we simply run out of data.",
      "start": 966.48,
      "duration": 3.39
    },
    {
      "text": "There's only so much data on the internet",
      "start": 969.87,
      "duration": 1.89
    },
    {
      "text": "and there's issues with the\nquality of the data, right?",
      "start": 971.76,
      "duration": 2.16
    },
    {
      "text": "You can get hundreds of trillions\nof words on the internet,",
      "start": 973.92,
      "duration": 3.99
    },
    {
      "text": "but a lot of it is repetitive",
      "start": 977.91,
      "duration": 2.91
    },
    {
      "text": "or it's search engine, you know,",
      "start": 980.82,
      "duration": 1.83
    },
    {
      "text": "search engine optimization\ndrivel, or maybe in the future",
      "start": 982.65,
      "duration": 3.03
    },
    {
      "text": "it'll even be text\ngenerated by AIs itself.",
      "start": 985.68,
      "duration": 2.523
    },
    {
      "text": "And so I think there are limits",
      "start": 989.07,
      "duration": 2.77
    },
    {
      "text": "to what can be produced in this way.",
      "start": 992.801,
      "duration": 2.059
    },
    {
      "text": "That said, we and I would\nguess other companies",
      "start": 994.86,
      "duration": 2.82
    },
    {
      "text": "are working on ways to make data synthetic",
      "start": 997.68,
      "duration": 3.87
    },
    {
      "text": "where you can, you know,",
      "start": 1001.55,
      "duration": 0.833
    },
    {
      "text": "you can use the model\nto generate more data",
      "start": 1002.383,
      "duration": 2.797
    },
    {
      "text": "of the type that you have already",
      "start": 1005.18,
      "duration": 2.4
    },
    {
      "text": "or even generate data from scratch.",
      "start": 1007.58,
      "duration": 2.25
    },
    {
      "text": "If you think about what was done",
      "start": 1009.83,
      "duration": 2.01
    },
    {
      "text": "with DeepMind's AlphaGo Zero,",
      "start": 1011.84,
      "duration": 2.25
    },
    {
      "text": "they managed to get a\nbot all the way from,",
      "start": 1014.09,
      "duration": 2.58
    },
    {
      "text": "you know, no ability to play Go whatsoever",
      "start": 1016.67,
      "duration": 2.37
    },
    {
      "text": "to above human level just\nby playing against itself.",
      "start": 1019.04,
      "duration": 2.91
    },
    {
      "text": "There was no example\ndata from humans required",
      "start": 1021.95,
      "duration": 2.64
    },
    {
      "text": "in the AlphaGo Zero version of it.",
      "start": 1024.59,
      "duration": 2.82
    },
    {
      "text": "The other direction, of course,\nis these reasoning models",
      "start": 1027.41,
      "duration": 2.58
    },
    {
      "text": "that do chain of thought and stop to think",
      "start": 1029.99,
      "duration": 2.4
    },
    {
      "text": "and reflect on their own thinking.",
      "start": 1032.39,
      "duration": 1.98
    },
    {
      "text": "In a way, that's another\nkind of synthetic data",
      "start": 1034.37,
      "duration": 2.79
    },
    {
      "text": "coupled with reinforcement learning.",
      "start": 1037.16,
      "duration": 2.01
    },
    {
      "text": "So my guess is with one of those methods,",
      "start": 1039.17,
      "duration": 2.52
    },
    {
      "text": "we'll get around the data limitation",
      "start": 1041.69,
      "duration": 1.71
    },
    {
      "text": "or there may be other sources of data",
      "start": 1043.4,
      "duration": 2.19
    },
    {
      "text": "that are available.",
      "start": 1045.59,
      "duration": 1.89
    },
    {
      "text": "We could just observe that",
      "start": 1047.48,
      "duration": 1.71
    },
    {
      "text": "even if there's no problem with data,",
      "start": 1049.19,
      "duration": 1.74
    },
    {
      "text": "as we start to scale models up,",
      "start": 1050.93,
      "duration": 1.83
    },
    {
      "text": "they just stop getting better.",
      "start": 1052.76,
      "duration": 1.8
    },
    {
      "text": "It seemed to be a reliable observation",
      "start": 1054.56,
      "duration": 2.73
    },
    {
      "text": "that they've gotten better,\nthat could just stop",
      "start": 1057.29,
      "duration": 2.19
    },
    {
      "text": "at some point for a reason\nwe don't understand.",
      "start": 1059.48,
      "duration": 2.373
    },
    {
      "text": "The answer could be that we need to,",
      "start": 1063.02,
      "duration": 3.45
    },
    {
      "text": "you know, we need to invent\nsome new architecture.",
      "start": 1066.47,
      "duration": 2.523
    },
    {
      "text": "There have been problems in the past with,",
      "start": 1070.07,
      "duration": 2.4
    },
    {
      "text": "say, numerical stability of models",
      "start": 1072.47,
      "duration": 2.43
    },
    {
      "text": "where it looked like\nthings were leveling off,",
      "start": 1074.9,
      "duration": 2.64
    },
    {
      "text": "but actually, you know,",
      "start": 1077.54,
      "duration": 2.42
    },
    {
      "text": "when we found the right unblocker,",
      "start": 1079.96,
      "duration": 1.6
    },
    {
      "text": "they didn't end up doing so.",
      "start": 1081.56,
      "duration": 1.2
    },
    {
      "text": "So perhaps there's some\nnew optimization method",
      "start": 1082.76,
      "duration": 3.69
    },
    {
      "text": "or some new technique we\nneed to unblock things.",
      "start": 1086.45,
      "duration": 3.36
    },
    {
      "text": "I've seen no evidence of that so far.",
      "start": 1089.81,
      "duration": 1.86
    },
    {
      "text": "But if things were to slow down,",
      "start": 1091.67,
      "duration": 2.01
    },
    {
      "text": "that perhaps could be one reason.",
      "start": 1093.68,
      "duration": 2.13
    },
    {
      "text": "- What about the limits of compute?",
      "start": 1095.81,
      "duration": 2.52
    },
    {
      "text": "Meaning the expensive nature",
      "start": 1098.33,
      "duration": 3.39
    },
    {
      "text": "of building bigger and\nbigger data centers.",
      "start": 1101.72,
      "duration": 1.98
    },
    {
      "text": "- So right now, I think, you know,",
      "start": 1103.7,
      "duration": 2.43
    },
    {
      "text": "most of the frontier model companies",
      "start": 1106.13,
      "duration": 2.01
    },
    {
      "text": "I would guess are operating in, you know,",
      "start": 1108.14,
      "duration": 1.8
    },
    {
      "text": "roughly, you know, $1 billion scale,",
      "start": 1109.94,
      "duration": 3.09
    },
    {
      "text": "plus or minus a factor of three, right?",
      "start": 1113.03,
      "duration": 1.74
    },
    {
      "text": "Those are the models that exist now",
      "start": 1114.77,
      "duration": 1.65
    },
    {
      "text": "or are being trained now.",
      "start": 1116.42,
      "duration": 2.13
    },
    {
      "text": "I think next year, we're\ngonna go to a few billion,",
      "start": 1118.55,
      "duration": 2.58
    },
    {
      "text": "and then 2026, we may go to,\nyou know, above 10 billion,",
      "start": 1121.13,
      "duration": 5.0
    },
    {
      "text": "and probably by 2027,\ntheir ambitions to build",
      "start": 1126.83,
      "duration": 3.49
    },
    {
      "text": "100 billion dollar clusters,",
      "start": 1132.115,
      "duration": 1.285
    },
    {
      "text": "and I think all of that\nactually will happen.",
      "start": 1133.4,
      "duration": 2.58
    },
    {
      "text": "There's a lot of determination to build",
      "start": 1135.98,
      "duration": 1.86
    },
    {
      "text": "the compute to do it within this country,",
      "start": 1137.84,
      "duration": 2.85
    },
    {
      "text": "and I would guess that\nit actually does happen.",
      "start": 1140.69,
      "duration": 1.95
    },
    {
      "text": "Now, if we get to 100 billion,",
      "start": 1142.64,
      "duration": 2.52
    },
    {
      "text": "that's still not enough compute,",
      "start": 1145.16,
      "duration": 1.32
    },
    {
      "text": "that's still not enough scale",
      "start": 1146.48,
      "duration": 1.62
    },
    {
      "text": "then either we need even more scale",
      "start": 1148.1,
      "duration": 2.34
    },
    {
      "text": "or we need to develop some way",
      "start": 1150.44,
      "duration": 2.16
    },
    {
      "text": "of doing it more efficiently\nof shifting the curve.",
      "start": 1152.6,
      "duration": 2.43
    },
    {
      "text": "I think between all of these,",
      "start": 1155.03,
      "duration": 1.86
    },
    {
      "text": "one of the reasons I'm bullish",
      "start": 1156.89,
      "duration": 1.29
    },
    {
      "text": "about powerful AI happening so fast",
      "start": 1158.18,
      "duration": 2.67
    },
    {
      "text": "is just that if you extrapolate",
      "start": 1160.85,
      "duration": 1.86
    },
    {
      "text": "the next few points on the curve,",
      "start": 1162.71,
      "duration": 1.86
    },
    {
      "text": "we're very quickly getting towards",
      "start": 1164.57,
      "duration": 2.13
    },
    {
      "text": "human level ability, right?",
      "start": 1166.7,
      "duration": 1.47
    },
    {
      "text": "Some of the new models that we developed,",
      "start": 1168.17,
      "duration": 3.03
    },
    {
      "text": "some reasoning models that\nhave come from other companies,",
      "start": 1171.2,
      "duration": 2.64
    },
    {
      "text": "they're starting to get\nto what I would call",
      "start": 1173.84,
      "duration": 1.8
    },
    {
      "text": "the PhD or professional level, right?",
      "start": 1175.64,
      "duration": 2.16
    },
    {
      "text": "If you look at their coding ability,",
      "start": 1177.8,
      "duration": 2.97
    },
    {
      "text": "the latest model we released, Sonnet 3.5,",
      "start": 1180.77,
      "duration": 3.57
    },
    {
      "text": "the new or updated version,",
      "start": 1184.34,
      "duration": 2.01
    },
    {
      "text": "it gets something like 50% on SWE-bench,",
      "start": 1186.35,
      "duration": 2.82
    },
    {
      "text": "and SWE-bench is an example of a bunch",
      "start": 1189.17,
      "duration": 1.8
    },
    {
      "text": "of professional, real world\nsoftware engineering tasks.",
      "start": 1190.97,
      "duration": 3.72
    },
    {
      "text": "At the beginning of the year,",
      "start": 1194.69,
      "duration": 1.59
    },
    {
      "text": "I think the state of the art was 3 or 4%.",
      "start": 1196.28,
      "duration": 3.63
    },
    {
      "text": "So in 10 months we've gone\nfrom 3% to 50% on this task,",
      "start": 1199.91,
      "duration": 4.56
    },
    {
      "text": "and I think in another year,\nwe'll probably be at 90%.",
      "start": 1204.47,
      "duration": 2.67
    },
    {
      "text": "I mean, I don't know, but\nmight even be less than that.",
      "start": 1207.14,
      "duration": 4.47
    },
    {
      "text": "We've seen similar things\nin graduate level math,",
      "start": 1211.61,
      "duration": 3.3
    },
    {
      "text": "physics, and biology from\nmodels like OpenAI's o1.",
      "start": 1214.91,
      "duration": 4.47
    },
    {
      "text": "So if we just continue to\nextrapolate this, right,",
      "start": 1219.38,
      "duration": 4.05
    },
    {
      "text": "in terms of skill that we have,",
      "start": 1223.43,
      "duration": 2.37
    },
    {
      "text": "I think if we extrapolate\nthe straight curve,",
      "start": 1225.8,
      "duration": 2.64
    },
    {
      "text": "within a few years, we will\nget to these models being,",
      "start": 1228.44,
      "duration": 3.39
    },
    {
      "text": "you know, above the\nhighest professional level",
      "start": 1231.83,
      "duration": 2.46
    },
    {
      "text": "in terms of humans.",
      "start": 1234.29,
      "duration": 0.833
    },
    {
      "text": "Now, will that curve continue?",
      "start": 1235.123,
      "duration": 1.807
    },
    {
      "text": "You've pointed to, and I've\npointed to a lot of reasons why,",
      "start": 1236.93,
      "duration": 2.73
    },
    {
      "text": "you know, possible reasons\nwhy that might not happen.",
      "start": 1239.66,
      "duration": 2.55
    },
    {
      "text": "But if the extrapolation curve continues,",
      "start": 1242.21,
      "duration": 2.37
    },
    {
      "text": "that is the trajectory we're on.",
      "start": 1244.58,
      "duration": 1.65
    },
    {
      "text": "- So Anthropic has several competitors.",
      "start": 1246.23,
      "duration": 2.76
    },
    {
      "text": "It'd be interesting to get\nyour sort of view of it all.",
      "start": 1248.99,
      "duration": 2.34
    },
    {
      "text": "OpenAI, Google, xAI, Meta.",
      "start": 1251.33,
      "duration": 2.46
    },
    {
      "text": "What does it take to win",
      "start": 1253.79,
      "duration": 2.25
    },
    {
      "text": "in the broad sense of win in this space?",
      "start": 1256.04,
      "duration": 2.34
    },
    {
      "text": "- Yeah, so I want to separate\nout a couple things, right?",
      "start": 1258.38,
      "duration": 2.7
    },
    {
      "text": "So, you know, Anthropic's mission",
      "start": 1261.08,
      "duration": 2.85
    },
    {
      "text": "is to kind of try to make\nthis all go well, right?",
      "start": 1263.93,
      "duration": 2.94
    },
    {
      "text": "And you know, we have a theory of change",
      "start": 1266.87,
      "duration": 2.7
    },
    {
      "text": "called race to the top, right?",
      "start": 1269.57,
      "duration": 2.22
    },
    {
      "text": "Race to the top is about trying to push",
      "start": 1271.79,
      "duration": 3.81
    },
    {
      "text": "the other players to do the right thing",
      "start": 1275.6,
      "duration": 2.55
    },
    {
      "text": "by setting an example.",
      "start": 1278.15,
      "duration": 1.26
    },
    {
      "text": "It's not about being the good guy,",
      "start": 1279.41,
      "duration": 1.29
    },
    {
      "text": "it's about setting things up so that",
      "start": 1280.7,
      "duration": 1.68
    },
    {
      "text": "all of us can be the good guy.",
      "start": 1282.38,
      "duration": 1.74
    },
    {
      "text": "I'll give a few examples of this.",
      "start": 1284.12,
      "duration": 1.5
    },
    {
      "text": "Early in the history of Anthropic,",
      "start": 1285.62,
      "duration": 1.98
    },
    {
      "text": "one of our co-founders, Chris Olah,",
      "start": 1287.6,
      "duration": 1.59
    },
    {
      "text": "who I believe you're interviewing soon,",
      "start": 1289.19,
      "duration": 2.37
    },
    {
      "text": "you know, he's the co-founder of the field",
      "start": 1291.56,
      "duration": 1.65
    },
    {
      "text": "of mechanistic interpretability,\nwhich is an attempt",
      "start": 1293.21,
      "duration": 2.22
    },
    {
      "text": "to understand what's\ngoing on inside AI models.",
      "start": 1295.43,
      "duration": 4.02
    },
    {
      "text": "So we had him and one of our early teams",
      "start": 1299.45,
      "duration": 3.24
    },
    {
      "text": "focus on this area of interpretability,",
      "start": 1302.69,
      "duration": 1.92
    },
    {
      "text": "which we think is good",
      "start": 1304.61,
      "duration": 1.05
    },
    {
      "text": "for making models safe and transparent.",
      "start": 1305.66,
      "duration": 3.06
    },
    {
      "text": "For three or four years,",
      "start": 1308.72,
      "duration": 1.77
    },
    {
      "text": "that had no commercial\napplication whatsoever.",
      "start": 1310.49,
      "duration": 2.22
    },
    {
      "text": "It still doesn't today.",
      "start": 1312.71,
      "duration": 1.2
    },
    {
      "text": "We're doing some early betas with it,",
      "start": 1313.91,
      "duration": 1.92
    },
    {
      "text": "and probably it will eventually,",
      "start": 1315.83,
      "duration": 1.47
    },
    {
      "text": "but you know, this is a\nvery, very long research bed",
      "start": 1317.3,
      "duration": 3.93
    },
    {
      "text": "and one in which we've built in public",
      "start": 1321.23,
      "duration": 2.37
    },
    {
      "text": "and shared our results publicly.",
      "start": 1323.6,
      "duration": 1.95
    },
    {
      "text": "And we did this because, you know,",
      "start": 1325.55,
      "duration": 1.77
    },
    {
      "text": "we think it's a way to make models safer.",
      "start": 1327.32,
      "duration": 2.28
    },
    {
      "text": "An interesting thing is\nthat as we've done this,",
      "start": 1329.6,
      "duration": 3.06
    },
    {
      "text": "other companies have\nstarted doing it as well,",
      "start": 1332.66,
      "duration": 2.31
    },
    {
      "text": "in some cases because\nthey've been inspired by it,",
      "start": 1334.97,
      "duration": 2.64
    },
    {
      "text": "in some cases because they're\nworried that, you know,",
      "start": 1337.61,
      "duration": 4.11
    },
    {
      "text": "if other companies are doing this",
      "start": 1341.72,
      "duration": 1.79
    },
    {
      "text": "to look more responsible,",
      "start": 1343.51,
      "duration": 1.63
    },
    {
      "text": "they wanna look more responsible too.",
      "start": 1345.14,
      "duration": 1.83
    },
    {
      "text": "No one wants to look like\nthe irresponsible actor,",
      "start": 1346.97,
      "duration": 2.67
    },
    {
      "text": "and so they adopt this as well.",
      "start": 1349.64,
      "duration": 3.21
    },
    {
      "text": "When folks come to Anthropic,",
      "start": 1352.85,
      "duration": 1.92
    },
    {
      "text": "interpretability often a draw,",
      "start": 1354.77,
      "duration": 1.35
    },
    {
      "text": "and I tell them, the other\nplaces you didn't go,",
      "start": 1356.12,
      "duration": 2.91
    },
    {
      "text": "tell them why you came here,",
      "start": 1359.03,
      "duration": 1.712
    },
    {
      "text": "and then you see soon",
      "start": 1360.742,
      "duration": 2.848
    },
    {
      "text": "that there's interpretability\nteams elsewhere as well.",
      "start": 1363.59,
      "duration": 3.93
    },
    {
      "text": "And in a way, that takes away\nour competitive advantage",
      "start": 1367.52,
      "duration": 2.46
    },
    {
      "text": "because it's like, oh, now\nothers are doing it as well,",
      "start": 1369.98,
      "duration": 3.99
    },
    {
      "text": "but it's good for the broader system,",
      "start": 1373.97,
      "duration": 2.397
    },
    {
      "text": "and so we have to invent\nsome new thing that",
      "start": 1376.367,
      "duration": 1.683
    },
    {
      "text": "we're doing that others\naren't doing as well.",
      "start": 1378.05,
      "duration": 1.467
    },
    {
      "text": "And the hope is to basically\nbid up the importance",
      "start": 1379.517,
      "duration": 4.783
    },
    {
      "text": "of doing the right thing.",
      "start": 1385.239,
      "duration": 1.001
    },
    {
      "text": "And it's not about us\nin particular, right?",
      "start": 1386.24,
      "duration": 2.22
    },
    {
      "text": "It's not about having\none particular good guy.",
      "start": 1388.46,
      "duration": 3.24
    },
    {
      "text": "Other companies can do this as well.",
      "start": 1391.7,
      "duration": 2.07
    },
    {
      "text": "If they join the race\nto do this, you know,",
      "start": 1393.77,
      "duration": 3.06
    },
    {
      "text": "that's the best news ever, right?",
      "start": 1396.83,
      "duration": 2.192
    },
    {
      "text": "It's just, it's about kind\nof shaping the incentives",
      "start": 1399.022,
      "duration": 2.818
    },
    {
      "text": "to point upward instead of shaping",
      "start": 1401.84,
      "duration": 1.92
    },
    {
      "text": "the incentives to point downward.",
      "start": 1403.76,
      "duration": 2.04
    },
    {
      "text": "- And we should say this\nexample of the field",
      "start": 1405.8,
      "duration": 1.68
    },
    {
      "text": "of mechanistic interpretability",
      "start": 1407.48,
      "duration": 2.43
    },
    {
      "text": "is just a rigorous, non-hand\nwavy way of doing AI safety,",
      "start": 1409.91,
      "duration": 4.83
    },
    {
      "text": "or it's tending that way.",
      "start": 1414.74,
      "duration": 1.68
    },
    {
      "text": "- Trying to, I mean, I\nthink we're still early",
      "start": 1416.42,
      "duration": 3.03
    },
    {
      "text": "in terms of our ability to see things,",
      "start": 1419.45,
      "duration": 2.25
    },
    {
      "text": "but I've been surprised at how much",
      "start": 1421.7,
      "duration": 1.95
    },
    {
      "text": "we've been able to look\ninside these systems",
      "start": 1423.65,
      "duration": 2.19
    },
    {
      "text": "and understand what we see, right?",
      "start": 1425.84,
      "duration": 2.49
    },
    {
      "text": "Unlike with the scaling laws",
      "start": 1428.33,
      "duration": 1.5
    },
    {
      "text": "where it feels like\nthere's some, you know,",
      "start": 1429.83,
      "duration": 2.16
    },
    {
      "text": "law that's driving these\nmodels to perform better,",
      "start": 1431.99,
      "duration": 3.3
    },
    {
      "text": "on the inside, the\nmodels aren't, you know,",
      "start": 1435.29,
      "duration": 2.46
    },
    {
      "text": "there's no reason why\nthey should be designed",
      "start": 1437.75,
      "duration": 1.65
    },
    {
      "text": "for us to understand them, right?",
      "start": 1439.4,
      "duration": 1.2
    },
    {
      "text": "They're designed to operate,\nthey're designed to work,",
      "start": 1440.6,
      "duration": 2.047
    },
    {
      "text": "just like the human brain\nor human biochemistry.",
      "start": 1442.647,
      "duration": 3.083
    },
    {
      "text": "They're not designed for a\nhuman to open up the hatch,",
      "start": 1445.73,
      "duration": 2.28
    },
    {
      "text": "look inside and understand them.",
      "start": 1448.01,
      "duration": 1.86
    },
    {
      "text": "But we have found, and you know,",
      "start": 1449.87,
      "duration": 1.497
    },
    {
      "text": "you can talk in much more\ndetail about this to Chris,",
      "start": 1451.367,
      "duration": 2.943
    },
    {
      "text": "that when we open them up,",
      "start": 1454.31,
      "duration": 1.35
    },
    {
      "text": "when we do look inside them,",
      "start": 1455.66,
      "duration": 1.65
    },
    {
      "text": "we find things that are\nsurprisingly interesting.",
      "start": 1457.31,
      "duration": 2.73
    },
    {
      "text": "- And as a side effect, you also get",
      "start": 1460.04,
      "duration": 1.53
    },
    {
      "text": "to see the beauty of these models.",
      "start": 1461.57,
      "duration": 1.59
    },
    {
      "text": "You get to explore sort\nof the beautiful nature",
      "start": 1463.16,
      "duration": 3.45
    },
    {
      "text": "of large neural networks",
      "start": 1466.61,
      "duration": 1.11
    },
    {
      "text": "through the mech interp\nkind of methodology.",
      "start": 1467.72,
      "duration": 1.792
    },
    {
      "text": "- I'm amazed at how clean it's been.",
      "start": 1469.512,
      "duration": 2.378
    },
    {
      "text": "I'm amazed at things like induction heads.",
      "start": 1471.89,
      "duration": 3.06
    },
    {
      "text": "I'm amazed at things like, you know,",
      "start": 1474.95,
      "duration": 3.96
    },
    {
      "text": "that we can, you know,\nuse sparse auto-encoders",
      "start": 1478.91,
      "duration": 2.88
    },
    {
      "text": "to find these directions\nwithin the networks,",
      "start": 1481.79,
      "duration": 3.33
    },
    {
      "text": "and that the directions correspond",
      "start": 1485.12,
      "duration": 2.25
    },
    {
      "text": "to these very clear concepts.",
      "start": 1487.37,
      "duration": 1.89
    },
    {
      "text": "We demonstrated this a bit",
      "start": 1489.26,
      "duration": 1.38
    },
    {
      "text": "with the Golden Gate Bridge Claude.",
      "start": 1490.64,
      "duration": 1.53
    },
    {
      "text": "So this was an experiment\nwhere we found a direction",
      "start": 1492.17,
      "duration": 3.51
    },
    {
      "text": "inside one of the neural network's layers",
      "start": 1495.68,
      "duration": 2.46
    },
    {
      "text": "that corresponded to\nthe Golden Gate Bridge",
      "start": 1498.14,
      "duration": 1.89
    },
    {
      "text": "and we just turned that way up.",
      "start": 1500.03,
      "duration": 1.47
    },
    {
      "text": "And so we released this model as a demo,",
      "start": 1501.5,
      "duration": 3.06
    },
    {
      "text": "it was kind of half a\njoke, for a couple days,",
      "start": 1504.56,
      "duration": 3.15
    },
    {
      "text": "but it was illustrative of\nthe method we developed.",
      "start": 1507.71,
      "duration": 3.3
    },
    {
      "text": "And you could take the Golden Gate,",
      "start": 1511.01,
      "duration": 3.0
    },
    {
      "text": "you could take the model, you\ncould ask it about anything,",
      "start": 1514.01,
      "duration": 2.13
    },
    {
      "text": "you know, it would be like, you could say,",
      "start": 1516.14,
      "duration": 2.377
    },
    {
      "text": "\"How was your day\" and anything you asked,",
      "start": 1518.517,
      "duration": 2.093
    },
    {
      "text": "because this feature was activated,",
      "start": 1520.61,
      "duration": 1.38
    },
    {
      "text": "would connect to the Golden Gate Bridge.",
      "start": 1521.99,
      "duration": 1.38
    },
    {
      "text": "So it would say, you know,",
      "start": 1523.37,
      "duration": 1.657
    },
    {
      "text": "\"I'm feeling relaxed and expansive,",
      "start": 1525.027,
      "duration": 2.603
    },
    {
      "text": "much like the arches of\nthe Golden Gate Bridge\"",
      "start": 1527.63,
      "duration": 2.49
    },
    {
      "text": "or, you know.",
      "start": 1530.12,
      "duration": 0.93
    },
    {
      "text": "- It would masterfully change topic",
      "start": 1531.05,
      "duration": 1.817
    },
    {
      "text": "to the Golden Gate Bridge\nand it integrate it.",
      "start": 1532.867,
      "duration": 1.73
    },
    {
      "text": "There was also a sadness to it,",
      "start": 1534.597,
      "duration": 2.203
    },
    {
      "text": "to the focus it had on\nthe Golden Gate Bridge.",
      "start": 1536.8,
      "duration": 1.99
    },
    {
      "text": "I think people quickly fell\nin love with it, I think,",
      "start": 1538.79,
      "duration": 2.91
    },
    {
      "text": "so people already miss it",
      "start": 1541.7,
      "duration": 1.697
    },
    {
      "text": "'cause it was taken down\nI think after a day.",
      "start": 1543.397,
      "duration": 2.173
    },
    {
      "text": "- Somehow these interventions on the model",
      "start": 1545.57,
      "duration": 2.92
    },
    {
      "text": "where you kind of adjust its behavior",
      "start": 1550.01,
      "duration": 2.04
    },
    {
      "text": "somehow emotionally\nmade it seem more human",
      "start": 1552.05,
      "duration": 3.18
    },
    {
      "text": "than any other version of the model.",
      "start": 1555.23,
      "duration": 0.833
    },
    {
      "text": "- It has a strong\npersonality, strong identity.",
      "start": 1556.063,
      "duration": 2.257
    },
    {
      "text": "- It has a strong personality.",
      "start": 1558.32,
      "duration": 1.56
    },
    {
      "text": "It has these kind of\nlike obsessive interests.",
      "start": 1559.88,
      "duration": 3.09
    },
    {
      "text": "You know, we can all think of someone",
      "start": 1562.97,
      "duration": 0.99
    },
    {
      "text": "who's like obsessed with something.",
      "start": 1563.96,
      "duration": 1.38
    },
    {
      "text": "So it does make it feel\nsomehow a bit more human.",
      "start": 1565.34,
      "duration": 3.0
    },
    {
      "text": "- Let's talk about the present.",
      "start": 1568.34,
      "duration": 1.02
    },
    {
      "text": "Let's talk about Claude.",
      "start": 1569.36,
      "duration": 1.2
    },
    {
      "text": "So this year, a lot has happened.",
      "start": 1570.56,
      "duration": 2.79
    },
    {
      "text": "In March, Claude 3, Opus,\nSonnet, Haiku were released,",
      "start": 1573.35,
      "duration": 4.83
    },
    {
      "text": "then Claude 3.5 Sonnet in July,",
      "start": 1578.18,
      "duration": 3.42
    },
    {
      "text": "with an updated version just now released,",
      "start": 1581.6,
      "duration": 2.67
    },
    {
      "text": "and then also Claude\n3.5 Haiku was released.",
      "start": 1584.27,
      "duration": 3.06
    },
    {
      "text": "Okay, can you explain the difference",
      "start": 1587.33,
      "duration": 2.19
    },
    {
      "text": "between Opus, Sonnet and Haiku,",
      "start": 1589.52,
      "duration": 3.03
    },
    {
      "text": "and how we should think\nabout the different versions?",
      "start": 1592.55,
      "duration": 2.07
    },
    {
      "text": "- Yeah, so let's go back to March",
      "start": 1594.62,
      "duration": 1.71
    },
    {
      "text": "when we first released these three models.",
      "start": 1596.33,
      "duration": 2.61
    },
    {
      "text": "So, you know, our thinking was, you know,",
      "start": 1598.94,
      "duration": 2.82
    },
    {
      "text": "different companies produce\nkind of large and small models,",
      "start": 1601.76,
      "duration": 3.42
    },
    {
      "text": "better and worse models.",
      "start": 1605.18,
      "duration": 1.95
    },
    {
      "text": "We felt that there was demand",
      "start": 1607.13,
      "duration": 2.37
    },
    {
      "text": "both for a really\npowerful model, you know,",
      "start": 1609.5,
      "duration": 3.66
    },
    {
      "text": "and you that might be a little bit slower",
      "start": 1613.16,
      "duration": 1.65
    },
    {
      "text": "that you'd have to pay more for,",
      "start": 1614.81,
      "duration": 1.62
    },
    {
      "text": "and also for fast, cheap models",
      "start": 1616.43,
      "duration": 3.3
    },
    {
      "text": "that are as smart as they can be",
      "start": 1619.73,
      "duration": 1.92
    },
    {
      "text": "for how fast and cheap, right?",
      "start": 1621.65,
      "duration": 1.62
    },
    {
      "text": "Whenever you wanna do some\nkind of like, you know,",
      "start": 1623.27,
      "duration": 2.73
    },
    {
      "text": "difficult analysis, like if, you know,",
      "start": 1626.0,
      "duration": 1.53
    },
    {
      "text": "I wanna write code, for instance,",
      "start": 1627.53,
      "duration": 1.98
    },
    {
      "text": "or you know, I wanna brainstorm ideas,",
      "start": 1629.51,
      "duration": 2.76
    },
    {
      "text": "or I wanna do creative writing,",
      "start": 1632.27,
      "duration": 1.65
    },
    {
      "text": "I want the really powerful model.",
      "start": 1633.92,
      "duration": 1.74
    },
    {
      "text": "But then there's a lot\nof practical applications",
      "start": 1635.66,
      "duration": 2.52
    },
    {
      "text": "in a business sense where it's like",
      "start": 1638.18,
      "duration": 1.71
    },
    {
      "text": "I'm interacting with a website.",
      "start": 1639.89,
      "duration": 1.83
    },
    {
      "text": "You know, like, I'm like doing my taxes,",
      "start": 1641.72,
      "duration": 3.06
    },
    {
      "text": "or I'm, you know, talking to a, you know,",
      "start": 1644.78,
      "duration": 1.747
    },
    {
      "text": "to like a legal advisor and\nI want to analyze a contract",
      "start": 1646.527,
      "duration": 3.383
    },
    {
      "text": "or, you know, we have plenty of companies",
      "start": 1649.91,
      "duration": 2.04
    },
    {
      "text": "that are just like, you know,",
      "start": 1651.95,
      "duration": 1.53
    },
    {
      "text": "I wanna do auto complete\non my IDE or something.",
      "start": 1653.48,
      "duration": 3.99
    },
    {
      "text": "And for all of those\nthings, you want to act fast",
      "start": 1657.47,
      "duration": 3.06
    },
    {
      "text": "and you want to use\nthe model very broadly.",
      "start": 1660.53,
      "duration": 2.19
    },
    {
      "text": "So we wanted to serve that\nwhole spectrum of needs.",
      "start": 1662.72,
      "duration": 4.23
    },
    {
      "text": "So we ended up with this, you know,",
      "start": 1666.95,
      "duration": 1.89
    },
    {
      "text": "this kind of poetry theme.",
      "start": 1668.84,
      "duration": 1.35
    },
    {
      "text": "And so what's a really short poem?",
      "start": 1670.19,
      "duration": 1.32
    },
    {
      "text": "It's a haiku.",
      "start": 1671.51,
      "duration": 1.11
    },
    {
      "text": "And so Haiku is the small,\nfast, cheap model that is,",
      "start": 1672.62,
      "duration": 4.02
    },
    {
      "text": "you know, was at the time\nwas released surprisingly,",
      "start": 1676.64,
      "duration": 2.46
    },
    {
      "text": "surprisingly intelligent for\nhow fast and cheap it was.",
      "start": 1679.1,
      "duration": 4.08
    },
    {
      "text": "Sonnet is a medium sized poem,",
      "start": 1683.18,
      "duration": 2.22
    },
    {
      "text": "right, a couple paragraphs,",
      "start": 1685.4,
      "duration": 1.2
    },
    {
      "text": "and so Sonnet was the middle model.",
      "start": 1686.6,
      "duration": 1.92
    },
    {
      "text": "It is smarter but also\na little bit slower,",
      "start": 1688.52,
      "duration": 2.73
    },
    {
      "text": "a little bit more expensive.",
      "start": 1691.25,
      "duration": 1.2
    },
    {
      "text": "And Opus, like a magnum\nopus is a large work,",
      "start": 1692.45,
      "duration": 3.54
    },
    {
      "text": "Opus was the largest,\nsmartest model at the time.",
      "start": 1695.99,
      "duration": 4.41
    },
    {
      "text": "So that was the original\nkind of thinking behind it.",
      "start": 1700.4,
      "duration": 2.943
    },
    {
      "text": "And our thinking then was,\nwell, each new generation",
      "start": 1704.87,
      "duration": 3.45
    },
    {
      "text": "of models should shift\nthat trade-off curve.",
      "start": 1708.32,
      "duration": 3.0
    },
    {
      "text": "So when we released Sonnet 3.5,",
      "start": 1711.32,
      "duration": 2.76
    },
    {
      "text": "it has the same, roughly\nthe same, you know,",
      "start": 1714.08,
      "duration": 2.94
    },
    {
      "text": "cost and speed as the Sonnet 3 model.",
      "start": 1717.02,
      "duration": 4.473
    },
    {
      "text": "But it increased its intelligence",
      "start": 1722.42,
      "duration": 4.14
    },
    {
      "text": "to the point where it was smarter",
      "start": 1726.56,
      "duration": 1.53
    },
    {
      "text": "than the original Opus 3 model,",
      "start": 1728.09,
      "duration": 2.19
    },
    {
      "text": "especially for code but\nalso just in general.",
      "start": 1730.28,
      "duration": 3.54
    },
    {
      "text": "And so now, you know, we've\nshown results for Haiku 3.5,",
      "start": 1733.82,
      "duration": 5.0
    },
    {
      "text": "and I believe Haiku 3.5,\nthe smallest new model",
      "start": 1738.83,
      "duration": 4.11
    },
    {
      "text": "is about as good as Opus\n3, the largest old model.",
      "start": 1742.94,
      "duration": 4.29
    },
    {
      "text": "So basically the aim here\nis to shift the curve,",
      "start": 1747.23,
      "duration": 2.55
    },
    {
      "text": "and then at some point,\nthere's gonna be an Opus 3.5.",
      "start": 1749.78,
      "duration": 2.56
    },
    {
      "text": "Now, every new generation\nof models has its own thing.",
      "start": 1753.26,
      "duration": 3.15
    },
    {
      "text": "They use new data, their\npersonality changes",
      "start": 1756.41,
      "duration": 3.0
    },
    {
      "text": "in ways that we kind of, you know,",
      "start": 1759.41,
      "duration": 2.19
    },
    {
      "text": "try to steer but are\nnot fully able to steer.",
      "start": 1761.6,
      "duration": 2.85
    },
    {
      "text": "And so there's never quite\nthat exact equivalence",
      "start": 1764.45,
      "duration": 3.63
    },
    {
      "text": "where the only thing you're\nchanging is intelligence.",
      "start": 1768.08,
      "duration": 2.49
    },
    {
      "text": "We always try and improve other things,",
      "start": 1770.57,
      "duration": 1.65
    },
    {
      "text": "and some things change without\nus knowing or measuring.",
      "start": 1772.22,
      "duration": 3.42
    },
    {
      "text": "So it's very much an inexact science.",
      "start": 1775.64,
      "duration": 2.61
    },
    {
      "text": "In many ways, the manner and\npersonality of these models",
      "start": 1778.25,
      "duration": 3.74
    },
    {
      "text": "is more an art than it is a science.",
      "start": 1781.99,
      "duration": 2.62
    },
    {
      "text": "- So what is sort of the reason",
      "start": 1784.61,
      "duration": 3.21
    },
    {
      "text": "for the span of time between, say,",
      "start": 1787.82,
      "duration": 4.71
    },
    {
      "text": "Claude Opus 3.0 and 3.5?",
      "start": 1792.53,
      "duration": 3.36
    },
    {
      "text": "What takes that time?",
      "start": 1795.89,
      "duration": 1.83
    },
    {
      "text": "If you can speak to.",
      "start": 1797.72,
      "duration": 0.833
    },
    {
      "text": "- Yeah, so there's different processes.",
      "start": 1798.553,
      "duration": 3.817
    },
    {
      "text": "There's pre-training, which is, you know,",
      "start": 1802.37,
      "duration": 1.8
    },
    {
      "text": "just kind of the normal\nlanguage model training,",
      "start": 1804.17,
      "duration": 2.34
    },
    {
      "text": "and that takes a very long time.",
      "start": 1806.51,
      "duration": 1.95
    },
    {
      "text": "That uses, you know, these days,",
      "start": 1808.46,
      "duration": 2.318
    },
    {
      "text": "you know, tens of thousands,",
      "start": 1810.778,
      "duration": 3.052
    },
    {
      "text": "sometimes many tens of\nthousands of GPUs or TPUs",
      "start": 1813.83,
      "duration": 4.427
    },
    {
      "text": "or Trainium, or you know,\nwe use different platforms,",
      "start": 1818.257,
      "duration": 2.833
    },
    {
      "text": "but, you know, accelerator chips,",
      "start": 1821.09,
      "duration": 1.773
    },
    {
      "text": "often training for months.",
      "start": 1823.85,
      "duration": 1.863
    },
    {
      "text": "There's then a kind of post-training phase",
      "start": 1826.64,
      "duration": 2.79
    },
    {
      "text": "where we do reinforcement\nlearning from human feedback,",
      "start": 1829.43,
      "duration": 3.24
    },
    {
      "text": "as well as other kinds of\nreinforcement learning.",
      "start": 1832.67,
      "duration": 2.88
    },
    {
      "text": "That phase is getting\nlarger and larger now,",
      "start": 1835.55,
      "duration": 3.84
    },
    {
      "text": "and, you know, often, that's\nless of an exact science.",
      "start": 1839.39,
      "duration": 3.66
    },
    {
      "text": "It often takes effort to get it right.",
      "start": 1843.05,
      "duration": 1.9
    },
    {
      "text": "Models are then tested with\nsome of our early partners",
      "start": 1845.84,
      "duration": 3.42
    },
    {
      "text": "to see how good they are,",
      "start": 1849.26,
      "duration": 1.65
    },
    {
      "text": "and they're then tested both internally",
      "start": 1850.91,
      "duration": 2.04
    },
    {
      "text": "and externally for their safety,",
      "start": 1852.95,
      "duration": 2.52
    },
    {
      "text": "particularly for catastrophic\nand autonomy risks.",
      "start": 1855.47,
      "duration": 3.153
    },
    {
      "text": "So we do internal testing",
      "start": 1859.46,
      "duration": 1.86
    },
    {
      "text": "according to our\nresponsible scaling policy,",
      "start": 1861.32,
      "duration": 2.52
    },
    {
      "text": "which I, you know, could talk\nmore about that in detail.",
      "start": 1863.84,
      "duration": 2.73
    },
    {
      "text": "And then we have an agreement",
      "start": 1866.57,
      "duration": 1.26
    },
    {
      "text": "with the US and the UK\nAI Safety Institute,",
      "start": 1867.83,
      "duration": 3.15
    },
    {
      "text": "as well as other third party testers",
      "start": 1870.98,
      "duration": 2.28
    },
    {
      "text": "in specific domains to test the models",
      "start": 1873.26,
      "duration": 2.46
    },
    {
      "text": "for what are called CBRN risks,",
      "start": 1875.72,
      "duration": 2.25
    },
    {
      "text": "chemical, biological,\nradiological and nuclear,",
      "start": 1877.97,
      "duration": 3.03
    },
    {
      "text": "which are, you know, we\ndon't think that models",
      "start": 1881.0,
      "duration": 2.892
    },
    {
      "text": "pose these risks seriously yet,",
      "start": 1883.892,
      "duration": 2.238
    },
    {
      "text": "but every new model, we wanna evaluate",
      "start": 1886.13,
      "duration": 1.95
    },
    {
      "text": "to see if we're starting to get close",
      "start": 1888.08,
      "duration": 1.32
    },
    {
      "text": "to some of these more\ndangerous capabilities.",
      "start": 1889.4,
      "duration": 5.0
    },
    {
      "text": "So those are the phases.",
      "start": 1895.49,
      "duration": 1.71
    },
    {
      "text": "And then, you know, then\nit just takes some time",
      "start": 1897.2,
      "duration": 3.12
    },
    {
      "text": "to get the model working\nin terms of inference",
      "start": 1900.32,
      "duration": 2.34
    },
    {
      "text": "and launching it in the API.",
      "start": 1902.66,
      "duration": 1.64
    },
    {
      "text": "So there's just a lot of steps",
      "start": 1904.3,
      "duration": 2.06
    },
    {
      "text": "to actually making a model work.",
      "start": 1907.94,
      "duration": 1.44
    },
    {
      "text": "And of course, you know,\nwe're always trying",
      "start": 1909.38,
      "duration": 1.68
    },
    {
      "text": "to make the processes as\nstreamlined as possible, right?",
      "start": 1911.06,
      "duration": 3.96
    },
    {
      "text": "We want our safety testing to be rigorous,",
      "start": 1915.02,
      "duration": 1.98
    },
    {
      "text": "but we want it to be rigorous",
      "start": 1917.0,
      "duration": 1.38
    },
    {
      "text": "and to be, you know, to be automatic,",
      "start": 1918.38,
      "duration": 2.46
    },
    {
      "text": "to happen as fast as it can\nwithout compromising on rigor.",
      "start": 1920.84,
      "duration": 3.51
    },
    {
      "text": "Same with our pre-training process",
      "start": 1924.35,
      "duration": 2.1
    },
    {
      "text": "and our post-training process.",
      "start": 1926.45,
      "duration": 1.32
    },
    {
      "text": "So, you know, it's just\nlike building anything else.",
      "start": 1927.77,
      "duration": 2.16
    },
    {
      "text": "It's just like building airplanes.",
      "start": 1929.93,
      "duration": 1.32
    },
    {
      "text": "You want to make them, you know,",
      "start": 1931.25,
      "duration": 1.41
    },
    {
      "text": "you want to make them safe,",
      "start": 1932.66,
      "duration": 1.02
    },
    {
      "text": "but you want to make\nthe process streamlined.",
      "start": 1933.68,
      "duration": 2.04
    },
    {
      "text": "And I think the creative\ntension between those is,",
      "start": 1935.72,
      "duration": 2.695
    },
    {
      "text": "you know, is an important thing\nin making the models work.",
      "start": 1938.415,
      "duration": 2.195
    },
    {
      "text": "- Yeah, rumor on the street,\nI forget who was saying",
      "start": 1940.61,
      "duration": 2.31
    },
    {
      "text": "that Anthropic has really good tooling,",
      "start": 1942.92,
      "duration": 2.28
    },
    {
      "text": "so probably a lot of the challenge here",
      "start": 1945.2,
      "duration": 4.11
    },
    {
      "text": "on the software engineering side",
      "start": 1949.31,
      "duration": 1.2
    },
    {
      "text": "is to build the tooling\nto have like a efficient,",
      "start": 1950.51,
      "duration": 3.96
    },
    {
      "text": "low friction interaction\nwith the infrastructure.",
      "start": 1954.47,
      "duration": 1.95
    },
    {
      "text": "- You would be surprised how\nmuch of the challenges of,",
      "start": 1956.42,
      "duration": 3.873
    },
    {
      "text": "you know, building these\nmodels comes down to, you know,",
      "start": 1961.224,
      "duration": 3.746
    },
    {
      "text": "software engineering, performance\nengineering, you know.",
      "start": 1964.97,
      "duration": 4.23
    },
    {
      "text": "From the outside you might think,",
      "start": 1969.2,
      "duration": 1.44
    },
    {
      "text": "oh, man, we had this\neureka breakthrough, right?",
      "start": 1970.64,
      "duration": 2.31
    },
    {
      "text": "You know, this movie with the science,",
      "start": 1972.95,
      "duration": 1.26
    },
    {
      "text": "we discovered it, we figured it out.",
      "start": 1974.21,
      "duration": 1.8
    },
    {
      "text": "But I think all things,",
      "start": 1976.906,
      "duration": 1.897
    },
    {
      "text": "even, you know, incredible discoveries,",
      "start": 1980.06,
      "duration": 1.95
    },
    {
      "text": "like, they almost always\ncome down to the details,",
      "start": 1982.01,
      "duration": 4.2
    },
    {
      "text": "and often super, super boring details.",
      "start": 1986.21,
      "duration": 2.91
    },
    {
      "text": "I can't speak to whether we have",
      "start": 1989.12,
      "duration": 1.41
    },
    {
      "text": "better tooling than other companies.",
      "start": 1990.53,
      "duration": 1.62
    },
    {
      "text": "I mean, you know, haven't\nbeen at those other companies,",
      "start": 1992.15,
      "duration": 1.98
    },
    {
      "text": "at least not recently,",
      "start": 1994.13,
      "duration": 1.98
    },
    {
      "text": "but it's certainly something\nwe give a lot of attention to.",
      "start": 1996.11,
      "duration": 1.98
    },
    {
      "text": "- I don't know if you\ncan say, but from three,",
      "start": 1998.09,
      "duration": 3.51
    },
    {
      "text": "from Claude 3 to Claude 3.5,",
      "start": 2001.6,
      "duration": 2.37
    },
    {
      "text": "is there any extra pre-training going on",
      "start": 2003.97,
      "duration": 2.06
    },
    {
      "text": "or is it mostly focused\non the post-training?",
      "start": 2006.03,
      "duration": 2.08
    },
    {
      "text": "There's been leaps in performance.",
      "start": 2008.11,
      "duration": 1.62
    },
    {
      "text": "- Yeah, I think at any given stage,",
      "start": 2009.73,
      "duration": 2.43
    },
    {
      "text": "we're focused on improving\neverything at once.",
      "start": 2012.16,
      "duration": 2.313
    },
    {
      "text": "Just naturally, like\nthere are different teams,",
      "start": 2015.7,
      "duration": 2.16
    },
    {
      "text": "each team makes progress\nin a particular area,",
      "start": 2017.86,
      "duration": 2.94
    },
    {
      "text": "in making a particular, you know,",
      "start": 2020.8,
      "duration": 2.61
    },
    {
      "text": "their particular segment\nof the relay race better.",
      "start": 2023.41,
      "duration": 2.67
    },
    {
      "text": "And it's just natural that\nwhen we make a new model,",
      "start": 2026.08,
      "duration": 2.31
    },
    {
      "text": "we put all of these things in at once.",
      "start": 2028.39,
      "duration": 1.95
    },
    {
      "text": "- So, the data you have,\nlike the preference data",
      "start": 2030.34,
      "duration": 2.34
    },
    {
      "text": "you get from RLHF, is that applicable,",
      "start": 2032.68,
      "duration": 3.69
    },
    {
      "text": "is there a ways to apply it",
      "start": 2036.37,
      "duration": 2.19
    },
    {
      "text": "to newer models as it get trained up?",
      "start": 2038.56,
      "duration": 2.13
    },
    {
      "text": "- Yeah, preference data from old models",
      "start": 2040.69,
      "duration": 1.89
    },
    {
      "text": "sometimes gets used for new models,",
      "start": 2042.58,
      "duration": 1.68
    },
    {
      "text": "although, of course, it\nperforms somewhat better",
      "start": 2044.26,
      "duration": 2.7
    },
    {
      "text": "when it's, you know, trained on,",
      "start": 2046.96,
      "duration": 1.35
    },
    {
      "text": "it's trained on the new models.",
      "start": 2048.31,
      "duration": 1.62
    },
    {
      "text": "Note that we have this, you know,",
      "start": 2049.93,
      "duration": 1.29
    },
    {
      "text": "Constitutional AI method",
      "start": 2051.22,
      "duration": 1.32
    },
    {
      "text": "such that we don't only\nuse preference data,",
      "start": 2052.54,
      "duration": 1.89
    },
    {
      "text": "we kind of, there's also\na post-training process",
      "start": 2054.43,
      "duration": 2.49
    },
    {
      "text": "where we train the model against itself",
      "start": 2056.92,
      "duration": 2.1
    },
    {
      "text": "and there's, you know, new types",
      "start": 2059.02,
      "duration": 1.71
    },
    {
      "text": "of post-training the model against itself",
      "start": 2060.73,
      "duration": 1.62
    },
    {
      "text": "that are used every day.",
      "start": 2062.35,
      "duration": 0.96
    },
    {
      "text": "So it's not just RLHF,",
      "start": 2063.31,
      "duration": 1.653
    },
    {
      "text": "it's a bunch of other methods as well.",
      "start": 2064.963,
      "duration": 2.337
    },
    {
      "text": "Post-training, I think, you know,",
      "start": 2067.3,
      "duration": 1.17
    },
    {
      "text": "is becoming more and more sophisticated.",
      "start": 2068.47,
      "duration": 2.07
    },
    {
      "text": "- Well, what explains the\nbig leap in performance",
      "start": 2070.54,
      "duration": 2.7
    },
    {
      "text": "for the new Sonnet 3.5?",
      "start": 2073.24,
      "duration": 2.1
    },
    {
      "text": "I mean, at least in the programming side.",
      "start": 2075.34,
      "duration": 1.77
    },
    {
      "text": "And maybe this is a good place\nto talk about benchmarks.",
      "start": 2077.11,
      "duration": 2.1
    },
    {
      "text": "What does it mean to get better?",
      "start": 2079.21,
      "duration": 1.62
    },
    {
      "text": "Just the number went up,\nbut, you know, I program,",
      "start": 2080.83,
      "duration": 4.62
    },
    {
      "text": "but I also love programming",
      "start": 2085.45,
      "duration": 1.23
    },
    {
      "text": "and Claude 3.5 through Cursor",
      "start": 2086.68,
      "duration": 3.75
    },
    {
      "text": "is what I use to assist me in programming.",
      "start": 2090.43,
      "duration": 2.67
    },
    {
      "text": "And there was, at least\nexperientially, anecdotally,",
      "start": 2093.1,
      "duration": 4.26
    },
    {
      "text": "it's gotten smarter at programming.",
      "start": 2097.36,
      "duration": 3.03
    },
    {
      "text": "So like, what does it\ntake to get it smarter?",
      "start": 2100.39,
      "duration": 3.39
    },
    {
      "text": "- We observed that as well, by the way.",
      "start": 2103.78,
      "duration": 1.86
    },
    {
      "text": "There were a couple very strong engineers",
      "start": 2105.64,
      "duration": 2.4
    },
    {
      "text": "here at Anthropic who\nall previous code models,",
      "start": 2108.04,
      "duration": 3.72
    },
    {
      "text": "both produced by us and produced\nby all the other companies,",
      "start": 2111.76,
      "duration": 2.82
    },
    {
      "text": "hadn't really been useful to them.",
      "start": 2114.58,
      "duration": 3.06
    },
    {
      "text": "You know, they said, you know,",
      "start": 2117.64,
      "duration": 1.29
    },
    {
      "text": "maybe this is useful to\nbeginner, it's not useful to me.",
      "start": 2118.93,
      "duration": 2.97
    },
    {
      "text": "But Sonnet 3.5, the original one",
      "start": 2121.9,
      "duration": 2.58
    },
    {
      "text": "for the first time they said,",
      "start": 2124.48,
      "duration": 0.877
    },
    {
      "text": "\"Oh my God, this helped me",
      "start": 2125.357,
      "duration": 1.013
    },
    {
      "text": "with something that, you know,",
      "start": 2126.37,
      "duration": 1.47
    },
    {
      "text": "that it would've taken me hours to do.",
      "start": 2127.84,
      "duration": 1.073
    },
    {
      "text": "This is the first model that's\nactually saved me time.\"",
      "start": 2128.913,
      "duration": 2.497
    },
    {
      "text": "So again, the waterline is rising.",
      "start": 2131.41,
      "duration": 2.46
    },
    {
      "text": "And then I think, you know,",
      "start": 2133.87,
      "duration": 0.987
    },
    {
      "text": "the new Sonnet has been even better.",
      "start": 2134.857,
      "duration": 2.463
    },
    {
      "text": "In terms of what it takes,",
      "start": 2137.32,
      "duration": 1.44
    },
    {
      "text": "I mean, I'll just say it's\nbeen across the board.",
      "start": 2138.76,
      "duration": 2.76
    },
    {
      "text": "It's in the pre-training,\nit's in the post-training,",
      "start": 2141.52,
      "duration": 2.76
    },
    {
      "text": "it's in various evaluations that we do.",
      "start": 2144.28,
      "duration": 3.09
    },
    {
      "text": "We've observed this as well.",
      "start": 2147.37,
      "duration": 1.65
    },
    {
      "text": "And if we go into the\ndetails of the benchmark,",
      "start": 2149.02,
      "duration": 2.25
    },
    {
      "text": "so Sowe bench is\nbasically since, you know,",
      "start": 2151.27,
      "duration": 4.5
    },
    {
      "text": "since you're a programmer, you know,",
      "start": 2155.77,
      "duration": 1.32
    },
    {
      "text": "you'll be familiar with like pull requests",
      "start": 2157.09,
      "duration": 2.31
    },
    {
      "text": "and, you know, just pull\nrequests are like the, you know,",
      "start": 2159.4,
      "duration": 3.69
    },
    {
      "text": "like a sort of atomic unit of work.",
      "start": 2163.09,
      "duration": 3.54
    },
    {
      "text": "You know, you could say, you know,",
      "start": 2166.63,
      "duration": 1.145
    },
    {
      "text": "I'm implementing one thing.",
      "start": 2167.775,
      "duration": 2.848
    },
    {
      "text": "And Sowe bench actually gives you",
      "start": 2172.27,
      "duration": 2.243
    },
    {
      "text": "kind of a real world situation",
      "start": 2174.513,
      "duration": 2.047
    },
    {
      "text": "where the code base is in a current state",
      "start": 2176.56,
      "duration": 1.8
    },
    {
      "text": "and I'm trying to implement\nsomething that's, you know,",
      "start": 2178.36,
      "duration": 2.34
    },
    {
      "text": "that's described in language.",
      "start": 2180.7,
      "duration": 2.43
    },
    {
      "text": "We have internal benchmarks",
      "start": 2183.13,
      "duration": 1.23
    },
    {
      "text": "where we measure the same thing",
      "start": 2184.36,
      "duration": 1.53
    },
    {
      "text": "and you say, just give the\nmodel free reign to like,",
      "start": 2185.89,
      "duration": 2.497
    },
    {
      "text": "you know, do anything, run\nanything, edit anything.",
      "start": 2188.387,
      "duration": 4.913
    },
    {
      "text": "How well is it able to\ncomplete these tasks?",
      "start": 2193.3,
      "duration": 3.27
    },
    {
      "text": "And it's that benchmark that's gone",
      "start": 2196.57,
      "duration": 2.16
    },
    {
      "text": "from it can do it 3% of the time",
      "start": 2198.73,
      "duration": 1.65
    },
    {
      "text": "to it can do it about 50% of the time.",
      "start": 2200.38,
      "duration": 2.61
    },
    {
      "text": "So I actually do believe that if we get,",
      "start": 2202.99,
      "duration": 2.4
    },
    {
      "text": "you can gain benchmarks,",
      "start": 2205.39,
      "duration": 1.11
    },
    {
      "text": "but I think if we get to\n100% on that benchmark",
      "start": 2206.5,
      "duration": 2.58
    },
    {
      "text": "in a way that isn't\nkind of like overtrained",
      "start": 2209.08,
      "duration": 2.88
    },
    {
      "text": "or game for that particular benchmark,",
      "start": 2211.96,
      "duration": 2.43
    },
    {
      "text": "probably represents a\nreal and serious increase",
      "start": 2214.39,
      "duration": 2.92
    },
    {
      "text": "in kind of programming ability.",
      "start": 2218.55,
      "duration": 2.23
    },
    {
      "text": "And I would suspect that\nif we can get to, you know,",
      "start": 2220.78,
      "duration": 3.486
    },
    {
      "text": "90, 95% that, you know,\nit will represent ability",
      "start": 2224.266,
      "duration": 4.974
    },
    {
      "text": "to autonomously do a significant fraction",
      "start": 2229.24,
      "duration": 2.25
    },
    {
      "text": "of software engineering tasks.",
      "start": 2231.49,
      "duration": 1.5
    },
    {
      "text": "- Well, ridiculous timeline question.",
      "start": 2233.83,
      "duration": 2.16
    },
    {
      "text": "When is Claude Opus 3.5 coming out?",
      "start": 2235.99,
      "duration": 3.75
    },
    {
      "text": "- Not giving you an exact date,",
      "start": 2239.74,
      "duration": 1.92
    },
    {
      "text": "but you know, there, you\nknow, as far as we know,",
      "start": 2241.66,
      "duration": 3.42
    },
    {
      "text": "the plan is still to\nhave a Claude 3.5 Opus.",
      "start": 2245.08,
      "duration": 2.37
    },
    {
      "text": "- Are we gonna get it\nbefore \"GTA 6\" or no?",
      "start": 2247.45,
      "duration": 3.09
    },
    {
      "text": "- Like \"Duke Nukem Forever.\"",
      "start": 2250.54,
      "duration": 1.207
    },
    {
      "text": "- \"Duke Nukem-\"\n- What was that game?",
      "start": 2251.747,
      "duration": 1.313
    },
    {
      "text": "There was some game that\nwas delayed 15 years.",
      "start": 2253.06,
      "duration": 1.435
    },
    {
      "text": "- That's right.\n- Was that",
      "start": 2254.495,
      "duration": 0.833
    },
    {
      "text": "\"Duke Nukem Forever?\"\n- Yeah.",
      "start": 2255.328,
      "duration": 1.122
    },
    {
      "text": "And I think \"GTA\" is now\njust releasing trailers.",
      "start": 2256.45,
      "duration": 2.7
    },
    {
      "text": "- You know, it's only been three months",
      "start": 2259.15,
      "duration": 1.44
    },
    {
      "text": "since we released the first Sonnet.",
      "start": 2260.59,
      "duration": 2.01
    },
    {
      "text": "- Yeah, it's the\nincredible pace of release.",
      "start": 2262.6,
      "duration": 2.289
    },
    {
      "text": "- It just tells you about the pace,",
      "start": 2264.889,
      "duration": 1.821
    },
    {
      "text": "the expectations for when\nthings are gonna come out.",
      "start": 2266.71,
      "duration": 2.4
    },
    {
      "text": "- So what about 4.0?",
      "start": 2269.11,
      "duration": 3.3
    },
    {
      "text": "So how do you think about",
      "start": 2272.41,
      "duration": 1.68
    },
    {
      "text": "sort of as these models\nget bigger and bigger,",
      "start": 2274.09,
      "duration": 1.68
    },
    {
      "text": "about versioning, and also\njust versioning in general,",
      "start": 2275.77,
      "duration": 3.0
    },
    {
      "text": "why Sonnet 3.5 updated with the date?",
      "start": 2278.77,
      "duration": 4.44
    },
    {
      "text": "Why not Sonnet 3.6, which a\nlot of people are calling it?",
      "start": 2283.21,
      "duration": 2.077
    },
    {
      "text": "- Yeah, naming is actually",
      "start": 2285.287,
      "duration": 2.273
    },
    {
      "text": "an interesting challenge here, right?",
      "start": 2287.56,
      "duration": 1.65
    },
    {
      "text": "Because I think a year ago,",
      "start": 2289.21,
      "duration": 1.68
    },
    {
      "text": "most of the model was pre-training,",
      "start": 2290.89,
      "duration": 1.847
    },
    {
      "text": "and so you could start from the beginning",
      "start": 2292.737,
      "duration": 2.083
    },
    {
      "text": "and just say, okay,\nwe're gonna have models",
      "start": 2294.82,
      "duration": 1.31
    },
    {
      "text": "of different sizes, we're\ngonna train them all together",
      "start": 2296.13,
      "duration": 2.23
    },
    {
      "text": "and you know, we'll have\na family of naming schemes",
      "start": 2298.36,
      "duration": 2.7
    },
    {
      "text": "and then we'll put some\nnew magic into them",
      "start": 2301.06,
      "duration": 2.343
    },
    {
      "text": "and then, you know, we'll\nhave the next generation.",
      "start": 2303.403,
      "duration": 3.297
    },
    {
      "text": "The trouble starts already",
      "start": 2306.7,
      "duration": 1.05
    },
    {
      "text": "when some of them take a lot longer",
      "start": 2307.75,
      "duration": 1.59
    },
    {
      "text": "than others to train, right?",
      "start": 2309.34,
      "duration": 1.08
    },
    {
      "text": "That already messes up\nyour time a little bit.",
      "start": 2310.42,
      "duration": 2.91
    },
    {
      "text": "But as you make big\nimprovements in pre-training,",
      "start": 2313.33,
      "duration": 5.0
    },
    {
      "text": "then you suddenly notice,",
      "start": 2318.7,
      "duration": 1.5
    },
    {
      "text": "oh, I can make better pre-train model",
      "start": 2320.2,
      "duration": 2.037
    },
    {
      "text": "and that doesn't take very long to do,",
      "start": 2322.237,
      "duration": 2.82
    },
    {
      "text": "but you know, clearly it has the same,",
      "start": 2325.057,
      "duration": 1.983
    },
    {
      "text": "you know, size and shape\nof previous models.",
      "start": 2327.04,
      "duration": 2.2
    },
    {
      "text": "So I think those two together",
      "start": 2330.28,
      "duration": 1.65
    },
    {
      "text": "as well as the timing issues,",
      "start": 2331.93,
      "duration": 1.92
    },
    {
      "text": "any kind of scheme you come up with,",
      "start": 2333.85,
      "duration": 2.94
    },
    {
      "text": "you know, the reality tends",
      "start": 2336.79,
      "duration": 2.61
    },
    {
      "text": "to kind of frustrate that scheme, right?",
      "start": 2339.4,
      "duration": 1.86
    },
    {
      "text": "Tend tends to kind of\nbreak out of the scheme.",
      "start": 2341.26,
      "duration": 3.03
    },
    {
      "text": "It's not like software where you can say,",
      "start": 2344.29,
      "duration": 1.47
    },
    {
      "text": "oh, this is like, you\nknow, 3.7, this is 3.8.",
      "start": 2345.76,
      "duration": 3.72
    },
    {
      "text": "No, you have models with\ndifferent trade-offs.",
      "start": 2349.48,
      "duration": 2.94
    },
    {
      "text": "You can change some things in your models,",
      "start": 2352.42,
      "duration": 1.74
    },
    {
      "text": "you can train, you can\nchange other things.",
      "start": 2354.16,
      "duration": 2.28
    },
    {
      "text": "Some are faster and slower at inference,",
      "start": 2356.44,
      "duration": 1.86
    },
    {
      "text": "some have to be more expensive,",
      "start": 2358.3,
      "duration": 1.32
    },
    {
      "text": "some have to be less expensive.",
      "start": 2359.62,
      "duration": 1.8
    },
    {
      "text": "And so I think all the companies\nhave struggled with this.",
      "start": 2361.42,
      "duration": 3.39
    },
    {
      "text": "I think we did very, you know,",
      "start": 2364.81,
      "duration": 2.023
    },
    {
      "text": "I think we were in a good position",
      "start": 2366.833,
      "duration": 2.087
    },
    {
      "text": "in terms of naming when we\nhad Haiku, Sonnet and Opus.",
      "start": 2368.92,
      "duration": 3.0
    },
    {
      "text": "- [Lex] It was great, great start.",
      "start": 2371.92,
      "duration": 0.9
    },
    {
      "text": "- We're trying to maintain it,",
      "start": 2372.82,
      "duration": 1.26
    },
    {
      "text": "but it's not perfect,",
      "start": 2374.08,
      "duration": 2.22
    },
    {
      "text": "so we'll try and get\nback to the simplicity,",
      "start": 2376.3,
      "duration": 2.7
    },
    {
      "text": "but just the nature of the field,",
      "start": 2379.0,
      "duration": 4.08
    },
    {
      "text": "I feel like no one's figured out naming.",
      "start": 2383.08,
      "duration": 1.86
    },
    {
      "text": "It's somehow a different paradigm",
      "start": 2384.94,
      "duration": 1.56
    },
    {
      "text": "from like normal software and so we just,",
      "start": 2386.5,
      "duration": 4.47
    },
    {
      "text": "none of the companies\nhave been perfect at it.",
      "start": 2390.97,
      "duration": 2.19
    },
    {
      "text": "It's something we struggle with",
      "start": 2393.16,
      "duration": 1.32
    },
    {
      "text": "surprisingly much relative to,",
      "start": 2394.48,
      "duration": 1.747
    },
    {
      "text": "you know, how relative\nto how trivial it is to,",
      "start": 2396.227,
      "duration": 2.903
    },
    {
      "text": "you know, for the grand\nscience of training the models.",
      "start": 2399.13,
      "duration": 3.96
    },
    {
      "text": "- So, from the user\nside, the user experience",
      "start": 2403.09,
      "duration": 3.12
    },
    {
      "text": "of the updated Sonnet 3.5",
      "start": 2406.21,
      "duration": 2.25
    },
    {
      "text": "is just different than the previous",
      "start": 2408.46,
      "duration": 3.27
    },
    {
      "text": "June 2024 Sonnet 3.5.",
      "start": 2411.73,
      "duration": 2.25
    },
    {
      "text": "It would be nice to come up",
      "start": 2413.98,
      "duration": 0.99
    },
    {
      "text": "with some kind of labeling\nthat embodies that",
      "start": 2414.97,
      "duration": 2.79
    },
    {
      "text": "because people talk about Sonnet 3.5,",
      "start": 2417.76,
      "duration": 2.46
    },
    {
      "text": "but now there's a different one,",
      "start": 2420.22,
      "duration": 1.857
    },
    {
      "text": "and so how do you refer to the\nprevious one and the new one",
      "start": 2422.077,
      "duration": 2.95
    },
    {
      "text": "when there's a distinct improvement?",
      "start": 2427.06,
      "duration": 2.49
    },
    {
      "text": "It just makes conversation\nabout it just challenging.",
      "start": 2429.55,
      "duration": 4.983
    },
    {
      "text": "- Yeah, yeah.",
      "start": 2434.533,
      "duration": 0.833
    },
    {
      "text": "I definitely think this question",
      "start": 2435.366,
      "duration": 1.684
    },
    {
      "text": "of there are lots of\nproperties of the models",
      "start": 2437.05,
      "duration": 3.33
    },
    {
      "text": "that are not reflected in the benchmarks.",
      "start": 2440.38,
      "duration": 2.73
    },
    {
      "text": "I think that's definitely\nthe case and everyone agrees.",
      "start": 2443.11,
      "duration": 3.63
    },
    {
      "text": "And not all of them are capabilities.",
      "start": 2446.74,
      "duration": 1.86
    },
    {
      "text": "Some of them are, you know,\nmodels can be polite or brusque.",
      "start": 2448.6,
      "duration": 5.0
    },
    {
      "text": "They can be, you know, very reactive",
      "start": 2454.21,
      "duration": 4.56
    },
    {
      "text": "or they can ask you questions.",
      "start": 2458.77,
      "duration": 2.43
    },
    {
      "text": "They can have what feels\nlike a warm personality",
      "start": 2461.2,
      "duration": 2.73
    },
    {
      "text": "or a cold personality.",
      "start": 2463.93,
      "duration": 1.62
    },
    {
      "text": "They can be boring or they\ncan be very distinctive,",
      "start": 2465.55,
      "duration": 2.31
    },
    {
      "text": "like Golden Gate Claude was.",
      "start": 2467.86,
      "duration": 1.683
    },
    {
      "text": "And we have a whole, you know,",
      "start": 2470.56,
      "duration": 1.13
    },
    {
      "text": "we have a whole team kind of focused on,",
      "start": 2471.69,
      "duration": 2.29
    },
    {
      "text": "I think we call it Claude character.",
      "start": 2473.98,
      "duration": 1.83
    },
    {
      "text": "Amanda leads that team",
      "start": 2475.81,
      "duration": 1.77
    },
    {
      "text": "and we'll talk to you about that.",
      "start": 2477.58,
      "duration": 1.8
    },
    {
      "text": "But it's still a very inexact science,",
      "start": 2479.38,
      "duration": 2.94
    },
    {
      "text": "and often we find that\nmodels have properties",
      "start": 2482.32,
      "duration": 3.39
    },
    {
      "text": "that we're not aware of.",
      "start": 2485.71,
      "duration": 1.5
    },
    {
      "text": "The fact of the matter is that you can,",
      "start": 2487.21,
      "duration": 2.689
    },
    {
      "text": "you know, talk to a model 10,000 times",
      "start": 2489.899,
      "duration": 1.961
    },
    {
      "text": "and there are some\nbehaviors you might not see,",
      "start": 2491.86,
      "duration": 2.35
    },
    {
      "text": "just like with a human, right?",
      "start": 2495.55,
      "duration": 1.17
    },
    {
      "text": "I can know someone for a few months and,",
      "start": 2496.72,
      "duration": 1.237
    },
    {
      "text": "you know, not know that\nthey have a certain skill,",
      "start": 2497.957,
      "duration": 2.813
    },
    {
      "text": "or not know that there's\na certain side to them.",
      "start": 2500.77,
      "duration": 2.31
    },
    {
      "text": "And so I think we just have to get used",
      "start": 2503.08,
      "duration": 1.83
    },
    {
      "text": "to this idea and we're always\nlooking for better ways",
      "start": 2504.91,
      "duration": 3.03
    },
    {
      "text": "of testing our models to\ndemonstrate these capabilities,",
      "start": 2507.94,
      "duration": 3.48
    },
    {
      "text": "and also to decide which are\nthe personality properties",
      "start": 2511.42,
      "duration": 4.89
    },
    {
      "text": "we want models to have and\nwhich we don't want to have.",
      "start": 2516.31,
      "duration": 2.52
    },
    {
      "text": "That itself, the normative question",
      "start": 2518.83,
      "duration": 1.65
    },
    {
      "text": "is also super interesting.",
      "start": 2520.48,
      "duration": 1.8
    },
    {
      "text": "- I gotta ask you a question from Reddit.",
      "start": 2522.28,
      "duration": 1.86
    },
    {
      "text": "- From Reddit? Oh, boy. (laughs)",
      "start": 2524.14,
      "duration": 2.97
    },
    {
      "text": "- You know, there just this fascinating,",
      "start": 2527.11,
      "duration": 1.71
    },
    {
      "text": "to me at least, it's a\npsychological social phenomenon",
      "start": 2528.82,
      "duration": 3.72
    },
    {
      "text": "where people report that Claude",
      "start": 2532.54,
      "duration": 2.49
    },
    {
      "text": "has gotten dumber for them over time.",
      "start": 2535.03,
      "duration": 2.43
    },
    {
      "text": "And so the question is,",
      "start": 2537.46,
      "duration": 2.16
    },
    {
      "text": "does the user complaint\nabout the dumbing down",
      "start": 2539.62,
      "duration": 2.1
    },
    {
      "text": "of Claude 3.5 Sonnet hold any water?",
      "start": 2541.72,
      "duration": 2.61
    },
    {
      "text": "So are these anecdotal reports\na kind of social phenomena",
      "start": 2544.33,
      "duration": 4.8
    },
    {
      "text": "or did Claude, is there any cases",
      "start": 2549.13,
      "duration": 2.91
    },
    {
      "text": "where Claude would get dumber?",
      "start": 2552.04,
      "duration": 1.59
    },
    {
      "text": "- So this actually doesn't apply,",
      "start": 2553.63,
      "duration": 2.61
    },
    {
      "text": "this isn't just about Claude.",
      "start": 2556.24,
      "duration": 1.45
    },
    {
      "text": "I believe I've seen these complaints",
      "start": 2559.0,
      "duration": 2.28
    },
    {
      "text": "for every foundation model\nproduced by a major company.",
      "start": 2561.28,
      "duration": 3.72
    },
    {
      "text": "People said this about GPT-4,",
      "start": 2565.0,
      "duration": 1.65
    },
    {
      "text": "they said it about GPT-4 Turbo.",
      "start": 2566.65,
      "duration": 2.073
    },
    {
      "text": "So, a couple things.",
      "start": 2570.847,
      "duration": 1.173
    },
    {
      "text": "One, the actual weights\nof the model, right,",
      "start": 2572.02,
      "duration": 2.64
    },
    {
      "text": "the actual brain of the\nmodel, that does not change",
      "start": 2574.66,
      "duration": 3.27
    },
    {
      "text": "unless we introduce a new model.",
      "start": 2577.93,
      "duration": 2.82
    },
    {
      "text": "There are just a number of reasons",
      "start": 2580.75,
      "duration": 1.71
    },
    {
      "text": "why it would not make sense practically",
      "start": 2582.46,
      "duration": 1.92
    },
    {
      "text": "to be randomly substituting in",
      "start": 2584.38,
      "duration": 1.93
    },
    {
      "text": "new versions of the model.",
      "start": 2587.77,
      "duration": 1.5
    },
    {
      "text": "It's difficult from an\ninference perspective",
      "start": 2589.27,
      "duration": 2.01
    },
    {
      "text": "and it's actually hard to\ncontrol all the consequences",
      "start": 2591.28,
      "duration": 3.51
    },
    {
      "text": "of changing the weight of the model.",
      "start": 2594.79,
      "duration": 1.44
    },
    {
      "text": "Let's say you wanted to fine\ntune the model to be like,",
      "start": 2596.23,
      "duration": 2.7
    },
    {
      "text": "I don't know, to like\nto say \"certainly\" less,",
      "start": 2598.93,
      "duration": 2.61
    },
    {
      "text": "which, you know, an old\nversion of Sonnet used to do.",
      "start": 2601.54,
      "duration": 3.0
    },
    {
      "text": "You actually end up\nchanging 100 things as well.",
      "start": 2604.54,
      "duration": 1.83
    },
    {
      "text": "So we have a whole process for it,",
      "start": 2606.37,
      "duration": 1.5
    },
    {
      "text": "and we have a whole process\nfor modifying the model.",
      "start": 2607.87,
      "duration": 2.73
    },
    {
      "text": "We do a bunch of testing on it,",
      "start": 2610.6,
      "duration": 1.41
    },
    {
      "text": "we do a bunch of user\ntesting and early customers.",
      "start": 2612.01,
      "duration": 4.35
    },
    {
      "text": "So we both have never changed",
      "start": 2616.36,
      "duration": 3.03
    },
    {
      "text": "the weights of the model\nwithout telling anyone,",
      "start": 2619.39,
      "duration": 2.49
    },
    {
      "text": "and it wouldn't, certainly\nin the current setup,",
      "start": 2621.88,
      "duration": 3.15
    },
    {
      "text": "it would not make sense to do that.",
      "start": 2625.03,
      "duration": 1.77
    },
    {
      "text": "Now, there are a couple things\nthat we do occasionally do.",
      "start": 2626.8,
      "duration": 3.51
    },
    {
      "text": "One is sometimes we run A/B tests,",
      "start": 2630.31,
      "duration": 3.09
    },
    {
      "text": "but those are typically\nvery close to when a model",
      "start": 2633.4,
      "duration": 3.29
    },
    {
      "text": "is being released and for a\nvery small fraction of time.",
      "start": 2636.69,
      "duration": 4.42
    },
    {
      "text": "So, you know, like, the day\nbefore the new Sonnet 3.5.",
      "start": 2641.11,
      "duration": 4.68
    },
    {
      "text": "I agree, we should have had a better name.",
      "start": 2645.79,
      "duration": 2.22
    },
    {
      "text": "It's clunky to refer to it.",
      "start": 2648.01,
      "duration": 1.593
    },
    {
      "text": "There were some comments from people that",
      "start": 2650.649,
      "duration": 1.291
    },
    {
      "text": "like it's gotten a lot better,",
      "start": 2651.94,
      "duration": 2.025
    },
    {
      "text": "and that's because, you know, a fraction",
      "start": 2653.965,
      "duration": 1.425
    },
    {
      "text": "were exposed to an A/B test",
      "start": 2655.39,
      "duration": 1.78
    },
    {
      "text": "for those one or two days.",
      "start": 2658.57,
      "duration": 1.473
    },
    {
      "text": "The other is that occasionally,",
      "start": 2660.91,
      "duration": 1.26
    },
    {
      "text": "the system prompt will change.",
      "start": 2662.17,
      "duration": 2.19
    },
    {
      "text": "The system prompt can have some effects,",
      "start": 2664.36,
      "duration": 1.86
    },
    {
      "text": "although it's unlikely\nto dumb down models.",
      "start": 2666.22,
      "duration": 3.57
    },
    {
      "text": "It's unlikely to make them dumber.",
      "start": 2669.79,
      "duration": 1.713
    },
    {
      "text": "And we've seen that\nwhile these two things,",
      "start": 2672.928,
      "duration": 2.322
    },
    {
      "text": "which I'm listing to be very complete,",
      "start": 2675.25,
      "duration": 1.983
    },
    {
      "text": "happened relatively,\nhappened quite infrequently,",
      "start": 2678.25,
      "duration": 3.87
    },
    {
      "text": "the complaints about,",
      "start": 2682.12,
      "duration": 2.01
    },
    {
      "text": "for us and for other model\ncompanies about the model change,",
      "start": 2684.13,
      "duration": 3.27
    },
    {
      "text": "the model isn't good at this.",
      "start": 2687.4,
      "duration": 1.14
    },
    {
      "text": "The model got more censored.",
      "start": 2688.54,
      "duration": 1.23
    },
    {
      "text": "The model was dumbed down.",
      "start": 2689.77,
      "duration": 1.38
    },
    {
      "text": "Those complaints are constant.",
      "start": 2691.15,
      "duration": 1.71
    },
    {
      "text": "And so I don't wanna say like people",
      "start": 2692.86,
      "duration": 1.8
    },
    {
      "text": "are imagining it or\nanything, but like the models",
      "start": 2694.66,
      "duration": 2.49
    },
    {
      "text": "are for the most part not changing.",
      "start": 2697.15,
      "duration": 3.24
    },
    {
      "text": "If I were to offer a theory,\nI think it actually relates",
      "start": 2700.39,
      "duration": 4.47
    },
    {
      "text": "to one of the things I said before,",
      "start": 2704.86,
      "duration": 1.86
    },
    {
      "text": "which is that models are very complex",
      "start": 2706.72,
      "duration": 4.2
    },
    {
      "text": "and have many aspects to them.",
      "start": 2710.92,
      "duration": 1.59
    },
    {
      "text": "And so often, you know,",
      "start": 2712.51,
      "duration": 2.54
    },
    {
      "text": "if I ask the model a question,\nyou know, if I'm like,",
      "start": 2715.05,
      "duration": 3.017
    },
    {
      "text": "\"Do task X\" versus \"Can you do task X?\"",
      "start": 2718.067,
      "duration": 3.653
    },
    {
      "text": "the model might respond in different ways.",
      "start": 2721.72,
      "duration": 2.343
    },
    {
      "text": "And so there are all\nkinds of subtle things",
      "start": 2724.93,
      "duration": 2.07
    },
    {
      "text": "that you can change about\nthe way you interact",
      "start": 2727.0,
      "duration": 2.64
    },
    {
      "text": "with the model that can give\nyou very different results.",
      "start": 2729.64,
      "duration": 3.54
    },
    {
      "text": "To be clear, this itself\nis like a failing by us",
      "start": 2733.18,
      "duration": 3.66
    },
    {
      "text": "and by the other model\nproviders that the models",
      "start": 2736.84,
      "duration": 2.64
    },
    {
      "text": "are just often sensitive to\nlike small changes in wording.",
      "start": 2739.48,
      "duration": 3.96
    },
    {
      "text": "It's yet another way in which the science",
      "start": 2743.44,
      "duration": 2.49
    },
    {
      "text": "of how these models work\nis very poorly developed.",
      "start": 2745.93,
      "duration": 3.27
    },
    {
      "text": "And so, you know, if I\ngo to sleep one night",
      "start": 2749.2,
      "duration": 1.77
    },
    {
      "text": "and I was like talking to\nthe model in a certain way",
      "start": 2750.97,
      "duration": 1.947
    },
    {
      "text": "and I like slightly changed the phrasing",
      "start": 2752.917,
      "duration": 2.238
    },
    {
      "text": "of how I talk to the model, you know,",
      "start": 2755.155,
      "duration": 2.295
    },
    {
      "text": "I could get different results.",
      "start": 2757.45,
      "duration": 1.67
    },
    {
      "text": "So that's one possible way.",
      "start": 2759.12,
      "duration": 1.87
    },
    {
      "text": "The other thing is, man,",
      "start": 2760.99,
      "duration": 1.02
    },
    {
      "text": "it's just hard to quantify this stuff.",
      "start": 2762.01,
      "duration": 2.04
    },
    {
      "text": "It's hard to quantify this stuff.",
      "start": 2764.05,
      "duration": 1.68
    },
    {
      "text": "I think people are very excited",
      "start": 2765.73,
      "duration": 1.32
    },
    {
      "text": "by new models when they come out",
      "start": 2767.05,
      "duration": 1.74
    },
    {
      "text": "and then as time goes on,",
      "start": 2768.79,
      "duration": 1.77
    },
    {
      "text": "they become very aware of the limitations,",
      "start": 2770.56,
      "duration": 3.27
    },
    {
      "text": "so that may be another effect.",
      "start": 2773.83,
      "duration": 1.29
    },
    {
      "text": "But that's all a very\nlong-winded way of saying",
      "start": 2775.12,
      "duration": 2.85
    },
    {
      "text": "for the most part, with some\nfairly narrow exceptions,",
      "start": 2777.97,
      "duration": 3.27
    },
    {
      "text": "the models are not changing.",
      "start": 2781.24,
      "duration": 1.621
    },
    {
      "text": "- I think there is a psychological effect.",
      "start": 2782.861,
      "duration": 1.799
    },
    {
      "text": "You just start getting used to it.",
      "start": 2784.66,
      "duration": 1.29
    },
    {
      "text": "The baseline raises.",
      "start": 2785.95,
      "duration": 0.87
    },
    {
      "text": "Like when people first\ngotten wifi on airplanes,",
      "start": 2786.82,
      "duration": 3.78
    },
    {
      "text": "it's like amazing, magic.\n- It's like amazing, yeah.",
      "start": 2790.6,
      "duration": 1.983
    },
    {
      "text": "- And then-\n- And now I'm like,",
      "start": 2792.583,
      "duration": 1.112
    },
    {
      "text": "I can't get this thing to work.",
      "start": 2793.695,
      "duration": 1.308
    },
    {
      "text": "This is such a piece of crap.",
      "start": 2795.003,
      "duration": 1.957
    },
    {
      "text": "- Exactly, so then it's easy\nto have the conspiracy theory",
      "start": 2796.96,
      "duration": 2.55
    },
    {
      "text": "of they're making wifi slower and slower.",
      "start": 2799.51,
      "duration": 1.83
    },
    {
      "text": "This is probably something I'll talk",
      "start": 2801.34,
      "duration": 2.28
    },
    {
      "text": "to Amanda much more about.",
      "start": 2803.62,
      "duration": 1.11
    },
    {
      "text": "But another Reddit question,",
      "start": 2804.73,
      "duration": 2.737
    },
    {
      "text": "\"When will Claude stop trying",
      "start": 2807.467,
      "duration": 1.613
    },
    {
      "text": "to be my puritanical grandmother",
      "start": 2809.08,
      "duration": 2.82
    },
    {
      "text": "imposing its moral worldview\non me as a paying customer?",
      "start": 2811.9,
      "duration": 3.72
    },
    {
      "text": "And also, what is the psychology",
      "start": 2815.62,
      "duration": 1.56
    },
    {
      "text": "behind making Claude overly apologetic?\"",
      "start": 2817.18,
      "duration": 2.82
    },
    {
      "text": "So this kind of reports\nabout the experience,",
      "start": 2820.0,
      "duration": 3.69
    },
    {
      "text": "a different angle on the frustration,",
      "start": 2823.69,
      "duration": 1.53
    },
    {
      "text": "it has to do with the character.",
      "start": 2825.22,
      "duration": 0.9
    },
    {
      "text": "- Yeah, so a couple points on this first.",
      "start": 2826.12,
      "duration": 2.16
    },
    {
      "text": "One is like things that people\nsay on Reddit and Twitter,",
      "start": 2828.28,
      "duration": 4.53
    },
    {
      "text": "or X or whatever it is,\nthere's actually a huge",
      "start": 2832.81,
      "duration": 3.21
    },
    {
      "text": "distribution shift between like the stuff",
      "start": 2836.02,
      "duration": 2.28
    },
    {
      "text": "that people complain loudly\nabout on social media",
      "start": 2838.3,
      "duration": 2.28
    },
    {
      "text": "and what actually kind of like, you know,",
      "start": 2840.58,
      "duration": 2.82
    },
    {
      "text": "statistically users care about",
      "start": 2843.4,
      "duration": 1.407
    },
    {
      "text": "and that drives people to use the models.",
      "start": 2844.807,
      "duration": 1.833
    },
    {
      "text": "Like people are frustrated\nwith, you know, things like,",
      "start": 2846.64,
      "duration": 3.16
    },
    {
      "text": "you know, the model not\nwriting out all the code",
      "start": 2849.8,
      "duration": 2.48
    },
    {
      "text": "or the model, you know, just not being",
      "start": 2852.28,
      "duration": 2.76
    },
    {
      "text": "as good at code as it could be,",
      "start": 2855.04,
      "duration": 1.62
    },
    {
      "text": "even though it's the best\nmodel in the world on code.",
      "start": 2856.66,
      "duration": 2.55
    },
    {
      "text": "I think the majority\nthings are about that.",
      "start": 2859.21,
      "duration": 2.913
    },
    {
      "text": "But certainly a kind\nof vocal minority are,",
      "start": 2863.11,
      "duration": 4.62
    },
    {
      "text": "you know, kind of raise\nthese concerns, right?",
      "start": 2867.73,
      "duration": 2.67
    },
    {
      "text": "Are frustrated by the\nmodel refusing things",
      "start": 2870.4,
      "duration": 2.163
    },
    {
      "text": "that it shouldn't refuse,",
      "start": 2872.563,
      "duration": 1.497
    },
    {
      "text": "or like apologizing too much,",
      "start": 2874.06,
      "duration": 1.65
    },
    {
      "text": "or just having these kind of\nlike annoying verbal ticks.",
      "start": 2875.71,
      "duration": 4.11
    },
    {
      "text": "The second caveat, and\nI just wanna say this",
      "start": 2879.82,
      "duration": 1.86
    },
    {
      "text": "like super clearly because I think",
      "start": 2881.68,
      "duration": 1.41
    },
    {
      "text": "it's like some people don't know it,",
      "start": 2883.09,
      "duration": 2.67
    },
    {
      "text": "others like kind of know it but forget it.",
      "start": 2885.76,
      "duration": 2.67
    },
    {
      "text": "Like it is very difficult to control",
      "start": 2888.43,
      "duration": 3.09
    },
    {
      "text": "across the board how the models behave.",
      "start": 2891.52,
      "duration": 2.04
    },
    {
      "text": "You cannot just reach in there",
      "start": 2893.56,
      "duration": 1.71
    },
    {
      "text": "and say, \"Oh, I want the\nmodel to like apologize less.\"",
      "start": 2895.27,
      "duration": 3.06
    },
    {
      "text": "Like you can do that, you\ncan include training data",
      "start": 2898.33,
      "duration": 2.37
    },
    {
      "text": "that says like, \"Oh, the model\nshould like apologize less,\"",
      "start": 2900.7,
      "duration": 3.33
    },
    {
      "text": "but then in some other\nsituation they end up",
      "start": 2904.03,
      "duration": 2.25
    },
    {
      "text": "being like super rude\nor like overconfident",
      "start": 2906.28,
      "duration": 2.52
    },
    {
      "text": "in a way that's like misleading people.",
      "start": 2908.8,
      "duration": 1.59
    },
    {
      "text": "So there are all these trade-offs.",
      "start": 2910.39,
      "duration": 2.193
    },
    {
      "text": "For example, another thing",
      "start": 2913.9,
      "duration": 1.14
    },
    {
      "text": "is there was a period during which models,",
      "start": 2915.04,
      "duration": 3.33
    },
    {
      "text": "ours and I think others as\nwell were too verbose, right?",
      "start": 2918.37,
      "duration": 3.45
    },
    {
      "text": "They would like repeat themselves,",
      "start": 2921.82,
      "duration": 1.32
    },
    {
      "text": "they would say too much.",
      "start": 2923.14,
      "duration": 1.2
    },
    {
      "text": "You can cut down on the\nverbosity by penalizing",
      "start": 2925.48,
      "duration": 2.43
    },
    {
      "text": "the models for just talking for too long.",
      "start": 2927.91,
      "duration": 2.67
    },
    {
      "text": "What happens when you do that,",
      "start": 2930.58,
      "duration": 1.74
    },
    {
      "text": "if you do it in a crude way\nis when the models are coding,",
      "start": 2932.32,
      "duration": 3.06
    },
    {
      "text": "sometimes they'll say rest\nof the code goes here, right?",
      "start": 2935.38,
      "duration": 3.12
    },
    {
      "text": "Because they've learned that\nthat's the way to economize",
      "start": 2938.5,
      "duration": 2.386
    },
    {
      "text": "and that they see it,",
      "start": 2940.886,
      "duration": 1.304
    },
    {
      "text": "and then so that leads the model",
      "start": 2942.19,
      "duration": 1.41
    },
    {
      "text": "to be so-called lazy in coding",
      "start": 2943.6,
      "duration": 2.278
    },
    {
      "text": "where they're just like,",
      "start": 2945.878,
      "duration": 1.052
    },
    {
      "text": "ah, you can finish the rest of it.",
      "start": 2946.93,
      "duration": 1.32
    },
    {
      "text": "It's not because we wanna,",
      "start": 2948.25,
      "duration": 1.92
    },
    {
      "text": "you know, save on compute",
      "start": 2950.17,
      "duration": 1.89
    },
    {
      "text": "or because you know, the models are lazy,",
      "start": 2952.06,
      "duration": 2.07
    },
    {
      "text": "and you know, during winter break,",
      "start": 2954.13,
      "duration": 2.34
    },
    {
      "text": "or any of the other kind\nof conspiracy theories",
      "start": 2956.47,
      "duration": 2.158
    },
    {
      "text": "that have come up.",
      "start": 2958.628,
      "duration": 1.292
    },
    {
      "text": "It's actually, it's just very hard",
      "start": 2959.92,
      "duration": 2.43
    },
    {
      "text": "to control the behavior of the model,",
      "start": 2962.35,
      "duration": 1.98
    },
    {
      "text": "to steer the behavior of the model",
      "start": 2964.33,
      "duration": 1.71
    },
    {
      "text": "in all circumstances at once.",
      "start": 2966.04,
      "duration": 2.01
    },
    {
      "text": "You can kind of, there's\nthis whack-a-mole aspect",
      "start": 2968.05,
      "duration": 3.0
    },
    {
      "text": "where you push on one\nthing and like, you know,",
      "start": 2971.05,
      "duration": 5.0
    },
    {
      "text": "these other things start to move as well",
      "start": 2976.42,
      "duration": 1.92
    },
    {
      "text": "that you may not even notice or measure.",
      "start": 2978.34,
      "duration": 2.31
    },
    {
      "text": "And so one of the reasons\nthat I care so much about,",
      "start": 2980.65,
      "duration": 3.783
    },
    {
      "text": "you know, kind of grand alignment",
      "start": 2985.69,
      "duration": 1.62
    },
    {
      "text": "of these AI systems in the\nfuture is actually these systems",
      "start": 2987.31,
      "duration": 3.09
    },
    {
      "text": "are actually quite unpredictable.",
      "start": 2990.4,
      "duration": 1.68
    },
    {
      "text": "They're actually quite\nhard to steer and control.",
      "start": 2992.08,
      "duration": 3.12
    },
    {
      "text": "And this version we're seeing today",
      "start": 2995.2,
      "duration": 2.64
    },
    {
      "text": "of you make one thing better,\nit makes another thing worse,",
      "start": 2997.84,
      "duration": 4.083
    },
    {
      "text": "I think that's like a present day analog",
      "start": 3003.36,
      "duration": 3.69
    },
    {
      "text": "of future control problems in AI systems",
      "start": 3007.05,
      "duration": 3.42
    },
    {
      "text": "that we can start to study today, right?",
      "start": 3010.47,
      "duration": 1.8
    },
    {
      "text": "I think that that difficulty",
      "start": 3012.27,
      "duration": 4.2
    },
    {
      "text": "in steering the behavior\nand in making sure",
      "start": 3016.47,
      "duration": 3.45
    },
    {
      "text": "that if we push an AI\nsystem in one direction,",
      "start": 3019.92,
      "duration": 2.79
    },
    {
      "text": "it doesn't push it in another direction",
      "start": 3022.71,
      "duration": 2.01
    },
    {
      "text": "in some other ways that we didn't want.",
      "start": 3024.72,
      "duration": 1.95
    },
    {
      "text": "I think that's kind of an early sign",
      "start": 3027.51,
      "duration": 3.78
    },
    {
      "text": "of things to come,",
      "start": 3031.29,
      "duration": 1.17
    },
    {
      "text": "and if we can do a good job\nof solving this problem,",
      "start": 3032.46,
      "duration": 2.34
    },
    {
      "text": "right, of like you ask the model to like,",
      "start": 3034.8,
      "duration": 2.64
    },
    {
      "text": "you know, to like make\nand distribute smallpox",
      "start": 3037.44,
      "duration": 2.73
    },
    {
      "text": "and it says no, but it's\nwilling to like help you",
      "start": 3040.17,
      "duration": 2.76
    },
    {
      "text": "in your graduate level virology class.",
      "start": 3042.93,
      "duration": 1.83
    },
    {
      "text": "Like how do we get both\nof those things at once?",
      "start": 3044.76,
      "duration": 2.85
    },
    {
      "text": "It's hard.",
      "start": 3047.61,
      "duration": 0.833
    },
    {
      "text": "It's very easy to go to\none side or the other",
      "start": 3048.443,
      "duration": 2.557
    },
    {
      "text": "and it's a multidimensional problem.",
      "start": 3051.0,
      "duration": 1.68
    },
    {
      "text": "And so, you know, I think these questions",
      "start": 3052.68,
      "duration": 3.15
    },
    {
      "text": "of like shaping the model's personality,",
      "start": 3055.83,
      "duration": 2.43
    },
    {
      "text": "I think they're very hard.",
      "start": 3058.26,
      "duration": 1.83
    },
    {
      "text": "I think we haven't done perfectly on them.",
      "start": 3060.09,
      "duration": 2.64
    },
    {
      "text": "I think we've actually done the best",
      "start": 3062.73,
      "duration": 1.77
    },
    {
      "text": "of all the AI companies, but\nstill so far from perfect.",
      "start": 3064.5,
      "duration": 4.32
    },
    {
      "text": "And I think if we can get this right,",
      "start": 3068.82,
      "duration": 1.89
    },
    {
      "text": "if we can control, you know,\ncontrol the false positives",
      "start": 3070.71,
      "duration": 4.32
    },
    {
      "text": "and false negatives in this",
      "start": 3075.03,
      "duration": 2.25
    },
    {
      "text": "very kind of controlled\npresent day environment,",
      "start": 3077.28,
      "duration": 3.42
    },
    {
      "text": "we'll be much better at doing it",
      "start": 3080.7,
      "duration": 1.32
    },
    {
      "text": "for the future when\nour worry is, you know,",
      "start": 3082.02,
      "duration": 2.499
    },
    {
      "text": "will the models be super autonomous?",
      "start": 3084.519,
      "duration": 1.551
    },
    {
      "text": "Will they be able to, you know,\nmake very dangerous things?",
      "start": 3086.07,
      "duration": 3.33
    },
    {
      "text": "Will they be able to\nautonomously, you know,",
      "start": 3089.4,
      "duration": 1.65
    },
    {
      "text": "build whole companies?",
      "start": 3091.05,
      "duration": 1.11
    },
    {
      "text": "And are those companies aligned?",
      "start": 3092.16,
      "duration": 1.68
    },
    {
      "text": "So, I think of this present\ntask as both vexing,",
      "start": 3093.84,
      "duration": 4.2
    },
    {
      "text": "but also good practice for the future.",
      "start": 3098.04,
      "duration": 2.37
    },
    {
      "text": "- What's the current best way of gathering",
      "start": 3100.41,
      "duration": 2.76
    },
    {
      "text": "sort of user feedback?",
      "start": 3103.17,
      "duration": 1.44
    },
    {
      "text": "Like not anecdotal data,",
      "start": 3104.61,
      "duration": 2.22
    },
    {
      "text": "but just large scale\ndata about pain points",
      "start": 3106.83,
      "duration": 3.84
    },
    {
      "text": "or the opposite of pain\npoints, positive things,",
      "start": 3110.67,
      "duration": 2.49
    },
    {
      "text": "so on, is it internal testing?",
      "start": 3113.16,
      "duration": 1.62
    },
    {
      "text": "Is it a specific group\ntesting, A/B testing?",
      "start": 3114.78,
      "duration": 2.91
    },
    {
      "text": "What works?",
      "start": 3117.69,
      "duration": 1.29
    },
    {
      "text": "- So, typically we'll have\ninternal model bashings",
      "start": 3118.98,
      "duration": 3.03
    },
    {
      "text": "where all of Anthropic,",
      "start": 3122.01,
      "duration": 1.11
    },
    {
      "text": "Anthropic is almost 1000 people,",
      "start": 3123.12,
      "duration": 2.31
    },
    {
      "text": "you know, people just\ntry and break the model.",
      "start": 3125.43,
      "duration": 2.04
    },
    {
      "text": "They try and interact\nwith it various ways.",
      "start": 3127.47,
      "duration": 2.193
    },
    {
      "text": "We have a suite of evals for, you know,",
      "start": 3130.89,
      "duration": 2.97
    },
    {
      "text": "oh, is the model refusing\nin ways that it couldn't?",
      "start": 3133.86,
      "duration": 2.7
    },
    {
      "text": "I think we even had a certainly eval",
      "start": 3136.56,
      "duration": 2.1
    },
    {
      "text": "because, you know, our model, again,",
      "start": 3138.66,
      "duration": 3.0
    },
    {
      "text": "one point, model had this problem",
      "start": 3141.66,
      "duration": 1.47
    },
    {
      "text": "where like it had this annoying tick",
      "start": 3143.13,
      "duration": 1.38
    },
    {
      "text": "where it would like respond",
      "start": 3144.51,
      "duration": 1.53
    },
    {
      "text": "to a wide range of questions by saying",
      "start": 3146.04,
      "duration": 1.717
    },
    {
      "text": "\"Certainly I can help you with that.",
      "start": 3147.757,
      "duration": 2.033
    },
    {
      "text": "Certainly I would be happy to do that.",
      "start": 3149.79,
      "duration": 1.98
    },
    {
      "text": "Certainly this is correct.\"",
      "start": 3151.77,
      "duration": 1.827
    },
    {
      "text": "And so we had a, like, certainly eval,",
      "start": 3154.53,
      "duration": 1.74
    },
    {
      "text": "which is like, how often\ndoes the model say certainly?",
      "start": 3156.27,
      "duration": 2.733
    },
    {
      "text": "But look, this is just a whack-a-mole.",
      "start": 3160.38,
      "duration": 1.35
    },
    {
      "text": "Like, what if it switches\nfrom certainly to definitely?",
      "start": 3161.73,
      "duration": 2.82
    },
    {
      "text": "Like, so, you know, every\ntime we add a new eval,",
      "start": 3164.55,
      "duration": 4.287
    },
    {
      "text": "and we're always evaluating\nfor all of the old things.",
      "start": 3168.837,
      "duration": 2.133
    },
    {
      "text": "So we have hundreds of these evaluations,",
      "start": 3170.97,
      "duration": 2.34
    },
    {
      "text": "but we find that there's no substitute",
      "start": 3173.31,
      "duration": 1.77
    },
    {
      "text": "for human interacting with it.",
      "start": 3175.08,
      "duration": 1.59
    },
    {
      "text": "And so it's very much like",
      "start": 3176.67,
      "duration": 1.08
    },
    {
      "text": "the ordinary product development process.",
      "start": 3177.75,
      "duration": 1.98
    },
    {
      "text": "We have like hundreds of people",
      "start": 3179.73,
      "duration": 1.74
    },
    {
      "text": "within Anthropic bash the model,",
      "start": 3181.47,
      "duration": 1.653
    },
    {
      "text": "you know, then we do external A/B tests.",
      "start": 3184.75,
      "duration": 2.42
    },
    {
      "text": "Sometimes we'll run\ntests with contractors.",
      "start": 3187.17,
      "duration": 2.4
    },
    {
      "text": "We pay contractors to\ninteract with the model.",
      "start": 3189.57,
      "duration": 2.94
    },
    {
      "text": "So you put all of these things together",
      "start": 3192.51,
      "duration": 2.37
    },
    {
      "text": "and it's still not perfect.",
      "start": 3194.88,
      "duration": 1.83
    },
    {
      "text": "You still see behaviors",
      "start": 3196.71,
      "duration": 0.99
    },
    {
      "text": "that you don't quite wanna see, right?",
      "start": 3197.7,
      "duration": 1.65
    },
    {
      "text": "You know, you still see the model",
      "start": 3199.35,
      "duration": 1.74
    },
    {
      "text": "like refusing things that",
      "start": 3201.09,
      "duration": 1.5
    },
    {
      "text": "it just doesn't make sense to refuse.",
      "start": 3202.59,
      "duration": 2.52
    },
    {
      "text": "But I think trying to solve\nthis challenge, right?",
      "start": 3205.11,
      "duration": 4.02
    },
    {
      "text": "Trying to stop the model\nfrom doing, you know,",
      "start": 3209.13,
      "duration": 3.03
    },
    {
      "text": "genuinely bad things that, you know,",
      "start": 3212.16,
      "duration": 1.92
    },
    {
      "text": "everyone agrees it shouldn't do, right?",
      "start": 3214.08,
      "duration": 1.62
    },
    {
      "text": "You know, everyone agrees that, you know,",
      "start": 3215.7,
      "duration": 2.667
    },
    {
      "text": "the model shouldn't talk about, you know,",
      "start": 3218.367,
      "duration": 2.373
    },
    {
      "text": "I don't know, child abuse material, right?",
      "start": 3220.74,
      "duration": 2.04
    },
    {
      "text": "Like, everyone agrees the\nmodel shouldn't do that.",
      "start": 3222.78,
      "duration": 2.55
    },
    {
      "text": "But at the same time",
      "start": 3225.33,
      "duration": 0.93
    },
    {
      "text": "that it doesn't refuse in\nthese dumb and stupid ways.",
      "start": 3226.26,
      "duration": 3.36
    },
    {
      "text": "I think drawing that line\nas finely as possible,",
      "start": 3229.62,
      "duration": 4.2
    },
    {
      "text": "approaching perfectly is still a challenge",
      "start": 3233.82,
      "duration": 2.4
    },
    {
      "text": "and we're getting better at it every day.",
      "start": 3236.22,
      "duration": 1.8
    },
    {
      "text": "But there's a lot to be solved.",
      "start": 3238.02,
      "duration": 1.56
    },
    {
      "text": "And again, I would point to that",
      "start": 3239.58,
      "duration": 2.31
    },
    {
      "text": "as an indicator of the challenge ahead",
      "start": 3241.89,
      "duration": 2.61
    },
    {
      "text": "in terms of steering much\nmore powerful models.",
      "start": 3244.5,
      "duration": 3.24
    },
    {
      "text": "- Do you think Claude\n4.0 is ever coming out?",
      "start": 3247.74,
      "duration": 3.78
    },
    {
      "text": "- I don't want to commit\nto any naming scheme,",
      "start": 3251.52,
      "duration": 2.61
    },
    {
      "text": "'cause if I say here",
      "start": 3254.13,
      "duration": 2.317
    },
    {
      "text": "\"We're gonna have Claude 4 next year,\"",
      "start": 3256.447,
      "duration": 2.093
    },
    {
      "text": "and then, you know, then\nwe decide that like,",
      "start": 3258.54,
      "duration": 1.507
    },
    {
      "text": "you know, we should start over,",
      "start": 3260.047,
      "duration": 1.703
    },
    {
      "text": "'cause there's a new type of model.",
      "start": 3261.75,
      "duration": 1.14
    },
    {
      "text": "Like I don't want to commit to it.",
      "start": 3262.89,
      "duration": 2.67
    },
    {
      "text": "I would expect in a\nnormal course of business",
      "start": 3265.56,
      "duration": 2.34
    },
    {
      "text": "that Claude 4 would come after Claude 3.5.",
      "start": 3267.9,
      "duration": 2.64
    },
    {
      "text": "But you know, you never know\nin this wacky field, right?",
      "start": 3270.54,
      "duration": 3.6
    },
    {
      "text": "- But the sort of, this idea\nof scaling is continuing.",
      "start": 3274.14,
      "duration": 4.65
    },
    {
      "text": "- Scaling is continuing.",
      "start": 3278.79,
      "duration": 1.38
    },
    {
      "text": "There will definitely\nbe more powerful models",
      "start": 3280.17,
      "duration": 2.43
    },
    {
      "text": "coming from us than the\nmodels that exist today.",
      "start": 3282.6,
      "duration": 1.98
    },
    {
      "text": "That is certain.",
      "start": 3284.58,
      "duration": 1.35
    },
    {
      "text": "Or if there aren't, we've\ndeeply failed as a company.",
      "start": 3285.93,
      "duration": 3.21
    },
    {
      "text": "- Okay, can you explain the\nResponsible Scaling Policy",
      "start": 3289.14,
      "duration": 2.7
    },
    {
      "text": "and the AI Safety Level\nStandards, ASL Levels?",
      "start": 3291.84,
      "duration": 3.6
    },
    {
      "text": "- As much as I am excited\nabout the benefits",
      "start": 3295.44,
      "duration": 2.7
    },
    {
      "text": "of these models, and, you\nknow, we'll talk about that",
      "start": 3298.14,
      "duration": 2.07
    },
    {
      "text": "if we talk about \"Machines\nof Loving Grace,\"",
      "start": 3300.21,
      "duration": 2.97
    },
    {
      "text": "I'm worried about the risks",
      "start": 3303.18,
      "duration": 1.41
    },
    {
      "text": "and I continue to be\nworried about the risks.",
      "start": 3304.59,
      "duration": 2.55
    },
    {
      "text": "No one should think that, you know,",
      "start": 3307.14,
      "duration": 1.327
    },
    {
      "text": "\"Machines of Loving Grace\"\nwas me saying, you know,",
      "start": 3308.467,
      "duration": 3.233
    },
    {
      "text": "I'm no longer worried about\nthe risks of these models.",
      "start": 3311.7,
      "duration": 2.52
    },
    {
      "text": "I think they're two\nsides of the same coin.",
      "start": 3314.22,
      "duration": 2.46
    },
    {
      "text": "The power of the models",
      "start": 3316.68,
      "duration": 2.79
    },
    {
      "text": "and their ability to solve\nall these problems in,",
      "start": 3319.47,
      "duration": 2.797
    },
    {
      "text": "you know, biology, neuroscience,",
      "start": 3322.267,
      "duration": 2.303
    },
    {
      "text": "economic development,",
      "start": 3324.57,
      "duration": 1.8
    },
    {
      "text": "governance and peace,\nlarge parts of the economy,",
      "start": 3326.37,
      "duration": 3.3
    },
    {
      "text": "those come with risks as well, right?",
      "start": 3329.67,
      "duration": 2.1
    },
    {
      "text": "With great power comes\ngreat responsibility, right?",
      "start": 3331.77,
      "duration": 2.604
    },
    {
      "text": "The two are paired.",
      "start": 3334.374,
      "duration": 2.496
    },
    {
      "text": "Things that are powerful\ncan do good things",
      "start": 3336.87,
      "duration": 1.83
    },
    {
      "text": "and they can do bad things.",
      "start": 3338.7,
      "duration": 1.35
    },
    {
      "text": "I think of those risks\nas being in, you know,",
      "start": 3340.89,
      "duration": 2.28
    },
    {
      "text": "several different categories.",
      "start": 3343.17,
      "duration": 2.1
    },
    {
      "text": "Perhaps the two biggest\nrisks that I think about,",
      "start": 3345.27,
      "duration": 2.88
    },
    {
      "text": "and that's not to say that",
      "start": 3348.15,
      "duration": 0.93
    },
    {
      "text": "there aren't risks today\nthat are important,",
      "start": 3349.08,
      "duration": 2.49
    },
    {
      "text": "but when I think of the\nreally the, you know,",
      "start": 3351.57,
      "duration": 1.98
    },
    {
      "text": "the things that would happen\non the grandest scale,",
      "start": 3353.55,
      "duration": 3.03
    },
    {
      "text": "one is what I call catastrophic misuse.",
      "start": 3356.58,
      "duration": 2.88
    },
    {
      "text": "These are misuse of the\nmodels in domains like cyber,",
      "start": 3359.46,
      "duration": 5.0
    },
    {
      "text": "bio, radiological, nuclear, right?",
      "start": 3364.8,
      "duration": 2.43
    },
    {
      "text": "Things that could, you\nknow, that could harm",
      "start": 3367.23,
      "duration": 3.42
    },
    {
      "text": "or even kill thousands, even millions",
      "start": 3370.65,
      "duration": 3.03
    },
    {
      "text": "of people if they really, really go wrong.",
      "start": 3373.68,
      "duration": 2.4
    },
    {
      "text": "Like these are the, you know,",
      "start": 3376.08,
      "duration": 1.11
    },
    {
      "text": "number one priority to prevent.",
      "start": 3377.19,
      "duration": 2.73
    },
    {
      "text": "And here I would just\nmake a simple observation,",
      "start": 3379.92,
      "duration": 3.12
    },
    {
      "text": "which is that the models, you know,",
      "start": 3383.04,
      "duration": 3.69
    },
    {
      "text": "if I look today at people",
      "start": 3386.73,
      "duration": 1.53
    },
    {
      "text": "who have done really\nbad things in the world,",
      "start": 3388.26,
      "duration": 2.283
    },
    {
      "text": "I think actually humanity\nhas been protected",
      "start": 3391.92,
      "duration": 2.37
    },
    {
      "text": "by the fact that the overlap",
      "start": 3394.29,
      "duration": 2.13
    },
    {
      "text": "between really smart, well-educated people",
      "start": 3396.42,
      "duration": 2.67
    },
    {
      "text": "and people who want to\ndo really horrific things",
      "start": 3399.09,
      "duration": 2.43
    },
    {
      "text": "has generally been small.",
      "start": 3401.52,
      "duration": 1.68
    },
    {
      "text": "Like, you know, let's say I'm someone who,",
      "start": 3403.2,
      "duration": 3.093
    },
    {
      "text": "you know, I have a PhD in this field,",
      "start": 3407.303,
      "duration": 1.507
    },
    {
      "text": "I have a well paying job.",
      "start": 3408.81,
      "duration": 2.4
    },
    {
      "text": "There's so much to lose.",
      "start": 3411.21,
      "duration": 1.38
    },
    {
      "text": "Why do I wanna, like, you know,",
      "start": 3412.59,
      "duration": 1.65
    },
    {
      "text": "even assuming I'm completely evil,",
      "start": 3414.24,
      "duration": 1.92
    },
    {
      "text": "which most people are not.",
      "start": 3416.16,
      "duration": 1.563
    },
    {
      "text": "You know, why would such\na person risk their life,",
      "start": 3418.74,
      "duration": 4.62
    },
    {
      "text": "risk their legacy, their\nreputation to do something like,",
      "start": 3423.36,
      "duration": 3.837
    },
    {
      "text": "you know, truly, truly evil?",
      "start": 3427.197,
      "duration": 1.953
    },
    {
      "text": "If we had a lot more people like that,",
      "start": 3429.15,
      "duration": 1.83
    },
    {
      "text": "the world would be a much\nmore dangerous place.",
      "start": 3430.98,
      "duration": 1.98
    },
    {
      "text": "And so my worry is that by being",
      "start": 3432.96,
      "duration": 3.48
    },
    {
      "text": "a much more intelligent agent,",
      "start": 3436.44,
      "duration": 2.67
    },
    {
      "text": "AI could break that correlation,",
      "start": 3439.11,
      "duration": 2.04
    },
    {
      "text": "and so I do have serious\nworries about that.",
      "start": 3441.15,
      "duration": 3.09
    },
    {
      "text": "I believe we can prevent those worries.",
      "start": 3444.24,
      "duration": 2.4
    },
    {
      "text": "But, you know, I think as a counterpoint",
      "start": 3446.64,
      "duration": 2.16
    },
    {
      "text": "to \"Machines of Loving\nGrace,\" I want to say that",
      "start": 3448.8,
      "duration": 2.85
    },
    {
      "text": "there's still serious risks.",
      "start": 3451.65,
      "duration": 1.77
    },
    {
      "text": "And the second range of risks",
      "start": 3453.42,
      "duration": 1.65
    },
    {
      "text": "would be the autonomy\nrisks, which is the idea",
      "start": 3455.07,
      "duration": 2.52
    },
    {
      "text": "that models might on their own,",
      "start": 3457.59,
      "duration": 2.16
    },
    {
      "text": "particularly as we give them more agency",
      "start": 3459.75,
      "duration": 2.31
    },
    {
      "text": "than they've had in the past,",
      "start": 3462.06,
      "duration": 1.59
    },
    {
      "text": "particularly as we give them",
      "start": 3463.65,
      "duration": 1.95
    },
    {
      "text": "supervision over wider\ntasks like, you know,",
      "start": 3465.6,
      "duration": 3.84
    },
    {
      "text": "writing whole code bases or someday even,",
      "start": 3469.44,
      "duration": 3.18
    },
    {
      "text": "you know, effectively\noperating entire companies,",
      "start": 3472.62,
      "duration": 3.96
    },
    {
      "text": "they're on a long enough leash,",
      "start": 3476.58,
      "duration": 1.5
    },
    {
      "text": "are they doing what we\nreally want them to do?",
      "start": 3478.08,
      "duration": 2.82
    },
    {
      "text": "It's very difficult to even understand",
      "start": 3480.9,
      "duration": 2.13
    },
    {
      "text": "in detail what they're\ndoing, let alone control it.",
      "start": 3483.03,
      "duration": 3.96
    },
    {
      "text": "And like I said, these early signs that",
      "start": 3486.99,
      "duration": 3.6
    },
    {
      "text": "it's hard to perfectly draw the boundary",
      "start": 3490.59,
      "duration": 2.28
    },
    {
      "text": "between things the model should do",
      "start": 3492.87,
      "duration": 1.41
    },
    {
      "text": "and things the model shouldn't do that,",
      "start": 3494.28,
      "duration": 2.16
    },
    {
      "text": "you know, if you go to one side,",
      "start": 3496.44,
      "duration": 2.31
    },
    {
      "text": "you get things that are\nannoying and useless,",
      "start": 3498.75,
      "duration": 1.83
    },
    {
      "text": "you go to the other side,\nyou get other behaviors.",
      "start": 3500.58,
      "duration": 2.19
    },
    {
      "text": "If you fix one thing, it\ncreates other problems.",
      "start": 3502.77,
      "duration": 2.7
    },
    {
      "text": "We're getting better and\nbetter at solving this.",
      "start": 3505.47,
      "duration": 1.8
    },
    {
      "text": "I don't think this is\nan unsolvable problem.",
      "start": 3507.27,
      "duration": 2.25
    },
    {
      "text": "I think, you know, this is a science,",
      "start": 3509.52,
      "duration": 2.37
    },
    {
      "text": "like the safety of airplanes\nor the safety of cars,",
      "start": 3511.89,
      "duration": 2.55
    },
    {
      "text": "or the safety of drugs.",
      "start": 3514.44,
      "duration": 1.71
    },
    {
      "text": "You know, I don't think there's\nany big thing we're missing.",
      "start": 3516.15,
      "duration": 2.55
    },
    {
      "text": "I just think we need to get better",
      "start": 3518.7,
      "duration": 1.29
    },
    {
      "text": "at controlling these models.",
      "start": 3519.99,
      "duration": 1.83
    },
    {
      "text": "And so these are the two\nrisks I'm worried about.",
      "start": 3521.82,
      "duration": 2.49
    },
    {
      "text": "And our Responsible Scaling Plan,",
      "start": 3524.31,
      "duration": 2.07
    },
    {
      "text": "which I'll recognize is\na very long-winded answer",
      "start": 3526.38,
      "duration": 2.88
    },
    {
      "text": "to your question.",
      "start": 3529.26,
      "duration": 1.083
    },
    {
      "text": "- I love it. I love it.",
      "start": 3530.343,
      "duration": 0.897
    },
    {
      "text": "- Our Responsible Scaling Plan is designed",
      "start": 3531.24,
      "duration": 2.37
    },
    {
      "text": "to address these two types of risks.",
      "start": 3533.61,
      "duration": 2.88
    },
    {
      "text": "And so every time we develop a new model,",
      "start": 3536.49,
      "duration": 3.0
    },
    {
      "text": "we basically test it for its ability",
      "start": 3539.49,
      "duration": 3.48
    },
    {
      "text": "to do both of these bad things.",
      "start": 3542.97,
      "duration": 3.09
    },
    {
      "text": "So if I were to back up a little bit,",
      "start": 3546.06,
      "duration": 1.893
    },
    {
      "text": "I think we have an interesting dilemma",
      "start": 3549.331,
      "duration": 2.489
    },
    {
      "text": "with AI systems where they're\nnot yet powerful enough",
      "start": 3551.82,
      "duration": 3.99
    },
    {
      "text": "to present these catastrophes.",
      "start": 3555.81,
      "duration": 2.55
    },
    {
      "text": "I don't know that they'll ever\nprevent these catastrophes,",
      "start": 3558.36,
      "duration": 3.12
    },
    {
      "text": "it's possible they won't,\nbut the case for worry,",
      "start": 3561.48,
      "duration": 3.66
    },
    {
      "text": "the case for risk is strong enough",
      "start": 3565.14,
      "duration": 1.71
    },
    {
      "text": "that we should act now.",
      "start": 3566.85,
      "duration": 2.31
    },
    {
      "text": "And they're getting better\nvery, very fast, right?",
      "start": 3569.16,
      "duration": 3.57
    },
    {
      "text": "You know, I testified in the Senate that,",
      "start": 3572.73,
      "duration": 1.327
    },
    {
      "text": "you know, we might have serious bio risks",
      "start": 3574.057,
      "duration": 2.543
    },
    {
      "text": "within two to three years.",
      "start": 3576.6,
      "duration": 0.96
    },
    {
      "text": "That was about a year ago.",
      "start": 3577.56,
      "duration": 1.29
    },
    {
      "text": "Things have proceeded at pace.",
      "start": 3578.85,
      "duration": 3.033
    },
    {
      "text": "So we have this thing where it's like,",
      "start": 3583.14,
      "duration": 2.25
    },
    {
      "text": "it's surprisingly hard\nto address these risks",
      "start": 3585.39,
      "duration": 3.54
    },
    {
      "text": "because they're not here today.",
      "start": 3588.93,
      "duration": 1.68
    },
    {
      "text": "They don't exist.",
      "start": 3590.61,
      "duration": 0.833
    },
    {
      "text": "They're like ghosts, but\nthey're coming at us so fast",
      "start": 3591.443,
      "duration": 2.617
    },
    {
      "text": "because the models are improving so fast.",
      "start": 3594.06,
      "duration": 1.92
    },
    {
      "text": "So how do you deal with\nsomething that's not here today,",
      "start": 3595.98,
      "duration": 3.57
    },
    {
      "text": "doesn't exist but is\ncoming at us very fast?",
      "start": 3599.55,
      "duration": 4.44
    },
    {
      "text": "So the solution we came up with for that",
      "start": 3603.99,
      "duration": 2.58
    },
    {
      "text": "in collaboration with, you know,",
      "start": 3606.57,
      "duration": 2.25
    },
    {
      "text": "people like the organization METR",
      "start": 3608.82,
      "duration": 2.55
    },
    {
      "text": "and Paul Christiano is, okay,",
      "start": 3611.37,
      "duration": 2.7
    },
    {
      "text": "what you need for that are you need tests",
      "start": 3614.07,
      "duration": 3.39
    },
    {
      "text": "to tell you when the\nrisk is getting close.",
      "start": 3617.46,
      "duration": 2.22
    },
    {
      "text": "You need an early warning system.",
      "start": 3619.68,
      "duration": 1.89
    },
    {
      "text": "And so every time we have a new model,",
      "start": 3621.57,
      "duration": 3.42
    },
    {
      "text": "we test it for its capability",
      "start": 3624.99,
      "duration": 1.86
    },
    {
      "text": "to do these CBRN tasks,",
      "start": 3626.85,
      "duration": 2.19
    },
    {
      "text": "as well as testing it for, you know,",
      "start": 3629.04,
      "duration": 2.7
    },
    {
      "text": "how capable it is of doing\ntasks autonomously on its own.",
      "start": 3631.74,
      "duration": 3.9
    },
    {
      "text": "And in the latest version of our RSP,",
      "start": 3635.64,
      "duration": 2.82
    },
    {
      "text": "which we released in\nthe last month or two,",
      "start": 3638.46,
      "duration": 3.003
    },
    {
      "text": "the way we test autonomy\nrisks is the model,",
      "start": 3642.96,
      "duration": 2.52
    },
    {
      "text": "the AI model's ability to do\naspects of AI research itself,",
      "start": 3645.48,
      "duration": 5.0
    },
    {
      "text": "which when the AI models\ncan do AI research,",
      "start": 3651.15,
      "duration": 2.55
    },
    {
      "text": "they become kind of truly autonomous.",
      "start": 3653.7,
      "duration": 2.52
    },
    {
      "text": "And you know, that threshold",
      "start": 3656.22,
      "duration": 1.56
    },
    {
      "text": "is important for a bunch of other ways.",
      "start": 3657.78,
      "duration": 1.92
    },
    {
      "text": "And so what do we then\ndo with these tasks?",
      "start": 3659.7,
      "duration": 3.99
    },
    {
      "text": "The RSP basically develops",
      "start": 3663.69,
      "duration": 2.22
    },
    {
      "text": "what we've called an if then structure,",
      "start": 3665.91,
      "duration": 2.34
    },
    {
      "text": "which is if the models\npass a certain capability,",
      "start": 3668.25,
      "duration": 3.84
    },
    {
      "text": "then we impose a certain set of safety",
      "start": 3672.09,
      "duration": 2.76
    },
    {
      "text": "and security requirements on them.",
      "start": 3674.85,
      "duration": 1.95
    },
    {
      "text": "So today's models are\nwhat's called ASL two.",
      "start": 3676.8,
      "duration": 3.24
    },
    {
      "text": "Models that were, ASL\none is for systems that",
      "start": 3680.04,
      "duration": 3.93
    },
    {
      "text": "manifestly don't pose any\nrisk of autonomy or misuse.",
      "start": 3683.97,
      "duration": 4.5
    },
    {
      "text": "So for example, a chess playing bot,",
      "start": 3688.47,
      "duration": 2.52
    },
    {
      "text": "Deep Blue would be ASL one.",
      "start": 3690.99,
      "duration": 2.13
    },
    {
      "text": "It's just manifestly the case",
      "start": 3693.12,
      "duration": 1.62
    },
    {
      "text": "that you can't use Deep Blue",
      "start": 3694.74,
      "duration": 2.19
    },
    {
      "text": "for anything other than chess.",
      "start": 3696.93,
      "duration": 1.23
    },
    {
      "text": "It was just designed for chess.",
      "start": 3698.16,
      "duration": 1.62
    },
    {
      "text": "No one's gonna use it to like, you know,",
      "start": 3699.78,
      "duration": 2.34
    },
    {
      "text": "to conduct a masterful cyber attack or to,",
      "start": 3702.12,
      "duration": 2.377
    },
    {
      "text": "you know, run wild and\ntake over the world.",
      "start": 3704.497,
      "duration": 2.873
    },
    {
      "text": "ASL two is today's AI systems\nwhere we've measured them",
      "start": 3707.37,
      "duration": 4.44
    },
    {
      "text": "and we think these systems are\nsimply not smart enough to,",
      "start": 3711.81,
      "duration": 3.753
    },
    {
      "text": "you know, autonomously self-replicate",
      "start": 3716.64,
      "duration": 2.43
    },
    {
      "text": "or conduct a bunch of tasks,",
      "start": 3719.07,
      "duration": 2.7
    },
    {
      "text": "and also not smart enough to provide",
      "start": 3721.77,
      "duration": 2.97
    },
    {
      "text": "meaningful information about CBRN risks",
      "start": 3724.74,
      "duration": 3.99
    },
    {
      "text": "and how to build CBRN\nweapons above and beyond",
      "start": 3728.73,
      "duration": 3.54
    },
    {
      "text": "what can be known from looking at Google.",
      "start": 3732.27,
      "duration": 2.313
    },
    {
      "text": "In fact, sometimes they\ndo provide information,",
      "start": 3735.42,
      "duration": 2.16
    },
    {
      "text": "but not above and beyond a search engine,",
      "start": 3737.58,
      "duration": 2.4
    },
    {
      "text": "but not in a way that\ncan be stitched together,",
      "start": 3739.98,
      "duration": 3.09
    },
    {
      "text": "not in a way that kind of end\nto end is dangerous enough.",
      "start": 3743.07,
      "duration": 3.24
    },
    {
      "text": "So ASL three is gonna be the point",
      "start": 3746.31,
      "duration": 2.79
    },
    {
      "text": "at which the models are helpful enough",
      "start": 3749.1,
      "duration": 4.44
    },
    {
      "text": "to enhance the capabilities\nof non-state actors, right?",
      "start": 3753.54,
      "duration": 3.96
    },
    {
      "text": "State actors can already do a lot of,",
      "start": 3757.5,
      "duration": 3.09
    },
    {
      "text": "unfortunately, to a high\nlevel of proficiency,",
      "start": 3760.59,
      "duration": 2.16
    },
    {
      "text": "a lot of these very dangerous\nand destructive things.",
      "start": 3762.75,
      "duration": 3.03
    },
    {
      "text": "The difference is that",
      "start": 3765.78,
      "duration": 1.15
    },
    {
      "text": "non-state actors are not capable of it.",
      "start": 3767.958,
      "duration": 1.962
    },
    {
      "text": "And so when we get to ASL three,",
      "start": 3769.92,
      "duration": 2.25
    },
    {
      "text": "we'll take special security precautions",
      "start": 3772.17,
      "duration": 3.09
    },
    {
      "text": "designed to be sufficient to prevent",
      "start": 3775.26,
      "duration": 2.13
    },
    {
      "text": "theft of the model by non-state actors,",
      "start": 3777.39,
      "duration": 2.49
    },
    {
      "text": "and misuse of the model as it's deployed.",
      "start": 3779.88,
      "duration": 3.09
    },
    {
      "text": "We'll have to have enhanced filters",
      "start": 3782.97,
      "duration": 2.34
    },
    {
      "text": "targeted at these particular areas.",
      "start": 3785.31,
      "duration": 2.07
    },
    {
      "text": "- Cyber, bio, nuclear.",
      "start": 3787.38,
      "duration": 1.59
    },
    {
      "text": "- Cyber, bio, nuclear and model autonomy,",
      "start": 3788.97,
      "duration": 3.51
    },
    {
      "text": "which is less a misuse risk",
      "start": 3792.48,
      "duration": 1.32
    },
    {
      "text": "and more risk of the model\ndoing bad things itself.",
      "start": 3793.8,
      "duration": 3.39
    },
    {
      "text": "ASL four, getting to the\npoint where these models",
      "start": 3797.19,
      "duration": 3.96
    },
    {
      "text": "could enhance the capability",
      "start": 3801.15,
      "duration": 2.68
    },
    {
      "text": "of a already knowledgeable state actor",
      "start": 3804.75,
      "duration": 3.06
    },
    {
      "text": "and/or become, you know, the\nmain source of such a risk.",
      "start": 3807.81,
      "duration": 4.14
    },
    {
      "text": "Like if you wanted to\nengage in such a risk,",
      "start": 3811.95,
      "duration": 2.22
    },
    {
      "text": "the main way you would\ndo it is through a model.",
      "start": 3814.17,
      "duration": 2.13
    },
    {
      "text": "And then I think ASL four\non the autonomy side,",
      "start": 3816.3,
      "duration": 2.55
    },
    {
      "text": "it's some amount of acceleration",
      "start": 3818.85,
      "duration": 3.03
    },
    {
      "text": "in AI research capabilities\nwithin an AI model.",
      "start": 3821.88,
      "duration": 2.94
    },
    {
      "text": "And then ASL five is where\nwe would get to the models",
      "start": 3824.82,
      "duration": 2.518
    },
    {
      "text": "that are, you know, that are kind of,",
      "start": 3827.338,
      "duration": 1.809
    },
    {
      "text": "you know, truly capable,",
      "start": 3829.147,
      "duration": 1.733
    },
    {
      "text": "that could exceed\nhumanity in their ability",
      "start": 3830.88,
      "duration": 2.885
    },
    {
      "text": "to do any of these tasks.",
      "start": 3833.765,
      "duration": 1.582
    },
    {
      "text": "And so the point of if\nthen structure commitment",
      "start": 3835.347,
      "duration": 5.0
    },
    {
      "text": "is basically to say, look, I don't know,",
      "start": 3840.45,
      "duration": 4.11
    },
    {
      "text": "I've been working with these models",
      "start": 3844.56,
      "duration": 1.29
    },
    {
      "text": "for many years and I've been worried",
      "start": 3845.85,
      "duration": 1.41
    },
    {
      "text": "about risk for many years.",
      "start": 3847.26,
      "duration": 1.68
    },
    {
      "text": "It's actually kind of\ndangerous to cry wolf.",
      "start": 3848.94,
      "duration": 2.37
    },
    {
      "text": "It's actually kind of\ndangerous to say this,",
      "start": 3851.31,
      "duration": 2.58
    },
    {
      "text": "you know, this model is risky.",
      "start": 3853.89,
      "duration": 3.09
    },
    {
      "text": "And, you know, people look at it",
      "start": 3856.98,
      "duration": 1.26
    },
    {
      "text": "and they say, this is\nmanifestly not dangerous.",
      "start": 3858.24,
      "duration": 2.28
    },
    {
      "text": "Again, it's the delicacy",
      "start": 3860.52,
      "duration": 4.05
    },
    {
      "text": "of the risk isn't here today",
      "start": 3864.57,
      "duration": 1.74
    },
    {
      "text": "but it's coming at us fast.",
      "start": 3866.31,
      "duration": 1.47
    },
    {
      "text": "How do you deal with that?",
      "start": 3867.78,
      "duration": 1.14
    },
    {
      "text": "It's really vexing to a risk\nplanner to deal with it.",
      "start": 3868.92,
      "duration": 2.97
    },
    {
      "text": "And so this if then\nstructure basically says,",
      "start": 3871.89,
      "duration": 2.49
    },
    {
      "text": "look, we don't wanna\nantagonize a bunch of people,",
      "start": 3874.38,
      "duration": 3.15
    },
    {
      "text": "we don't wanna harm our own, you know,",
      "start": 3877.53,
      "duration": 2.25
    },
    {
      "text": "our kind of own ability to have a place",
      "start": 3879.78,
      "duration": 2.25
    },
    {
      "text": "in the conversation by imposing these",
      "start": 3882.03,
      "duration": 2.65
    },
    {
      "text": "very onerous burdens on models",
      "start": 3886.41,
      "duration": 3.27
    },
    {
      "text": "that are not dangerous today.",
      "start": 3889.68,
      "duration": 1.53
    },
    {
      "text": "So if then, the trigger commitment",
      "start": 3891.21,
      "duration": 2.22
    },
    {
      "text": "is basically a way to deal with this.",
      "start": 3893.43,
      "duration": 1.71
    },
    {
      "text": "Says you clamp down hard",
      "start": 3895.14,
      "duration": 1.77
    },
    {
      "text": "when you can show that\nthe model is dangerous.",
      "start": 3896.91,
      "duration": 1.83
    },
    {
      "text": "And of course what has to\ncome with that is, you know,",
      "start": 3898.74,
      "duration": 2.31
    },
    {
      "text": "enough of a buffer\nthreshold that, you know,",
      "start": 3901.05,
      "duration": 2.2
    },
    {
      "text": "you're not at high risk of\nkind of missing the danger.",
      "start": 3906.72,
      "duration": 2.13
    },
    {
      "text": "It's not a perfect framework.",
      "start": 3908.85,
      "duration": 1.47
    },
    {
      "text": "We've had to change it every, you know,",
      "start": 3910.32,
      "duration": 2.79
    },
    {
      "text": "we came out with a new\none just a few weeks ago,",
      "start": 3913.11,
      "duration": 2.43
    },
    {
      "text": "and probably going forward,",
      "start": 3915.54,
      "duration": 1.86
    },
    {
      "text": "we might release new ones\nmultiple times a year",
      "start": 3917.4,
      "duration": 2.07
    },
    {
      "text": "because it's hard to get\nthese policies right,",
      "start": 3919.47,
      "duration": 2.64
    },
    {
      "text": "like technically, organizationally,",
      "start": 3922.11,
      "duration": 2.19
    },
    {
      "text": "from a research perspective.",
      "start": 3924.3,
      "duration": 1.77
    },
    {
      "text": "But that is the proposal,\nif then commitments",
      "start": 3926.07,
      "duration": 2.7
    },
    {
      "text": "and triggers in order to minimize burdens",
      "start": 3928.77,
      "duration": 4.2
    },
    {
      "text": "and false alarms now,",
      "start": 3932.97,
      "duration": 1.59
    },
    {
      "text": "but really react appropriately\nwhen the dangers are here.",
      "start": 3934.56,
      "duration": 2.79
    },
    {
      "text": "- What do you think the timeline",
      "start": 3937.35,
      "duration": 1.11
    },
    {
      "text": "for ASL three is where several\nof the triggers are fired?",
      "start": 3938.46,
      "duration": 3.69
    },
    {
      "text": "And what do you think the\ntimeline is for ASL four?",
      "start": 3942.15,
      "duration": 2.52
    },
    {
      "text": "- Yeah, so that is hotly\ndebated within the company.",
      "start": 3944.67,
      "duration": 2.6
    },
    {
      "text": "We are working actively\nto prepare ASL three",
      "start": 3948.24,
      "duration": 4.23
    },
    {
      "text": "security measures as well as\nASL three deployment measures.",
      "start": 3953.94,
      "duration": 3.33
    },
    {
      "text": "I'm not gonna go into detail,",
      "start": 3957.27,
      "duration": 1.38
    },
    {
      "text": "but we've made a lot of progress on both,",
      "start": 3958.65,
      "duration": 2.22
    },
    {
      "text": "and, you know, we're prepared to be,",
      "start": 3960.87,
      "duration": 2.4
    },
    {
      "text": "I think, ready quite soon.",
      "start": 3963.27,
      "duration": 1.473
    },
    {
      "text": "I would not be surprised at all",
      "start": 3967.775,
      "duration": 1.615
    },
    {
      "text": "if we hit ASL three next year.",
      "start": 3969.39,
      "duration": 2.79
    },
    {
      "text": "There was some concern that",
      "start": 3972.18,
      "duration": 1.11
    },
    {
      "text": "we might even hit it this year.",
      "start": 3973.29,
      "duration": 2.37
    },
    {
      "text": "That's still possible,\nthat could still happen.",
      "start": 3975.66,
      "duration": 2.34
    },
    {
      "text": "It's like very hard to say,",
      "start": 3978.0,
      "duration": 1.26
    },
    {
      "text": "but like I would be very, very surprised",
      "start": 3979.26,
      "duration": 2.31
    },
    {
      "text": "if it was like 2030.",
      "start": 3981.57,
      "duration": 1.14
    },
    {
      "text": "I think it's much sooner than that.",
      "start": 3982.71,
      "duration": 2.22
    },
    {
      "text": "- So there's protocols\nfor detecting it, if then,",
      "start": 3984.93,
      "duration": 3.42
    },
    {
      "text": "and then there's protocols\nfor how to respond to it.",
      "start": 3988.35,
      "duration": 3.0
    },
    {
      "text": "- Yes.",
      "start": 3991.35,
      "duration": 0.93
    },
    {
      "text": "- How difficult is the second, the latter?",
      "start": 3992.28,
      "duration": 2.52
    },
    {
      "text": "- Yeah, I think for ASL three,",
      "start": 3994.8,
      "duration": 1.89
    },
    {
      "text": "it's primarily about security and about,",
      "start": 3996.69,
      "duration": 4.38
    },
    {
      "text": "you know, filters on the model",
      "start": 4001.07,
      "duration": 1.86
    },
    {
      "text": "relating to a very narrow set of areas",
      "start": 4002.93,
      "duration": 2.37
    },
    {
      "text": "when we deploy the model.",
      "start": 4005.3,
      "duration": 1.08
    },
    {
      "text": "Because at ASL three, the\nmodel isn't autonomous yet,",
      "start": 4006.38,
      "duration": 3.813
    },
    {
      "text": "and so you don't have to\nworry about, you know,",
      "start": 4011.42,
      "duration": 1.83
    },
    {
      "text": "kind of the model itself\nbehaving in a bad way,",
      "start": 4013.25,
      "duration": 2.64
    },
    {
      "text": "even when it's deployed internally.",
      "start": 4015.89,
      "duration": 2.43
    },
    {
      "text": "So I think the ASL three measures are,",
      "start": 4018.32,
      "duration": 3.21
    },
    {
      "text": "I won't say straightforward,",
      "start": 4021.53,
      "duration": 2.1
    },
    {
      "text": "they're rigorous, but they're\neasier to reason about.",
      "start": 4023.63,
      "duration": 2.64
    },
    {
      "text": "I think once we get to ASL four,",
      "start": 4026.27,
      "duration": 3.273
    },
    {
      "text": "we start to have worries about the models",
      "start": 4030.41,
      "duration": 2.34
    },
    {
      "text": "being smart enough that\nthey might sandbag tests,",
      "start": 4032.75,
      "duration": 3.57
    },
    {
      "text": "they might not tell the truth about tests.",
      "start": 4036.32,
      "duration": 2.58
    },
    {
      "text": "We had some results came out\nabout like sleeper agents",
      "start": 4038.9,
      "duration": 2.76
    },
    {
      "text": "and there was a more recent\npaper about, you know,",
      "start": 4041.66,
      "duration": 2.82
    },
    {
      "text": "can the models mislead\nattempts to, you know,",
      "start": 4044.48,
      "duration": 4.02
    },
    {
      "text": "sandbag their own abilities, right?",
      "start": 4048.5,
      "duration": 2.07
    },
    {
      "text": "Show them, you know, present themselves",
      "start": 4050.57,
      "duration": 3.15
    },
    {
      "text": "as being less capable than they are.",
      "start": 4053.72,
      "duration": 1.74
    },
    {
      "text": "And so I think with ASL four,",
      "start": 4055.46,
      "duration": 2.28
    },
    {
      "text": "there's gonna be an important component",
      "start": 4057.74,
      "duration": 2.01
    },
    {
      "text": "of using other things\nthan just interacting",
      "start": 4059.75,
      "duration": 2.7
    },
    {
      "text": "with the models, for\nexample, interpretability",
      "start": 4062.45,
      "duration": 2.67
    },
    {
      "text": "or hidden chains of thought\nwhere you have to look",
      "start": 4065.12,
      "duration": 2.76
    },
    {
      "text": "inside the model and verify\nvia some other mechanism",
      "start": 4067.88,
      "duration": 3.9
    },
    {
      "text": "that is not, you know, is\nnot as easily corrupted",
      "start": 4071.78,
      "duration": 2.49
    },
    {
      "text": "as what the model says,",
      "start": 4074.27,
      "duration": 1.473
    },
    {
      "text": "you know, that the model\nindeed has some property.",
      "start": 4077.09,
      "duration": 3.36
    },
    {
      "text": "So we're still working on ASL four.",
      "start": 4080.45,
      "duration": 1.83
    },
    {
      "text": "One of the properties of the RSP is that",
      "start": 4082.28,
      "duration": 3.6
    },
    {
      "text": "we don't specify ASL four\nuntil we've hit ASL three.",
      "start": 4085.88,
      "duration": 5.0
    },
    {
      "text": "And I think that's proven\nto be a wise decision",
      "start": 4090.89,
      "duration": 2.28
    },
    {
      "text": "because even with ASL three,",
      "start": 4093.17,
      "duration": 2.28
    },
    {
      "text": "again, it's hard to know\nthis stuff in detail,",
      "start": 4095.45,
      "duration": 2.28
    },
    {
      "text": "and we wanna take as much time",
      "start": 4097.73,
      "duration": 2.64
    },
    {
      "text": "as we can possibly take\nto get these things right.",
      "start": 4100.37,
      "duration": 2.97
    },
    {
      "text": "- So for ASL three,",
      "start": 4103.34,
      "duration": 1.14
    },
    {
      "text": "the bad actor will be the humans.",
      "start": 4104.48,
      "duration": 2.075
    },
    {
      "text": "- [Dario] Humans, yes.",
      "start": 4106.555,
      "duration": 0.895
    },
    {
      "text": "- And so there, it's a little bit more-",
      "start": 4107.45,
      "duration": 1.857
    },
    {
      "text": "- For ASL four, it's both, I think, both.",
      "start": 4109.307,
      "duration": 1.686
    },
    {
      "text": "- It's both, and so deception,",
      "start": 4110.993,
      "duration": 1.647
    },
    {
      "text": "and that's where\nmechanistic interpretability",
      "start": 4112.64,
      "duration": 3.18
    },
    {
      "text": "comes into play and hopefully\nthe techniques used for that",
      "start": 4115.82,
      "duration": 4.23
    },
    {
      "text": "are not made accessible to the model.",
      "start": 4120.05,
      "duration": 2.4
    },
    {
      "text": "- Yeah, I mean, of course you can hook up",
      "start": 4122.45,
      "duration": 2.07
    },
    {
      "text": "the mechanistic interpretability\nto the model itself,",
      "start": 4124.52,
      "duration": 2.91
    },
    {
      "text": "but then you've kind of lost it",
      "start": 4127.43,
      "duration": 2.49
    },
    {
      "text": "as a reliable indicator\nof the model state.",
      "start": 4129.92,
      "duration": 4.89
    },
    {
      "text": "There are a bunch of exotic ways",
      "start": 4134.81,
      "duration": 1.26
    },
    {
      "text": "you can think of that it\nmight also not be reliable.",
      "start": 4136.07,
      "duration": 2.25
    },
    {
      "text": "Like if the, you know,\nmodel gets smart enough",
      "start": 4138.32,
      "duration": 2.25
    },
    {
      "text": "that it can like, you know, jump computers",
      "start": 4140.57,
      "duration": 2.25
    },
    {
      "text": "and like read the code",
      "start": 4142.82,
      "duration": 1.05
    },
    {
      "text": "where you're like looking\nat its internal state.",
      "start": 4143.87,
      "duration": 2.58
    },
    {
      "text": "We've thought about some of those.",
      "start": 4146.45,
      "duration": 1.255
    },
    {
      "text": "I think there're exotic enough,",
      "start": 4147.705,
      "duration": 1.085
    },
    {
      "text": "there are ways to render them unlikely.",
      "start": 4148.79,
      "duration": 1.71
    },
    {
      "text": "But yeah, generally you wanna preserve",
      "start": 4150.5,
      "duration": 2.88
    },
    {
      "text": "mechanistic interpretability",
      "start": 4153.38,
      "duration": 1.38
    },
    {
      "text": "as a kind of verification set or test set",
      "start": 4154.76,
      "duration": 2.61
    },
    {
      "text": "that's separate from the\ntraining process of the model.",
      "start": 4157.37,
      "duration": 1.98
    },
    {
      "text": "- See, I think as these\nmodels become better",
      "start": 4159.35,
      "duration": 2.43
    },
    {
      "text": "and better conversation\nand become smarter,",
      "start": 4161.78,
      "duration": 2.55
    },
    {
      "text": "social engineering becomes\na threat too 'cause they-",
      "start": 4164.33,
      "duration": 2.88
    },
    {
      "text": "- [Dario] Oh, yeah.",
      "start": 4167.21,
      "duration": 0.833
    },
    {
      "text": "- That could start being very convincing",
      "start": 4168.043,
      "duration": 0.877
    },
    {
      "text": "to the engineers inside companies.",
      "start": 4168.92,
      "duration": 1.89
    },
    {
      "text": "- Oh yeah, yeah.",
      "start": 4170.81,
      "duration": 1.11
    },
    {
      "text": "It's actually like, you know,",
      "start": 4171.92,
      "duration": 1.29
    },
    {
      "text": "we've seen lots of examples of demagoguery",
      "start": 4173.21,
      "duration": 2.16
    },
    {
      "text": "in our life from humans and, you know,",
      "start": 4175.37,
      "duration": 1.77
    },
    {
      "text": "there's a concern that models\nthat could do that as well.",
      "start": 4177.14,
      "duration": 2.85
    },
    {
      "text": "- One of the ways that\nClaude has been getting",
      "start": 4180.92,
      "duration": 1.29
    },
    {
      "text": "more and more powerful is it's now able",
      "start": 4182.21,
      "duration": 2.16
    },
    {
      "text": "to do some agentic stuff, computer use.",
      "start": 4184.37,
      "duration": 4.5
    },
    {
      "text": "There's also an analysis\nwithin the sandbox",
      "start": 4188.87,
      "duration": 1.98
    },
    {
      "text": "of claude.ai itself.",
      "start": 4190.85,
      "duration": 1.2
    },
    {
      "text": "But let's talk about computer use.",
      "start": 4192.05,
      "duration": 2.04
    },
    {
      "text": "That seems to me super exciting",
      "start": 4194.09,
      "duration": 2.49
    },
    {
      "text": "that you can just give Claude a task",
      "start": 4196.58,
      "duration": 2.16
    },
    {
      "text": "and it takes a bunch of\nactions, figures it out,",
      "start": 4198.74,
      "duration": 2.61
    },
    {
      "text": "and has access to your\ncomputer through screenshots.",
      "start": 4201.35,
      "duration": 4.89
    },
    {
      "text": "So can you explain how that works?",
      "start": 4206.24,
      "duration": 1.923
    },
    {
      "text": "And where that's headed?",
      "start": 4209.81,
      "duration": 0.96
    },
    {
      "text": "- Yeah, it's actually relatively simple.",
      "start": 4210.77,
      "duration": 1.68
    },
    {
      "text": "So Claude has had for a long time,",
      "start": 4212.45,
      "duration": 1.897
    },
    {
      "text": "since Claude 3.0 back in March,",
      "start": 4214.347,
      "duration": 2.723
    },
    {
      "text": "the ability to analyze images",
      "start": 4217.07,
      "duration": 2.34
    },
    {
      "text": "and respond to them with text.",
      "start": 4219.41,
      "duration": 1.74
    },
    {
      "text": "The only new thing we added",
      "start": 4221.15,
      "duration": 1.68
    },
    {
      "text": "is those images can be\nscreenshots of a computer.",
      "start": 4222.83,
      "duration": 3.87
    },
    {
      "text": "And in response, we trained the model",
      "start": 4226.7,
      "duration": 2.28
    },
    {
      "text": "to give a location on the screen",
      "start": 4228.98,
      "duration": 1.89
    },
    {
      "text": "where you can click and/or\nbuttons on the keyboard",
      "start": 4230.87,
      "duration": 3.06
    },
    {
      "text": "you can press in order to take action.",
      "start": 4233.93,
      "duration": 2.58
    },
    {
      "text": "And it turns out that",
      "start": 4236.51,
      "duration": 1.77
    },
    {
      "text": "with actually not all that\nmuch additional training,",
      "start": 4238.28,
      "duration": 2.79
    },
    {
      "text": "the models can get\nquite good at that task.",
      "start": 4241.07,
      "duration": 2.07
    },
    {
      "text": "It's a good example of generalization.",
      "start": 4243.14,
      "duration": 2.52
    },
    {
      "text": "You know, people sometimes say,",
      "start": 4245.66,
      "duration": 1.05
    },
    {
      "text": "if you get to lower earth orbit,",
      "start": 4246.71,
      "duration": 1.35
    },
    {
      "text": "you're like halfway to anywhere, right?",
      "start": 4248.06,
      "duration": 1.29
    },
    {
      "text": "Because of how much it\ntakes to escape the gravity.",
      "start": 4249.35,
      "duration": 1.83
    },
    {
      "text": "Well, if you have a\nstrong pre-trained model,",
      "start": 4251.18,
      "duration": 2.25
    },
    {
      "text": "I feel like you're halfway to anywhere",
      "start": 4253.43,
      "duration": 2.49
    },
    {
      "text": "in terms of the intelligence space.",
      "start": 4255.92,
      "duration": 2.103
    },
    {
      "text": "And so actually, it\ndidn't take all that much",
      "start": 4260.48,
      "duration": 2.61
    },
    {
      "text": "to get Claude to do this.",
      "start": 4263.09,
      "duration": 2.34
    },
    {
      "text": "And you can just set that in a loop,",
      "start": 4265.43,
      "duration": 2.7
    },
    {
      "text": "give the model a screenshot,\ntell it what to click on,",
      "start": 4268.13,
      "duration": 2.1
    },
    {
      "text": "give it the next screenshot,\ntell it what to click on,",
      "start": 4270.23,
      "duration": 2.25
    },
    {
      "text": "and that turns into a full",
      "start": 4272.48,
      "duration": 1.59
    },
    {
      "text": "kind of almost 3D video\ninteraction of the model.",
      "start": 4274.07,
      "duration": 3.777
    },
    {
      "text": "And it's able to do all\nof these tasks, right?",
      "start": 4277.847,
      "duration": 2.791
    },
    {
      "text": "You know, we showed these demos",
      "start": 4280.638,
      "duration": 1.202
    },
    {
      "text": "where it's able to like\nfill out spreadsheets.",
      "start": 4281.84,
      "duration": 2.49
    },
    {
      "text": "It's able to kind of like\ninteract with a website.",
      "start": 4284.33,
      "duration": 2.88
    },
    {
      "text": "It's able to, you know,",
      "start": 4287.21,
      "duration": 1.773
    },
    {
      "text": "it's able to open all kinds\nof, you know, programs,",
      "start": 4290.54,
      "duration": 2.34
    },
    {
      "text": "different operating systems,\nWindows, Linux, Mac.",
      "start": 4292.88,
      "duration": 2.973
    },
    {
      "text": "So, you know, I think all\nof that is very exciting.",
      "start": 4297.44,
      "duration": 2.61
    },
    {
      "text": "I will say, while in theory,",
      "start": 4300.05,
      "duration": 2.76
    },
    {
      "text": "there's nothing you could do there",
      "start": 4302.81,
      "duration": 1.26
    },
    {
      "text": "that you couldn't have done through",
      "start": 4304.07,
      "duration": 1.23
    },
    {
      "text": "just giving the model the API\nto drive the computer screen.",
      "start": 4305.3,
      "duration": 3.243
    },
    {
      "text": "This really lowers the barrier.",
      "start": 4309.41,
      "duration": 1.56
    },
    {
      "text": "And you know, there's a\nlot of folks who either,",
      "start": 4310.97,
      "duration": 3.843
    },
    {
      "text": "you know, aren't in a position to interact",
      "start": 4316.356,
      "duration": 2.024
    },
    {
      "text": "with those APIs or it takes\nthem a long time to do.",
      "start": 4318.38,
      "duration": 2.55
    },
    {
      "text": "It's just the screen is\njust a universal interface",
      "start": 4320.93,
      "duration": 2.22
    },
    {
      "text": "that's a lot easier to interact with.",
      "start": 4323.15,
      "duration": 1.47
    },
    {
      "text": "And so I expect over time,",
      "start": 4324.62,
      "duration": 1.86
    },
    {
      "text": "this is gonna lower a bunch of barriers.",
      "start": 4326.48,
      "duration": 2.16
    },
    {
      "text": "Now, honestly, the current model has,",
      "start": 4328.64,
      "duration": 2.91
    },
    {
      "text": "it leaves a lot still to be desired,",
      "start": 4331.55,
      "duration": 1.83
    },
    {
      "text": "and we were honest about\nthat in the blog, right?",
      "start": 4333.38,
      "duration": 2.22
    },
    {
      "text": "It makes mistakes, it misclicks.",
      "start": 4335.6,
      "duration": 2.58
    },
    {
      "text": "And, you know, we were\ncareful to warn people,",
      "start": 4338.18,
      "duration": 2.67
    },
    {
      "text": "hey, this thing isn't, you\ncan't just leave this thing to,",
      "start": 4340.85,
      "duration": 2.58
    },
    {
      "text": "you know, run on your computer\nfor minutes and minutes.",
      "start": 4343.43,
      "duration": 2.75
    },
    {
      "text": "You gotta give this thing\nboundaries and guardrails.",
      "start": 4347.3,
      "duration": 2.13
    },
    {
      "text": "And I think that's one of the reasons",
      "start": 4349.43,
      "duration": 1.26
    },
    {
      "text": "we released it first in an API form",
      "start": 4350.69,
      "duration": 2.58
    },
    {
      "text": "rather than kind of, you know,",
      "start": 4353.27,
      "duration": 1.62
    },
    {
      "text": "this kind of just hand it to the consumer",
      "start": 4354.89,
      "duration": 2.7
    },
    {
      "text": "and give it control of their computer.",
      "start": 4357.59,
      "duration": 3.423
    },
    {
      "text": "But you know, I definitely\nfeel that it's important",
      "start": 4361.91,
      "duration": 2.76
    },
    {
      "text": "to get these capabilities out there.",
      "start": 4364.67,
      "duration": 1.41
    },
    {
      "text": "As models get more powerful,",
      "start": 4366.08,
      "duration": 1.8
    },
    {
      "text": "we're gonna have to\ngrapple with, you know,",
      "start": 4367.88,
      "duration": 2.04
    },
    {
      "text": "how do we use these capabilities safely?",
      "start": 4369.92,
      "duration": 1.92
    },
    {
      "text": "How do we prevent them from being abused?",
      "start": 4371.84,
      "duration": 2.397
    },
    {
      "text": "And you know, I think releasing the model",
      "start": 4374.237,
      "duration": 4.023
    },
    {
      "text": "while the capabilities are, you know,",
      "start": 4378.26,
      "duration": 2.985
    },
    {
      "text": "are still limited is very\nhelpful in terms of doing that.",
      "start": 4381.245,
      "duration": 5.0
    },
    {
      "text": "You know, I think since\nit's been released,",
      "start": 4386.656,
      "duration": 1.774
    },
    {
      "text": "a number of customers,",
      "start": 4388.43,
      "duration": 1.35
    },
    {
      "text": "I think Replit was maybe\none of the most quickest",
      "start": 4389.78,
      "duration": 4.69
    },
    {
      "text": "to deploy things.",
      "start": 4396.496,
      "duration": 1.027
    },
    {
      "text": "You know, have made use\nof it in various ways.",
      "start": 4398.51,
      "duration": 1.92
    },
    {
      "text": "People have hooked up demos for, you know,",
      "start": 4400.43,
      "duration": 2.4
    },
    {
      "text": "Windows desktops, Macs,\nyou know, Linux machines.",
      "start": 4402.83,
      "duration": 5.0
    },
    {
      "text": "So yeah, it's been very exciting.",
      "start": 4409.07,
      "duration": 2.79
    },
    {
      "text": "I think as with anything else, you know,",
      "start": 4411.86,
      "duration": 2.043
    },
    {
      "text": "it comes with new exciting abilities",
      "start": 4414.8,
      "duration": 2.64
    },
    {
      "text": "and then, you know,",
      "start": 4417.44,
      "duration": 0.95
    },
    {
      "text": "with those new exciting abilities,",
      "start": 4419.42,
      "duration": 1.23
    },
    {
      "text": "we have to think about how to, you know,",
      "start": 4420.65,
      "duration": 1.92
    },
    {
      "text": "make the model, you know, safe,",
      "start": 4422.57,
      "duration": 2.46
    },
    {
      "text": "reliable, do what humans want them to do.",
      "start": 4425.03,
      "duration": 1.8
    },
    {
      "text": "I mean, it's the same story\nfor everything, right?",
      "start": 4426.83,
      "duration": 2.4
    },
    {
      "text": "Same thing. It's that same tension.",
      "start": 4429.23,
      "duration": 1.8
    },
    {
      "text": "- But the possibility of use cases here,",
      "start": 4431.03,
      "duration": 2.163
    },
    {
      "text": "just the range is incredible.",
      "start": 4433.193,
      "duration": 1.977
    },
    {
      "text": "So how much to make it work\nreally well in the future?",
      "start": 4435.17,
      "duration": 4.11
    },
    {
      "text": "How much do you have to specially",
      "start": 4439.28,
      "duration": 1.47
    },
    {
      "text": "kind of go beyond what's the\npre-trained model's doing,",
      "start": 4440.75,
      "duration": 3.78
    },
    {
      "text": "do more post-training, RLHF\nor supervised fine tuning,",
      "start": 4444.53,
      "duration": 3.99
    },
    {
      "text": "or synthetic data just\nfor the agentic stuff?",
      "start": 4448.52,
      "duration": 1.938
    },
    {
      "text": "- Yeah, I think speaking at a high level,",
      "start": 4450.458,
      "duration": 2.172
    },
    {
      "text": "it's our intention to\nkeep investing a lot in,",
      "start": 4452.63,
      "duration": 2.617
    },
    {
      "text": "you know, making the model better.",
      "start": 4455.247,
      "duration": 2.453
    },
    {
      "text": "Like I think, you know,\nwe look at some of the,",
      "start": 4457.7,
      "duration": 2.887
    },
    {
      "text": "you know, some of the benchmarks",
      "start": 4460.587,
      "duration": 1.763
    },
    {
      "text": "where previous models were like,",
      "start": 4462.35,
      "duration": 1.26
    },
    {
      "text": "oh, could do it 6% of the time,",
      "start": 4463.61,
      "duration": 1.59
    },
    {
      "text": "and now our model would do\nit 14 or 22% of the time.",
      "start": 4465.2,
      "duration": 3.24
    },
    {
      "text": "And yeah, we wanna get up to, you know,",
      "start": 4468.44,
      "duration": 1.74
    },
    {
      "text": "the human level reliability",
      "start": 4470.18,
      "duration": 1.26
    },
    {
      "text": "of 80, 90% just like anywhere else, right?",
      "start": 4471.44,
      "duration": 1.98
    },
    {
      "text": "We're on the same curve that\nwe were on with SWE-bench,",
      "start": 4473.42,
      "duration": 2.58
    },
    {
      "text": "where I think I would\nguess a year from now,",
      "start": 4476.0,
      "duration": 1.89
    },
    {
      "text": "the models can do this\nvery, very reliably,",
      "start": 4477.89,
      "duration": 1.92
    },
    {
      "text": "but you gotta start somewhere.",
      "start": 4479.81,
      "duration": 1.53
    },
    {
      "text": "- So you think it's possible to get",
      "start": 4481.34,
      "duration": 1.217
    },
    {
      "text": "to the human level, 90%,",
      "start": 4482.557,
      "duration": 2.743
    },
    {
      "text": "basically doing the same\nthing you're doing now?",
      "start": 4485.3,
      "duration": 1.96
    },
    {
      "text": "Or it has to be special for computer use?",
      "start": 4487.26,
      "duration": 2.42
    },
    {
      "text": "- I mean, it depends what\nyou mean by, you know,",
      "start": 4489.68,
      "duration": 4.219
    },
    {
      "text": "special and special in general.",
      "start": 4493.899,
      "duration": 2.111
    },
    {
      "text": "But, you know, I\ngenerally think, you know,",
      "start": 4496.01,
      "duration": 3.27
    },
    {
      "text": "the same kinds of techniques",
      "start": 4499.28,
      "duration": 1.5
    },
    {
      "text": "that we've been using to\ntrain the current model,",
      "start": 4500.78,
      "duration": 1.8
    },
    {
      "text": "I expect that doubling\ndown on those techniques",
      "start": 4502.58,
      "duration": 2.34
    },
    {
      "text": "in the same way that we have for code",
      "start": 4504.92,
      "duration": 2.862
    },
    {
      "text": "for models in general, you know,",
      "start": 4507.782,
      "duration": 1.6
    },
    {
      "text": "for image input, you know, for voice.",
      "start": 4510.38,
      "duration": 4.44
    },
    {
      "text": "I expect those same\ntechniques will scale here,",
      "start": 4514.82,
      "duration": 2.13
    },
    {
      "text": "as they have everywhere else.",
      "start": 4516.95,
      "duration": 1.23
    },
    {
      "text": "- But this is giving sort of the power",
      "start": 4518.18,
      "duration": 2.37
    },
    {
      "text": "of action to Claude,",
      "start": 4520.55,
      "duration": 1.98
    },
    {
      "text": "and so you could do a lot\nof really powerful things,",
      "start": 4522.53,
      "duration": 2.243
    },
    {
      "text": "but you could do a lot of damage also.",
      "start": 4524.773,
      "duration": 2.004
    },
    {
      "text": "- Yeah, yeah, no, and we've\nbeen very aware of that.",
      "start": 4526.777,
      "duration": 2.413
    },
    {
      "text": "Look, my view actually is computer use",
      "start": 4529.19,
      "duration": 3.12
    },
    {
      "text": "isn't a fundamentally new capability,",
      "start": 4532.31,
      "duration": 2.61
    },
    {
      "text": "like the CBRN or autonomy\ncapabilities are.",
      "start": 4534.92,
      "duration": 4.29
    },
    {
      "text": "It's more like, it kind of\nopens the aperture for the model",
      "start": 4539.21,
      "duration": 3.06
    },
    {
      "text": "to use and apply its existing abilities.",
      "start": 4542.27,
      "duration": 2.463
    },
    {
      "text": "And so the way we think about it,",
      "start": 4545.81,
      "duration": 1.74
    },
    {
      "text": "going back to our RSP,",
      "start": 4547.55,
      "duration": 1.5
    },
    {
      "text": "is nothing that this model is\ndoing inherently increases,",
      "start": 4549.05,
      "duration": 5.0
    },
    {
      "text": "you know, the risk from\nan RSP perspective.",
      "start": 4555.56,
      "duration": 3.36
    },
    {
      "text": "But as the models get more\npowerful, having this capability",
      "start": 4558.92,
      "duration": 4.14
    },
    {
      "text": "may make it scarier once it, you know,",
      "start": 4563.06,
      "duration": 2.46
    },
    {
      "text": "once it has the cognitive\ncapability to, you know,",
      "start": 4565.52,
      "duration": 3.267
    },
    {
      "text": "to do something at the ASL\nthree and ASL four level,",
      "start": 4569.68,
      "duration": 3.103
    },
    {
      "text": "you know, this may be the thing",
      "start": 4573.68,
      "duration": 2.34
    },
    {
      "text": "that kind of unbounds it from doing so.",
      "start": 4576.02,
      "duration": 1.74
    },
    {
      "text": "So, going forward, certainly\nthis modality of interaction",
      "start": 4577.76,
      "duration": 4.2
    },
    {
      "text": "is something we have tested for,",
      "start": 4581.96,
      "duration": 1.38
    },
    {
      "text": "and that we will continue to\ntest for in RSP going forward.",
      "start": 4583.34,
      "duration": 3.18
    },
    {
      "text": "I think it's probably better to have,",
      "start": 4586.52,
      "duration": 2.07
    },
    {
      "text": "to learn and explore this capability",
      "start": 4588.59,
      "duration": 1.74
    },
    {
      "text": "before the model is super,\nyou know, super capable.",
      "start": 4590.33,
      "duration": 2.7
    },
    {
      "text": "- Yeah, and there's a lot\nof interesting attacks,",
      "start": 4593.03,
      "duration": 1.86
    },
    {
      "text": "like prompt injection,",
      "start": 4594.89,
      "duration": 1.14
    },
    {
      "text": "because now you've widened the aperture,",
      "start": 4596.03,
      "duration": 1.71
    },
    {
      "text": "so you can prompt inject\nthrough stuff on screen.",
      "start": 4597.74,
      "duration": 3.3
    },
    {
      "text": "So if this becomes more and more useful,",
      "start": 4601.04,
      "duration": 2.67
    },
    {
      "text": "then there's more and more benefit",
      "start": 4603.71,
      "duration": 1.23
    },
    {
      "text": "to inject stuff into the model.",
      "start": 4604.94,
      "duration": 3.016
    },
    {
      "text": "If it goes to a certain webpage,",
      "start": 4607.956,
      "duration": 1.394
    },
    {
      "text": "it could be harmless\nstuff like advertisements",
      "start": 4609.35,
      "duration": 2.04
    },
    {
      "text": "or it could be like harmful stuff, right?",
      "start": 4611.39,
      "duration": 2.19
    },
    {
      "text": "- Yeah, I mean, we've\nthought a lot about things",
      "start": 4613.58,
      "duration": 1.47
    },
    {
      "text": "like spam, CAPTCHA, you know, mass camp.",
      "start": 4615.05,
      "duration": 3.06
    },
    {
      "text": "There's all, you know, every,",
      "start": 4618.11,
      "duration": 1.68
    },
    {
      "text": "like one secret I'll tell you,",
      "start": 4619.79,
      "duration": 2.37
    },
    {
      "text": "if you've invented a new technology,",
      "start": 4622.16,
      "duration": 1.95
    },
    {
      "text": "not necessarily the biggest misuse,",
      "start": 4624.11,
      "duration": 2.357
    },
    {
      "text": "but the first misuse you'll see,",
      "start": 4626.467,
      "duration": 2.383
    },
    {
      "text": "scams, just petty scams.",
      "start": 4628.85,
      "duration": 1.98
    },
    {
      "text": "Like you'll just, it's\nlike a thing as old,",
      "start": 4630.83,
      "duration": 3.9
    },
    {
      "text": "people scamming each other,",
      "start": 4634.73,
      "duration": 1.11
    },
    {
      "text": "it's this thing as old as time,",
      "start": 4635.84,
      "duration": 2.55
    },
    {
      "text": "and it's just every time\nyou gotta deal with it.",
      "start": 4638.39,
      "duration": 3.54
    },
    {
      "text": "- It's almost like silly\nto say but it's true,",
      "start": 4641.93,
      "duration": 2.55
    },
    {
      "text": "sort of bots and spam\nin general is a thing",
      "start": 4644.48,
      "duration": 3.4
    },
    {
      "text": "as it gets more and more intelligent.",
      "start": 4647.88,
      "duration": 2.003
    },
    {
      "text": "It's harder and harder to fight.",
      "start": 4649.883,
      "duration": 1.557
    },
    {
      "text": "- There are a lot of like I said,",
      "start": 4651.44,
      "duration": 1.32
    },
    {
      "text": "like there are a lot of\npetty criminals in the world.",
      "start": 4652.76,
      "duration": 1.92
    },
    {
      "text": "And you know, it's like\nevery new technology",
      "start": 4654.68,
      "duration": 2.91
    },
    {
      "text": "is like a new way for petty\ncriminals to do something,",
      "start": 4657.59,
      "duration": 3.27
    },
    {
      "text": "you know, something stupid and malicious.",
      "start": 4660.86,
      "duration": 2.05
    },
    {
      "text": "- Is there any ideas about sandboxing it?",
      "start": 4665.63,
      "duration": 2.13
    },
    {
      "text": "Like how difficult is the sandboxing task?",
      "start": 4667.76,
      "duration": 2.19
    },
    {
      "text": "- Yeah, we sandbox during training.",
      "start": 4669.95,
      "duration": 1.86
    },
    {
      "text": "So for example, during training,",
      "start": 4671.81,
      "duration": 1.2
    },
    {
      "text": "we didn't expose the\nmodel to the internet.",
      "start": 4673.01,
      "duration": 2.22
    },
    {
      "text": "I think that's probably a\nbad idea during training",
      "start": 4675.23,
      "duration": 2.19
    },
    {
      "text": "because, you know, the model\ncan be changing its policy,",
      "start": 4677.42,
      "duration": 2.7
    },
    {
      "text": "it can be changing what it's doing,",
      "start": 4680.12,
      "duration": 1.2
    },
    {
      "text": "and it's having an\neffect in the real world.",
      "start": 4681.32,
      "duration": 2.2
    },
    {
      "text": "You know, in terms of actually\ndeploying the model, right,",
      "start": 4684.83,
      "duration": 4.127
    },
    {
      "text": "it kind of depends on the application.",
      "start": 4688.957,
      "duration": 1.453
    },
    {
      "text": "Like, you know, sometimes\nyou want the model",
      "start": 4690.41,
      "duration": 1.59
    },
    {
      "text": "to do something in the real world,",
      "start": 4692.0,
      "duration": 1.5
    },
    {
      "text": "but of course, you can always put",
      "start": 4693.5,
      "duration": 2.612
    },
    {
      "text": "guardrails on the outside, right?",
      "start": 4696.112,
      "duration": 1.588
    },
    {
      "text": "You can say, okay, well, you know,",
      "start": 4697.7,
      "duration": 1.83
    },
    {
      "text": "this model's not gonna move\ndata from my, you know,",
      "start": 4699.53,
      "duration": 3.12
    },
    {
      "text": "model's not gonna move\nany files from my computer",
      "start": 4702.65,
      "duration": 2.55
    },
    {
      "text": "or my web server to anywhere else.",
      "start": 4705.2,
      "duration": 2.16
    },
    {
      "text": "Now, when you talk about sandboxing,",
      "start": 4707.36,
      "duration": 2.4
    },
    {
      "text": "again, when we get to ASL four,",
      "start": 4709.76,
      "duration": 2.34
    },
    {
      "text": "none of these precautions",
      "start": 4712.1,
      "duration": 1.44
    },
    {
      "text": "are going to make sense there, right,",
      "start": 4713.54,
      "duration": 1.92
    },
    {
      "text": "where when you talk about ASL four,",
      "start": 4715.46,
      "duration": 2.31
    },
    {
      "text": "you're then, the model is\nbeing kind of, you know,",
      "start": 4717.77,
      "duration": 3.03
    },
    {
      "text": "there's a theoretical worry",
      "start": 4720.8,
      "duration": 1.86
    },
    {
      "text": "the model could be smart enough",
      "start": 4722.66,
      "duration": 2.214
    },
    {
      "text": "to kind of break out of any box.",
      "start": 4724.874,
      "duration": 1.716
    },
    {
      "text": "And so there we need to think about",
      "start": 4726.59,
      "duration": 1.86
    },
    {
      "text": "mechanistic interpretability\nabout, you know,",
      "start": 4728.45,
      "duration": 3.03
    },
    {
      "text": "if we're gonna have a sandbox,",
      "start": 4731.48,
      "duration": 1.68
    },
    {
      "text": "it would need to be a\nmathematically provable sand.",
      "start": 4733.16,
      "duration": 2.04
    },
    {
      "text": "You know, that's a whole different world",
      "start": 4735.2,
      "duration": 3.12
    },
    {
      "text": "than what we're dealing\nwith with the models today.",
      "start": 4738.32,
      "duration": 3.06
    },
    {
      "text": "- Yeah, the science of building a box",
      "start": 4741.38,
      "duration": 2.79
    },
    {
      "text": "from which ASL four AI\nsystem cannot escape.",
      "start": 4744.17,
      "duration": 3.9
    },
    {
      "text": "- I think it's probably\nnot the right approach.",
      "start": 4748.07,
      "duration": 2.25
    },
    {
      "text": "I think the right approach\ninstead of having something,",
      "start": 4750.32,
      "duration": 2.73
    },
    {
      "text": "you know, unaligned\nthat like you're trying",
      "start": 4753.05,
      "duration": 2.61
    },
    {
      "text": "to prevent it from escaping.",
      "start": 4755.66,
      "duration": 1.38
    },
    {
      "text": "I think it's better to\njust design the model",
      "start": 4757.04,
      "duration": 2.37
    },
    {
      "text": "the right way or have a loop where,",
      "start": 4759.41,
      "duration": 1.5
    },
    {
      "text": "you know, you look inside,\nyou look inside the model",
      "start": 4760.91,
      "duration": 2.61
    },
    {
      "text": "and you're able to verify properties,",
      "start": 4763.52,
      "duration": 1.56
    },
    {
      "text": "and that gives you an opportunity",
      "start": 4765.08,
      "duration": 1.83
    },
    {
      "text": "to like iterate and actually get it right.",
      "start": 4766.91,
      "duration": 2.85
    },
    {
      "text": "I think containing bad models",
      "start": 4769.76,
      "duration": 3.074
    },
    {
      "text": "is much worse solution\nthan having good models.",
      "start": 4772.834,
      "duration": 3.136
    },
    {
      "text": "- Let me ask about regulation.",
      "start": 4775.97,
      "duration": 1.38
    },
    {
      "text": "What's the role of regulation\nin keeping AI safe?",
      "start": 4777.35,
      "duration": 3.33
    },
    {
      "text": "So for example, can you describe\nCalifornia AI regulation",
      "start": 4780.68,
      "duration": 3.27
    },
    {
      "text": "Bill SB 1047 that was ultimately\nvetoed by the governor?",
      "start": 4783.95,
      "duration": 4.77
    },
    {
      "text": "What are the pros and cons\nof this bill in general?",
      "start": 4788.72,
      "duration": 1.593
    },
    {
      "text": "- Yes, we ended up making some suggestions",
      "start": 4790.313,
      "duration": 1.857
    },
    {
      "text": "to the bill, and then\nsome of those were adopted",
      "start": 4792.17,
      "duration": 3.0
    },
    {
      "text": "and, you know, we felt,\nI think quite positively,",
      "start": 4795.17,
      "duration": 2.613
    },
    {
      "text": "quite positively about the\nbill by the end of that.",
      "start": 4799.16,
      "duration": 4.59
    },
    {
      "text": "It did still have some downsides,",
      "start": 4803.75,
      "duration": 1.893
    },
    {
      "text": "and, you know, of course it got vetoed.",
      "start": 4807.14,
      "duration": 2.46
    },
    {
      "text": "I think at a high level, I\nthink some of the key ideas",
      "start": 4809.6,
      "duration": 3.27
    },
    {
      "text": "behind the bill are, you know,",
      "start": 4812.87,
      "duration": 2.43
    },
    {
      "text": "I would say similar to\nideas behind our RSPs.",
      "start": 4815.3,
      "duration": 2.427
    },
    {
      "text": "And I think it's very important\nthat some jurisdiction,",
      "start": 4817.727,
      "duration": 3.033
    },
    {
      "text": "whether it's California\nor the federal government",
      "start": 4820.76,
      "duration": 2.94
    },
    {
      "text": "and/or other countries",
      "start": 4823.7,
      "duration": 1.77
    },
    {
      "text": "and other states passes\nsome regulation like this.",
      "start": 4825.47,
      "duration": 3.72
    },
    {
      "text": "And I can talk through why\nI think that's so important.",
      "start": 4829.19,
      "duration": 2.94
    },
    {
      "text": "So I feel good about our RSP.",
      "start": 4832.13,
      "duration": 1.77
    },
    {
      "text": "It's not perfect, it needs\nto be iterated on a lot,",
      "start": 4833.9,
      "duration": 3.21
    },
    {
      "text": "but it's been a good forcing function",
      "start": 4837.11,
      "duration": 2.13
    },
    {
      "text": "for getting the company to\ntake these risks seriously,",
      "start": 4839.24,
      "duration": 4.05
    },
    {
      "text": "to put them into product\nplanning, to really make them",
      "start": 4843.29,
      "duration": 3.12
    },
    {
      "text": "a central part of work at Anthropic",
      "start": 4846.41,
      "duration": 2.67
    },
    {
      "text": "and to make sure that all of 1000 people,",
      "start": 4849.08,
      "duration": 1.98
    },
    {
      "text": "and it's almost 1000\npeople now at Anthropic,",
      "start": 4851.06,
      "duration": 2.13
    },
    {
      "text": "understand that this is one\nof the highest priorities",
      "start": 4853.19,
      "duration": 2.28
    },
    {
      "text": "of the company, if not\nthe highest priority.",
      "start": 4855.47,
      "duration": 2.313
    },
    {
      "text": "But one, there are still some companies",
      "start": 4858.65,
      "duration": 4.86
    },
    {
      "text": "that don't have RSP like mechanisms,",
      "start": 4863.51,
      "duration": 2.22
    },
    {
      "text": "like OpenAI, Google did\nadopt these mechanisms",
      "start": 4865.73,
      "duration": 3.66
    },
    {
      "text": "a couple months after Anthropic did,",
      "start": 4869.39,
      "duration": 3.213
    },
    {
      "text": "but there are other companies out there",
      "start": 4873.62,
      "duration": 2.4
    },
    {
      "text": "that don't have these mechanisms at all.",
      "start": 4876.02,
      "duration": 2.64
    },
    {
      "text": "And so if some companies",
      "start": 4878.66,
      "duration": 1.59
    },
    {
      "text": "adopt these mechanisms and others don't,",
      "start": 4880.25,
      "duration": 3.54
    },
    {
      "text": "it's really gonna create\na situation where,",
      "start": 4883.79,
      "duration": 2.13
    },
    {
      "text": "you know, some of these\ndangers have the property",
      "start": 4885.92,
      "duration": 2.49
    },
    {
      "text": "that it doesn't matter\nif three out of five",
      "start": 4888.41,
      "duration": 1.95
    },
    {
      "text": "of the companies are being safe,",
      "start": 4890.36,
      "duration": 1.08
    },
    {
      "text": "if the other two are being unsafe,",
      "start": 4891.44,
      "duration": 2.61
    },
    {
      "text": "it creates this negative externality.",
      "start": 4894.05,
      "duration": 2.46
    },
    {
      "text": "And I think the lack of\nuniformity is not fair",
      "start": 4896.51,
      "duration": 2.46
    },
    {
      "text": "to those of us who have\nput a lot of effort",
      "start": 4898.97,
      "duration": 1.68
    },
    {
      "text": "into being very thoughtful\nabout these procedures.",
      "start": 4900.65,
      "duration": 2.97
    },
    {
      "text": "The second thing is,",
      "start": 4903.62,
      "duration": 1.59
    },
    {
      "text": "I don't think you can\ntrust these companies",
      "start": 4905.21,
      "duration": 1.83
    },
    {
      "text": "to adhere to these voluntary\nplans in their own, right?",
      "start": 4907.04,
      "duration": 4.17
    },
    {
      "text": "I like to think that Anthropic will.",
      "start": 4911.21,
      "duration": 1.89
    },
    {
      "text": "We do everything we can that we will.",
      "start": 4913.1,
      "duration": 1.863
    },
    {
      "text": "Our RSP is checked by our\nlong-term benefit trust.",
      "start": 4916.1,
      "duration": 4.08
    },
    {
      "text": "So, you know, we do everything we can",
      "start": 4920.18,
      "duration": 2.23
    },
    {
      "text": "to adhere to our own RSP.",
      "start": 4923.9,
      "duration": 3.003
    },
    {
      "text": "But you know, you hear lots of things",
      "start": 4927.8,
      "duration": 1.92
    },
    {
      "text": "about various companies saying,",
      "start": 4929.72,
      "duration": 1.95
    },
    {
      "text": "oh, they said they would give",
      "start": 4931.67,
      "duration": 1.62
    },
    {
      "text": "this much compute and they didn't.",
      "start": 4933.29,
      "duration": 1.41
    },
    {
      "text": "They said they would do\nthis thing and they didn't.",
      "start": 4934.7,
      "duration": 2.88
    },
    {
      "text": "You know, I don't think it makes sense to,",
      "start": 4937.58,
      "duration": 1.95
    },
    {
      "text": "you know, to litigate particular things",
      "start": 4939.53,
      "duration": 3.123
    },
    {
      "text": "that companies have done.",
      "start": 4942.653,
      "duration": 1.617
    },
    {
      "text": "But I think this broad principle",
      "start": 4944.27,
      "duration": 1.92
    },
    {
      "text": "that like if there's\nnothing watching over them,",
      "start": 4946.19,
      "duration": 2.73
    },
    {
      "text": "there's nothing watching\nover us as an industry,",
      "start": 4948.92,
      "duration": 2.43
    },
    {
      "text": "there's no guarantee that\nwe'll do the right thing,",
      "start": 4951.35,
      "duration": 2.01
    },
    {
      "text": "and the stakes are very high.",
      "start": 4953.36,
      "duration": 1.74
    },
    {
      "text": "And so I think it's important",
      "start": 4955.1,
      "duration": 2.13
    },
    {
      "text": "to have a uniform standard\nthat everyone follows,",
      "start": 4957.23,
      "duration": 4.35
    },
    {
      "text": "and to make sure that simply\nthat the industry does",
      "start": 4961.58,
      "duration": 4.23
    },
    {
      "text": "what a majority of the industry",
      "start": 4965.81,
      "duration": 1.41
    },
    {
      "text": "has already said is important\nand has already said",
      "start": 4967.22,
      "duration": 2.613
    },
    {
      "text": "that they definitely will do.",
      "start": 4969.833,
      "duration": 2.277
    },
    {
      "text": "Right, some people, you know,",
      "start": 4972.11,
      "duration": 1.613
    },
    {
      "text": "I think there's a class of people",
      "start": 4973.723,
      "duration": 1.717
    },
    {
      "text": "who are against regulation on principle.",
      "start": 4975.44,
      "duration": 3.3
    },
    {
      "text": "I understand where that comes from.",
      "start": 4978.74,
      "duration": 1.38
    },
    {
      "text": "If you go to Europe and, you know,",
      "start": 4980.12,
      "duration": 1.2
    },
    {
      "text": "you see something like GDPR,",
      "start": 4981.32,
      "duration": 2.07
    },
    {
      "text": "you see some of the other\nstuff that they've done.",
      "start": 4983.39,
      "duration": 4.14
    },
    {
      "text": "You know, some of it's good,",
      "start": 4987.53,
      "duration": 0.96
    },
    {
      "text": "but some of it is really\nunnecessarily burdensome,",
      "start": 4988.49,
      "duration": 3.21
    },
    {
      "text": "and I think it's fair to say",
      "start": 4991.7,
      "duration": 1.068
    },
    {
      "text": "really has slowed innovation.",
      "start": 4992.768,
      "duration": 2.562
    },
    {
      "text": "And so I understand where people\nare coming from on priors.",
      "start": 4995.33,
      "duration": 3.21
    },
    {
      "text": "I understand why people come from,",
      "start": 4998.54,
      "duration": 1.65
    },
    {
      "text": "start from that position.",
      "start": 5000.19,
      "duration": 2.343
    },
    {
      "text": "But again, I think AI is different.",
      "start": 5003.4,
      "duration": 2.07
    },
    {
      "text": "If we go to the very\nserious risks of autonomy",
      "start": 5005.47,
      "duration": 3.72
    },
    {
      "text": "and misuse that I talked about,",
      "start": 5009.19,
      "duration": 3.18
    },
    {
      "text": "you know, just a few minutes ago,",
      "start": 5012.37,
      "duration": 2.55
    },
    {
      "text": "I think that those are unusual",
      "start": 5014.92,
      "duration": 3.42
    },
    {
      "text": "and they warrant an\nunusually strong response.",
      "start": 5018.34,
      "duration": 4.35
    },
    {
      "text": "And so I think it's very important.",
      "start": 5022.69,
      "duration": 1.68
    },
    {
      "text": "Again, we need something\nthat everyone can get behind.",
      "start": 5024.37,
      "duration": 4.71
    },
    {
      "text": "You know, I think one of\nthe issues with SB 1047,",
      "start": 5029.08,
      "duration": 3.84
    },
    {
      "text": "especially the original version of it,",
      "start": 5032.92,
      "duration": 3.06
    },
    {
      "text": "was it had a bunch of\nthe structure of RSPs,",
      "start": 5035.98,
      "duration": 5.0
    },
    {
      "text": "but it also had a bunch of\nstuff that was either clunky",
      "start": 5041.2,
      "duration": 3.36
    },
    {
      "text": "or that just would've created",
      "start": 5044.56,
      "duration": 3.03
    },
    {
      "text": "a bunch of burdens, a bunch of hassle,",
      "start": 5047.59,
      "duration": 2.67
    },
    {
      "text": "and might even have missed the target",
      "start": 5050.26,
      "duration": 1.86
    },
    {
      "text": "in terms of addressing the risks.",
      "start": 5052.12,
      "duration": 2.4
    },
    {
      "text": "You don't really hear about it on Twitter.",
      "start": 5054.52,
      "duration": 1.89
    },
    {
      "text": "You just hear about kind of, you know,",
      "start": 5056.41,
      "duration": 2.55
    },
    {
      "text": "people are cheering for any regulation,",
      "start": 5058.96,
      "duration": 2.37
    },
    {
      "text": "and then the folks who are against",
      "start": 5061.33,
      "duration": 1.89
    },
    {
      "text": "make up these often quite\nintellectually dishonest arguments",
      "start": 5063.22,
      "duration": 3.0
    },
    {
      "text": "about how, you know,",
      "start": 5066.22,
      "duration": 2.22
    },
    {
      "text": "it'll make us move away from California.",
      "start": 5068.44,
      "duration": 2.61
    },
    {
      "text": "Bill doesn't apply if you're\nheadquartered in California,",
      "start": 5071.05,
      "duration": 2.64
    },
    {
      "text": "bill only applies if you\ndo business in California.",
      "start": 5073.69,
      "duration": 2.61
    },
    {
      "text": "Or that it would damage\nthe open source ecosystem,",
      "start": 5076.3,
      "duration": 3.15
    },
    {
      "text": "or that it would, you know,",
      "start": 5079.45,
      "duration": 1.62
    },
    {
      "text": "it would cause all of these things.",
      "start": 5081.07,
      "duration": 2.073
    },
    {
      "text": "I think those were mostly nonsense,",
      "start": 5084.365,
      "duration": 1.715
    },
    {
      "text": "but there are better\narguments against regulation.",
      "start": 5086.08,
      "duration": 3.15
    },
    {
      "text": "There's one guy, Dean Ball,\nwho's really, you know,",
      "start": 5089.23,
      "duration": 2.287
    },
    {
      "text": "I think a very scholarly,\nscholarly analyst",
      "start": 5091.517,
      "duration": 3.413
    },
    {
      "text": "who looks at what happens when\na regulation is put in place",
      "start": 5094.93,
      "duration": 2.937
    },
    {
      "text": "and ways that they can kind\nof get a life of their own,",
      "start": 5097.867,
      "duration": 3.723
    },
    {
      "text": "or how they can be poorly designed.",
      "start": 5101.59,
      "duration": 1.95
    },
    {
      "text": "And so our interest has always been,",
      "start": 5103.54,
      "duration": 2.61
    },
    {
      "text": "we do think there should be\nregulation in this space,",
      "start": 5106.15,
      "duration": 2.79
    },
    {
      "text": "but we wanna be an actor who makes sure",
      "start": 5108.94,
      "duration": 3.49
    },
    {
      "text": "that that regulation is\nsomething that's surgical,",
      "start": 5113.375,
      "duration": 3.215
    },
    {
      "text": "that's targeted at the serious risks",
      "start": 5116.59,
      "duration": 2.37
    },
    {
      "text": "and is something people\ncan actually comply with.",
      "start": 5118.96,
      "duration": 2.64
    },
    {
      "text": "Because something I think the advocates",
      "start": 5121.6,
      "duration": 1.8
    },
    {
      "text": "of regulation don't understand\nas well as they could",
      "start": 5123.4,
      "duration": 3.75
    },
    {
      "text": "is if we get something in place",
      "start": 5127.15,
      "duration": 1.84
    },
    {
      "text": "that's poorly targeted,",
      "start": 5131.83,
      "duration": 1.74
    },
    {
      "text": "that wastes a bunch of people's time,",
      "start": 5133.57,
      "duration": 2.97
    },
    {
      "text": "what's gonna happen is\npeople are gonna say,",
      "start": 5136.54,
      "duration": 1.77
    },
    {
      "text": "see, these safety risks,",
      "start": 5138.31,
      "duration": 2.73
    },
    {
      "text": "you know, this is nonsense.",
      "start": 5141.04,
      "duration": 2.07
    },
    {
      "text": "You know, I just had to hire 10 lawyers,",
      "start": 5143.11,
      "duration": 2.88
    },
    {
      "text": "you know, to fill out all these forms.",
      "start": 5145.99,
      "duration": 1.77
    },
    {
      "text": "I had to run all these tests for something",
      "start": 5147.76,
      "duration": 1.65
    },
    {
      "text": "that was clearly not dangerous.",
      "start": 5149.41,
      "duration": 1.89
    },
    {
      "text": "And after six months of that,",
      "start": 5151.3,
      "duration": 1.53
    },
    {
      "text": "there will be a groundswell\nand we'll end up",
      "start": 5152.83,
      "duration": 3.63
    },
    {
      "text": "with a durable consensus\nagainst regulation.",
      "start": 5156.46,
      "duration": 2.49
    },
    {
      "text": "And so I think the worst enemy of those",
      "start": 5158.95,
      "duration": 4.02
    },
    {
      "text": "who want real accountability\nis badly designed regulation.",
      "start": 5162.97,
      "duration": 3.72
    },
    {
      "text": "We need to actually get it right.",
      "start": 5166.69,
      "duration": 2.427
    },
    {
      "text": "And this is, if there's\none thing I could say",
      "start": 5169.117,
      "duration": 2.033
    },
    {
      "text": "to the advocates, it\nwould be that I want them",
      "start": 5171.15,
      "duration": 2.83
    },
    {
      "text": "to understand this dynamic better,",
      "start": 5173.98,
      "duration": 1.74
    },
    {
      "text": "and we need to be really careful",
      "start": 5175.72,
      "duration": 1.41
    },
    {
      "text": "and we need to talk to people",
      "start": 5177.13,
      "duration": 1.45
    },
    {
      "text": "who actually have experience",
      "start": 5179.83,
      "duration": 1.77
    },
    {
      "text": "seeing how regulations\nplay out in practice.",
      "start": 5181.6,
      "duration": 2.7
    },
    {
      "text": "And the people who have\nseen that understand",
      "start": 5184.3,
      "duration": 2.43
    },
    {
      "text": "to be very careful.",
      "start": 5186.73,
      "duration": 1.44
    },
    {
      "text": "If this was some lesser issue,",
      "start": 5188.17,
      "duration": 1.59
    },
    {
      "text": "I might be against regulation at all.",
      "start": 5189.76,
      "duration": 2.25
    },
    {
      "text": "But what I want the\nopponents to understand",
      "start": 5192.01,
      "duration": 3.72
    },
    {
      "text": "is that the underlying\nissues are actually serious.",
      "start": 5195.73,
      "duration": 3.573
    },
    {
      "text": "They're not something that\nI or the other companies",
      "start": 5200.59,
      "duration": 3.06
    },
    {
      "text": "are just making up because\nof regulatory capture.",
      "start": 5203.65,
      "duration": 3.45
    },
    {
      "text": "They're not sci-fi fantasies.",
      "start": 5207.1,
      "duration": 2.49
    },
    {
      "text": "They're not any of these things.",
      "start": 5209.59,
      "duration": 2.61
    },
    {
      "text": "You know, every time we have a new model,",
      "start": 5212.2,
      "duration": 2.55
    },
    {
      "text": "every few months, we measure\nthe behavior of these models",
      "start": 5214.75,
      "duration": 3.36
    },
    {
      "text": "and they're getting better and better",
      "start": 5218.11,
      "duration": 1.83
    },
    {
      "text": "at these concerning tasks,",
      "start": 5219.94,
      "duration": 1.32
    },
    {
      "text": "just as they are getting\nbetter and better at,",
      "start": 5221.26,
      "duration": 2.343
    },
    {
      "text": "you know, good, valuable,\neconomically useful tasks.",
      "start": 5225.13,
      "duration": 3.447
    },
    {
      "text": "And so I would just love\nit if some of the former,",
      "start": 5228.577,
      "duration": 4.593
    },
    {
      "text": "you know, I think SB\n1047 was very polarizing.",
      "start": 5233.17,
      "duration": 3.27
    },
    {
      "text": "I would love it if some of\nthe most reasonable opponents",
      "start": 5236.44,
      "duration": 4.65
    },
    {
      "text": "and some of the most reasonable proponents",
      "start": 5241.09,
      "duration": 4.98
    },
    {
      "text": "would sit down together.",
      "start": 5246.07,
      "duration": 1.32
    },
    {
      "text": "And, you know I think that, you know,",
      "start": 5247.39,
      "duration": 2.76
    },
    {
      "text": "the different AI companies, you know,",
      "start": 5250.15,
      "duration": 2.01
    },
    {
      "text": "Anthropic was the only AI\ncompany that, you know,",
      "start": 5252.16,
      "duration": 4.29
    },
    {
      "text": "felt positively in a very detailed way.",
      "start": 5256.45,
      "duration": 1.68
    },
    {
      "text": "I think Elon tweeted\nbriefly something positive.",
      "start": 5258.13,
      "duration": 3.432
    },
    {
      "text": "But, you know, some of the big ones,",
      "start": 5261.562,
      "duration": 2.478
    },
    {
      "text": "like Google, OpenAI, Meta, Microsoft",
      "start": 5264.04,
      "duration": 2.94
    },
    {
      "text": "were pretty staunchly against.",
      "start": 5266.98,
      "duration": 2.34
    },
    {
      "text": "So I would really like is if, you know,",
      "start": 5269.32,
      "duration": 2.22
    },
    {
      "text": "some of the key stakeholders,",
      "start": 5271.54,
      "duration": 1.38
    },
    {
      "text": "some of the, you know,\nmost thoughtful proponents",
      "start": 5272.92,
      "duration": 2.43
    },
    {
      "text": "and some of the most thoughtful\nopponents would sit down",
      "start": 5275.35,
      "duration": 2.94
    },
    {
      "text": "and say, how do we solve\nthis problem in a way",
      "start": 5278.29,
      "duration": 3.39
    },
    {
      "text": "that the proponents feel brings\na real reduction in risk,",
      "start": 5281.68,
      "duration": 4.59
    },
    {
      "text": "and that the opponents feel that",
      "start": 5286.27,
      "duration": 2.77
    },
    {
      "text": "it is not hampering the industry",
      "start": 5290.404,
      "duration": 2.496
    },
    {
      "text": "or hampering innovation any\nmore necessary than it needs to.",
      "start": 5292.9,
      "duration": 5.0
    },
    {
      "text": "And I think for whatever reason",
      "start": 5298.36,
      "duration": 2.49
    },
    {
      "text": "that things got too polarized",
      "start": 5300.85,
      "duration": 2.28
    },
    {
      "text": "and those two groups\ndidn't get to sit down",
      "start": 5303.13,
      "duration": 2.67
    },
    {
      "text": "in the way that they should.",
      "start": 5305.8,
      "duration": 1.74
    },
    {
      "text": "And I feel urgency.",
      "start": 5307.54,
      "duration": 1.65
    },
    {
      "text": "I really think we need\nto do something in 2025.",
      "start": 5309.19,
      "duration": 2.553
    },
    {
      "text": "You know, if we get to the end of 2025",
      "start": 5313.21,
      "duration": 2.25
    },
    {
      "text": "and we've still done nothing about this,",
      "start": 5315.46,
      "duration": 1.95
    },
    {
      "text": "then I'm gonna be worried.",
      "start": 5317.41,
      "duration": 1.29
    },
    {
      "text": "I'm not worried yet, because again,",
      "start": 5318.7,
      "duration": 2.64
    },
    {
      "text": "the risks aren't here yet,",
      "start": 5321.34,
      "duration": 1.407
    },
    {
      "text": "but I think time is running short.",
      "start": 5322.747,
      "duration": 1.893
    },
    {
      "text": "- Yeah, and come up with\nsomething surgical, like you said.",
      "start": 5324.64,
      "duration": 2.04
    },
    {
      "text": "- Yeah, yeah, yeah, exactly.",
      "start": 5326.68,
      "duration": 1.74
    },
    {
      "text": "And we need to get away",
      "start": 5328.42,
      "duration": 1.8
    },
    {
      "text": "from this intense pro-safety",
      "start": 5330.22,
      "duration": 5.0
    },
    {
      "text": "versus intense anti-regulatory\nrhetoric, right?",
      "start": 5336.07,
      "duration": 2.88
    },
    {
      "text": "It's turned into these\nflame wars on Twitter",
      "start": 5338.95,
      "duration": 3.15
    },
    {
      "text": "and nothing good's gonna come of that.",
      "start": 5342.1,
      "duration": 2.55
    },
    {
      "text": "- So there's a lot of curiosity about",
      "start": 5344.65,
      "duration": 1.32
    },
    {
      "text": "the different players in the game.",
      "start": 5345.97,
      "duration": 1.17
    },
    {
      "text": "One of the OGs is OpenAI.",
      "start": 5347.14,
      "duration": 2.8
    },
    {
      "text": "You've had several years\nof experience at OpenAI.",
      "start": 5349.94,
      "duration": 2.21
    },
    {
      "text": "What's your story and history there?",
      "start": 5352.15,
      "duration": 1.98
    },
    {
      "text": "- Yeah, so I was at OpenAI\nfor roughly five years.",
      "start": 5354.13,
      "duration": 4.65
    },
    {
      "text": "For the last, I think it was couple years,",
      "start": 5358.78,
      "duration": 1.417
    },
    {
      "text": "you know, I was vice\npresident of research there.",
      "start": 5360.197,
      "duration": 4.853
    },
    {
      "text": "Probably myself and Ilya\nSutskever were the ones who,",
      "start": 5365.05,
      "duration": 2.46
    },
    {
      "text": "you know, really kind of\nset the research direction.",
      "start": 5367.51,
      "duration": 3.24
    },
    {
      "text": "Around 2016 or 2017, I first\nstarted to really believe in",
      "start": 5370.75,
      "duration": 4.41
    },
    {
      "text": "or at least confirm my belief\nin the Scaling Hypothesis",
      "start": 5375.16,
      "duration": 2.76
    },
    {
      "text": "when Ilya famously said to me,",
      "start": 5377.92,
      "duration": 2.047
    },
    {
      "text": "\"The thing you need to understand",
      "start": 5379.967,
      "duration": 1.193
    },
    {
      "text": "about these models is\nthey just wanna learn.",
      "start": 5381.16,
      "duration": 2.34
    },
    {
      "text": "The models just wanna learn.\"",
      "start": 5383.5,
      "duration": 2.28
    },
    {
      "text": "And again, sometimes there\nare these one sentences,",
      "start": 5385.78,
      "duration": 3.39
    },
    {
      "text": "these zen koans that you hear them",
      "start": 5389.17,
      "duration": 1.68
    },
    {
      "text": "and you're like, ah,\nthat explains everything.",
      "start": 5390.85,
      "duration": 3.3
    },
    {
      "text": "That explains like 1000\nthings that I've seen.",
      "start": 5394.15,
      "duration": 2.49
    },
    {
      "text": "And then I, you know,",
      "start": 5396.64,
      "duration": 1.68
    },
    {
      "text": "ever after I had this\nvisualization in my head of like,",
      "start": 5398.32,
      "duration": 2.97
    },
    {
      "text": "you optimize the models in the right way,",
      "start": 5401.29,
      "duration": 1.65
    },
    {
      "text": "you point the models in the right way.",
      "start": 5402.94,
      "duration": 1.62
    },
    {
      "text": "They just wanna learn.",
      "start": 5404.56,
      "duration": 1.05
    },
    {
      "text": "They just wanna solve the problem,",
      "start": 5405.61,
      "duration": 1.29
    },
    {
      "text": "regardless of what the problem is.",
      "start": 5406.9,
      "duration": 1.44
    },
    {
      "text": "- So get out of their way, basically.",
      "start": 5408.34,
      "duration": 1.68
    },
    {
      "text": "- Get out of their way, yeah.",
      "start": 5410.02,
      "duration": 1.53
    },
    {
      "text": "Don't impose your own ideas\nabout how they should learn.",
      "start": 5411.55,
      "duration": 3.032
    },
    {
      "text": "And you know, this was the same thing",
      "start": 5414.582,
      "duration": 1.198
    },
    {
      "text": "as Rich Sutton put out\nin the Bitter Lesson",
      "start": 5415.78,
      "duration": 2.16
    },
    {
      "text": "or Gwern put out in\nThe Scaling Hypothesis.",
      "start": 5417.94,
      "duration": 2.227
    },
    {
      "text": "You know, I think generally\nthe dynamic was, you know,",
      "start": 5420.167,
      "duration": 3.953
    },
    {
      "text": "I got this kind of inspiration\nfrom Ilya and from others,",
      "start": 5424.12,
      "duration": 4.86
    },
    {
      "text": "folks like Alec Radford\nwho did the original GPT-1,",
      "start": 5428.98,
      "duration": 5.0
    },
    {
      "text": "and then ran really hard with it.",
      "start": 5434.47,
      "duration": 2.22
    },
    {
      "text": "Me and my collaborators on GPT-2, GPT-3.",
      "start": 5436.69,
      "duration": 4.14
    },
    {
      "text": "RL from Human Feedback,\nwhich was an attempt",
      "start": 5440.83,
      "duration": 1.89
    },
    {
      "text": "to kind of deal with the\nearly safety and durability.",
      "start": 5442.72,
      "duration": 2.91
    },
    {
      "text": "Things like debate and amplification.",
      "start": 5445.63,
      "duration": 2.34
    },
    {
      "text": "Heavy on interpretability.",
      "start": 5447.97,
      "duration": 1.41
    },
    {
      "text": "So again, the combination\nof safety plus scaling.",
      "start": 5449.38,
      "duration": 3.63
    },
    {
      "text": "Probably 2018, 2019, 2020,",
      "start": 5453.01,
      "duration": 3.285
    },
    {
      "text": "those were kind of the years",
      "start": 5456.295,
      "duration": 2.055
    },
    {
      "text": "when myself and my collaborators\nprobably, you know,",
      "start": 5458.35,
      "duration": 5.0
    },
    {
      "text": "many of whom became\nco-founders of Anthropic,",
      "start": 5464.29,
      "duration": 2.67
    },
    {
      "text": "kind of really had a vision\nand like drove the direction.",
      "start": 5466.96,
      "duration": 4.26
    },
    {
      "text": "- Why'd you leave?",
      "start": 5471.22,
      "duration": 0.93
    },
    {
      "text": "Why'd you decide to leave?",
      "start": 5472.15,
      "duration": 1.38
    },
    {
      "text": "- Yeah, so look, I'm\ngonna put things this way",
      "start": 5473.53,
      "duration": 2.64
    },
    {
      "text": "and, you know, I think it ties",
      "start": 5476.17,
      "duration": 2.595
    },
    {
      "text": "to the race to the top, right?",
      "start": 5478.765,
      "duration": 1.995
    },
    {
      "text": "Which is, you know, in my time at OpenAI,",
      "start": 5480.76,
      "duration": 2.73
    },
    {
      "text": "what I'd come to see is\nI'd come to appreciate",
      "start": 5483.49,
      "duration": 2.214
    },
    {
      "text": "the Scaling Hypothesis,",
      "start": 5485.704,
      "duration": 1.571
    },
    {
      "text": "and as I'd come to appreciate\nkind of the importance",
      "start": 5487.275,
      "duration": 2.215
    },
    {
      "text": "of safety along with\nthe Scaling Hypothesis.",
      "start": 5489.49,
      "duration": 2.55
    },
    {
      "text": "The first one I think, you know,",
      "start": 5492.04,
      "duration": 1.328
    },
    {
      "text": "OpenAI was getting on board with.",
      "start": 5493.368,
      "duration": 2.545
    },
    {
      "text": "The second one in a way\nhad always been part",
      "start": 5496.78,
      "duration": 2.46
    },
    {
      "text": "of OpenAI's messaging,",
      "start": 5499.24,
      "duration": 1.653
    },
    {
      "text": "but, you know, over many years",
      "start": 5501.76,
      "duration": 2.52
    },
    {
      "text": "of the time that I spent there,",
      "start": 5504.28,
      "duration": 2.76
    },
    {
      "text": "I think I had a particular vision",
      "start": 5507.04,
      "duration": 1.74
    },
    {
      "text": "of how we should handle these things,",
      "start": 5508.78,
      "duration": 2.7
    },
    {
      "text": "how we should be brought out in the world,",
      "start": 5511.48,
      "duration": 2.07
    },
    {
      "text": "the kind of principles that\nthe organization should have.",
      "start": 5513.55,
      "duration": 3.75
    },
    {
      "text": "And look, I mean, there were\nlike many, many discussions",
      "start": 5517.3,
      "duration": 3.63
    },
    {
      "text": "about like, you know, should the org do,",
      "start": 5520.93,
      "duration": 1.62
    },
    {
      "text": "should the company do this?",
      "start": 5522.55,
      "duration": 1.05
    },
    {
      "text": "Should the company do that?",
      "start": 5523.6,
      "duration": 1.44
    },
    {
      "text": "Like, there's a bunch of\nmisinformation out there.",
      "start": 5525.04,
      "duration": 2.34
    },
    {
      "text": "People say like, we left",
      "start": 5527.38,
      "duration": 1.56
    },
    {
      "text": "because we didn't like\nthe deal with Microsoft.",
      "start": 5528.94,
      "duration": 1.98
    },
    {
      "text": "False, although, you know, it\nwas like a lot of discussion,",
      "start": 5530.92,
      "duration": 3.0
    },
    {
      "text": "a lot of questions about exactly",
      "start": 5533.92,
      "duration": 1.38
    },
    {
      "text": "how we do the deal with Microsoft.",
      "start": 5535.3,
      "duration": 2.16
    },
    {
      "text": "We left because we didn't\nlike commercialization.",
      "start": 5537.46,
      "duration": 2.04
    },
    {
      "text": "That's not true, we built GPT-3,",
      "start": 5539.5,
      "duration": 1.77
    },
    {
      "text": "which was the model\nthat was commercialized.",
      "start": 5541.27,
      "duration": 2.16
    },
    {
      "text": "I was involved in commercialization.",
      "start": 5543.43,
      "duration": 2.22
    },
    {
      "text": "It's more again about, how do you do it?",
      "start": 5545.65,
      "duration": 2.76
    },
    {
      "text": "Like civilization is going down this path",
      "start": 5548.41,
      "duration": 3.21
    },
    {
      "text": "to very powerful AI.",
      "start": 5551.62,
      "duration": 1.65
    },
    {
      "text": "What's the way to do it that is cautious,",
      "start": 5553.27,
      "duration": 3.57
    },
    {
      "text": "straightforward, honest,",
      "start": 5556.84,
      "duration": 2.583
    },
    {
      "text": "that builds trust in the\norganization and individuals?",
      "start": 5560.29,
      "duration": 4.74
    },
    {
      "text": "How do we get from here to there?",
      "start": 5565.03,
      "duration": 2.25
    },
    {
      "text": "And how do we have a real\nvision for how to get it right?",
      "start": 5567.28,
      "duration": 2.64
    },
    {
      "text": "How can safety not just\nbe something we say",
      "start": 5569.92,
      "duration": 3.3
    },
    {
      "text": "because it helps with recruiting?",
      "start": 5573.22,
      "duration": 2.07
    },
    {
      "text": "And, you know, I think\nat the end of the day,",
      "start": 5575.29,
      "duration": 2.793
    },
    {
      "text": "if you have a vision for that,",
      "start": 5578.95,
      "duration": 1.56
    },
    {
      "text": "forget about anyone else's vision.",
      "start": 5580.51,
      "duration": 1.41
    },
    {
      "text": "I don't wanna talk about\nanyone else's vision.",
      "start": 5581.92,
      "duration": 1.8
    },
    {
      "text": "If you have a vision for how\nto do it, you should go off",
      "start": 5583.72,
      "duration": 2.82
    },
    {
      "text": "and you should do that vision.",
      "start": 5586.54,
      "duration": 1.56
    },
    {
      "text": "It is incredibly unproductive to try",
      "start": 5588.1,
      "duration": 2.52
    },
    {
      "text": "and argue with someone else's vision.",
      "start": 5590.62,
      "duration": 2.04
    },
    {
      "text": "You might think they're\nnot doing it the right way.",
      "start": 5592.66,
      "duration": 2.04
    },
    {
      "text": "You might think they're dishonest.",
      "start": 5594.7,
      "duration": 2.28
    },
    {
      "text": "Who knows, maybe you're\nright, maybe you're not.",
      "start": 5596.98,
      "duration": 3.06
    },
    {
      "text": "But what you should do",
      "start": 5600.04,
      "duration": 1.83
    },
    {
      "text": "is you should take some people you trust",
      "start": 5601.87,
      "duration": 1.62
    },
    {
      "text": "and you should go off together",
      "start": 5603.49,
      "duration": 1.44
    },
    {
      "text": "and you should make your vision happen.",
      "start": 5604.93,
      "duration": 1.56
    },
    {
      "text": "And if your vision is compelling,",
      "start": 5606.49,
      "duration": 1.68
    },
    {
      "text": "if you can make it appeal to people,",
      "start": 5608.17,
      "duration": 2.01
    },
    {
      "text": "you know, some combination of ethically,",
      "start": 5610.18,
      "duration": 3.18
    },
    {
      "text": "you know, in the market, you know,",
      "start": 5613.36,
      "duration": 3.11
    },
    {
      "text": "if you can make a company that's\na place people wanna join,",
      "start": 5616.47,
      "duration": 4.51
    },
    {
      "text": "that, you know, engages in practices",
      "start": 5620.98,
      "duration": 2.07
    },
    {
      "text": "that people think are reasonable,",
      "start": 5623.05,
      "duration": 2.34
    },
    {
      "text": "while managing to maintain its position",
      "start": 5625.39,
      "duration": 2.55
    },
    {
      "text": "in the ecosystem at the same time,",
      "start": 5627.94,
      "duration": 1.77
    },
    {
      "text": "if you do that, people will copy it.",
      "start": 5629.71,
      "duration": 2.7
    },
    {
      "text": "And the fact that you are\ndoing it, especially the fact",
      "start": 5632.41,
      "duration": 2.67
    },
    {
      "text": "that you're doing it better than they are",
      "start": 5635.08,
      "duration": 2.28
    },
    {
      "text": "causes them to change their behavior",
      "start": 5637.36,
      "duration": 1.89
    },
    {
      "text": "in a much more compelling way",
      "start": 5639.25,
      "duration": 1.86
    },
    {
      "text": "than if they're your boss\nand you're arguing with them.",
      "start": 5641.11,
      "duration": 2.58
    },
    {
      "text": "I just, I don't know how to be",
      "start": 5643.69,
      "duration": 1.26
    },
    {
      "text": "any more specific about it than that,",
      "start": 5644.95,
      "duration": 1.92
    },
    {
      "text": "but I think it's generally\nvery unproductive",
      "start": 5646.87,
      "duration": 2.76
    },
    {
      "text": "to try and get someone else's vision",
      "start": 5649.63,
      "duration": 1.89
    },
    {
      "text": "to look like your vision.",
      "start": 5651.52,
      "duration": 1.86
    },
    {
      "text": "It's much more productive to go off",
      "start": 5653.38,
      "duration": 2.43
    },
    {
      "text": "and do a clean experiment\nand say, this is our vision,",
      "start": 5655.81,
      "duration": 2.97
    },
    {
      "text": "this is how we're gonna do things.",
      "start": 5658.78,
      "duration": 2.4
    },
    {
      "text": "Your choice is you can ignore us,",
      "start": 5661.18,
      "duration": 3.69
    },
    {
      "text": "you can reject what we're doing,",
      "start": 5664.87,
      "duration": 1.59
    },
    {
      "text": "or you can start to become more like us,",
      "start": 5666.46,
      "duration": 3.69
    },
    {
      "text": "and imitation is the\nsincerest form of flattery.",
      "start": 5670.15,
      "duration": 2.403
    },
    {
      "text": "And you know, that plays out\nin the behavior of customers,",
      "start": 5673.39,
      "duration": 4.08
    },
    {
      "text": "that plays out in the\nbehavior of the public,",
      "start": 5677.47,
      "duration": 1.98
    },
    {
      "text": "that plays out in the behavior",
      "start": 5679.45,
      "duration": 1.32
    },
    {
      "text": "of where people choose to work.",
      "start": 5680.77,
      "duration": 2.31
    },
    {
      "text": "And again, at the end,",
      "start": 5683.08,
      "duration": 1.95
    },
    {
      "text": "it's not about one company winning",
      "start": 5685.03,
      "duration": 2.16
    },
    {
      "text": "or another company winning.",
      "start": 5687.19,
      "duration": 1.35
    },
    {
      "text": "If we or another company are\nengaging in some practice that,",
      "start": 5689.494,
      "duration": 5.0
    },
    {
      "text": "you know, people find genuinely appealing,",
      "start": 5694.51,
      "duration": 3.18
    },
    {
      "text": "and I want it to be in substance,",
      "start": 5697.69,
      "duration": 1.38
    },
    {
      "text": "not just in appearance.",
      "start": 5699.07,
      "duration": 1.8
    },
    {
      "text": "And, you know, I think researchers",
      "start": 5700.87,
      "duration": 2.43
    },
    {
      "text": "are sophisticated and\nthey look at substance.",
      "start": 5703.3,
      "duration": 2.25
    },
    {
      "text": "And then other companies\nstart copying that practice",
      "start": 5706.48,
      "duration": 3.09
    },
    {
      "text": "and they win because they\ncopied that practice,",
      "start": 5709.57,
      "duration": 2.31
    },
    {
      "text": "that's great, that's success.",
      "start": 5711.88,
      "duration": 1.62
    },
    {
      "text": "That's like the race to the top.",
      "start": 5713.5,
      "duration": 1.65
    },
    {
      "text": "It doesn't matter who wins in the end,",
      "start": 5715.15,
      "duration": 1.92
    },
    {
      "text": "as long as everyone is copying",
      "start": 5717.07,
      "duration": 1.38
    },
    {
      "text": "everyone else's good practices, right?",
      "start": 5718.45,
      "duration": 1.92
    },
    {
      "text": "One way I think of it is like,",
      "start": 5720.37,
      "duration": 1.77
    },
    {
      "text": "the thing we're all afraid of",
      "start": 5722.14,
      "duration": 1.02
    },
    {
      "text": "is the race to the bottom, right?",
      "start": 5723.16,
      "duration": 0.867
    },
    {
      "text": "And the race to the bottom,",
      "start": 5724.027,
      "duration": 1.473
    },
    {
      "text": "doesn't matter who wins\nbecause we all lose, right?",
      "start": 5725.5,
      "duration": 2.61
    },
    {
      "text": "Like, you know, in the most extreme world,",
      "start": 5728.11,
      "duration": 1.65
    },
    {
      "text": "we make this autonomous AI that, you know,",
      "start": 5729.76,
      "duration": 2.4
    },
    {
      "text": "the robots enslave us or whatever, right?",
      "start": 5732.16,
      "duration": 1.313
    },
    {
      "text": "I mean, that's half joking, but you know,",
      "start": 5733.473,
      "duration": 2.257
    },
    {
      "text": "that is the most extreme\nthing that could happen.",
      "start": 5735.73,
      "duration": 4.44
    },
    {
      "text": "Then it doesn't matter\nwhich company was ahead.",
      "start": 5740.17,
      "duration": 2.85
    },
    {
      "text": "If instead you create a race to the top",
      "start": 5743.02,
      "duration": 2.4
    },
    {
      "text": "where people are competing to engage",
      "start": 5745.42,
      "duration": 2.91
    },
    {
      "text": "in good practices, then, you know,",
      "start": 5748.33,
      "duration": 3.05
    },
    {
      "text": "at the end of the day, you know,",
      "start": 5751.38,
      "duration": 1.57
    },
    {
      "text": "it doesn't matter who ends up winning,",
      "start": 5752.95,
      "duration": 2.55
    },
    {
      "text": "doesn't even matter who\nstarted the race at the top.",
      "start": 5755.5,
      "duration": 2.28
    },
    {
      "text": "The point isn't to be virtuous.",
      "start": 5757.78,
      "duration": 1.65
    },
    {
      "text": "The point is to get the system",
      "start": 5759.43,
      "duration": 1.56
    },
    {
      "text": "into a better equilibrium\nthan it was before,",
      "start": 5760.99,
      "duration": 2.46
    },
    {
      "text": "and individual companies can\nplay some role in doing this.",
      "start": 5763.45,
      "duration": 2.97
    },
    {
      "text": "Individual companies can, you know,",
      "start": 5766.42,
      "duration": 2.61
    },
    {
      "text": "can help to start it,",
      "start": 5769.03,
      "duration": 1.56
    },
    {
      "text": "can help to accelerate it.",
      "start": 5770.59,
      "duration": 1.77
    },
    {
      "text": "And frankly, I think\nindividuals at other companies",
      "start": 5772.36,
      "duration": 2.25
    },
    {
      "text": "have done this as well, right?",
      "start": 5774.61,
      "duration": 1.35
    },
    {
      "text": "The individuals that\nwhen we put out an RSP",
      "start": 5775.96,
      "duration": 3.12
    },
    {
      "text": "react by pushing harder to\nget something similar done,",
      "start": 5779.08,
      "duration": 3.93
    },
    {
      "text": "get something similar\ndone at other companies.",
      "start": 5783.01,
      "duration": 2.82
    },
    {
      "text": "Sometimes other companies\ndo something that's like,",
      "start": 5785.83,
      "duration": 1.83
    },
    {
      "text": "we're like, oh, it's a good practice.",
      "start": 5787.66,
      "duration": 1.29
    },
    {
      "text": "We think that's good.",
      "start": 5788.95,
      "duration": 1.47
    },
    {
      "text": "We should adopt it too.",
      "start": 5790.42,
      "duration": 1.32
    },
    {
      "text": "The only difference is,\nyou know, I think we are,",
      "start": 5791.74,
      "duration": 3.69
    },
    {
      "text": "we try to be more forward-leaning.",
      "start": 5795.43,
      "duration": 1.8
    },
    {
      "text": "We try and adopt more\nof these practices first",
      "start": 5797.23,
      "duration": 2.37
    },
    {
      "text": "and adopt them more quickly\nwhen others invent them.",
      "start": 5799.6,
      "duration": 2.91
    },
    {
      "text": "But I think this dynamic is\nwhat we should be pointing at.",
      "start": 5802.51,
      "duration": 3.3
    },
    {
      "text": "And I think it abstracts\naway the question of,",
      "start": 5805.81,
      "duration": 4.14
    },
    {
      "text": "you know, which company's\nwinning, who trusts who.",
      "start": 5809.95,
      "duration": 3.03
    },
    {
      "text": "I think all these questions of drama",
      "start": 5812.98,
      "duration": 3.45
    },
    {
      "text": "are profoundly uninteresting,",
      "start": 5816.43,
      "duration": 1.89
    },
    {
      "text": "and the thing that\nmatters is the ecosystem",
      "start": 5818.32,
      "duration": 2.88
    },
    {
      "text": "that we all operate in and how\nto make that ecosystem better",
      "start": 5821.2,
      "duration": 2.91
    },
    {
      "text": "because that constrains all the players.",
      "start": 5824.11,
      "duration": 1.89
    },
    {
      "text": "- And so Anthropic is this kind",
      "start": 5826.0,
      "duration": 1.26
    },
    {
      "text": "of clean experiment built on a foundation",
      "start": 5827.26,
      "duration": 2.993
    },
    {
      "text": "of like what concretely AI\nsafety should look like.",
      "start": 5830.253,
      "duration": 3.727
    },
    {
      "text": "- Look, I'm sure we've made plenty",
      "start": 5833.98,
      "duration": 1.32
    },
    {
      "text": "of mistakes along the way.",
      "start": 5835.3,
      "duration": 1.38
    },
    {
      "text": "The perfect organization doesn't exist.",
      "start": 5836.68,
      "duration": 2.28
    },
    {
      "text": "It has to deal with the imperfection",
      "start": 5838.96,
      "duration": 2.61
    },
    {
      "text": "of 1000 employees.",
      "start": 5841.57,
      "duration": 1.62
    },
    {
      "text": "It has to deal with the imperfection",
      "start": 5843.19,
      "duration": 1.38
    },
    {
      "text": "of our leaders, including me.",
      "start": 5844.57,
      "duration": 1.59
    },
    {
      "text": "It has to deal with the imperfection",
      "start": 5846.16,
      "duration": 1.65
    },
    {
      "text": "of the people we've put to, you know,",
      "start": 5847.81,
      "duration": 2.715
    },
    {
      "text": "to oversee the imperfection\nof the leaders,",
      "start": 5850.525,
      "duration": 2.325
    },
    {
      "text": "like the board and the\nlong-term benefit trust.",
      "start": 5852.85,
      "duration": 2.7
    },
    {
      "text": "It's all a set of imperfect people",
      "start": 5855.55,
      "duration": 3.27
    },
    {
      "text": "trying to aim imperfectly at some ideal",
      "start": 5858.82,
      "duration": 2.13
    },
    {
      "text": "that will never perfectly be achieved.",
      "start": 5860.95,
      "duration": 2.34
    },
    {
      "text": "That's what you sign up for,",
      "start": 5863.29,
      "duration": 1.14
    },
    {
      "text": "that's what it will always be.",
      "start": 5864.43,
      "duration": 1.53
    },
    {
      "text": "But imperfect doesn't\nmean you just give up.",
      "start": 5865.96,
      "duration": 4.05
    },
    {
      "text": "There's better and there's worse.",
      "start": 5870.01,
      "duration": 1.59
    },
    {
      "text": "And hopefully we can begin to build,",
      "start": 5871.6,
      "duration": 3.36
    },
    {
      "text": "we can do well enough\nthat we can begin to build",
      "start": 5874.96,
      "duration": 2.46
    },
    {
      "text": "some practices that the\nwhole industry engages in.",
      "start": 5877.42,
      "duration": 3.03
    },
    {
      "text": "And then, you know, my guess is that",
      "start": 5880.45,
      "duration": 1.95
    },
    {
      "text": "multiple of these companies\nwill be successful.",
      "start": 5882.4,
      "duration": 2.1
    },
    {
      "text": "Anthropic will be successful.",
      "start": 5884.5,
      "duration": 1.59
    },
    {
      "text": "These other companies,",
      "start": 5886.09,
      "duration": 0.93
    },
    {
      "text": "like ones I've been at the\npast will also be successful,",
      "start": 5887.02,
      "duration": 3.09
    },
    {
      "text": "and some will be more\nsuccessful than others.",
      "start": 5890.11,
      "duration": 2.16
    },
    {
      "text": "That's less important than, again,",
      "start": 5892.27,
      "duration": 2.22
    },
    {
      "text": "that we align the\nincentives of the industry.",
      "start": 5894.49,
      "duration": 2.22
    },
    {
      "text": "And that happens partly\nthrough the race to the top,",
      "start": 5896.71,
      "duration": 2.49
    },
    {
      "text": "partly through things like RSP,",
      "start": 5899.2,
      "duration": 1.89
    },
    {
      "text": "partly through again\nselected surgical regulation.",
      "start": 5901.09,
      "duration": 4.05
    },
    {
      "text": "- You said talent density\nbeats talent mass.",
      "start": 5905.14,
      "duration": 3.543
    },
    {
      "text": "So can you explain that?",
      "start": 5909.52,
      "duration": 1.38
    },
    {
      "text": "Can you expand on that?",
      "start": 5910.9,
      "duration": 0.833
    },
    {
      "text": "Can you just talk about what it takes",
      "start": 5911.733,
      "duration": 1.657
    },
    {
      "text": "to build a great team of AI\nresearchers and engineers?",
      "start": 5913.39,
      "duration": 4.08
    },
    {
      "text": "- This is one of these statements",
      "start": 5917.47,
      "duration": 1.2
    },
    {
      "text": "that's like more true every month.",
      "start": 5918.67,
      "duration": 2.55
    },
    {
      "text": "Every month I see this statement",
      "start": 5921.22,
      "duration": 1.2
    },
    {
      "text": "as more true than I did the month before.",
      "start": 5922.42,
      "duration": 2.13
    },
    {
      "text": "So if I were to do a thought experiment,",
      "start": 5924.55,
      "duration": 2.04
    },
    {
      "text": "let's say you have a team of 100 people",
      "start": 5926.59,
      "duration": 3.99
    },
    {
      "text": "that are super smart, motivated,",
      "start": 5930.58,
      "duration": 1.86
    },
    {
      "text": "and aligned with the mission,\nand that's your company.",
      "start": 5932.44,
      "duration": 2.16
    },
    {
      "text": "Or you can have a team of 1000 people",
      "start": 5934.6,
      "duration": 2.73
    },
    {
      "text": "where 200 people are super smart,",
      "start": 5937.33,
      "duration": 2.13
    },
    {
      "text": "super aligned with the mission,",
      "start": 5939.46,
      "duration": 1.56
    },
    {
      "text": "and then like 800 people are,",
      "start": 5941.02,
      "duration": 4.5
    },
    {
      "text": "let's just say you pick 800",
      "start": 5945.52,
      "duration": 1.29
    },
    {
      "text": "like random big tech employees,",
      "start": 5946.81,
      "duration": 2.76
    },
    {
      "text": "which would you rather have, right?",
      "start": 5949.57,
      "duration": 1.5
    },
    {
      "text": "The talent mass is greater",
      "start": 5951.07,
      "duration": 2.231
    },
    {
      "text": "in the group of 1000 people, right?",
      "start": 5953.301,
      "duration": 3.109
    },
    {
      "text": "You have even a larger number",
      "start": 5956.41,
      "duration": 2.46
    },
    {
      "text": "of incredibly talented,\nincredibly aligned,",
      "start": 5958.87,
      "duration": 2.43
    },
    {
      "text": "incredibly smart people.",
      "start": 5961.3,
      "duration": 1.503
    },
    {
      "text": "But the issue is just that if every time",
      "start": 5964.09,
      "duration": 5.0
    },
    {
      "text": "someone super talented looks around,",
      "start": 5969.22,
      "duration": 1.95
    },
    {
      "text": "they see someone else super\ntalented and super dedicated,",
      "start": 5971.17,
      "duration": 3.06
    },
    {
      "text": "that sets the tone for everything, right?",
      "start": 5974.23,
      "duration": 2.22
    },
    {
      "text": "That sets the tone for\neveryone is super inspired",
      "start": 5976.45,
      "duration": 2.61
    },
    {
      "text": "to work at the same place.",
      "start": 5979.06,
      "duration": 1.5
    },
    {
      "text": "Everyone trusts everyone else.",
      "start": 5980.56,
      "duration": 1.95
    },
    {
      "text": "If you have 1000 or 10,000 people",
      "start": 5982.51,
      "duration": 3.24
    },
    {
      "text": "and things have really regressed, right?",
      "start": 5985.75,
      "duration": 2.01
    },
    {
      "text": "You are not able to do selection",
      "start": 5987.76,
      "duration": 1.71
    },
    {
      "text": "and you're choosing random people,",
      "start": 5989.47,
      "duration": 1.83
    },
    {
      "text": "what happens is then you need\nto put a lot of processes",
      "start": 5991.3,
      "duration": 2.61
    },
    {
      "text": "and a lot of guardrails in place",
      "start": 5993.91,
      "duration": 1.81
    },
    {
      "text": "just because people don't\nfully trust each other,",
      "start": 5997.0,
      "duration": 2.18
    },
    {
      "text": "or you have to adjudicate\npolitical battles.",
      "start": 5999.18,
      "duration": 2.44
    },
    {
      "text": "Like there are so many things",
      "start": 6001.62,
      "duration": 2.01
    },
    {
      "text": "that slow down the org's\nability to operate.",
      "start": 6003.63,
      "duration": 2.55
    },
    {
      "text": "And so we're nearly 1000 people",
      "start": 6006.18,
      "duration": 1.89
    },
    {
      "text": "and you know, we've\ntried to make it so that",
      "start": 6008.07,
      "duration": 2.88
    },
    {
      "text": "as large a fraction of those 1000 people",
      "start": 6010.95,
      "duration": 2.16
    },
    {
      "text": "as possible are like super\ntalented, super skilled.",
      "start": 6013.11,
      "duration": 3.96
    },
    {
      "text": "It's one of the reasons",
      "start": 6017.07,
      "duration": 1.65
    },
    {
      "text": "we've slowed down hiring a\nlot in the last few months.",
      "start": 6018.72,
      "duration": 3.03
    },
    {
      "text": "We grew from 300 to 800, I believe,",
      "start": 6021.75,
      "duration": 2.94
    },
    {
      "text": "I think in the first seven,\neight months of the year.",
      "start": 6024.69,
      "duration": 2.94
    },
    {
      "text": "And now we've slowed down.",
      "start": 6027.63,
      "duration": 1.32
    },
    {
      "text": "We're at like, you know,\nthe last three months,",
      "start": 6028.95,
      "duration": 1.95
    },
    {
      "text": "we went from 800 to 900,\n950, something like that.",
      "start": 6030.9,
      "duration": 3.36
    },
    {
      "text": "Don't quote me on the exact numbers,",
      "start": 6034.26,
      "duration": 1.86
    },
    {
      "text": "but I think there's an\ninflection point around 1000,",
      "start": 6036.12,
      "duration": 2.804
    },
    {
      "text": "and we want to be much\nmore careful how we grow.",
      "start": 6038.924,
      "duration": 3.766
    },
    {
      "text": "Early on, and now as well, you know,",
      "start": 6042.69,
      "duration": 1.92
    },
    {
      "text": "we've hired a lot of physicists.",
      "start": 6044.61,
      "duration": 2.4
    },
    {
      "text": "You know, theoretical physicists",
      "start": 6047.01,
      "duration": 1.11
    },
    {
      "text": "can learn things really fast.",
      "start": 6048.12,
      "duration": 1.923
    },
    {
      "text": "Even more recently as we've\ncontinued to hire that,",
      "start": 6051.69,
      "duration": 2.047
    },
    {
      "text": "you know, we've really had a high bar for,",
      "start": 6053.737,
      "duration": 3.323
    },
    {
      "text": "on both the research side",
      "start": 6057.06,
      "duration": 1.29
    },
    {
      "text": "and the software engineering side",
      "start": 6058.35,
      "duration": 1.71
    },
    {
      "text": "have hired a lot of senior people,",
      "start": 6060.06,
      "duration": 1.83
    },
    {
      "text": "including folks who used to be",
      "start": 6061.89,
      "duration": 1.963
    },
    {
      "text": "at other companies in this space.",
      "start": 6063.853,
      "duration": 1.937
    },
    {
      "text": "And we've just continued\nto be very selective.",
      "start": 6065.79,
      "duration": 3.12
    },
    {
      "text": "It's very easy to go from 100 to 1000",
      "start": 6068.91,
      "duration": 3.277
    },
    {
      "text": "and 1000 to 10,000\nwithout paying attention",
      "start": 6072.187,
      "duration": 2.813
    },
    {
      "text": "to making sure everyone\nhas a unified purpose.",
      "start": 6075.0,
      "duration": 3.45
    },
    {
      "text": "It's so powerful.",
      "start": 6078.45,
      "duration": 1.32
    },
    {
      "text": "If your company consists of\na lot of different fiefdoms",
      "start": 6079.77,
      "duration": 3.81
    },
    {
      "text": "that all wanna do their own thing,",
      "start": 6083.58,
      "duration": 1.47
    },
    {
      "text": "they're all optimizing\nfor their own thing,",
      "start": 6085.05,
      "duration": 2.15
    },
    {
      "text": "it's very hard to get anything done.",
      "start": 6088.2,
      "duration": 1.8
    },
    {
      "text": "But if everyone sees the\nbroader purpose of the company,",
      "start": 6090.0,
      "duration": 3.0
    },
    {
      "text": "if there's trust and there's dedication",
      "start": 6093.0,
      "duration": 2.43
    },
    {
      "text": "to doing the right thing,\nthat is a superpower.",
      "start": 6095.43,
      "duration": 2.55
    },
    {
      "text": "That in itself, I think,",
      "start": 6097.98,
      "duration": 1.38
    },
    {
      "text": "can overcome almost\nevery other disadvantage.",
      "start": 6099.36,
      "duration": 2.49
    },
    {
      "text": "- And you know, as to\nSteve Jobs, A players.",
      "start": 6101.85,
      "duration": 2.31
    },
    {
      "text": "A players wanna look around",
      "start": 6104.16,
      "duration": 1.08
    },
    {
      "text": "and see other A players is\nanother way of saying that.",
      "start": 6105.24,
      "duration": 3.056
    },
    {
      "text": "I don't know what that\nis about human nature,",
      "start": 6108.296,
      "duration": 1.324
    },
    {
      "text": "but it is demotivating to see people",
      "start": 6109.62,
      "duration": 2.88
    },
    {
      "text": "who are not obsessively driving\ntowards a singular mission.",
      "start": 6112.5,
      "duration": 3.69
    },
    {
      "text": "And it is, on the flip side of that,",
      "start": 6116.19,
      "duration": 1.92
    },
    {
      "text": "super motivating to see that.",
      "start": 6118.11,
      "duration": 1.89
    },
    {
      "text": "It's interesting.",
      "start": 6120.0,
      "duration": 1.47
    },
    {
      "text": "What's it take to be a great AI researcher",
      "start": 6121.47,
      "duration": 3.57
    },
    {
      "text": "or engineer from everything you've seen,",
      "start": 6125.04,
      "duration": 2.25
    },
    {
      "text": "from working with so many amazing people?",
      "start": 6127.29,
      "duration": 1.68
    },
    {
      "text": "- Yeah, I think the number one quality,",
      "start": 6128.97,
      "duration": 5.0
    },
    {
      "text": "especially on the research side,",
      "start": 6134.16,
      "duration": 1.32
    },
    {
      "text": "but really both is open-mindedness.",
      "start": 6135.48,
      "duration": 2.61
    },
    {
      "text": "Sounds easy to be open-minded, right?",
      "start": 6138.09,
      "duration": 1.83
    },
    {
      "text": "You're just like, oh,\nI'm open to anything.",
      "start": 6139.92,
      "duration": 2.46
    },
    {
      "text": "But, you know, if I think about",
      "start": 6142.38,
      "duration": 2.13
    },
    {
      "text": "my own early history in\nthe Scaling Hypothesis,",
      "start": 6144.51,
      "duration": 3.033
    },
    {
      "text": "I was seeing the same\ndata others were seeing.",
      "start": 6148.65,
      "duration": 2.79
    },
    {
      "text": "I don't think I was\nlike a better programmer",
      "start": 6151.44,
      "duration": 2.58
    },
    {
      "text": "or better at coming up with research ideas",
      "start": 6154.02,
      "duration": 2.73
    },
    {
      "text": "than any of the hundreds of\npeople that I worked with.",
      "start": 6156.75,
      "duration": 2.13
    },
    {
      "text": "In some ways, I was worse.",
      "start": 6158.88,
      "duration": 2.493
    },
    {
      "text": "You know, like I've never like, you know,",
      "start": 6162.57,
      "duration": 1.92
    },
    {
      "text": "precise programming of like,\nyou know, finding the bug,",
      "start": 6164.49,
      "duration": 3.27
    },
    {
      "text": "writing the GPU kernels.",
      "start": 6167.76,
      "duration": 1.35
    },
    {
      "text": "Like, I could point you to 100 people here",
      "start": 6169.11,
      "duration": 2.04
    },
    {
      "text": "who are better at that than I am.",
      "start": 6171.15,
      "duration": 1.833
    },
    {
      "text": "But the thing that I think I did have",
      "start": 6174.27,
      "duration": 3.48
    },
    {
      "text": "that was different was\nthat I was just willing",
      "start": 6177.75,
      "duration": 3.87
    },
    {
      "text": "to look at something with new eyes, right?",
      "start": 6181.62,
      "duration": 1.92
    },
    {
      "text": "People said, oh, you know,",
      "start": 6183.54,
      "duration": 1.837
    },
    {
      "text": "\"We don't have the right algorithms yet.",
      "start": 6185.377,
      "duration": 1.643
    },
    {
      "text": "We haven't come up with the\nright way to do things.\"",
      "start": 6187.02,
      "duration": 3.087
    },
    {
      "text": "And I was just like, oh, I don't know,",
      "start": 6190.107,
      "duration": 2.223
    },
    {
      "text": "like, you know, this neural net",
      "start": 6192.33,
      "duration": 2.43
    },
    {
      "text": "has like 30 billion,\n30 million parameters.",
      "start": 6194.76,
      "duration": 2.67
    },
    {
      "text": "Like, what if we gave\nit 50 million instead?",
      "start": 6197.43,
      "duration": 1.95
    },
    {
      "text": "Like, let's plot some graphs.",
      "start": 6199.38,
      "duration": 1.74
    },
    {
      "text": "Like that basic scientific\nmindset of like,",
      "start": 6201.12,
      "duration": 3.33
    },
    {
      "text": "oh, man, like I just, like, you know,",
      "start": 6204.45,
      "duration": 2.16
    },
    {
      "text": "I see some variable that I could change.",
      "start": 6207.87,
      "duration": 2.22
    },
    {
      "text": "Like, what happens when it changes?",
      "start": 6210.09,
      "duration": 1.89
    },
    {
      "text": "Like, let's try these different things",
      "start": 6211.98,
      "duration": 2.007
    },
    {
      "text": "and like create a graph.",
      "start": 6213.987,
      "duration": 1.353
    },
    {
      "text": "For even, this was like",
      "start": 6215.34,
      "duration": 1.11
    },
    {
      "text": "the simplest thing in the world, right?",
      "start": 6216.45,
      "duration": 1.68
    },
    {
      "text": "Change the number of, you know,",
      "start": 6218.13,
      "duration": 1.29
    },
    {
      "text": "this wasn't like PhD\nlevel experimental design.",
      "start": 6219.42,
      "duration": 3.18
    },
    {
      "text": "This was like simple and stupid.",
      "start": 6222.6,
      "duration": 2.73
    },
    {
      "text": "Like, anyone could have done this",
      "start": 6225.33,
      "duration": 1.98
    },
    {
      "text": "if you just told them\nthat it was important.",
      "start": 6227.31,
      "duration": 2.7
    },
    {
      "text": "It's also not hard to understand.",
      "start": 6230.01,
      "duration": 1.47
    },
    {
      "text": "You didn't need to be\nbrilliant to come up with this.",
      "start": 6231.48,
      "duration": 2.76
    },
    {
      "text": "But you put the two things together",
      "start": 6234.24,
      "duration": 1.86
    },
    {
      "text": "and, you know, some tiny number of people,",
      "start": 6236.1,
      "duration": 2.37
    },
    {
      "text": "some single digit number of people",
      "start": 6238.47,
      "duration": 1.53
    },
    {
      "text": "have driven forward the whole\nfield by realizing this.",
      "start": 6240.0,
      "duration": 3.273
    },
    {
      "text": "And you know, it's often like that.",
      "start": 6244.11,
      "duration": 1.83
    },
    {
      "text": "If you look back at the\ndiscovery, you know,",
      "start": 6245.94,
      "duration": 1.8
    },
    {
      "text": "the discoveries in history,",
      "start": 6247.74,
      "duration": 2.37
    },
    {
      "text": "they're often like that.",
      "start": 6250.11,
      "duration": 1.217
    },
    {
      "text": "And so this open-mindedness",
      "start": 6251.327,
      "duration": 2.053
    },
    {
      "text": "and this willingness to see with new eyes",
      "start": 6253.38,
      "duration": 2.52
    },
    {
      "text": "that often comes from\nbeing newer to the field.",
      "start": 6255.9,
      "duration": 2.25
    },
    {
      "text": "Often experience is a\ndisadvantage for this.",
      "start": 6258.15,
      "duration": 3.0
    },
    {
      "text": "That is the most important thing.",
      "start": 6261.15,
      "duration": 1.23
    },
    {
      "text": "It's very hard to look for and test for.",
      "start": 6262.38,
      "duration": 2.07
    },
    {
      "text": "But I think it's the most important thing",
      "start": 6264.45,
      "duration": 1.92
    },
    {
      "text": "because when you find something,",
      "start": 6266.37,
      "duration": 2.52
    },
    {
      "text": "some really new way of\nthinking about things,",
      "start": 6268.89,
      "duration": 2.46
    },
    {
      "text": "when you have the initiative to do that,",
      "start": 6271.35,
      "duration": 1.56
    },
    {
      "text": "it's absolutely transformative.",
      "start": 6272.91,
      "duration": 1.53
    },
    {
      "text": "- And also be able to do kind\nof rapid experimentation,",
      "start": 6274.44,
      "duration": 2.82
    },
    {
      "text": "and in the face of that,\nbe open-minded and curious",
      "start": 6277.26,
      "duration": 2.85
    },
    {
      "text": "and looking at the data,\njust these fresh eyes",
      "start": 6280.11,
      "duration": 2.43
    },
    {
      "text": "and seeing what is that\nit's actually saying.",
      "start": 6282.54,
      "duration": 1.62
    },
    {
      "text": "That applies in mechanism\ninterpretability.",
      "start": 6284.16,
      "duration": 2.76
    },
    {
      "text": "- It's another example of this.",
      "start": 6286.92,
      "duration": 1.32
    },
    {
      "text": "Like some of the early work",
      "start": 6288.24,
      "duration": 1.8
    },
    {
      "text": "in mechanistic\ninterpretability, so simple,",
      "start": 6290.04,
      "duration": 2.85
    },
    {
      "text": "it's just no one thought to\ncare about this question before.",
      "start": 6292.89,
      "duration": 3.18
    },
    {
      "text": "- You said what it takes to\nbe a great AI researcher.",
      "start": 6296.07,
      "duration": 2.16
    },
    {
      "text": "Can we rewind the clock back?",
      "start": 6298.23,
      "duration": 2.19
    },
    {
      "text": "What advice would you give\nto people interested in AI?",
      "start": 6300.42,
      "duration": 2.7
    },
    {
      "text": "They're young, looking forward to,",
      "start": 6303.12,
      "duration": 1.23
    },
    {
      "text": "how can I make an impact on the world?",
      "start": 6304.35,
      "duration": 2.34
    },
    {
      "text": "- I think my number one piece of advice",
      "start": 6306.69,
      "duration": 1.95
    },
    {
      "text": "is to just start playing with the models.",
      "start": 6308.64,
      "duration": 2.79
    },
    {
      "text": "This was actually, I worry a little,",
      "start": 6311.43,
      "duration": 2.16
    },
    {
      "text": "this seems like obvious advice now.",
      "start": 6313.59,
      "duration": 1.8
    },
    {
      "text": "I think three years ago,",
      "start": 6315.39,
      "duration": 1.32
    },
    {
      "text": "it wasn't obvious and people started by,",
      "start": 6316.71,
      "duration": 2.31
    },
    {
      "text": "oh, let me read the latest\nReinforcement Learning paper.",
      "start": 6319.02,
      "duration": 2.554
    },
    {
      "text": "Let me, you know, let me kind of, I mean,",
      "start": 6321.574,
      "duration": 2.312
    },
    {
      "text": "that was really,",
      "start": 6323.886,
      "duration": 1.434
    },
    {
      "text": "and I mean, you should do that as well.",
      "start": 6325.32,
      "duration": 2.58
    },
    {
      "text": "But now, you know, with wider availability",
      "start": 6327.9,
      "duration": 2.7
    },
    {
      "text": "of models and APIs, people\nare doing this more.",
      "start": 6330.6,
      "duration": 2.34
    },
    {
      "text": "But I think just experiential knowledge.",
      "start": 6332.94,
      "duration": 3.933
    },
    {
      "text": "These models are new artifacts",
      "start": 6337.92,
      "duration": 2.31
    },
    {
      "text": "that no one really understands,",
      "start": 6340.23,
      "duration": 1.95
    },
    {
      "text": "and so getting experience\nplaying with them.",
      "start": 6342.18,
      "duration": 2.73
    },
    {
      "text": "I would also say, again,\nin line with the like,",
      "start": 6344.91,
      "duration": 2.58
    },
    {
      "text": "do something new, think\nin some new direction.",
      "start": 6347.49,
      "duration": 2.61
    },
    {
      "text": "Like there are all these things\nthat haven't been explored.",
      "start": 6350.1,
      "duration": 3.21
    },
    {
      "text": "Like for example,",
      "start": 6353.31,
      "duration": 0.833
    },
    {
      "text": "mechanistic interpretability\nis still very new.",
      "start": 6354.143,
      "duration": 2.617
    },
    {
      "text": "It's probably better to work on that",
      "start": 6356.76,
      "duration": 1.65
    },
    {
      "text": "than it is to work on\nnew model architectures",
      "start": 6358.41,
      "duration": 2.22
    },
    {
      "text": "because, you know, it's more\npopular than it was before.",
      "start": 6360.63,
      "duration": 2.85
    },
    {
      "text": "There are probably like\n100 people working on it,",
      "start": 6363.48,
      "duration": 1.71
    },
    {
      "text": "but there aren't like\n10,000 people working on it.",
      "start": 6365.19,
      "duration": 2.4
    },
    {
      "text": "And it's just this fertile area for study.",
      "start": 6367.59,
      "duration": 4.11
    },
    {
      "text": "Like, you know, there's so\nmuch like low hanging fruit.",
      "start": 6371.7,
      "duration": 5.0
    },
    {
      "text": "You can just walk by and, you know,",
      "start": 6377.85,
      "duration": 1.043
    },
    {
      "text": "you can just walk by\nand you can pick things.",
      "start": 6378.893,
      "duration": 2.5
    },
    {
      "text": "And the only reason, for whatever reason,",
      "start": 6382.23,
      "duration": 2.79
    },
    {
      "text": "people aren't interested in it enough.",
      "start": 6385.02,
      "duration": 2.37
    },
    {
      "text": "I think there are some things around",
      "start": 6387.39,
      "duration": 1.84
    },
    {
      "text": "long horizon learning\nand long horizon tasks",
      "start": 6390.54,
      "duration": 2.91
    },
    {
      "text": "where there's a lot to be done.",
      "start": 6393.45,
      "duration": 1.53
    },
    {
      "text": "I think evaluations are still,",
      "start": 6394.98,
      "duration": 1.83
    },
    {
      "text": "we're still very early in our\nability to study evaluations,",
      "start": 6396.81,
      "duration": 2.73
    },
    {
      "text": "particularly for dynamic\nsystems acting in the world.",
      "start": 6399.54,
      "duration": 3.12
    },
    {
      "text": "I think there's some\nstuff around multi-agent.",
      "start": 6402.66,
      "duration": 2.523
    },
    {
      "text": "Skate where the puck\nis going is my advice.",
      "start": 6406.74,
      "duration": 2.97
    },
    {
      "text": "And you don't have to be\nbrilliant to think of it.",
      "start": 6409.71,
      "duration": 1.77
    },
    {
      "text": "Like all the things that\nare gonna be exciting",
      "start": 6411.48,
      "duration": 2.4
    },
    {
      "text": "in five years, like people\neven mention them as like,",
      "start": 6413.88,
      "duration": 3.93
    },
    {
      "text": "you know, conventional wisdom, but like,",
      "start": 6417.81,
      "duration": 2.07
    },
    {
      "text": "it's just somehow\nthere's this barrier that",
      "start": 6419.88,
      "duration": 3.18
    },
    {
      "text": "people don't double down\nas much as they could,",
      "start": 6423.06,
      "duration": 2.61
    },
    {
      "text": "or they're afraid to do something",
      "start": 6425.67,
      "duration": 1.35
    },
    {
      "text": "that's not the popular thing.",
      "start": 6427.02,
      "duration": 1.65
    },
    {
      "text": "I don't know why it happens,",
      "start": 6428.67,
      "duration": 1.2
    },
    {
      "text": "but like, getting over that barrier,",
      "start": 6429.87,
      "duration": 2.01
    },
    {
      "text": "that's the my number one piece of advice.",
      "start": 6431.88,
      "duration": 2.49
    },
    {
      "text": "- Let's talk if we could\na bit about post-training.",
      "start": 6434.37,
      "duration": 2.13
    },
    {
      "text": "So it seems that the\nmodern post-training recipe",
      "start": 6436.5,
      "duration": 4.86
    },
    {
      "text": "has a little bit of everything.",
      "start": 6441.36,
      "duration": 2.07
    },
    {
      "text": "So supervised fine tuning, RLHF,",
      "start": 6443.43,
      "duration": 2.553
    },
    {
      "text": "the Constitutional AI with RLAIF.",
      "start": 6447.267,
      "duration": 4.983
    },
    {
      "text": "- Best acronym.",
      "start": 6452.25,
      "duration": 1.08
    },
    {
      "text": "- It's, again, that name thing.",
      "start": 6453.33,
      "duration": 0.874
    },
    {
      "text": "- [Dario] RLAIF.",
      "start": 6454.204,
      "duration": 1.203
    },
    {
      "text": "(both laughing)",
      "start": 6455.407,
      "duration": 1.913
    },
    {
      "text": "- And then synthetic data,",
      "start": 6457.32,
      "duration": 1.41
    },
    {
      "text": "seems like a lot of synthetic data,",
      "start": 6458.73,
      "duration": 1.44
    },
    {
      "text": "or at least trying to figure out ways",
      "start": 6460.17,
      "duration": 1.71
    },
    {
      "text": "to have high quality synthetic data.",
      "start": 6461.88,
      "duration": 1.41
    },
    {
      "text": "So what's the, if this is a secret sauce",
      "start": 6463.29,
      "duration": 3.15
    },
    {
      "text": "that makes Anthropic Claude so incredible,",
      "start": 6466.44,
      "duration": 3.93
    },
    {
      "text": "how much of the magic\nis in the pre-training?",
      "start": 6470.37,
      "duration": 2.31
    },
    {
      "text": "How much of is in the post-training?",
      "start": 6472.68,
      "duration": 1.38
    },
    {
      "text": "- Yeah, I mean, so first of all,",
      "start": 6474.06,
      "duration": 2.04
    },
    {
      "text": "we're not perfectly able\nto measure that ourselves.",
      "start": 6476.1,
      "duration": 1.98
    },
    {
      "text": "- [Lex] True.",
      "start": 6478.08,
      "duration": 1.348
    },
    {
      "text": "- You know, when you see\nsome great character ability,",
      "start": 6479.428,
      "duration": 2.552
    },
    {
      "text": "sometimes it's hard to tell",
      "start": 6481.98,
      "duration": 0.96
    },
    {
      "text": "whether it came from\npre-training or post-training.",
      "start": 6482.94,
      "duration": 2.73
    },
    {
      "text": "We've developed ways to try",
      "start": 6485.67,
      "duration": 1.53
    },
    {
      "text": "and distinguish between those two",
      "start": 6487.2,
      "duration": 1.5
    },
    {
      "text": "but they're not perfect.",
      "start": 6488.7,
      "duration": 1.44
    },
    {
      "text": "You know, the second thing\nI would say is, you know,",
      "start": 6490.14,
      "duration": 2.34
    },
    {
      "text": "when there is an advantage,",
      "start": 6492.48,
      "duration": 0.99
    },
    {
      "text": "and I think we've been pretty good",
      "start": 6493.47,
      "duration": 2.113
    },
    {
      "text": "in general at RL, perhaps the best.",
      "start": 6495.583,
      "duration": 2.353
    },
    {
      "text": "Although I don't know 'cause I don't see",
      "start": 6497.936,
      "duration": 1.594
    },
    {
      "text": "what goes on inside other companies.",
      "start": 6499.53,
      "duration": 1.893
    },
    {
      "text": "Usually it isn't, oh my God,",
      "start": 6503.7,
      "duration": 1.56
    },
    {
      "text": "we have this secret magic method",
      "start": 6505.26,
      "duration": 1.41
    },
    {
      "text": "that others don't have, right?",
      "start": 6506.67,
      "duration": 1.41
    },
    {
      "text": "Usually it's like, well, you know,",
      "start": 6508.08,
      "duration": 2.19
    },
    {
      "text": "we got better at the infrastructure,",
      "start": 6510.27,
      "duration": 1.65
    },
    {
      "text": "so we could run it for longer.",
      "start": 6511.92,
      "duration": 1.41
    },
    {
      "text": "Or, you know, we were able\nto get higher quality data,",
      "start": 6513.33,
      "duration": 2.58
    },
    {
      "text": "or we were able to filter our data better,",
      "start": 6515.91,
      "duration": 2.1
    },
    {
      "text": "or we were able to, you know,",
      "start": 6518.01,
      "duration": 1.14
    },
    {
      "text": "combine these methods in practice.",
      "start": 6519.15,
      "duration": 1.89
    },
    {
      "text": "It's usually some boring matter",
      "start": 6521.04,
      "duration": 1.98
    },
    {
      "text": "of kind of practiced and trade craft.",
      "start": 6523.02,
      "duration": 4.653
    },
    {
      "text": "So, you know, when I think about",
      "start": 6528.51,
      "duration": 1.56
    },
    {
      "text": "how to do something special",
      "start": 6530.07,
      "duration": 1.29
    },
    {
      "text": "in terms of how we train these\nmodels, both pre-training,",
      "start": 6531.36,
      "duration": 2.22
    },
    {
      "text": "but even more so post-training, you know,",
      "start": 6533.58,
      "duration": 3.003
    },
    {
      "text": "I really think of it a little more, again,",
      "start": 6537.421,
      "duration": 2.129
    },
    {
      "text": "as like designing airplanes or cars.",
      "start": 6539.55,
      "duration": 2.37
    },
    {
      "text": "Like, you know, it's\nnot just like, oh, man,",
      "start": 6541.92,
      "duration": 1.95
    },
    {
      "text": "I have the blueprint.",
      "start": 6543.87,
      "duration": 1.11
    },
    {
      "text": "Like maybe that makes you\nmake the next airplane.",
      "start": 6544.98,
      "duration": 1.71
    },
    {
      "text": "But like, there's some\ncultural trade craft",
      "start": 6546.69,
      "duration": 3.96
    },
    {
      "text": "of how we think about the design process",
      "start": 6550.65,
      "duration": 2.34
    },
    {
      "text": "that I think is more\nimportant than, you know,",
      "start": 6552.99,
      "duration": 2.103
    },
    {
      "text": "than any particular gizmo\nwe're able to invent.",
      "start": 6555.093,
      "duration": 2.757
    },
    {
      "text": "- Okay, well, let me ask you\nabout specific techniques.",
      "start": 6557.85,
      "duration": 2.61
    },
    {
      "text": "So first on RLHF, why do\nyou think, just zooming out,",
      "start": 6560.46,
      "duration": 3.9
    },
    {
      "text": "intuition almost philosophy,",
      "start": 6564.36,
      "duration": 1.56
    },
    {
      "text": "why do you think RLHF works so well?",
      "start": 6565.92,
      "duration": 2.52
    },
    {
      "text": "- If I go back to like\nthe Scaling Hypothesis,",
      "start": 6568.44,
      "duration": 3.0
    },
    {
      "text": "one of the ways to skate\nthe Scaling Hypothesis",
      "start": 6571.44,
      "duration": 2.43
    },
    {
      "text": "is if you train for X",
      "start": 6573.87,
      "duration": 1.68
    },
    {
      "text": "and you throw enough compute\nat it, then you get X.",
      "start": 6575.55,
      "duration": 3.42
    },
    {
      "text": "And so RLHF is good at doing",
      "start": 6578.97,
      "duration": 3.36
    },
    {
      "text": "what humans want the model to do,",
      "start": 6582.33,
      "duration": 1.71
    },
    {
      "text": "or at least to state it more precisely,",
      "start": 6584.04,
      "duration": 3.0
    },
    {
      "text": "doing what humans who look at the model",
      "start": 6587.04,
      "duration": 1.92
    },
    {
      "text": "for a brief period of time",
      "start": 6588.96,
      "duration": 1.32
    },
    {
      "text": "and consider different possible responses,",
      "start": 6590.28,
      "duration": 2.22
    },
    {
      "text": "what they prefer as the response,",
      "start": 6592.5,
      "duration": 1.95
    },
    {
      "text": "which is not perfect from both the safety",
      "start": 6594.45,
      "duration": 1.83
    },
    {
      "text": "and capabilities perspective,",
      "start": 6596.28,
      "duration": 1.71
    },
    {
      "text": "in that humans are often not\nable to perfectly identify",
      "start": 6597.99,
      "duration": 3.54
    },
    {
      "text": "what the model wants,",
      "start": 6601.53,
      "duration": 0.87
    },
    {
      "text": "and what humans want in\nthe moment may not be",
      "start": 6602.4,
      "duration": 1.59
    },
    {
      "text": "what they want in the long term.",
      "start": 6603.99,
      "duration": 1.23
    },
    {
      "text": "So there's a lot of subtlety there,",
      "start": 6605.22,
      "duration": 2.64
    },
    {
      "text": "but the models are good at, you know,",
      "start": 6607.86,
      "duration": 2.88
    },
    {
      "text": "producing what the humans\nin some shallow sense want.",
      "start": 6610.74,
      "duration": 3.363
    },
    {
      "text": "And it actually turns out\nthat you don't even have",
      "start": 6615.03,
      "duration": 2.4
    },
    {
      "text": "to throw that much compute at\nit because of another thing,",
      "start": 6617.43,
      "duration": 3.15
    },
    {
      "text": "which is this thing about\na strong pre-trained model",
      "start": 6620.58,
      "duration": 3.54
    },
    {
      "text": "being halfway to anywhere.",
      "start": 6624.12,
      "duration": 1.323
    },
    {
      "text": "So once you have the pre-trained model,",
      "start": 6626.7,
      "duration": 1.95
    },
    {
      "text": "you have all the representations you need",
      "start": 6628.65,
      "duration": 2.05
    },
    {
      "text": "to get the model where you want it to go.",
      "start": 6631.904,
      "duration": 2.056
    },
    {
      "text": "- So do you think RLHF\nmakes the model smarter",
      "start": 6633.96,
      "duration": 4.89
    },
    {
      "text": "or just appears smarter to the humans?",
      "start": 6638.85,
      "duration": 2.76
    },
    {
      "text": "- I don't think it\nmakes the model smarter.",
      "start": 6641.61,
      "duration": 2.43
    },
    {
      "text": "I don't think it just makes\nthe model appear smarter.",
      "start": 6644.04,
      "duration": 2.7
    },
    {
      "text": "It's like RLHF like bridges the gap",
      "start": 6646.74,
      "duration": 3.6
    },
    {
      "text": "between the human and the model, right?",
      "start": 6650.34,
      "duration": 1.86
    },
    {
      "text": "I could have something really smart",
      "start": 6652.2,
      "duration": 1.68
    },
    {
      "text": "that like can't communicate at all, right?",
      "start": 6653.88,
      "duration": 1.77
    },
    {
      "text": "We all know people like this,\npeople who are really smart,",
      "start": 6655.65,
      "duration": 2.91
    },
    {
      "text": "but that, you know,",
      "start": 6658.56,
      "duration": 0.833
    },
    {
      "text": "you can't understand what they're saying.",
      "start": 6659.393,
      "duration": 1.844
    },
    {
      "text": "So I think RLHF just bridges that gap.",
      "start": 6662.37,
      "duration": 3.66
    },
    {
      "text": "I think it's not the\nonly kind of RL we do,",
      "start": 6666.03,
      "duration": 2.46
    },
    {
      "text": "it's not the only kind of RL\nthat will happen in the future.",
      "start": 6668.49,
      "duration": 2.52
    },
    {
      "text": "I think RL has the potential\nto make models smarter,",
      "start": 6671.01,
      "duration": 3.33
    },
    {
      "text": "to make them reason better,\nto make them operate better,",
      "start": 6674.34,
      "duration": 3.57
    },
    {
      "text": "to make them develop new skills even.",
      "start": 6677.91,
      "duration": 2.67
    },
    {
      "text": "And perhaps that could be done, you know,",
      "start": 6680.58,
      "duration": 2.22
    },
    {
      "text": "even in some cases with human feedback.",
      "start": 6682.8,
      "duration": 1.77
    },
    {
      "text": "But the kind of RLHF we do today",
      "start": 6684.57,
      "duration": 2.43
    },
    {
      "text": "mostly doesn't do that yet,",
      "start": 6687.0,
      "duration": 1.35
    },
    {
      "text": "although we're very quickly\nstarting to be able to.",
      "start": 6688.35,
      "duration": 2.25
    },
    {
      "text": "- But it appears to sort of increase,",
      "start": 6690.6,
      "duration": 1.883
    },
    {
      "text": "if you look at the metric of helpfulness,",
      "start": 6692.483,
      "duration": 2.367
    },
    {
      "text": "it increases that.",
      "start": 6694.85,
      "duration": 1.51
    },
    {
      "text": "- It also increases, what was this word",
      "start": 6696.36,
      "duration": 3.06
    },
    {
      "text": "in Leopold's essay, unhobbling,",
      "start": 6699.42,
      "duration": 2.1
    },
    {
      "text": "where basically the models are hobbled",
      "start": 6701.52,
      "duration": 1.65
    },
    {
      "text": "and then you do various trainings\nto them to unhobble them.",
      "start": 6703.17,
      "duration": 2.82
    },
    {
      "text": "So, you know, I like that word",
      "start": 6705.99,
      "duration": 2.07
    },
    {
      "text": "'cause it's like a rare word.",
      "start": 6708.06,
      "duration": 1.08
    },
    {
      "text": "But so I think RLHF unhobbles\nthe models in some ways.",
      "start": 6709.14,
      "duration": 3.9
    },
    {
      "text": "And then there are other ways",
      "start": 6713.04,
      "duration": 1.11
    },
    {
      "text": "where that model hasn't yet been unhobbled",
      "start": 6714.15,
      "duration": 1.68
    },
    {
      "text": "and, you know, needs to unhobble.",
      "start": 6715.83,
      "duration": 2.22
    },
    {
      "text": "- If you can say in terms of cost,",
      "start": 6718.05,
      "duration": 2.76
    },
    {
      "text": "is pre-training the most expensive thing?",
      "start": 6720.81,
      "duration": 1.8
    },
    {
      "text": "Or is post-training creep up to that?",
      "start": 6722.61,
      "duration": 3.12
    },
    {
      "text": "- At the present moment,",
      "start": 6725.73,
      "duration": 1.17
    },
    {
      "text": "it is still the case that pre-training",
      "start": 6726.9,
      "duration": 2.12
    },
    {
      "text": "is the majority of the cost.",
      "start": 6729.02,
      "duration": 1.75
    },
    {
      "text": "I don't know what to expect in the future,",
      "start": 6730.77,
      "duration": 1.83
    },
    {
      "text": "but I could certainly anticipate a future",
      "start": 6732.6,
      "duration": 1.68
    },
    {
      "text": "where post-training is\nthe majority of the cost.",
      "start": 6734.28,
      "duration": 2.22
    },
    {
      "text": "- In that future you anticipate,",
      "start": 6736.5,
      "duration": 1.5
    },
    {
      "text": "would it be the humans or the AI",
      "start": 6738.0,
      "duration": 2.07
    },
    {
      "text": "that's the cost of thing\nfor the post-training?",
      "start": 6740.07,
      "duration": 2.35
    },
    {
      "text": "- I don't think you can\nscale up humans enough",
      "start": 6743.927,
      "duration": 2.113
    },
    {
      "text": "to get high quality.",
      "start": 6746.04,
      "duration": 1.68
    },
    {
      "text": "Any kind of method that relies on humans",
      "start": 6747.72,
      "duration": 2.04
    },
    {
      "text": "and uses a large amount of compute,",
      "start": 6749.76,
      "duration": 1.77
    },
    {
      "text": "it's gonna have to rely on some\nscaled superposition method,",
      "start": 6751.53,
      "duration": 3.0
    },
    {
      "text": "like you know, debate or\niterated amplification",
      "start": 6755.58,
      "duration": 3.0
    },
    {
      "text": "or something like that.",
      "start": 6758.58,
      "duration": 0.99
    },
    {
      "text": "- So on that super\ninteresting set of ideas",
      "start": 6759.57,
      "duration": 3.96
    },
    {
      "text": "around Constitutional AI,",
      "start": 6763.53,
      "duration": 1.83
    },
    {
      "text": "can you describe what it is,",
      "start": 6765.36,
      "duration": 1.5
    },
    {
      "text": "as first detailed in December 2022 paper",
      "start": 6766.86,
      "duration": 3.78
    },
    {
      "text": "and beyond that, what is it?",
      "start": 6770.64,
      "duration": 2.67
    },
    {
      "text": "- Yes, so this was from two years ago.",
      "start": 6773.31,
      "duration": 2.34
    },
    {
      "text": "The basic idea is, so we\ndescribe what RLHF is.",
      "start": 6775.65,
      "duration": 3.663
    },
    {
      "text": "You have a model and it,\nyou know, spits out two,",
      "start": 6780.96,
      "duration": 3.6
    },
    {
      "text": "you know, like you just\nsample from it twice,",
      "start": 6784.56,
      "duration": 2.04
    },
    {
      "text": "it spits out two possible responses,",
      "start": 6786.6,
      "duration": 1.5
    },
    {
      "text": "and you're like, \"Human, which\nresponse do you like better?\"",
      "start": 6788.1,
      "duration": 2.55
    },
    {
      "text": "Or another variant of it is,",
      "start": 6790.65,
      "duration": 1.507
    },
    {
      "text": "\"Rate this response on a\nscale of one to seven.\"",
      "start": 6792.157,
      "duration": 2.633
    },
    {
      "text": "So that's hard because you need",
      "start": 6794.79,
      "duration": 1.32
    },
    {
      "text": "to scale up human interaction\nand it's very implicit, right?",
      "start": 6796.11,
      "duration": 3.63
    },
    {
      "text": "I don't have a sense of\nwhat I want the model to do.",
      "start": 6799.74,
      "duration": 2.61
    },
    {
      "text": "I just have a sense of\nlike what this average",
      "start": 6802.35,
      "duration": 2.37
    },
    {
      "text": "of a 1000 humans wants the model to do.",
      "start": 6804.72,
      "duration": 2.94
    },
    {
      "text": "So two ideas.",
      "start": 6807.66,
      "duration": 0.96
    },
    {
      "text": "One is, could the AI system itself",
      "start": 6808.62,
      "duration": 3.36
    },
    {
      "text": "decide which response is better, right?",
      "start": 6811.98,
      "duration": 3.42
    },
    {
      "text": "Could you show the AI\nsystem these two responses",
      "start": 6815.4,
      "duration": 2.67
    },
    {
      "text": "and ask which response is better?",
      "start": 6818.07,
      "duration": 2.49
    },
    {
      "text": "And then second, well, what\ncriterion should the AI use?",
      "start": 6820.56,
      "duration": 3.3
    },
    {
      "text": "And so then there's this idea,",
      "start": 6823.86,
      "duration": 1.53
    },
    {
      "text": "'cause you have a single document,",
      "start": 6825.39,
      "duration": 1.53
    },
    {
      "text": "a constitution, if you will, that says,",
      "start": 6826.92,
      "duration": 2.61
    },
    {
      "text": "these are the principles the model",
      "start": 6829.53,
      "duration": 1.35
    },
    {
      "text": "should be using to respond.",
      "start": 6830.88,
      "duration": 1.65
    },
    {
      "text": "And the AI system reads those,",
      "start": 6832.53,
      "duration": 2.493
    },
    {
      "text": "it reads those principles,",
      "start": 6835.98,
      "duration": 2.07
    },
    {
      "text": "as well as reading the\nenvironment and the response.",
      "start": 6838.05,
      "duration": 3.09
    },
    {
      "text": "And it says, well, how\ngood did the AI model do?",
      "start": 6841.14,
      "duration": 3.3
    },
    {
      "text": "It's basically a form of self play.",
      "start": 6844.44,
      "duration": 1.77
    },
    {
      "text": "You're kind of training\nthe model against itself.",
      "start": 6846.21,
      "duration": 2.757
    },
    {
      "text": "And so the AI gives the response",
      "start": 6848.967,
      "duration": 1.983
    },
    {
      "text": "and then you feed that back",
      "start": 6850.95,
      "duration": 1.62
    },
    {
      "text": "into what's called the preference model,",
      "start": 6852.57,
      "duration": 1.65
    },
    {
      "text": "which in turn feeds the\nmodel to make it better.",
      "start": 6854.22,
      "duration": 3.06
    },
    {
      "text": "So you have this triangle of like the AI,",
      "start": 6857.28,
      "duration": 2.43
    },
    {
      "text": "the preference model, and the\nimprovement of the AI itself.",
      "start": 6859.71,
      "duration": 2.94
    },
    {
      "text": "- And we should say that\nin the constitution,",
      "start": 6862.65,
      "duration": 1.83
    },
    {
      "text": "the set of principles are\nlike human interpretable.",
      "start": 6864.48,
      "duration": 2.58
    },
    {
      "text": "They're like-\n- Yeah, yeah.",
      "start": 6867.06,
      "duration": 1.08
    },
    {
      "text": "It's something both the human\nand the AI system can read.",
      "start": 6868.14,
      "duration": 3.27
    },
    {
      "text": "So it has this nice kind of\ntranslatability or symmetry.",
      "start": 6871.41,
      "duration": 3.873
    },
    {
      "text": "You know, in practice we\nboth use a model constitution",
      "start": 6876.27,
      "duration": 2.94
    },
    {
      "text": "and we use RLHF and we use\nsome of these other methods.",
      "start": 6879.21,
      "duration": 3.3
    },
    {
      "text": "So it's turned into one tool in a toolkit",
      "start": 6882.51,
      "duration": 4.11
    },
    {
      "text": "that both reduces the need for RLHF",
      "start": 6886.62,
      "duration": 2.79
    },
    {
      "text": "and increases the value we get",
      "start": 6889.41,
      "duration": 1.87
    },
    {
      "text": "from using each data point of RLHF.",
      "start": 6892.488,
      "duration": 2.742
    },
    {
      "text": "It also interacts in interesting ways",
      "start": 6895.23,
      "duration": 1.65
    },
    {
      "text": "with kind of future\nreasoning type RL methods.",
      "start": 6896.88,
      "duration": 2.94
    },
    {
      "text": "So it's one tool in the toolkit,",
      "start": 6899.82,
      "duration": 2.82
    },
    {
      "text": "but I think it is a very important tool.",
      "start": 6902.64,
      "duration": 2.58
    },
    {
      "text": "- Well, it's a compelling\none to us humans.",
      "start": 6905.22,
      "duration": 2.58
    },
    {
      "text": "You know, thinking about\nthe founding fathers",
      "start": 6907.8,
      "duration": 1.62
    },
    {
      "text": "and the founding of the United States,",
      "start": 6909.42,
      "duration": 1.9
    },
    {
      "text": "the natural question is,",
      "start": 6912.57,
      "duration": 1.08
    },
    {
      "text": "who and how do you think it\ngets to define the constitution,",
      "start": 6913.65,
      "duration": 4.77
    },
    {
      "text": "the set of principles in the constitution?",
      "start": 6918.42,
      "duration": 1.95
    },
    {
      "text": "- Yeah, so I'll give\nlike a practical answer",
      "start": 6920.37,
      "duration": 2.94
    },
    {
      "text": "and a more abstract answer.",
      "start": 6923.31,
      "duration": 1.35
    },
    {
      "text": "I think the practical\nanswer is like, look,",
      "start": 6924.66,
      "duration": 1.59
    },
    {
      "text": "in practice models get used by all kinds",
      "start": 6926.25,
      "duration": 2.61
    },
    {
      "text": "of different like customers, right?",
      "start": 6928.86,
      "duration": 2.13
    },
    {
      "text": "And so you can have this\nidea where, you know,",
      "start": 6930.99,
      "duration": 3.12
    },
    {
      "text": "the model can have specialized\nrules or principles.",
      "start": 6934.11,
      "duration": 3.03
    },
    {
      "text": "You know, we fine tune\nversions of models implicitly.",
      "start": 6937.14,
      "duration": 3.18
    },
    {
      "text": "We've talked about doing it explicitly,",
      "start": 6940.32,
      "duration": 1.59
    },
    {
      "text": "having special principles that people",
      "start": 6941.91,
      "duration": 2.31
    },
    {
      "text": "can build into the models.",
      "start": 6944.22,
      "duration": 1.683
    },
    {
      "text": "So from a practical perspective,",
      "start": 6947.46,
      "duration": 1.59
    },
    {
      "text": "the answer can be very\ndifferent from different people.",
      "start": 6949.05,
      "duration": 2.64
    },
    {
      "text": "You know, customer\nservice agent, you know,",
      "start": 6951.69,
      "duration": 1.92
    },
    {
      "text": "behaves very differently from a lawyer",
      "start": 6953.61,
      "duration": 1.47
    },
    {
      "text": "and obeys different principles.",
      "start": 6955.08,
      "duration": 2.37
    },
    {
      "text": "But I think at the base of it,\nthere are specific principles",
      "start": 6957.45,
      "duration": 3.18
    },
    {
      "text": "that models, you know, have to obey.",
      "start": 6960.63,
      "duration": 3.06
    },
    {
      "text": "I think a lot of them are things",
      "start": 6963.69,
      "duration": 0.833
    },
    {
      "text": "that people would agree with.",
      "start": 6964.523,
      "duration": 1.717
    },
    {
      "text": "Everyone agrees that, you know,",
      "start": 6966.24,
      "duration": 1.89
    },
    {
      "text": "we don't want models to\npresent these CBRN risks.",
      "start": 6968.13,
      "duration": 4.05
    },
    {
      "text": "I think we can go a little further",
      "start": 6972.18,
      "duration": 1.35
    },
    {
      "text": "and agree with some basic principles",
      "start": 6973.53,
      "duration": 2.13
    },
    {
      "text": "of democracy in the rule of law.",
      "start": 6975.66,
      "duration": 2.04
    },
    {
      "text": "Beyond that, it gets,\nyou know, very uncertain,",
      "start": 6977.7,
      "duration": 2.37
    },
    {
      "text": "and there, our goal is\ngenerally for the models",
      "start": 6980.07,
      "duration": 2.28
    },
    {
      "text": "to be more neutral,",
      "start": 6982.35,
      "duration": 2.13
    },
    {
      "text": "to not espouse a particular point of view,",
      "start": 6984.48,
      "duration": 2.4
    },
    {
      "text": "and, you know, more just\nbe kind of like wise agents",
      "start": 6986.88,
      "duration": 4.47
    },
    {
      "text": "or advisors that will help\nyou think things through",
      "start": 6991.35,
      "duration": 2.46
    },
    {
      "text": "and will, you know, present\npossible considerations,",
      "start": 6993.81,
      "duration": 3.39
    },
    {
      "text": "but, you know, don't express,",
      "start": 6997.2,
      "duration": 1.56
    },
    {
      "text": "you know, strong or specific opinions.",
      "start": 6998.76,
      "duration": 2.867
    },
    {
      "text": "- OpenAI released a model spec",
      "start": 7001.627,
      "duration": 2.203
    },
    {
      "text": "where it kind of clearly,\nconcretely defines",
      "start": 7003.83,
      "duration": 2.76
    },
    {
      "text": "some of the goals of the model,",
      "start": 7006.59,
      "duration": 2.31
    },
    {
      "text": "and specific examples, like A/B,",
      "start": 7008.9,
      "duration": 3.09
    },
    {
      "text": "how the model should behave.",
      "start": 7011.99,
      "duration": 1.32
    },
    {
      "text": "Do you find that interesting?",
      "start": 7013.31,
      "duration": 1.32
    },
    {
      "text": "By the way, I should mention,",
      "start": 7014.63,
      "duration": 1.29
    },
    {
      "text": "I believe the brilliant John\nSchulman was a part of that.",
      "start": 7015.92,
      "duration": 3.51
    },
    {
      "text": "He's now at Anthropic.",
      "start": 7019.43,
      "duration": 1.233
    },
    {
      "text": "Do you think this is a useful direction?",
      "start": 7022.1,
      "duration": 1.35
    },
    {
      "text": "Might Anthropic release\na model spec as well?",
      "start": 7023.45,
      "duration": 2.1
    },
    {
      "text": "- Yeah, so I think that's\na pretty useful direction.",
      "start": 7025.55,
      "duration": 2.58
    },
    {
      "text": "Again, it has a lot in common\nwith Constitutional AI.",
      "start": 7028.13,
      "duration": 3.48
    },
    {
      "text": "So again, another example of\nlike a race to the top, right?",
      "start": 7031.61,
      "duration": 3.36
    },
    {
      "text": "We have something that's like we think,",
      "start": 7034.97,
      "duration": 2.25
    },
    {
      "text": "you know, a better and more\nresponsible way of doing things.",
      "start": 7037.22,
      "duration": 3.3
    },
    {
      "text": "It's also a competitive advantage.",
      "start": 7040.52,
      "duration": 2.4
    },
    {
      "text": "Then others kind of, you know,",
      "start": 7042.92,
      "duration": 2.13
    },
    {
      "text": "discover that it has advantages",
      "start": 7045.05,
      "duration": 1.68
    },
    {
      "text": "and then start to do that thing.",
      "start": 7046.73,
      "duration": 2.22
    },
    {
      "text": "We then no longer have\nthe competitive advantage,",
      "start": 7048.95,
      "duration": 2.25
    },
    {
      "text": "but it's good from the perspective",
      "start": 7051.2,
      "duration": 1.32
    },
    {
      "text": "that now everyone has\nadopted a positive practice",
      "start": 7052.52,
      "duration": 3.72
    },
    {
      "text": "that others were not adopting.",
      "start": 7056.24,
      "duration": 1.41
    },
    {
      "text": "And so our response to that is, well,",
      "start": 7057.65,
      "duration": 1.89
    },
    {
      "text": "looks like we need a new\ncompetitive advantage",
      "start": 7059.54,
      "duration": 1.83
    },
    {
      "text": "in order to keep driving\nthis race upwards.",
      "start": 7061.37,
      "duration": 2.46
    },
    {
      "text": "So that's how I generally feel about that.",
      "start": 7063.83,
      "duration": 2.16
    },
    {
      "text": "I also think every implementation",
      "start": 7065.99,
      "duration": 1.74
    },
    {
      "text": "of these things is different.",
      "start": 7067.73,
      "duration": 1.14
    },
    {
      "text": "So, you know, there were some things",
      "start": 7068.87,
      "duration": 1.65
    },
    {
      "text": "in the model spec that were\nnot in Constitutional AI,",
      "start": 7070.52,
      "duration": 2.82
    },
    {
      "text": "and so, you know, we can\nalways adopt those things",
      "start": 7073.34,
      "duration": 3.54
    },
    {
      "text": "or, you know, at least learn from them.",
      "start": 7076.88,
      "duration": 2.07
    },
    {
      "text": "So again, I think this is an example",
      "start": 7078.95,
      "duration": 1.65
    },
    {
      "text": "of like the positive dynamic",
      "start": 7080.6,
      "duration": 1.4
    },
    {
      "text": "that I think we should all\nwant the field to have.",
      "start": 7083.735,
      "duration": 2.415
    },
    {
      "text": "- Let's talk about the incredible essay",
      "start": 7086.15,
      "duration": 2.407
    },
    {
      "text": "\"Machines of Loving Grace.\"",
      "start": 7088.557,
      "duration": 1.403
    },
    {
      "text": "I recommend everybody read it.",
      "start": 7089.96,
      "duration": 1.307
    },
    {
      "text": "It's a long one.",
      "start": 7091.267,
      "duration": 1.333
    },
    {
      "text": "- It is rather long.\n- Yeah.",
      "start": 7092.6,
      "duration": 1.5
    },
    {
      "text": "It's really refreshing\nto read concrete ideas",
      "start": 7094.1,
      "duration": 2.7
    },
    {
      "text": "about what a positive future looks like.",
      "start": 7096.8,
      "duration": 2.25
    },
    {
      "text": "And you took sort of a bold stance",
      "start": 7099.05,
      "duration": 1.62
    },
    {
      "text": "because like, it's very possible",
      "start": 7100.67,
      "duration": 1.53
    },
    {
      "text": "that you might be wrong on the dates",
      "start": 7102.2,
      "duration": 1.56
    },
    {
      "text": "or specific applications.\n- Oh, yeah.",
      "start": 7103.76,
      "duration": 1.44
    },
    {
      "text": "I'm fully expecting to, you know,",
      "start": 7105.2,
      "duration": 2.47
    },
    {
      "text": "will definitely be wrong\nabout all the details.",
      "start": 7107.67,
      "duration": 2.18
    },
    {
      "text": "I might be just spectacularly wrong",
      "start": 7109.85,
      "duration": 2.79
    },
    {
      "text": "about the whole thing and people will,",
      "start": 7112.64,
      "duration": 1.5
    },
    {
      "text": "you know, will laugh at me for years.",
      "start": 7114.14,
      "duration": 1.85
    },
    {
      "text": "That's just how the future works. (laughs)",
      "start": 7117.5,
      "duration": 2.64
    },
    {
      "text": "- So you provided a bunch of concrete",
      "start": 7120.14,
      "duration": 1.77
    },
    {
      "text": "positive impacts of AI and how, you know,",
      "start": 7121.91,
      "duration": 3.15
    },
    {
      "text": "exactly a super intelligent\nAI might accelerate",
      "start": 7125.06,
      "duration": 2.34
    },
    {
      "text": "the rate of breakthroughs in, for example,",
      "start": 7127.4,
      "duration": 1.83
    },
    {
      "text": "biology and chemistry that would then lead",
      "start": 7129.23,
      "duration": 2.97
    },
    {
      "text": "to things like we cure most cancers,",
      "start": 7132.2,
      "duration": 2.91
    },
    {
      "text": "prevent all infectious disease,",
      "start": 7135.11,
      "duration": 2.52
    },
    {
      "text": "double the human lifespan and so on.",
      "start": 7137.63,
      "duration": 2.88
    },
    {
      "text": "So let's talk about this essay.",
      "start": 7140.51,
      "duration": 1.35
    },
    {
      "text": "First, can you give a high\nlevel vision of this essay",
      "start": 7141.86,
      "duration": 3.63
    },
    {
      "text": "and what key takeaways\nthat people would have?",
      "start": 7145.49,
      "duration": 3.42
    },
    {
      "text": "- Yeah, I have spent a lot of time,",
      "start": 7148.91,
      "duration": 1.707
    },
    {
      "text": "and Anthropic has spent a lot\nof effort on like, you know,",
      "start": 7150.617,
      "duration": 2.463
    },
    {
      "text": "how do we address the risks of AI, right?",
      "start": 7153.08,
      "duration": 2.49
    },
    {
      "text": "How do we think about those risks?",
      "start": 7155.57,
      "duration": 1.44
    },
    {
      "text": "Like we're trying to do a\nrace to the top, you know,",
      "start": 7157.01,
      "duration": 2.64
    },
    {
      "text": "that requires us to build\nall these capabilities",
      "start": 7159.65,
      "duration": 2.25
    },
    {
      "text": "and the capabilities are cool,",
      "start": 7161.9,
      "duration": 1.62
    },
    {
      "text": "but you know, we're like,",
      "start": 7163.52,
      "duration": 3.54
    },
    {
      "text": "a big part of what we're trying to do",
      "start": 7167.06,
      "duration": 1.98
    },
    {
      "text": "is like address the risks.",
      "start": 7169.04,
      "duration": 1.44
    },
    {
      "text": "And the justification for that is like,",
      "start": 7170.48,
      "duration": 2.46
    },
    {
      "text": "well, you know, all these positive things,",
      "start": 7172.94,
      "duration": 1.777
    },
    {
      "text": "you know, the market is this\nvery healthy organism, right?",
      "start": 7174.717,
      "duration": 3.203
    },
    {
      "text": "It's gonna produce all\nthe positive things.",
      "start": 7177.92,
      "duration": 1.98
    },
    {
      "text": "The risks, I don't know,",
      "start": 7179.9,
      "duration": 1.26
    },
    {
      "text": "we might mitigate them, we might not.",
      "start": 7181.16,
      "duration": 1.47
    },
    {
      "text": "And so we can have more impact",
      "start": 7182.63,
      "duration": 1.41
    },
    {
      "text": "by trying to mitigate the risks.",
      "start": 7184.04,
      "duration": 2.01
    },
    {
      "text": "But I noticed that one flaw\nin that way of thinking,",
      "start": 7186.05,
      "duration": 4.8
    },
    {
      "text": "and it's not a change in how\nseriously I take the risks.",
      "start": 7190.85,
      "duration": 3.0
    },
    {
      "text": "It's maybe a change in\nhow I talk about them.",
      "start": 7193.85,
      "duration": 2.703
    },
    {
      "text": "Is that, you know, no\nmatter how kind of logical",
      "start": 7197.51,
      "duration": 4.83
    },
    {
      "text": "or rational that line of reasoning",
      "start": 7202.34,
      "duration": 2.79
    },
    {
      "text": "that I just gave might be,",
      "start": 7205.13,
      "duration": 1.713
    },
    {
      "text": "if you kind of only talk about risks,",
      "start": 7208.46,
      "duration": 1.98
    },
    {
      "text": "your brain only thinks about risks.",
      "start": 7210.44,
      "duration": 2.04
    },
    {
      "text": "And so I think it's\nactually very important",
      "start": 7212.48,
      "duration": 2.07
    },
    {
      "text": "to understand, what if things do go well?",
      "start": 7214.55,
      "duration": 2.13
    },
    {
      "text": "And the whole reason we're\ntrying to prevent these risks",
      "start": 7216.68,
      "duration": 1.98
    },
    {
      "text": "is not because we're afraid of technology,",
      "start": 7218.66,
      "duration": 1.8
    },
    {
      "text": "not because we wanna slow it down.",
      "start": 7220.46,
      "duration": 2.22
    },
    {
      "text": "It's because if we can get",
      "start": 7222.68,
      "duration": 3.96
    },
    {
      "text": "to the other side of these risks, right?",
      "start": 7226.64,
      "duration": 1.59
    },
    {
      "text": "If we can run the gauntlet successfully,",
      "start": 7228.23,
      "duration": 2.073
    },
    {
      "text": "you know, to put it in stark terms,",
      "start": 7231.17,
      "duration": 2.19
    },
    {
      "text": "then on the other side of the gauntlet",
      "start": 7233.36,
      "duration": 1.68
    },
    {
      "text": "are all these great things",
      "start": 7235.04,
      "duration": 1.38
    },
    {
      "text": "and these things are worth fighting for,",
      "start": 7236.42,
      "duration": 1.83
    },
    {
      "text": "and these things can\nreally inspire people.",
      "start": 7238.25,
      "duration": 2.28
    },
    {
      "text": "And I think I imagine,",
      "start": 7240.53,
      "duration": 1.95
    },
    {
      "text": "because look, you have all\nthese investors, all these VCs,",
      "start": 7242.48,
      "duration": 3.42
    },
    {
      "text": "all these AI companies talking about",
      "start": 7245.9,
      "duration": 2.25
    },
    {
      "text": "all the positive benefits of AI.",
      "start": 7248.15,
      "duration": 1.95
    },
    {
      "text": "But as you point out, it's weird,",
      "start": 7250.1,
      "duration": 2.85
    },
    {
      "text": "there's actually a dearth",
      "start": 7252.95,
      "duration": 1.23
    },
    {
      "text": "of really getting specific about it.",
      "start": 7254.18,
      "duration": 1.92
    },
    {
      "text": "There's a lot of like\nrandom people on Twitter",
      "start": 7256.1,
      "duration": 2.82
    },
    {
      "text": "like posting these kind\nof like gleaning cities,",
      "start": 7258.92,
      "duration": 3.27
    },
    {
      "text": "and this just kind of\nlike vibe of like grind,",
      "start": 7262.19,
      "duration": 3.06
    },
    {
      "text": "accelerate harder, like\nkick out the, you know,",
      "start": 7265.25,
      "duration": 3.15
    },
    {
      "text": "it's just this very like\naggressive ideological.",
      "start": 7268.4,
      "duration": 3.93
    },
    {
      "text": "But then you're like, well,",
      "start": 7272.33,
      "duration": 1.343
    },
    {
      "text": "what are you actually excited about?",
      "start": 7275.183,
      "duration": 2.577
    },
    {
      "text": "And so I figured that, you know,",
      "start": 7277.76,
      "duration": 2.481
    },
    {
      "text": "I think it would be\ninteresting and valuable",
      "start": 7280.241,
      "duration": 2.109
    },
    {
      "text": "for someone who's actually coming",
      "start": 7282.35,
      "duration": 1.32
    },
    {
      "text": "from the risk side to try,",
      "start": 7283.67,
      "duration": 2.463
    },
    {
      "text": "and to try and really\nmake a try at explaining",
      "start": 7286.997,
      "duration": 4.65
    },
    {
      "text": "what the benefits are,",
      "start": 7293.33,
      "duration": 1.92
    },
    {
      "text": "both because I think it's\nsomething we can all get behind",
      "start": 7295.25,
      "duration": 3.33
    },
    {
      "text": "and I want people to understand.",
      "start": 7298.58,
      "duration": 1.59
    },
    {
      "text": "I want them to really understand that",
      "start": 7300.17,
      "duration": 2.91
    },
    {
      "text": "this isn't doomers versus accelerationist.",
      "start": 7303.08,
      "duration": 3.538
    },
    {
      "text": "This is that, if you\nhave a true understanding",
      "start": 7306.618,
      "duration": 4.852
    },
    {
      "text": "of where things are going with with AI,",
      "start": 7311.47,
      "duration": 1.96
    },
    {
      "text": "and maybe that's the more important axis.",
      "start": 7313.43,
      "duration": 2.07
    },
    {
      "text": "AI is moving fast versus\nAI is not moving fast,",
      "start": 7315.5,
      "duration": 2.79
    },
    {
      "text": "then you really appreciate the benefits",
      "start": 7318.29,
      "duration": 2.34
    },
    {
      "text": "and you really, you want humanity,",
      "start": 7320.63,
      "duration": 3.21
    },
    {
      "text": "our civilization to seize those benefits,",
      "start": 7323.84,
      "duration": 2.31
    },
    {
      "text": "but you also get very serious",
      "start": 7326.15,
      "duration": 1.44
    },
    {
      "text": "about anything that could derail them.",
      "start": 7327.59,
      "duration": 1.59
    },
    {
      "text": "- So I think the starting point",
      "start": 7329.18,
      "duration": 1.05
    },
    {
      "text": "is to talk about what this powerful AI,",
      "start": 7330.23,
      "duration": 3.03
    },
    {
      "text": "which is the term you like to use,",
      "start": 7333.26,
      "duration": 2.46
    },
    {
      "text": "most of the world uses AGI,\nbut you don't like the term",
      "start": 7335.72,
      "duration": 2.91
    },
    {
      "text": "because it's basically\nhas too much baggage,",
      "start": 7338.63,
      "duration": 3.42
    },
    {
      "text": "it's become meaningless.",
      "start": 7342.05,
      "duration": 1.35
    },
    {
      "text": "It's like we're stuck with the terms,",
      "start": 7343.4,
      "duration": 1.92
    },
    {
      "text": "whether we like them or not.\n- Maybe we're stuck",
      "start": 7345.32,
      "duration": 1.38
    },
    {
      "text": "with the terms and my efforts\nto change them are futile.",
      "start": 7346.7,
      "duration": 2.4
    },
    {
      "text": "- It's admirable.\n- I'll tell you",
      "start": 7349.1,
      "duration": 1.05
    },
    {
      "text": "what else I don't, this is like\na pointless semantic point,",
      "start": 7350.15,
      "duration": 3.09
    },
    {
      "text": "but I keep talking about it in public-",
      "start": 7353.24,
      "duration": 2.07
    },
    {
      "text": "- Back to naming again.\n- So I'm just gonna",
      "start": 7355.31,
      "duration": 1.11
    },
    {
      "text": "do it once more.",
      "start": 7356.42,
      "duration": 0.903
    },
    {
      "text": "I think it's a little like,\nlet's say it was like 1995",
      "start": 7358.94,
      "duration": 3.81
    },
    {
      "text": "and Moore's law is making\nthe computers faster.",
      "start": 7362.75,
      "duration": 2.28
    },
    {
      "text": "And like for some reason,",
      "start": 7365.03,
      "duration": 1.743
    },
    {
      "text": "there had been this like\nverbal tick that like,",
      "start": 7367.714,
      "duration": 2.206
    },
    {
      "text": "everyone was like, well,\nsomeday we're gonna have",
      "start": 7369.92,
      "duration": 1.59
    },
    {
      "text": "like super computers\nand like supercomputers",
      "start": 7371.51,
      "duration": 2.34
    },
    {
      "text": "are gonna be able to do\nall these things that like,",
      "start": 7373.85,
      "duration": 1.92
    },
    {
      "text": "you know, once we have supercomputers,",
      "start": 7375.77,
      "duration": 1.95
    },
    {
      "text": "we'll be able to like sequence the genome,",
      "start": 7377.72,
      "duration": 1.41
    },
    {
      "text": "we'll be able to do other things.",
      "start": 7379.13,
      "duration": 1.2
    },
    {
      "text": "And so, like one, it's true,",
      "start": 7380.33,
      "duration": 1.74
    },
    {
      "text": "the computers are getting\nfaster, and as they get faster,",
      "start": 7382.07,
      "duration": 2.07
    },
    {
      "text": "they're gonna be able to\ndo all these great things.",
      "start": 7384.14,
      "duration": 2.04
    },
    {
      "text": "But there's no discreet point",
      "start": 7386.18,
      "duration": 2.31
    },
    {
      "text": "at which you had a supercomputer",
      "start": 7388.49,
      "duration": 1.8
    },
    {
      "text": "and previous computers were not.",
      "start": 7390.29,
      "duration": 1.44
    },
    {
      "text": "Like supercomputer is a term we use,",
      "start": 7391.73,
      "duration": 1.62
    },
    {
      "text": "but like, it's a vague\nterm to just describe like",
      "start": 7393.35,
      "duration": 3.33
    },
    {
      "text": "computers that are faster\nthan what we have today.",
      "start": 7396.68,
      "duration": 2.88
    },
    {
      "text": "There's no point at which\nyou pass the threshold",
      "start": 7399.56,
      "duration": 1.74
    },
    {
      "text": "and you're like, oh my God,\nwe're doing a totally new type",
      "start": 7401.3,
      "duration": 1.98
    },
    {
      "text": "of computation and new.",
      "start": 7403.28,
      "duration": 1.59
    },
    {
      "text": "And so I feel that way about AGI like,",
      "start": 7404.87,
      "duration": 2.43
    },
    {
      "text": "there's just a smooth exponential",
      "start": 7407.3,
      "duration": 1.62
    },
    {
      "text": "and like if by AGI you mean",
      "start": 7408.92,
      "duration": 2.49
    },
    {
      "text": "like AI is getting better and better,",
      "start": 7411.41,
      "duration": 2.61
    },
    {
      "text": "and like gradually, it's gonna do more",
      "start": 7414.02,
      "duration": 1.86
    },
    {
      "text": "and more of what humans do",
      "start": 7415.88,
      "duration": 1.11
    },
    {
      "text": "until it's gonna be smarter than humans,",
      "start": 7416.99,
      "duration": 1.56
    },
    {
      "text": "and then it's gonna get\nsmarter even from there",
      "start": 7418.55,
      "duration": 1.86
    },
    {
      "text": "then yes, I believe in AGI.",
      "start": 7420.41,
      "duration": 2.43
    },
    {
      "text": "But if AGI is some\ndiscreet or separate thing,",
      "start": 7422.84,
      "duration": 3.33
    },
    {
      "text": "which is the way people\noften talk about it,",
      "start": 7426.17,
      "duration": 1.293
    },
    {
      "text": "then it's kind of a meaningless buzzword.",
      "start": 7427.463,
      "duration": 2.247
    },
    {
      "text": "- Yeah, I mean, to me it's\njust sort of a platonic form",
      "start": 7429.71,
      "duration": 3.6
    },
    {
      "text": "of a powerful AI, exactly\nhow you define it.",
      "start": 7433.31,
      "duration": 1.763
    },
    {
      "text": "I mean, you define it very nicely.",
      "start": 7435.073,
      "duration": 1.747
    },
    {
      "text": "So on the intelligence axis,",
      "start": 7436.82,
      "duration": 3.513
    },
    {
      "text": "just on pure intelligence,",
      "start": 7441.29,
      "duration": 1.32
    },
    {
      "text": "it's smarter than a Nobel Prize winner,",
      "start": 7442.61,
      "duration": 2.04
    },
    {
      "text": "as you describe, across\nmost relevant disciplines.",
      "start": 7444.65,
      "duration": 2.97
    },
    {
      "text": "So, okay, that's just intelligence.",
      "start": 7447.62,
      "duration": 1.32
    },
    {
      "text": "So it's both in creativity",
      "start": 7448.94,
      "duration": 3.15
    },
    {
      "text": "and be able to generate new ideas,",
      "start": 7452.09,
      "duration": 1.47
    },
    {
      "text": "all that kind of stuff,",
      "start": 7453.56,
      "duration": 1.23
    },
    {
      "text": "in every discipline, Nobel Prize winner,",
      "start": 7454.79,
      "duration": 2.37
    },
    {
      "text": "okay, in their prime. (laughs)",
      "start": 7457.16,
      "duration": 3.42
    },
    {
      "text": "It can use every modality,",
      "start": 7460.58,
      "duration": 1.77
    },
    {
      "text": "so that's kind of self-explanatory,",
      "start": 7462.35,
      "duration": 2.25
    },
    {
      "text": "but just to operate across all\nthe modalities of the world.",
      "start": 7464.6,
      "duration": 3.66
    },
    {
      "text": "It can go off for many hours,\ndays and weeks to do tasks,",
      "start": 7468.26,
      "duration": 4.89
    },
    {
      "text": "and do its own sort of detailed planning",
      "start": 7473.15,
      "duration": 2.4
    },
    {
      "text": "and only ask you help when it's needed.",
      "start": 7475.55,
      "duration": 2.403
    },
    {
      "text": "It can use, this is\nactually kind of interesting",
      "start": 7478.97,
      "duration": 1.643
    },
    {
      "text": "because I think in the essay you said,",
      "start": 7480.613,
      "duration": 2.15
    },
    {
      "text": "I mean, again, it's a bet, that\nit's not gonna be embodied,",
      "start": 7482.763,
      "duration": 3.257
    },
    {
      "text": "but it can control embodied tools.",
      "start": 7486.02,
      "duration": 3.39
    },
    {
      "text": "So it can control tools,\nrobots, laboratory equipment.",
      "start": 7489.41,
      "duration": 3.48
    },
    {
      "text": "The resources used to train\nit can then be repurposed",
      "start": 7492.89,
      "duration": 2.73
    },
    {
      "text": "to run millions of copies of it.",
      "start": 7495.62,
      "duration": 1.53
    },
    {
      "text": "And each of those copies\nwould be independent,",
      "start": 7497.15,
      "duration": 2.7
    },
    {
      "text": "they could do their own independent work.",
      "start": 7499.85,
      "duration": 1.037
    },
    {
      "text": "So you can do the cloning",
      "start": 7500.887,
      "duration": 1.543
    },
    {
      "text": "of the intelligence system software.",
      "start": 7502.43,
      "duration": 1.14
    },
    {
      "text": "- Yeah, yeah, I mean, you might imagine",
      "start": 7503.57,
      "duration": 1.35
    },
    {
      "text": "from outside the field that like,",
      "start": 7504.92,
      "duration": 1.26
    },
    {
      "text": "there's only one of these, right?",
      "start": 7506.18,
      "duration": 0.833
    },
    {
      "text": "That like, you made it,\nyou've only made one.",
      "start": 7507.013,
      "duration": 1.807
    },
    {
      "text": "But the truth is that like,\nthe scale up is very quick.",
      "start": 7508.82,
      "duration": 3.18
    },
    {
      "text": "Like we do this today, we make a model,",
      "start": 7512.0,
      "duration": 2.34
    },
    {
      "text": "and then we deploy thousands,",
      "start": 7514.34,
      "duration": 1.62
    },
    {
      "text": "maybe tens of thousands\nof instances of it.",
      "start": 7515.96,
      "duration": 2.52
    },
    {
      "text": "I think by the time, you know,",
      "start": 7518.48,
      "duration": 1.95
    },
    {
      "text": "certainly within two to three years,",
      "start": 7520.43,
      "duration": 1.35
    },
    {
      "text": "whether we have these\nsuper powerful AIs or not,",
      "start": 7521.78,
      "duration": 2.49
    },
    {
      "text": "clusters are gonna get to the size",
      "start": 7524.27,
      "duration": 1.68
    },
    {
      "text": "where you'll be able to\ndeploy millions of these",
      "start": 7525.95,
      "duration": 2.01
    },
    {
      "text": "and they'll be, you\nknow, faster than humans.",
      "start": 7527.96,
      "duration": 2.37
    },
    {
      "text": "And so if your picture is,",
      "start": 7530.33,
      "duration": 1.29
    },
    {
      "text": "oh, we'll have one and it'll\ntake a while to make them.",
      "start": 7531.62,
      "duration": 2.61
    },
    {
      "text": "My point, there was no,",
      "start": 7534.23,
      "duration": 1.74
    },
    {
      "text": "actually you have millions\nof them right away.",
      "start": 7535.97,
      "duration": 1.92
    },
    {
      "text": "- And in general they can learn and act",
      "start": 7537.89,
      "duration": 2.65
    },
    {
      "text": "10 to 100 times faster than humans.",
      "start": 7542.21,
      "duration": 3.09
    },
    {
      "text": "So that's a really nice\ndefinition of powerful AI, okay.",
      "start": 7545.3,
      "duration": 2.28
    },
    {
      "text": "So that, but you also write",
      "start": 7547.58,
      "duration": 2.16
    },
    {
      "text": "that clearly such an\nentity would be capable",
      "start": 7549.74,
      "duration": 2.22
    },
    {
      "text": "of solving very difficult\nproblems very fast,",
      "start": 7551.96,
      "duration": 2.19
    },
    {
      "text": "but it is not trivial\nto figure out how fast.",
      "start": 7554.15,
      "duration": 2.76
    },
    {
      "text": "Two extreme positions\nboth seem false to me.",
      "start": 7556.91,
      "duration": 2.7
    },
    {
      "text": "So the singularity is on the one extreme",
      "start": 7559.61,
      "duration": 3.03
    },
    {
      "text": "and the opposite on the other extreme.",
      "start": 7562.64,
      "duration": 1.38
    },
    {
      "text": "Can you describe each of the extremes?",
      "start": 7564.02,
      "duration": 1.542
    },
    {
      "text": "- Yeah, so.\n- And why.",
      "start": 7565.562,
      "duration": 1.26
    },
    {
      "text": "- So yeah, let's describe the extreme.",
      "start": 7566.822,
      "duration": 2.058
    },
    {
      "text": "So like one extreme would be, well, look,",
      "start": 7568.88,
      "duration": 4.083
    },
    {
      "text": "you know, if we look at kind\nof evolutionary history,",
      "start": 7574.25,
      "duration": 3.18
    },
    {
      "text": "like there was this big\nacceleration where, you know,",
      "start": 7577.43,
      "duration": 2.13
    },
    {
      "text": "for hundreds of thousands of years,",
      "start": 7579.56,
      "duration": 1.38
    },
    {
      "text": "we just had like, you know,\nsingle celled organisms,",
      "start": 7580.94,
      "duration": 2.7
    },
    {
      "text": "and then we had mammals,\nand then we had apes,",
      "start": 7583.64,
      "duration": 1.86
    },
    {
      "text": "and then that quickly turned to humans.",
      "start": 7585.5,
      "duration": 1.59
    },
    {
      "text": "Humans quickly built\nindustrial civilization.",
      "start": 7587.09,
      "duration": 2.49
    },
    {
      "text": "And so this is gonna keep speeding up",
      "start": 7589.58,
      "duration": 1.95
    },
    {
      "text": "and there's no ceiling at the human level.",
      "start": 7591.53,
      "duration": 2.28
    },
    {
      "text": "Once models get much,\nmuch smarter than humans,",
      "start": 7593.81,
      "duration": 2.91
    },
    {
      "text": "they'll get really good at\nbuilding the next models,",
      "start": 7596.72,
      "duration": 2.16
    },
    {
      "text": "and you know, if you write down",
      "start": 7598.88,
      "duration": 1.65
    },
    {
      "text": "like a simple differential equation,",
      "start": 7600.53,
      "duration": 1.5
    },
    {
      "text": "like this is an exponential.",
      "start": 7602.03,
      "duration": 1.56
    },
    {
      "text": "And so what's gonna happen is that",
      "start": 7603.59,
      "duration": 3.15
    },
    {
      "text": "models will build faster models,",
      "start": 7606.74,
      "duration": 1.44
    },
    {
      "text": "models will build faster models,",
      "start": 7608.18,
      "duration": 1.29
    },
    {
      "text": "and those models will build, you know,",
      "start": 7609.47,
      "duration": 1.65
    },
    {
      "text": "nanobots that can like take over the world",
      "start": 7611.12,
      "duration": 2.16
    },
    {
      "text": "and produce much more energy",
      "start": 7613.28,
      "duration": 1.38
    },
    {
      "text": "than you could produce otherwise.",
      "start": 7614.66,
      "duration": 1.65
    },
    {
      "text": "And so if you just kind of like solve",
      "start": 7616.31,
      "duration": 2.04
    },
    {
      "text": "this abstract differential equation,",
      "start": 7618.35,
      "duration": 1.5
    },
    {
      "text": "then like five days after we, you know,",
      "start": 7619.85,
      "duration": 2.24
    },
    {
      "text": "we build the first AI",
      "start": 7622.09,
      "duration": 1.66
    },
    {
      "text": "that's more powerful than humans,",
      "start": 7623.75,
      "duration": 1.35
    },
    {
      "text": "then, you know, like\nthe world will be filled",
      "start": 7625.1,
      "duration": 2.82
    },
    {
      "text": "with these AIs and every\npossible technology",
      "start": 7627.92,
      "duration": 1.98
    },
    {
      "text": "that could be invented\nlike will be invented.",
      "start": 7629.9,
      "duration": 2.34
    },
    {
      "text": "I'm caricaturing this a little bit,",
      "start": 7632.24,
      "duration": 1.833
    },
    {
      "text": "but, you know, I think that's one extreme.",
      "start": 7635.66,
      "duration": 2.19
    },
    {
      "text": "And the reason that I think\nthat's not the case is that,",
      "start": 7637.85,
      "duration": 5.0
    },
    {
      "text": "one, I think they just neglect\nlike the laws of physics.",
      "start": 7643.04,
      "duration": 3.27
    },
    {
      "text": "Like it's only possible to do things",
      "start": 7646.31,
      "duration": 1.5
    },
    {
      "text": "so fast in the physical world.",
      "start": 7647.81,
      "duration": 1.38
    },
    {
      "text": "Like some of those loops go through,",
      "start": 7649.19,
      "duration": 2.04
    },
    {
      "text": "you know, producing faster hardware.",
      "start": 7651.23,
      "duration": 1.863
    },
    {
      "text": "It takes a long time to\nproduce faster hardware.",
      "start": 7654.08,
      "duration": 2.64
    },
    {
      "text": "Things take a long time.",
      "start": 7656.72,
      "duration": 1.83
    },
    {
      "text": "There's this issue of complexity,",
      "start": 7658.55,
      "duration": 1.59
    },
    {
      "text": "like, I think no matter how smart you are,",
      "start": 7660.14,
      "duration": 2.7
    },
    {
      "text": "like, you know, people talk about,",
      "start": 7662.84,
      "duration": 1.86
    },
    {
      "text": "oh, we can make models\nof biological systems",
      "start": 7664.7,
      "duration": 2.16
    },
    {
      "text": "that'll do everything\nthe biological systems.",
      "start": 7666.86,
      "duration": 2.04
    },
    {
      "text": "Look, I think computational\nmodeling can do a lot.",
      "start": 7668.9,
      "duration": 1.98
    },
    {
      "text": "I did a lot of computational modeling",
      "start": 7670.88,
      "duration": 1.5
    },
    {
      "text": "when I worked in biology, but like, just,",
      "start": 7672.38,
      "duration": 3.153
    },
    {
      "text": "there are a lot of things",
      "start": 7676.88,
      "duration": 1.38
    },
    {
      "text": "that you can't predict\nhow they're, you know,",
      "start": 7678.26,
      "duration": 1.86
    },
    {
      "text": "they're complex enough\nthat like just iterating,",
      "start": 7680.12,
      "duration": 3.24
    },
    {
      "text": "just running the experiment\nis gonna beat any modeling,",
      "start": 7683.36,
      "duration": 2.64
    },
    {
      "text": "no matter how smart the\nsystem doing the modeling is.",
      "start": 7686.0,
      "duration": 2.43
    },
    {
      "text": "- Well, even if it's not interacting",
      "start": 7688.43,
      "duration": 1.32
    },
    {
      "text": "with the physical world,",
      "start": 7689.75,
      "duration": 0.84
    },
    {
      "text": "just the modeling is gonna be hard?",
      "start": 7690.59,
      "duration": 2.07
    },
    {
      "text": "- Yeah, I think, well, the\nmodeling's gonna be hard",
      "start": 7692.66,
      "duration": 2.28
    },
    {
      "text": "and getting the model to match",
      "start": 7694.94,
      "duration": 2.55
    },
    {
      "text": "the physical world is gonna be hard.",
      "start": 7697.49,
      "duration": 1.32
    },
    {
      "text": "- All right, so he does have to interact",
      "start": 7698.81,
      "duration": 1.359
    },
    {
      "text": "with the physical world to verify.",
      "start": 7700.169,
      "duration": 1.221
    },
    {
      "text": "- Yeah, but it's just, you know,",
      "start": 7701.39,
      "duration": 1.49
    },
    {
      "text": "you just look at even\nthe simplest problems.",
      "start": 7702.88,
      "duration": 2.07
    },
    {
      "text": "Like, you know, I think I\ntalk about like, you know,",
      "start": 7704.95,
      "duration": 2.05
    },
    {
      "text": "the three body problem or\nsimple chaotic prediction, like,",
      "start": 7707.0,
      "duration": 3.48
    },
    {
      "text": "you know, or like predicting the economy.",
      "start": 7710.48,
      "duration": 2.46
    },
    {
      "text": "It's really hard to predict\nthe economy two years out.",
      "start": 7712.94,
      "duration": 2.82
    },
    {
      "text": "Like maybe the case is like,\nyou know, normal, you know,",
      "start": 7715.76,
      "duration": 3.39
    },
    {
      "text": "humans can predict what's gonna happen",
      "start": 7719.15,
      "duration": 1.113
    },
    {
      "text": "in the economy next quarter,\nor they can't really do that.",
      "start": 7720.263,
      "duration": 3.268
    },
    {
      "text": "Maybe a AI system that's, you know,",
      "start": 7723.531,
      "duration": 3.059
    },
    {
      "text": "a zillion times smarter\ncan only predict it",
      "start": 7726.59,
      "duration": 2.19
    },
    {
      "text": "out a year or something\ninstead of, you know.",
      "start": 7728.78,
      "duration": 2.127
    },
    {
      "text": "You have these kind of\nexponential increase",
      "start": 7730.907,
      "duration": 2.613
    },
    {
      "text": "in computer intelligence\nfor linear increase",
      "start": 7733.52,
      "duration": 3.42
    },
    {
      "text": "in ability to predict.",
      "start": 7736.94,
      "duration": 1.65
    },
    {
      "text": "Same with, again, like, you know,",
      "start": 7738.59,
      "duration": 2.58
    },
    {
      "text": "biological molecules,\nmolecules interacting.",
      "start": 7741.17,
      "duration": 3.21
    },
    {
      "text": "You don't know what's gonna happen",
      "start": 7744.38,
      "duration": 2.259
    },
    {
      "text": "when you perturb a complex system.",
      "start": 7746.639,
      "duration": 1.611
    },
    {
      "text": "You can find simple parts in it",
      "start": 7748.25,
      "duration": 1.5
    },
    {
      "text": "if you're smarter, you're better",
      "start": 7749.75,
      "duration": 1.08
    },
    {
      "text": "at finding these simple parts.",
      "start": 7750.83,
      "duration": 1.65
    },
    {
      "text": "And then I think human institutions.",
      "start": 7752.48,
      "duration": 1.95
    },
    {
      "text": "Human institutions are\njust, are really difficult.",
      "start": 7754.43,
      "duration": 3.69
    },
    {
      "text": "Like, you know, it's\nbeen hard to get people,",
      "start": 7758.12,
      "duration": 4.47
    },
    {
      "text": "I won't give specific examples,",
      "start": 7762.59,
      "duration": 1.68
    },
    {
      "text": "but it's been hard to get people to adopt",
      "start": 7764.27,
      "duration": 3.09
    },
    {
      "text": "even the technologies\nthat we've developed,",
      "start": 7767.36,
      "duration": 2.04
    },
    {
      "text": "even ones where the\ncase for their efficacy",
      "start": 7769.4,
      "duration": 2.82
    },
    {
      "text": "is very, very strong.",
      "start": 7772.22,
      "duration": 1.533
    },
    {
      "text": "You know, people have concerns.",
      "start": 7775.37,
      "duration": 2.31
    },
    {
      "text": "They think things are conspiracy theories.",
      "start": 7777.68,
      "duration": 1.74
    },
    {
      "text": "Like it's just been,\nit's been very difficult.",
      "start": 7779.42,
      "duration": 2.79
    },
    {
      "text": "It's also been very\ndifficult to get, you know,",
      "start": 7782.21,
      "duration": 3.48
    },
    {
      "text": "very simple things through\nthe regulatory system, right?",
      "start": 7785.69,
      "duration": 2.97
    },
    {
      "text": "I think, and you know, I don't\nwanna disparage anyone who,",
      "start": 7788.66,
      "duration": 4.656
    },
    {
      "text": "you know, works in regulatory\nsystems of any technology.",
      "start": 7793.316,
      "duration": 3.774
    },
    {
      "text": "There are hard trade-offs\nthey have to deal with.",
      "start": 7797.09,
      "duration": 1.89
    },
    {
      "text": "They have to save lives.",
      "start": 7798.98,
      "duration": 1.98
    },
    {
      "text": "But the system as a whole",
      "start": 7800.96,
      "duration": 1.74
    },
    {
      "text": "I think makes some obvious trade-offs",
      "start": 7802.7,
      "duration": 2.01
    },
    {
      "text": "that are very far from\nmaximizing human welfare.",
      "start": 7804.71,
      "duration": 4.29
    },
    {
      "text": "And so if we bring AI systems into this,",
      "start": 7809.0,
      "duration": 5.0
    },
    {
      "text": "you know, into these human systems,",
      "start": 7814.07,
      "duration": 4.11
    },
    {
      "text": "often the level of intelligence",
      "start": 7818.18,
      "duration": 2.25
    },
    {
      "text": "may just not be the\nlimiting factor, right?",
      "start": 7820.43,
      "duration": 2.4
    },
    {
      "text": "It just may be that it takes\na long time to do something.",
      "start": 7822.83,
      "duration": 2.88
    },
    {
      "text": "Now, if the AI system\ncircumvented all governments,",
      "start": 7825.71,
      "duration": 3.99
    },
    {
      "text": "if it just said \"I'm dictator of the world",
      "start": 7829.7,
      "duration": 1.77
    },
    {
      "text": "and I'm gonna do whatever,\"",
      "start": 7831.47,
      "duration": 1.26
    },
    {
      "text": "some of these things it could do.",
      "start": 7832.73,
      "duration": 1.2
    },
    {
      "text": "Again, the things having\nto do with complexity,",
      "start": 7833.93,
      "duration": 1.8
    },
    {
      "text": "I still think a lot of\nthings would take a while.",
      "start": 7835.73,
      "duration": 2.58
    },
    {
      "text": "I don't think it helps that the AI systems",
      "start": 7838.31,
      "duration": 1.86
    },
    {
      "text": "can produce a lot of\nenergy or go to the Moon.",
      "start": 7840.17,
      "duration": 2.7
    },
    {
      "text": "Like some people in comments responded",
      "start": 7842.87,
      "duration": 2.52
    },
    {
      "text": "to the essay saying the AI system",
      "start": 7845.39,
      "duration": 1.65
    },
    {
      "text": "can produce a lot of energy\nin smarter AI systems.",
      "start": 7847.04,
      "duration": 3.21
    },
    {
      "text": "That's missing the point.",
      "start": 7850.25,
      "duration": 1.26
    },
    {
      "text": "That kind of cycle doesn't\nsolve the key problems",
      "start": 7851.51,
      "duration": 2.97
    },
    {
      "text": "that I'm talking about here.",
      "start": 7854.48,
      "duration": 1.59
    },
    {
      "text": "So I think a bunch of people\nmissed the point there.",
      "start": 7856.07,
      "duration": 2.34
    },
    {
      "text": "But even if it were completely on the line",
      "start": 7858.41,
      "duration": 2.193
    },
    {
      "text": "and, you know, could get around",
      "start": 7860.603,
      "duration": 1.767
    },
    {
      "text": "all these human obstacles,\nit would have trouble.",
      "start": 7862.37,
      "duration": 2.04
    },
    {
      "text": "But again, if you want\nthis to be an AI system",
      "start": 7864.41,
      "duration": 2.49
    },
    {
      "text": "that doesn't take over the world,",
      "start": 7866.9,
      "duration": 0.993
    },
    {
      "text": "that doesn't destroy humanity,",
      "start": 7867.893,
      "duration": 1.89
    },
    {
      "text": "then basically, you know, it's gonna need",
      "start": 7870.658,
      "duration": 2.482
    },
    {
      "text": "to follow basic human laws, right?",
      "start": 7873.14,
      "duration": 2.43
    },
    {
      "text": "You know, if we want to\nhave an actually good world,",
      "start": 7875.57,
      "duration": 2.73
    },
    {
      "text": "like we're gonna have to have an AI system",
      "start": 7878.3,
      "duration": 2.28
    },
    {
      "text": "that interacts with humans,",
      "start": 7880.58,
      "duration": 1.71
    },
    {
      "text": "not one that kind of\ncreates its own legal system",
      "start": 7882.29,
      "duration": 2.79
    },
    {
      "text": "or disregards all the laws or all of that.",
      "start": 7885.08,
      "duration": 1.98
    },
    {
      "text": "So as inefficient as these\nprocesses are, you know,",
      "start": 7887.06,
      "duration": 4.14
    },
    {
      "text": "we're gonna have to deal with them",
      "start": 7891.2,
      "duration": 1.38
    },
    {
      "text": "because there needs to be some popular",
      "start": 7892.58,
      "duration": 2.52
    },
    {
      "text": "and democratic legitimacy",
      "start": 7895.1,
      "duration": 1.47
    },
    {
      "text": "in how these systems are rolled out.",
      "start": 7896.57,
      "duration": 1.74
    },
    {
      "text": "We can't have a small group of people",
      "start": 7898.31,
      "duration": 2.13
    },
    {
      "text": "who are developing these systems say",
      "start": 7900.44,
      "duration": 1.35
    },
    {
      "text": "this is what's best for everyone, right?",
      "start": 7901.79,
      "duration": 2.01
    },
    {
      "text": "I think it's wrong,",
      "start": 7903.8,
      "duration": 0.833
    },
    {
      "text": "and I think in practice,\nit's not gonna work anyway.",
      "start": 7904.633,
      "duration": 2.257
    },
    {
      "text": "So you put all those things together",
      "start": 7906.89,
      "duration": 1.77
    },
    {
      "text": "and, you know, we're not gonna,",
      "start": 7908.66,
      "duration": 2.433
    },
    {
      "text": "you know, change the world",
      "start": 7912.14,
      "duration": 1.11
    },
    {
      "text": "and upload everyone in five minutes.",
      "start": 7913.25,
      "duration": 2.07
    },
    {
      "text": "I just, I don't think it,",
      "start": 7915.32,
      "duration": 2.43
    },
    {
      "text": "A, I don't think it's gonna happen,",
      "start": 7917.75,
      "duration": 2.31
    },
    {
      "text": "and B, you know,",
      "start": 7920.06,
      "duration": 1.8
    },
    {
      "text": "to the extent that it could happen,",
      "start": 7921.86,
      "duration": 2.07
    },
    {
      "text": "it's not the way to lead to a good world.",
      "start": 7923.93,
      "duration": 2.28
    },
    {
      "text": "So that's on one side.",
      "start": 7926.21,
      "duration": 1.14
    },
    {
      "text": "On the other side, there's\nanother set of perspectives,",
      "start": 7927.35,
      "duration": 2.79
    },
    {
      "text": "which I have actually in\nsome ways more sympathy for,",
      "start": 7930.14,
      "duration": 3.03
    },
    {
      "text": "which is, look, we've seen",
      "start": 7933.17,
      "duration": 1.89
    },
    {
      "text": "big productivity increases before, right?",
      "start": 7935.06,
      "duration": 2.31
    },
    {
      "text": "You know, economists are familiar",
      "start": 7937.37,
      "duration": 2.49
    },
    {
      "text": "with studying the productivity increases",
      "start": 7939.86,
      "duration": 1.83
    },
    {
      "text": "that came from the computer revolution",
      "start": 7941.69,
      "duration": 1.77
    },
    {
      "text": "and internet revolution.",
      "start": 7943.46,
      "duration": 1.26
    },
    {
      "text": "And generally, those\nproductivity increases",
      "start": 7944.72,
      "duration": 2.13
    },
    {
      "text": "were underwhelming.",
      "start": 7946.85,
      "duration": 1.08
    },
    {
      "text": "They were less than you might imagine.",
      "start": 7947.93,
      "duration": 2.82
    },
    {
      "text": "There was a quote from Robert Solow,",
      "start": 7950.75,
      "duration": 1.627
    },
    {
      "text": "\"You see the computer\nrevolution everywhere",
      "start": 7952.377,
      "duration": 1.883
    },
    {
      "text": "except the productivity statistics.\"",
      "start": 7954.26,
      "duration": 1.92
    },
    {
      "text": "So why is this the case?",
      "start": 7956.18,
      "duration": 1.71
    },
    {
      "text": "People point to the structure of firms,",
      "start": 7957.89,
      "duration": 3.15
    },
    {
      "text": "the structure of enterprises.",
      "start": 7961.04,
      "duration": 2.103
    },
    {
      "text": "You know, how slow it's been to roll out",
      "start": 7965.15,
      "duration": 2.46
    },
    {
      "text": "our existing technology to\nvery poor parts of the world,",
      "start": 7967.61,
      "duration": 2.67
    },
    {
      "text": "which I talk about in the essay, right?",
      "start": 7970.28,
      "duration": 1.8
    },
    {
      "text": "How do we get these technologies",
      "start": 7972.08,
      "duration": 1.59
    },
    {
      "text": "to the poorest parts of the world",
      "start": 7973.67,
      "duration": 2.04
    },
    {
      "text": "that are behind on cell phone technology,",
      "start": 7975.71,
      "duration": 2.61
    },
    {
      "text": "computers, medicine, let alone, you know,",
      "start": 7978.32,
      "duration": 3.09
    },
    {
      "text": "newfangled AI that\nhasn't been invented yet.",
      "start": 7981.41,
      "duration": 3.0
    },
    {
      "text": "So you could have a\nperspective that's like, well,",
      "start": 7984.41,
      "duration": 2.31
    },
    {
      "text": "this is amazing technically,\nbut it's all a nothing burger.",
      "start": 7986.72,
      "duration": 2.95
    },
    {
      "text": "You know, I think Tyler Cowen,",
      "start": 7990.59,
      "duration": 2.19
    },
    {
      "text": "who wrote something in response",
      "start": 7992.78,
      "duration": 1.53
    },
    {
      "text": "to my essay, has that perspective.",
      "start": 7994.31,
      "duration": 2.16
    },
    {
      "text": "I think he thinks the radical\nchange will happen eventually,",
      "start": 7996.47,
      "duration": 2.4
    },
    {
      "text": "but he thinks it'll take 50 or 100 years.",
      "start": 7998.87,
      "duration": 2.34
    },
    {
      "text": "And you could have even\nmore static perspectives",
      "start": 8001.21,
      "duration": 3.33
    },
    {
      "text": "on the whole thing.",
      "start": 8004.54,
      "duration": 1.05
    },
    {
      "text": "I think there's some truth to it.",
      "start": 8005.59,
      "duration": 1.2
    },
    {
      "text": "I think the timescale is just too long.",
      "start": 8006.79,
      "duration": 3.24
    },
    {
      "text": "And I can see it, I can actually see",
      "start": 8010.03,
      "duration": 2.07
    },
    {
      "text": "both sides with today's AI.",
      "start": 8012.1,
      "duration": 2.88
    },
    {
      "text": "So, you know, a lot of our\ncustomers are large enterprises",
      "start": 8014.98,
      "duration": 4.05
    },
    {
      "text": "who are used to doing\nthings a certain way.",
      "start": 8019.03,
      "duration": 2.67
    },
    {
      "text": "I've also seen it in talking\nto governments, right?",
      "start": 8021.7,
      "duration": 2.55
    },
    {
      "text": "Those are prototypical,\nyou know, institutions,",
      "start": 8024.25,
      "duration": 3.24
    },
    {
      "text": "entities that are slow to change.",
      "start": 8027.49,
      "duration": 1.833
    },
    {
      "text": "But the dynamic I see over\nand over again is, yes,",
      "start": 8030.22,
      "duration": 3.45
    },
    {
      "text": "it takes a long time to move the ship.",
      "start": 8033.67,
      "duration": 2.22
    },
    {
      "text": "Yes, there's a lot of resistance\nand lack of understanding.",
      "start": 8035.89,
      "duration": 3.03
    },
    {
      "text": "But the thing that makes\nme feel that progress",
      "start": 8038.92,
      "duration": 2.49
    },
    {
      "text": "will in the end happen moderately fast,",
      "start": 8041.41,
      "duration": 2.46
    },
    {
      "text": "not incredibly fast, but moderately fast,",
      "start": 8043.87,
      "duration": 2.58
    },
    {
      "text": "is that you talk to, what I find",
      "start": 8046.45,
      "duration": 2.97
    },
    {
      "text": "is I find over and over again,",
      "start": 8049.42,
      "duration": 1.44
    },
    {
      "text": "again, in large companies,\neven in governments,",
      "start": 8050.86,
      "duration": 3.39
    },
    {
      "text": "which have been actually\nsurprisingly forward-leaning,",
      "start": 8054.25,
      "duration": 2.7
    },
    {
      "text": "you find two things that\nmove things forward.",
      "start": 8058.06,
      "duration": 3.24
    },
    {
      "text": "One, you find a small fraction\nof people within a company,",
      "start": 8061.3,
      "duration": 4.26
    },
    {
      "text": "within a government who\nreally see the big picture,",
      "start": 8065.56,
      "duration": 2.82
    },
    {
      "text": "who see the whole Scaling Hypothesis,",
      "start": 8068.38,
      "duration": 2.13
    },
    {
      "text": "who understand where AI is\ngoing, or at least understand",
      "start": 8070.51,
      "duration": 2.73
    },
    {
      "text": "where it's going within their industry.",
      "start": 8073.24,
      "duration": 1.68
    },
    {
      "text": "And there are a few people like that",
      "start": 8074.92,
      "duration": 2.1
    },
    {
      "text": "within the current US government",
      "start": 8077.02,
      "duration": 1.44
    },
    {
      "text": "who really see the whole picture.",
      "start": 8078.46,
      "duration": 1.86
    },
    {
      "text": "And those people see that",
      "start": 8080.32,
      "duration": 1.493
    },
    {
      "text": "this is the most important\nthing in the world,",
      "start": 8081.813,
      "duration": 1.714
    },
    {
      "text": "and so they agitate for it.",
      "start": 8083.527,
      "duration": 1.503
    },
    {
      "text": "And the thing, they alone\nare not enough to succeed",
      "start": 8085.03,
      "duration": 3.0
    },
    {
      "text": "because they're a small set of people",
      "start": 8088.03,
      "duration": 1.8
    },
    {
      "text": "within a large organization.",
      "start": 8089.83,
      "duration": 1.92
    },
    {
      "text": "But as the technology starts to roll out,",
      "start": 8091.75,
      "duration": 3.87
    },
    {
      "text": "as it succeeds in some places,",
      "start": 8095.62,
      "duration": 2.19
    },
    {
      "text": "in the folks who are\nmost willing to adopt it,",
      "start": 8097.81,
      "duration": 3.09
    },
    {
      "text": "the specter of competition\ngives them a wind at their backs",
      "start": 8100.9,
      "duration": 4.35
    },
    {
      "text": "because they can point within\ntheir large organization,",
      "start": 8105.25,
      "duration": 2.97
    },
    {
      "text": "they can say, look, these other\nguys are doing this, right?",
      "start": 8108.22,
      "duration": 3.157
    },
    {
      "text": "You know, one bank can say, look,",
      "start": 8111.377,
      "duration": 2.153
    },
    {
      "text": "this newfangled hedge\nfund is doing this thing.",
      "start": 8113.53,
      "duration": 1.95
    },
    {
      "text": "They're going to eat our lunch.",
      "start": 8115.48,
      "duration": 1.47
    },
    {
      "text": "In the US, we can say we're afraid",
      "start": 8116.95,
      "duration": 1.59
    },
    {
      "text": "China's gonna get there before we are.",
      "start": 8118.54,
      "duration": 2.403
    },
    {
      "text": "And that combination, the\nspecter of competition",
      "start": 8121.78,
      "duration": 2.88
    },
    {
      "text": "plus a few visionaries within these,",
      "start": 8124.66,
      "duration": 3.03
    },
    {
      "text": "you know, within the organizations",
      "start": 8127.69,
      "duration": 1.95
    },
    {
      "text": "that in many ways are sclerotic,",
      "start": 8129.64,
      "duration": 2.22
    },
    {
      "text": "you put those two things together",
      "start": 8131.86,
      "duration": 1.53
    },
    {
      "text": "and it actually makes something happen.",
      "start": 8133.39,
      "duration": 1.86
    },
    {
      "text": "I mean, it's interesting.",
      "start": 8135.25,
      "duration": 0.96
    },
    {
      "text": "It's a balanced fight between the two",
      "start": 8136.21,
      "duration": 1.62
    },
    {
      "text": "because inertia is very powerful.",
      "start": 8137.83,
      "duration": 1.923
    },
    {
      "text": "But eventually over enough time,",
      "start": 8140.68,
      "duration": 2.58
    },
    {
      "text": "the innovative approach breaks through.",
      "start": 8143.26,
      "duration": 3.153
    },
    {
      "text": "And I've seen that happen.",
      "start": 8148.03,
      "duration": 1.65
    },
    {
      "text": "I've seen the arc of\nthat over and over again.",
      "start": 8149.68,
      "duration": 2.79
    },
    {
      "text": "And it's like the barriers are there.",
      "start": 8152.47,
      "duration": 3.003
    },
    {
      "text": "The barriers to progress, the complexity,",
      "start": 8156.406,
      "duration": 2.604
    },
    {
      "text": "not knowing how to use the model",
      "start": 8159.01,
      "duration": 1.46
    },
    {
      "text": "or how to deploy them are there,",
      "start": 8160.47,
      "duration": 2.23
    },
    {
      "text": "and for a bit, it seems like\nthey're gonna last forever,",
      "start": 8162.7,
      "duration": 3.45
    },
    {
      "text": "like change doesn't happen.",
      "start": 8166.15,
      "duration": 1.65
    },
    {
      "text": "But then eventually change happens",
      "start": 8167.8,
      "duration": 1.89
    },
    {
      "text": "and always comes from a few people.",
      "start": 8169.69,
      "duration": 2.16
    },
    {
      "text": "I felt the same way when I was an advocate",
      "start": 8171.85,
      "duration": 2.25
    },
    {
      "text": "of the Scaling Hypothesis\nwithin the AI field itself",
      "start": 8174.1,
      "duration": 2.73
    },
    {
      "text": "and others didn't get it.",
      "start": 8176.83,
      "duration": 1.14
    },
    {
      "text": "It felt like no one would ever get it.",
      "start": 8177.97,
      "duration": 2.712
    },
    {
      "text": "Then it felt like we had a\nsecret almost no one ever had,",
      "start": 8180.682,
      "duration": 3.258
    },
    {
      "text": "and then a couple years later,\neveryone has the secret.",
      "start": 8183.94,
      "duration": 2.88
    },
    {
      "text": "And so I think that's how it's gonna go",
      "start": 8186.82,
      "duration": 2.1
    },
    {
      "text": "with deployment to AI in the world.",
      "start": 8188.92,
      "duration": 1.92
    },
    {
      "text": "It's gonna, the barriers are\ngonna fall apart gradually",
      "start": 8190.84,
      "duration": 3.69
    },
    {
      "text": "and then all at once.",
      "start": 8194.53,
      "duration": 1.32
    },
    {
      "text": "And so I think this is gonna be more,",
      "start": 8195.85,
      "duration": 2.37
    },
    {
      "text": "and this is just an instinct.",
      "start": 8198.22,
      "duration": 1.71
    },
    {
      "text": "I could easily see how I'm wrong.",
      "start": 8199.93,
      "duration": 1.77
    },
    {
      "text": "I think it's gonna be more like",
      "start": 8201.7,
      "duration": 1.55
    },
    {
      "text": "5 or 10 years, as I say in the essay,",
      "start": 8204.17,
      "duration": 2.09
    },
    {
      "text": "than it's gonna be 50 or 100 years.",
      "start": 8206.26,
      "duration": 2.07
    },
    {
      "text": "I also think it's gonna be",
      "start": 8208.33,
      "duration": 0.903
    },
    {
      "text": "5 or 10 years more than it's gonna be,",
      "start": 8209.233,
      "duration": 3.387
    },
    {
      "text": "you know, 5 or 10 hours,",
      "start": 8212.62,
      "duration": 2.94
    },
    {
      "text": "because I've just seen\nhow human systems work.",
      "start": 8215.56,
      "duration": 3.54
    },
    {
      "text": "And I think a lot of these people",
      "start": 8219.1,
      "duration": 1.2
    },
    {
      "text": "who write down these\ndifferential equations",
      "start": 8220.3,
      "duration": 1.77
    },
    {
      "text": "who say AI is gonna make more powerful AI,",
      "start": 8222.07,
      "duration": 2.55
    },
    {
      "text": "who can't understand how it\ncould possibly be the case",
      "start": 8224.62,
      "duration": 2.7
    },
    {
      "text": "that these things won't change so fast,",
      "start": 8227.32,
      "duration": 2.13
    },
    {
      "text": "I think they don't\nunderstand these things.",
      "start": 8229.45,
      "duration": 2.13
    },
    {
      "text": "- So what to you is the timeline",
      "start": 8231.58,
      "duration": 2.43
    },
    {
      "text": "to where we achieve AGI, AKA powerful AI,",
      "start": 8234.01,
      "duration": 5.0
    },
    {
      "text": "AKA super useful AI?",
      "start": 8239.08,
      "duration": 1.68
    },
    {
      "text": "- Useful. (laughs)",
      "start": 8240.76,
      "duration": 1.798
    },
    {
      "text": "I'm gonna start calling it that.",
      "start": 8242.558,
      "duration": 1.416
    },
    {
      "text": "- It's a debate about naming.",
      "start": 8243.974,
      "duration": 2.669
    },
    {
      "text": "You know, on pure intelligence,",
      "start": 8248.26,
      "duration": 1.5
    },
    {
      "text": "it can smarter than a Nobel Prize winner",
      "start": 8249.76,
      "duration": 2.22
    },
    {
      "text": "in every relevant discipline",
      "start": 8251.98,
      "duration": 1.38
    },
    {
      "text": "and all the things we've said.",
      "start": 8253.36,
      "duration": 0.99
    },
    {
      "text": "Modality, can go and do stuff\non its own for days, weeks,",
      "start": 8254.35,
      "duration": 4.77
    },
    {
      "text": "and do biology experiments on its own.",
      "start": 8259.12,
      "duration": 2.61
    },
    {
      "text": "In one, you know what,\nlet's just stick to biology",
      "start": 8261.73,
      "duration": 2.76
    },
    {
      "text": "'cause you sold me on the whole biology",
      "start": 8264.49,
      "duration": 2.76
    },
    {
      "text": "and health section,\nthat's so exciting from,",
      "start": 8267.25,
      "duration": 3.093
    },
    {
      "text": "just I was getting giddy from\na scientific perspective.",
      "start": 8271.51,
      "duration": 3.12
    },
    {
      "text": "It made me wanna be a biologist.",
      "start": 8274.63,
      "duration": 1.077
    },
    {
      "text": "- It's almost, it's so, no, no,",
      "start": 8275.707,
      "duration": 2.583
    },
    {
      "text": "this was the feeling I\nhad when I was writing it",
      "start": 8278.29,
      "duration": 1.92
    },
    {
      "text": "that it's like this would\nbe such a beautiful future",
      "start": 8280.21,
      "duration": 3.52
    },
    {
      "text": "if we can just make it happen, right?",
      "start": 8285.49,
      "duration": 1.59
    },
    {
      "text": "If we can just get the\nlandmines out of the way",
      "start": 8287.08,
      "duration": 2.88
    },
    {
      "text": "and make it happen, there's so much,",
      "start": 8289.96,
      "duration": 3.24
    },
    {
      "text": "there's so much beauty",
      "start": 8293.2,
      "duration": 2.38
    },
    {
      "text": "and elegance and moral force\nbehind it if we can just.",
      "start": 8296.865,
      "duration": 4.165
    },
    {
      "text": "And it's something we should\nall be able to agree on, right?",
      "start": 8301.03,
      "duration": 2.61
    },
    {
      "text": "Like, as much as we fight",
      "start": 8303.64,
      "duration": 2.16
    },
    {
      "text": "about all these political questions,",
      "start": 8305.8,
      "duration": 2.28
    },
    {
      "text": "is this something that could\nactually bring us together?",
      "start": 8308.08,
      "duration": 3.33
    },
    {
      "text": "But you were asking when\nwhen will we get this?",
      "start": 8311.41,
      "duration": 1.677
    },
    {
      "text": "- When? When do you think?",
      "start": 8313.087,
      "duration": 1.078
    },
    {
      "text": "Just so putting numbers on that.",
      "start": 8314.165,
      "duration": 1.233
    },
    {
      "text": "- So you know, this is, of course,",
      "start": 8315.398,
      "duration": 1.559
    },
    {
      "text": "the thing I've been grappling\nwith for many years,",
      "start": 8316.957,
      "duration": 2.163
    },
    {
      "text": "and I'm not at all confident.",
      "start": 8319.12,
      "duration": 2.13
    },
    {
      "text": "Every time, if I say 2026 or 2027,",
      "start": 8321.25,
      "duration": 2.443
    },
    {
      "text": "there will be like a zillion",
      "start": 8323.693,
      "duration": 1.937
    },
    {
      "text": "like people on Twitter who will be like,",
      "start": 8325.63,
      "duration": 1.927
    },
    {
      "text": "\"AI CEO said 2026,\"",
      "start": 8327.557,
      "duration": 2.243
    },
    {
      "text": "and it'll be repeated for\nlike the next two years",
      "start": 8329.8,
      "duration": 2.76
    },
    {
      "text": "that like this is definitely\nwhen I think it's gonna happen.",
      "start": 8332.56,
      "duration": 2.94
    },
    {
      "text": "So whoever's extorting these clips",
      "start": 8335.5,
      "duration": 3.01
    },
    {
      "text": "will crop out the thing I just said",
      "start": 8339.893,
      "duration": 1.517
    },
    {
      "text": "and only say the thing I'm about to say,",
      "start": 8341.41,
      "duration": 2.7
    },
    {
      "text": "but I'll just say it anyway.",
      "start": 8344.11,
      "duration": 2.19
    },
    {
      "text": "- [Lex] Have fun with it.",
      "start": 8346.3,
      "duration": 1.193
    },
    {
      "text": "- So, if you extrapolate the curves",
      "start": 8348.52,
      "duration": 2.91
    },
    {
      "text": "that we've had so far, right?",
      "start": 8351.43,
      "duration": 1.2
    },
    {
      "text": "If you say, well, I don't know,",
      "start": 8352.63,
      "duration": 2.1
    },
    {
      "text": "we're starting to get to like PhD level,",
      "start": 8354.73,
      "duration": 2.16
    },
    {
      "text": "and last year we were\nat undergraduate level,",
      "start": 8356.89,
      "duration": 3.837
    },
    {
      "text": "and the year before we were at like",
      "start": 8360.727,
      "duration": 1.53
    },
    {
      "text": "the level of a high school student.",
      "start": 8362.257,
      "duration": 2.043
    },
    {
      "text": "Again, you can quibble with at what tasks",
      "start": 8364.3,
      "duration": 2.7
    },
    {
      "text": "and for what, we're\nstill missing modalities,",
      "start": 8367.0,
      "duration": 2.52
    },
    {
      "text": "but those are being added,\nlike computer use was added,",
      "start": 8369.52,
      "duration": 2.64
    },
    {
      "text": "like image in was added,",
      "start": 8372.16,
      "duration": 1.26
    },
    {
      "text": "like image generation has been added.",
      "start": 8373.42,
      "duration": 2.64
    },
    {
      "text": "If you just kind of like, and\nthis is totally unscientific,",
      "start": 8376.06,
      "duration": 3.0
    },
    {
      "text": "but if you just kind of like eyeball",
      "start": 8379.06,
      "duration": 2.19
    },
    {
      "text": "the rate at which these\ncapabilities are increasing,",
      "start": 8381.25,
      "duration": 3.0
    },
    {
      "text": "it does make you think",
      "start": 8384.25,
      "duration": 1.44
    },
    {
      "text": "that we'll get there by 2026 or 2027.",
      "start": 8385.69,
      "duration": 2.82
    },
    {
      "text": "Again, lots of things could derail it.",
      "start": 8388.51,
      "duration": 2.94
    },
    {
      "text": "We could run out of data.",
      "start": 8391.45,
      "duration": 1.98
    },
    {
      "text": "You know, we might not be able",
      "start": 8393.43,
      "duration": 1.35
    },
    {
      "text": "to scale clusters as much as we want.",
      "start": 8394.78,
      "duration": 2.04
    },
    {
      "text": "Like, you know, maybe Taiwan\ngets blown up or something",
      "start": 8396.82,
      "duration": 3.09
    },
    {
      "text": "and, you know, then we can't produce",
      "start": 8399.91,
      "duration": 1.23
    },
    {
      "text": "as many GPUs as we want.",
      "start": 8401.14,
      "duration": 1.41
    },
    {
      "text": "So there are all kinds of things",
      "start": 8402.55,
      "duration": 2.133
    },
    {
      "text": "that could derail the whole process.",
      "start": 8404.683,
      "duration": 2.577
    },
    {
      "text": "So I don't fully believe the\nstraight line extrapolation,",
      "start": 8407.26,
      "duration": 3.27
    },
    {
      "text": "but if you believe the\nstraight line extrapolation,",
      "start": 8410.53,
      "duration": 2.94
    },
    {
      "text": "we'll get there in 2026 or 2027.",
      "start": 8413.47,
      "duration": 2.97
    },
    {
      "text": "I think the most likely is that",
      "start": 8416.44,
      "duration": 1.073
    },
    {
      "text": "there's some mild delay relative to that.",
      "start": 8417.513,
      "duration": 2.8
    },
    {
      "text": "I don't know what that delay is,",
      "start": 8421.45,
      "duration": 1.44
    },
    {
      "text": "but I think it could happen on schedule.",
      "start": 8422.89,
      "duration": 1.44
    },
    {
      "text": "I think there could be a mild delay.",
      "start": 8424.33,
      "duration": 1.5
    },
    {
      "text": "I think there are still worlds",
      "start": 8425.83,
      "duration": 1.17
    },
    {
      "text": "where it doesn't happen in 100 years.",
      "start": 8427.0,
      "duration": 2.043
    },
    {
      "text": "The number of those worlds\nis rapidly decreasing.",
      "start": 8429.91,
      "duration": 2.76
    },
    {
      "text": "We are rapidly running out",
      "start": 8432.67,
      "duration": 1.44
    },
    {
      "text": "of truly convincing blockers,",
      "start": 8434.11,
      "duration": 2.43
    },
    {
      "text": "truly compelling reasons why",
      "start": 8436.54,
      "duration": 1.5
    },
    {
      "text": "this will not happen\nin the next few years.",
      "start": 8438.04,
      "duration": 1.89
    },
    {
      "text": "There were a lot more in 2020,",
      "start": 8439.93,
      "duration": 1.773
    },
    {
      "text": "although my guess, my hunch at that time",
      "start": 8442.78,
      "duration": 2.13
    },
    {
      "text": "was that we'll make it\nthrough all those blockers.",
      "start": 8444.91,
      "duration": 1.62
    },
    {
      "text": "So sitting as someone who has seen",
      "start": 8446.53,
      "duration": 2.31
    },
    {
      "text": "most of the blockers\ncleared out of the way,",
      "start": 8448.84,
      "duration": 2.1
    },
    {
      "text": "I kind of suspect, my\nhunch, my suspicion is that",
      "start": 8450.94,
      "duration": 2.82
    },
    {
      "text": "the rest of them will not block us.",
      "start": 8453.76,
      "duration": 2.31
    },
    {
      "text": "But, you know, look,\nat the end of the day,",
      "start": 8456.07,
      "duration": 3.3
    },
    {
      "text": "like I don't wanna represent this",
      "start": 8459.37,
      "duration": 1.35
    },
    {
      "text": "as a scientific prediction.",
      "start": 8460.72,
      "duration": 1.56
    },
    {
      "text": "People call them scaling laws.",
      "start": 8462.28,
      "duration": 1.62
    },
    {
      "text": "That's a misnomer, like\nMoore's law is a misnomer.",
      "start": 8463.9,
      "duration": 3.33
    },
    {
      "text": "Moore's laws, scaling laws,",
      "start": 8467.23,
      "duration": 1.29
    },
    {
      "text": "they're not laws of the universe.",
      "start": 8468.52,
      "duration": 1.47
    },
    {
      "text": "They're empirical regularities.",
      "start": 8469.99,
      "duration": 1.74
    },
    {
      "text": "I am going to bet in\nfavor of them continuing,",
      "start": 8471.73,
      "duration": 2.28
    },
    {
      "text": "but I'm not certain of that.",
      "start": 8474.01,
      "duration": 1.56
    },
    {
      "text": "- So you extensively describe",
      "start": 8475.57,
      "duration": 1.68
    },
    {
      "text": "sort of the compressed 21st century,",
      "start": 8477.25,
      "duration": 1.89
    },
    {
      "text": "how AGI will help set forth",
      "start": 8479.14,
      "duration": 4.95
    },
    {
      "text": "a chain of breakthroughs in biology",
      "start": 8484.09,
      "duration": 2.34
    },
    {
      "text": "and medicine that help us",
      "start": 8486.43,
      "duration": 2.16
    },
    {
      "text": "in all these kinds of\nways that I mentioned.",
      "start": 8488.59,
      "duration": 1.59
    },
    {
      "text": "So how do you think, what are\nthe early steps it might do?",
      "start": 8490.18,
      "duration": 3.18
    },
    {
      "text": "And by the way, I asked Claude\ngood questions to ask you,",
      "start": 8493.36,
      "duration": 2.85
    },
    {
      "text": "and Claude told me to ask,",
      "start": 8497.315,
      "duration": 2.592
    },
    {
      "text": "\"What do you think is a typical day",
      "start": 8499.907,
      "duration": 1.643
    },
    {
      "text": "for a biologists working on\nAGI look like in this future?\"",
      "start": 8501.55,
      "duration": 3.79
    },
    {
      "text": "- Yeah, yeah.\n- Claude is curious.",
      "start": 8505.34,
      "duration": 2.515
    },
    {
      "text": "- Well, let me start\nwith your first questions",
      "start": 8507.855,
      "duration": 1.915
    },
    {
      "text": "and then I'll answer that.",
      "start": 8509.77,
      "duration": 1.35
    },
    {
      "text": "Claude wants to know what's\nin his future, right?",
      "start": 8511.12,
      "duration": 1.74
    },
    {
      "text": "- Exactly.",
      "start": 8512.86,
      "duration": 1.62
    },
    {
      "text": "- Who am I gonna be working with?",
      "start": 8514.48,
      "duration": 1.14
    },
    {
      "text": "- Exactly.",
      "start": 8515.62,
      "duration": 1.32
    },
    {
      "text": "- So I think one of the things",
      "start": 8516.94,
      "duration": 1.62
    },
    {
      "text": "I went hard on, when I went\nhard on in the essay is,",
      "start": 8518.56,
      "duration": 3.36
    },
    {
      "text": "let me go back to this idea of,",
      "start": 8521.92,
      "duration": 1.77
    },
    {
      "text": "because it's really had, you know,",
      "start": 8523.69,
      "duration": 2.25
    },
    {
      "text": "had an impact on me.",
      "start": 8525.94,
      "duration": 1.32
    },
    {
      "text": "This idea that within large\norganizations and systems,",
      "start": 8527.26,
      "duration": 4.02
    },
    {
      "text": "there end up being a few people",
      "start": 8531.28,
      "duration": 1.695
    },
    {
      "text": "or a few new ideas who\nkind of cause things",
      "start": 8532.975,
      "duration": 2.985
    },
    {
      "text": "to go in a different direction\nthan they would've before,",
      "start": 8535.96,
      "duration": 2.25
    },
    {
      "text": "who kind of disproportionately\naffect the trajectory.",
      "start": 8538.21,
      "duration": 4.23
    },
    {
      "text": "There's a bunch of kind of the\nsame thing going on, right?",
      "start": 8542.44,
      "duration": 2.55
    },
    {
      "text": "If you think about the health world,",
      "start": 8544.99,
      "duration": 1.44
    },
    {
      "text": "there's like, you know,\ntrillions of dollars",
      "start": 8546.43,
      "duration": 2.4
    },
    {
      "text": "to pay out Medicare and you\nknow, other health insurance,",
      "start": 8548.83,
      "duration": 2.97
    },
    {
      "text": "and then the NIH is is 100 billion.",
      "start": 8551.8,
      "duration": 2.22
    },
    {
      "text": "And then if I think of like the few things",
      "start": 8554.02,
      "duration": 2.34
    },
    {
      "text": "that have really revolutionized anything,",
      "start": 8556.36,
      "duration": 1.53
    },
    {
      "text": "it could be encapsulated in\na small fraction of that.",
      "start": 8557.89,
      "duration": 3.21
    },
    {
      "text": "And so when I think of like,\nwhere will AI have an impact?",
      "start": 8561.1,
      "duration": 3.0
    },
    {
      "text": "I'm like, can AI turn that small fraction",
      "start": 8564.1,
      "duration": 2.52
    },
    {
      "text": "into a much larger fraction\nand raise its quality?",
      "start": 8566.62,
      "duration": 2.46
    },
    {
      "text": "And within biology, my\nexperience within biology",
      "start": 8569.08,
      "duration": 4.44
    },
    {
      "text": "is that the biggest problem of biology",
      "start": 8573.52,
      "duration": 2.64
    },
    {
      "text": "is that you can't see what's going on.",
      "start": 8576.16,
      "duration": 2.97
    },
    {
      "text": "You have very little ability\nto see what's going on",
      "start": 8579.13,
      "duration": 3.36
    },
    {
      "text": "and even less ability to change it, right?",
      "start": 8582.49,
      "duration": 2.043
    },
    {
      "text": "What you have is this, like from this,",
      "start": 8584.533,
      "duration": 4.137
    },
    {
      "text": "you have to infer that\nthere's a bunch of cells",
      "start": 8588.67,
      "duration": 2.73
    },
    {
      "text": "that within each cell is, you know,",
      "start": 8591.4,
      "duration": 3.57
    },
    {
      "text": "3 billion base pairs of DNA",
      "start": 8594.97,
      "duration": 2.34
    },
    {
      "text": "built according to a genetic code.",
      "start": 8597.31,
      "duration": 2.043
    },
    {
      "text": "And you know, there\nare all these processes",
      "start": 8600.58,
      "duration": 2.1
    },
    {
      "text": "that are just going on\nwithout any ability of us as,",
      "start": 8602.68,
      "duration": 2.977
    },
    {
      "text": "you know, unaugmented humans to affect it.",
      "start": 8605.657,
      "duration": 3.053
    },
    {
      "text": "These cells are dividing.",
      "start": 8608.71,
      "duration": 1.38
    },
    {
      "text": "Most of the time that's healthy,",
      "start": 8610.09,
      "duration": 1.59
    },
    {
      "text": "but sometimes that process\ngoes wrong and that's cancer.",
      "start": 8611.68,
      "duration": 4.02
    },
    {
      "text": "The cells are aging,",
      "start": 8615.7,
      "duration": 1.95
    },
    {
      "text": "your skin may change color,",
      "start": 8617.65,
      "duration": 2.1
    },
    {
      "text": "develops wrinkles as you age,",
      "start": 8619.75,
      "duration": 2.58
    },
    {
      "text": "and all of this is determined\nby these processes.",
      "start": 8622.33,
      "duration": 2.49
    },
    {
      "text": "All these proteins being produced,",
      "start": 8624.82,
      "duration": 1.83
    },
    {
      "text": "transported to various parts of the cells,",
      "start": 8626.65,
      "duration": 2.49
    },
    {
      "text": "binding to each other.",
      "start": 8629.14,
      "duration": 1.44
    },
    {
      "text": "And in our initial state about biology,",
      "start": 8630.58,
      "duration": 2.64
    },
    {
      "text": "we didn't even know that\nthese cells existed.",
      "start": 8633.22,
      "duration": 1.95
    },
    {
      "text": "We had to invent microscopes\nto observe the cells.",
      "start": 8635.17,
      "duration": 2.853
    },
    {
      "text": "We had to invent more\npowerful microscopes to see,",
      "start": 8639.773,
      "duration": 2.957
    },
    {
      "text": "you know, below the level of the cell",
      "start": 8642.73,
      "duration": 1.74
    },
    {
      "text": "to the level of molecules.",
      "start": 8644.47,
      "duration": 1.56
    },
    {
      "text": "We had to invent X-ray\ncrystallography to see the DNA.",
      "start": 8646.03,
      "duration": 3.27
    },
    {
      "text": "We had to invent gene\nsequencing to read the DNA.",
      "start": 8649.3,
      "duration": 3.15
    },
    {
      "text": "Now, you know, we had to invent",
      "start": 8652.45,
      "duration": 1.8
    },
    {
      "text": "protein folding technology to, you know,",
      "start": 8654.25,
      "duration": 2.31
    },
    {
      "text": "to predict how it would fold",
      "start": 8656.56,
      "duration": 1.45
    },
    {
      "text": "and how these things bind to each other.",
      "start": 8659.269,
      "duration": 2.901
    },
    {
      "text": "You know, we had to\ninvent various techniques",
      "start": 8662.17,
      "duration": 3.0
    },
    {
      "text": "for now we can edit the\nDNA as of, you know,",
      "start": 8665.17,
      "duration": 2.4
    },
    {
      "text": "with CRISPR, as of the last 12 years.",
      "start": 8667.57,
      "duration": 2.55
    },
    {
      "text": "So the whole history of biology,",
      "start": 8670.12,
      "duration": 2.13
    },
    {
      "text": "a whole big part of the history",
      "start": 8672.25,
      "duration": 2.43
    },
    {
      "text": "is basically our ability to read",
      "start": 8674.68,
      "duration": 4.23
    },
    {
      "text": "and understand what's going on,",
      "start": 8678.91,
      "duration": 1.32
    },
    {
      "text": "and our ability to reach in\nand selectively change things.",
      "start": 8680.23,
      "duration": 3.72
    },
    {
      "text": "And my view is that there's so much more",
      "start": 8683.95,
      "duration": 2.88
    },
    {
      "text": "we can still do there, right?",
      "start": 8686.83,
      "duration": 1.38
    },
    {
      "text": "You can do CRISPR but you can\ndo it for your whole body.",
      "start": 8688.21,
      "duration": 3.57
    },
    {
      "text": "Let's say I wanna do it for\none particular type of cell",
      "start": 8691.78,
      "duration": 3.12
    },
    {
      "text": "and I want the rate of targeting",
      "start": 8694.9,
      "duration": 2.13
    },
    {
      "text": "the wrong cell to be very low.",
      "start": 8697.03,
      "duration": 1.98
    },
    {
      "text": "That's still a challenge.",
      "start": 8699.01,
      "duration": 1.02
    },
    {
      "text": "That's still things people are working on.",
      "start": 8700.03,
      "duration": 1.89
    },
    {
      "text": "That's what we might need",
      "start": 8701.92,
      "duration": 1.02
    },
    {
      "text": "for gene therapy for certain diseases.",
      "start": 8702.94,
      "duration": 1.89
    },
    {
      "text": "And so the reason I'm saying all of this,",
      "start": 8704.83,
      "duration": 2.37
    },
    {
      "text": "and it goes beyond this to, you know,",
      "start": 8707.2,
      "duration": 3.47
    },
    {
      "text": "to gene sequencing, to new\ntypes of nano materials",
      "start": 8710.67,
      "duration": 3.55
    },
    {
      "text": "for observing what's going on\ninside cells for, you know,",
      "start": 8714.22,
      "duration": 3.72
    },
    {
      "text": "antibody drug conjugates.",
      "start": 8717.94,
      "duration": 2.16
    },
    {
      "text": "The reason I'm saying all this",
      "start": 8720.1,
      "duration": 1.31
    },
    {
      "text": "is that this could be a leverage point",
      "start": 8721.41,
      "duration": 2.02
    },
    {
      "text": "for the AI systems, right?",
      "start": 8723.43,
      "duration": 1.89
    },
    {
      "text": "That the number of such inventions,",
      "start": 8725.32,
      "duration": 2.823
    },
    {
      "text": "it's in the mid double\ndigits or something,",
      "start": 8729.348,
      "duration": 1.792
    },
    {
      "text": "you know, mid double digits.",
      "start": 8731.14,
      "duration": 1.14
    },
    {
      "text": "Maybe low triple digits\nover the history of biology.",
      "start": 8732.28,
      "duration": 3.06
    },
    {
      "text": "Let's say I have a million\nof these AIs like, you know,",
      "start": 8735.34,
      "duration": 2.34
    },
    {
      "text": "can they discover thousand,\nyou know, working together,",
      "start": 8737.68,
      "duration": 2.04
    },
    {
      "text": "can they discover thousands\nof these very quickly?",
      "start": 8739.72,
      "duration": 2.91
    },
    {
      "text": "And does that provide a huge lever,",
      "start": 8742.63,
      "duration": 2.55
    },
    {
      "text": "instead of trying to\nleverage the, you know,",
      "start": 8745.18,
      "duration": 2.22
    },
    {
      "text": "2 trillion a year we spend on,",
      "start": 8747.4,
      "duration": 1.59
    },
    {
      "text": "you know, Medicare or whatever,",
      "start": 8748.99,
      "duration": 1.5
    },
    {
      "text": "can we leverage the 1 billion a year,",
      "start": 8750.49,
      "duration": 2.255
    },
    {
      "text": "you know, that's spent to discover,",
      "start": 8752.745,
      "duration": 1.585
    },
    {
      "text": "but with much higher quality?",
      "start": 8754.33,
      "duration": 2.25
    },
    {
      "text": "And so what is it like, you know,",
      "start": 8756.58,
      "duration": 2.34
    },
    {
      "text": "being a scientist that\nworks with an AI system?",
      "start": 8758.92,
      "duration": 3.93
    },
    {
      "text": "The way I think about\nit actually is, well,",
      "start": 8762.85,
      "duration": 3.84
    },
    {
      "text": "so I think in the early stages,",
      "start": 8766.69,
      "duration": 2.79
    },
    {
      "text": "the AIs are gonna be like grad students.",
      "start": 8769.48,
      "duration": 2.82
    },
    {
      "text": "You're gonna give them a\nproject, you're gonna say,",
      "start": 8772.3,
      "duration": 2.407
    },
    {
      "text": "you know, I'm the experienced biologist,",
      "start": 8774.707,
      "duration": 2.243
    },
    {
      "text": "I've set up the lab.",
      "start": 8776.95,
      "duration": 1.35
    },
    {
      "text": "The biology professor",
      "start": 8778.3,
      "duration": 1.23
    },
    {
      "text": "or even the grad students\nthemselves will say,",
      "start": 8779.53,
      "duration": 2.373
    },
    {
      "text": "here's what you can do with an AI,",
      "start": 8784.766,
      "duration": 1.544
    },
    {
      "text": "you know, like AI system.",
      "start": 8786.31,
      "duration": 2.1
    },
    {
      "text": "I'd like to study this.",
      "start": 8788.41,
      "duration": 1.584
    },
    {
      "text": "And you know, the AI system,\nit has all the tools.",
      "start": 8789.994,
      "duration": 1.896
    },
    {
      "text": "It can like look up all the\nliterature to decide what to do.",
      "start": 8791.89,
      "duration": 3.36
    },
    {
      "text": "It can look at all the equipment.",
      "start": 8795.25,
      "duration": 1.29
    },
    {
      "text": "It can go to a website and say,",
      "start": 8796.54,
      "duration": 1.35
    },
    {
      "text": "hey, I'm gonna go to,\nyou know, Thermo Fisher",
      "start": 8797.89,
      "duration": 2.4
    },
    {
      "text": "or, you know, whatever the\nlab equipment company is,",
      "start": 8800.29,
      "duration": 2.572
    },
    {
      "text": "dominant lab equipment company is today.",
      "start": 8802.862,
      "duration": 1.958
    },
    {
      "text": "In my time, it was Thermo Fisher.",
      "start": 8804.82,
      "duration": 2.613
    },
    {
      "text": "You know, I'm gonna order\nthis new equipment to do this.",
      "start": 8808.3,
      "duration": 3.03
    },
    {
      "text": "I'm gonna run my experiments.",
      "start": 8811.33,
      "duration": 1.5
    },
    {
      "text": "I'm gonna, you know, write up\na report about my experiments.",
      "start": 8812.83,
      "duration": 3.33
    },
    {
      "text": "I'm gonna, you know, inspect\nthe images for contamination.",
      "start": 8816.16,
      "duration": 3.75
    },
    {
      "text": "I'm gonna decide what\nthe next experiment is.",
      "start": 8819.91,
      "duration": 2.4
    },
    {
      "text": "I'm gonna like write some code",
      "start": 8822.31,
      "duration": 1.8
    },
    {
      "text": "and run a statistical analysis.",
      "start": 8824.11,
      "duration": 2.04
    },
    {
      "text": "All the things a grad student would do,",
      "start": 8826.15,
      "duration": 1.77
    },
    {
      "text": "there will be a computer with an AI",
      "start": 8827.92,
      "duration": 1.86
    },
    {
      "text": "that like the professor talks\nto every once in a while",
      "start": 8829.78,
      "duration": 2.28
    },
    {
      "text": "and it says, this is what\nyou're gonna do today.",
      "start": 8832.06,
      "duration": 1.83
    },
    {
      "text": "The AI system comes to it with questions.",
      "start": 8833.89,
      "duration": 2.79
    },
    {
      "text": "When it's necessary to\nrun the lab equipment,",
      "start": 8836.68,
      "duration": 1.92
    },
    {
      "text": "it may be limited in some ways.",
      "start": 8838.6,
      "duration": 1.37
    },
    {
      "text": "It may have to hire a human lab assistant,",
      "start": 8839.97,
      "duration": 3.22
    },
    {
      "text": "you know, to do the experiment\nand explain how to do it.",
      "start": 8843.19,
      "duration": 2.64
    },
    {
      "text": "Or it could, you know,",
      "start": 8845.83,
      "duration": 1.17
    },
    {
      "text": "it could use advances in lab automation",
      "start": 8847.0,
      "duration": 1.95
    },
    {
      "text": "that are gradually being developed over,",
      "start": 8848.95,
      "duration": 2.01
    },
    {
      "text": "have been developed over\nthe last decade or so,",
      "start": 8850.96,
      "duration": 3.687
    },
    {
      "text": "and will continue to be developed.",
      "start": 8854.647,
      "duration": 3.573
    },
    {
      "text": "And so it'll look like\nthere's a human professor",
      "start": 8858.22,
      "duration": 2.04
    },
    {
      "text": "and 1000 AI grad students,",
      "start": 8860.26,
      "duration": 1.56
    },
    {
      "text": "and you know, if you go to one",
      "start": 8861.82,
      "duration": 1.68
    },
    {
      "text": "of these Nobel Prize\nwinning biologists or so,",
      "start": 8863.5,
      "duration": 2.49
    },
    {
      "text": "you'll say, okay, well, you know,",
      "start": 8865.99,
      "duration": 1.14
    },
    {
      "text": "you had like 50 grad students,",
      "start": 8867.13,
      "duration": 1.8
    },
    {
      "text": "well, now you have 1000\nand they're smarter",
      "start": 8868.93,
      "duration": 2.67
    },
    {
      "text": "than you are, by the way.",
      "start": 8871.6,
      "duration": 1.25
    },
    {
      "text": "Then I think at some point\nit'll flip around where,",
      "start": 8874.18,
      "duration": 2.356
    },
    {
      "text": "you know, the AI systems will, you know,",
      "start": 8876.536,
      "duration": 1.754
    },
    {
      "text": "will be the PIs, will be the leaders,",
      "start": 8878.29,
      "duration": 1.89
    },
    {
      "text": "and you know, they'll be ordering humans",
      "start": 8880.18,
      "duration": 2.67
    },
    {
      "text": "or other AI systems around.",
      "start": 8882.85,
      "duration": 1.86
    },
    {
      "text": "So I think that's how it'll\nwork on the research side.",
      "start": 8884.71,
      "duration": 1.95
    },
    {
      "text": "- And they would be the inventors",
      "start": 8886.66,
      "duration": 1.155
    },
    {
      "text": "of a CRISPR type technology.",
      "start": 8887.815,
      "duration": 1.228
    },
    {
      "text": "They would be the inventors\nof a CRISPR type technology.",
      "start": 8889.043,
      "duration": 4.847
    },
    {
      "text": "And then I think, you know,\nas I say in the essay,",
      "start": 8893.89,
      "duration": 2.1
    },
    {
      "text": "we'll want to turn,",
      "start": 8895.99,
      "duration": 1.5
    },
    {
      "text": "probably turning loose is the wrong term,",
      "start": 8897.49,
      "duration": 2.34
    },
    {
      "text": "but we'll want to harness the AI systems",
      "start": 8899.83,
      "duration": 4.26
    },
    {
      "text": "to improve the clinical\ntrial system as well.",
      "start": 8904.09,
      "duration": 3.48
    },
    {
      "text": "There's some amount of\nthis that's regulatory,",
      "start": 8907.57,
      "duration": 1.8
    },
    {
      "text": "that's a matter of societal\ndecisions and that'll be harder.",
      "start": 8909.37,
      "duration": 2.79
    },
    {
      "text": "But can we get better at predicting",
      "start": 8912.16,
      "duration": 2.55
    },
    {
      "text": "the results of clinical trials?",
      "start": 8914.71,
      "duration": 1.77
    },
    {
      "text": "Can we get better at\nstatistical design so that,",
      "start": 8916.48,
      "duration": 3.87
    },
    {
      "text": "you know, clinical trials\nthat used to require,",
      "start": 8920.35,
      "duration": 2.61
    },
    {
      "text": "you know, 5,000 people\nand therefore, you know,",
      "start": 8922.96,
      "duration": 2.82
    },
    {
      "text": "needed 100 million dollars\nin a year to enroll them.",
      "start": 8925.78,
      "duration": 3.15
    },
    {
      "text": "Now they need 500 people and\ntwo months to enroll them.",
      "start": 8928.93,
      "duration": 3.72
    },
    {
      "text": "That's where we should start.",
      "start": 8932.65,
      "duration": 1.95
    },
    {
      "text": "And you know, can we\nincrease the success rate",
      "start": 8934.6,
      "duration": 2.7
    },
    {
      "text": "of clinical trials by doing\nthings in animal trials",
      "start": 8937.3,
      "duration": 3.33
    },
    {
      "text": "that we used to do in clinical trials,",
      "start": 8940.63,
      "duration": 1.44
    },
    {
      "text": "and doing things in simulations",
      "start": 8942.07,
      "duration": 1.35
    },
    {
      "text": "that we used to do in animal trials?",
      "start": 8943.42,
      "duration": 1.29
    },
    {
      "text": "Again, we won't be able\nto simulate it all,",
      "start": 8944.71,
      "duration": 2.34
    },
    {
      "text": "AI's not God, but you know,",
      "start": 8947.05,
      "duration": 3.51
    },
    {
      "text": "can we shift the curve\nsubstantially and radically?",
      "start": 8950.56,
      "duration": 3.15
    },
    {
      "text": "So I don't know, that would be my picture.",
      "start": 8953.71,
      "duration": 2.16
    },
    {
      "text": "- Doing in vitro and doing it,",
      "start": 8955.87,
      "duration": 1.403
    },
    {
      "text": "I mean, you're still slowed down.",
      "start": 8957.273,
      "duration": 1.887
    },
    {
      "text": "It still takes time, but you\ncan do it much, much faster.",
      "start": 8959.16,
      "duration": 2.68
    },
    {
      "text": "- Yeah, yeah, yeah.",
      "start": 8961.84,
      "duration": 0.833
    },
    {
      "text": "Can we just one step at a time,",
      "start": 8962.673,
      "duration": 1.627
    },
    {
      "text": "and can that add up to a lot of steps?",
      "start": 8964.3,
      "duration": 3.15
    },
    {
      "text": "Even though we still need clinical trials,",
      "start": 8967.45,
      "duration": 2.01
    },
    {
      "text": "even though we still need laws,",
      "start": 8969.46,
      "duration": 1.68
    },
    {
      "text": "even though the FDA\nand other organizations",
      "start": 8971.14,
      "duration": 2.1
    },
    {
      "text": "will still not be perfect,",
      "start": 8973.24,
      "duration": 1.29
    },
    {
      "text": "can we just move everything\nin a positive direction?",
      "start": 8974.53,
      "duration": 2.37
    },
    {
      "text": "And when you add up all\nthose positive directions,",
      "start": 8976.9,
      "duration": 1.86
    },
    {
      "text": "do you get everything",
      "start": 8978.76,
      "duration": 1.26
    },
    {
      "text": "that was gonna happen from here to 2100",
      "start": 8980.02,
      "duration": 2.31
    },
    {
      "text": "instead happens from 2027\nto 2032 or something?",
      "start": 8982.33,
      "duration": 4.59
    },
    {
      "text": "- Another way that I think the world",
      "start": 8986.92,
      "duration": 1.74
    },
    {
      "text": "might be changing with AI even today,",
      "start": 8988.66,
      "duration": 4.08
    },
    {
      "text": "but moving towards this future",
      "start": 8992.74,
      "duration": 1.41
    },
    {
      "text": "of the powerful super\nuseful AI is programming.",
      "start": 8994.15,
      "duration": 4.35
    },
    {
      "text": "So how do you see the\nnature of programming?",
      "start": 8998.5,
      "duration": 3.87
    },
    {
      "text": "Because it's so intimate to\nthe actual act of building AI.",
      "start": 9002.37,
      "duration": 3.36
    },
    {
      "text": "How do you see that\nchanging for us humans?",
      "start": 9005.73,
      "duration": 3.42
    },
    {
      "text": "- I think that's gonna be one of the areas",
      "start": 9009.15,
      "duration": 1.65
    },
    {
      "text": "that changes fastest for two reasons.",
      "start": 9010.8,
      "duration": 2.49
    },
    {
      "text": "One, programming is a\nskill that's very close",
      "start": 9013.29,
      "duration": 3.18
    },
    {
      "text": "to the actual building of the AI.",
      "start": 9016.47,
      "duration": 2.58
    },
    {
      "text": "So the farther a skill is from the people",
      "start": 9019.05,
      "duration": 3.06
    },
    {
      "text": "who are building the AI,\nthe longer it's gonna take",
      "start": 9022.11,
      "duration": 2.73
    },
    {
      "text": "to get disrupted by the AI, right?",
      "start": 9024.84,
      "duration": 1.98
    },
    {
      "text": "Like I truly believe that like\nAI will disrupt agriculture.",
      "start": 9026.82,
      "duration": 3.39
    },
    {
      "text": "Maybe it already has in some ways,",
      "start": 9030.21,
      "duration": 1.71
    },
    {
      "text": "but that's just very\ndistant from the folks",
      "start": 9031.92,
      "duration": 2.31
    },
    {
      "text": "who are building AI and so I\nthink it's gonna take longer.",
      "start": 9034.23,
      "duration": 2.73
    },
    {
      "text": "But programming is the\nbread and butter of,",
      "start": 9036.96,
      "duration": 2.111
    },
    {
      "text": "you know, a large fraction",
      "start": 9039.071,
      "duration": 1.099
    },
    {
      "text": "of the employees who work at Anthropic",
      "start": 9040.17,
      "duration": 1.98
    },
    {
      "text": "and at the other companies",
      "start": 9042.15,
      "duration": 1.2
    },
    {
      "text": "and so it's gonna happen fast.",
      "start": 9043.35,
      "duration": 2.31
    },
    {
      "text": "The other reason it's gonna\nhappen fast is with programming,",
      "start": 9045.66,
      "duration": 2.25
    },
    {
      "text": "you close the loop,",
      "start": 9047.91,
      "duration": 1.11
    },
    {
      "text": "both when you're training the model",
      "start": 9049.02,
      "duration": 1.047
    },
    {
      "text": "and when you're applying the model.",
      "start": 9050.067,
      "duration": 2.043
    },
    {
      "text": "The idea that the model can write the code",
      "start": 9052.11,
      "duration": 2.22
    },
    {
      "text": "means that the model can then run the code",
      "start": 9054.33,
      "duration": 2.1
    },
    {
      "text": "and then see the results\nand interpret it back.",
      "start": 9057.29,
      "duration": 3.07
    },
    {
      "text": "And so it really has an\nability, unlike hardware,",
      "start": 9060.36,
      "duration": 3.06
    },
    {
      "text": "unlike biology, which we just discussed,",
      "start": 9063.42,
      "duration": 2.49
    },
    {
      "text": "the model has an ability\nto close the loop.",
      "start": 9065.91,
      "duration": 2.163
    },
    {
      "text": "And so I think those two things\nare gonna lead to the model",
      "start": 9069.12,
      "duration": 2.94
    },
    {
      "text": "getting good at programming very fast.",
      "start": 9072.06,
      "duration": 2.16
    },
    {
      "text": "As I saw on, you know, typical\nreal world programming tasks,",
      "start": 9074.22,
      "duration": 4.14
    },
    {
      "text": "models have gone from 3%\nin January of this year",
      "start": 9078.36,
      "duration": 4.32
    },
    {
      "text": "to 50% in October of this year.",
      "start": 9082.68,
      "duration": 2.79
    },
    {
      "text": "So, you know, we're on\nthat s-curve, right,",
      "start": 9085.47,
      "duration": 1.86
    },
    {
      "text": "where it's gonna start slowing down soon,",
      "start": 9087.33,
      "duration": 2.25
    },
    {
      "text": "'cause you can only get to 100 percent.",
      "start": 9089.58,
      "duration": 2.28
    },
    {
      "text": "But, you know, I would guess\nthat in another 10 months,",
      "start": 9091.86,
      "duration": 3.87
    },
    {
      "text": "we'll probably get pretty close.",
      "start": 9095.73,
      "duration": 1.41
    },
    {
      "text": "We'll be at least 90%.",
      "start": 9097.14,
      "duration": 2.07
    },
    {
      "text": "So again, I would guess, you know,",
      "start": 9099.21,
      "duration": 2.4
    },
    {
      "text": "I don't know how long it'll take,",
      "start": 9101.61,
      "duration": 1.08
    },
    {
      "text": "but I would guess again, 2026, 2027.",
      "start": 9102.69,
      "duration": 3.96
    },
    {
      "text": "Twitter people who crop out these numbers",
      "start": 9106.65,
      "duration": 3.99
    },
    {
      "text": "and get rid of the caveats,",
      "start": 9110.64,
      "duration": 2.22
    },
    {
      "text": "like, I don't know, I don't like you.",
      "start": 9112.86,
      "duration": 1.53
    },
    {
      "text": "Go away. (laughs)",
      "start": 9114.39,
      "duration": 1.65
    },
    {
      "text": "I would guess that the kind of task",
      "start": 9116.04,
      "duration": 2.52
    },
    {
      "text": "that the vast majority of\ncoders do, AI can probably,",
      "start": 9118.56,
      "duration": 4.593
    },
    {
      "text": "if we make the task very\nnarrow, like just write code,",
      "start": 9124.17,
      "duration": 3.96
    },
    {
      "text": "AI systems will be able to do that.",
      "start": 9128.13,
      "duration": 3.06
    },
    {
      "text": "Now that said, I think\ncomparative advantage is powerful.",
      "start": 9131.19,
      "duration": 2.82
    },
    {
      "text": "We'll find that when AIs\ncan do 80% of a coder's job,",
      "start": 9134.01,
      "duration": 4.98
    },
    {
      "text": "including most of it",
      "start": 9138.99,
      "duration": 0.93
    },
    {
      "text": "that's literally like write\ncode with a given spec,",
      "start": 9139.92,
      "duration": 2.88
    },
    {
      "text": "we'll find that the remaining parts",
      "start": 9142.8,
      "duration": 1.53
    },
    {
      "text": "of the job become more\nleveraged for humans, right?",
      "start": 9144.33,
      "duration": 2.82
    },
    {
      "text": "Humans will, there'll be more about",
      "start": 9147.15,
      "duration": 2.31
    },
    {
      "text": "like high level system design or,",
      "start": 9149.46,
      "duration": 2.73
    },
    {
      "text": "you know, looking at the app",
      "start": 9152.19,
      "duration": 1.38
    },
    {
      "text": "and like, is it architected well?",
      "start": 9153.57,
      "duration": 1.83
    },
    {
      "text": "And the design and UX aspects,",
      "start": 9155.4,
      "duration": 2.7
    },
    {
      "text": "and eventually AI will be able\nto do those as well, right?",
      "start": 9158.1,
      "duration": 2.97
    },
    {
      "text": "That's my vision of the, you\nknow, powerful AI system.",
      "start": 9161.07,
      "duration": 3.12
    },
    {
      "text": "But I think for much longer\nthan we might expect,",
      "start": 9164.19,
      "duration": 3.57
    },
    {
      "text": "we will see that",
      "start": 9167.76,
      "duration": 1.72
    },
    {
      "text": "small parts of the job\nthat humans still do",
      "start": 9171.66,
      "duration": 2.88
    },
    {
      "text": "will expand to fill their entire job",
      "start": 9174.54,
      "duration": 2.13
    },
    {
      "text": "in order for the overall\nproductivity to go up.",
      "start": 9176.67,
      "duration": 3.36
    },
    {
      "text": "That's something we've seen.",
      "start": 9180.03,
      "duration": 1.204
    },
    {
      "text": "You know, it used to be that, you know,",
      "start": 9181.234,
      "duration": 0.833
    },
    {
      "text": "writing and editing\nletters was very difficult",
      "start": 9184.094,
      "duration": 2.206
    },
    {
      "text": "and like writing the print was difficult.",
      "start": 9186.3,
      "duration": 1.44
    },
    {
      "text": "Well, as soon as you had word processors",
      "start": 9187.74,
      "duration": 2.35
    },
    {
      "text": "and then computers and it\nbecame easy to produce work",
      "start": 9191.847,
      "duration": 2.673
    },
    {
      "text": "and easy to share it,\nthen that became instant",
      "start": 9194.52,
      "duration": 3.21
    },
    {
      "text": "and all the focus was on the ideas.",
      "start": 9197.73,
      "duration": 1.98
    },
    {
      "text": "So this logic of comparative advantage",
      "start": 9199.71,
      "duration": 2.55
    },
    {
      "text": "that expands tiny parts of the tasks",
      "start": 9202.26,
      "duration": 2.97
    },
    {
      "text": "to large parts of the tasks",
      "start": 9205.23,
      "duration": 2.16
    },
    {
      "text": "and creates new tasks in\norder to expand productivity,",
      "start": 9207.39,
      "duration": 3.45
    },
    {
      "text": "I think that's going to be the case.",
      "start": 9210.84,
      "duration": 1.26
    },
    {
      "text": "Again, someday AI will\nbe better at everything",
      "start": 9212.1,
      "duration": 2.55
    },
    {
      "text": "in that logic won't apply,",
      "start": 9214.65,
      "duration": 1.59
    },
    {
      "text": "and then we all have, you\nknow, humanity will have",
      "start": 9216.24,
      "duration": 3.06
    },
    {
      "text": "to think about how to\ncollectively deal with that,",
      "start": 9219.3,
      "duration": 2.79
    },
    {
      "text": "and we're thinking about that every day.",
      "start": 9222.09,
      "duration": 2.0
    },
    {
      "text": "And you know, that's another one",
      "start": 9225.089,
      "duration": 1.011
    },
    {
      "text": "of the grand problems to deal with,",
      "start": 9226.1,
      "duration": 1.57
    },
    {
      "text": "aside from misuse and autonomy",
      "start": 9227.67,
      "duration": 1.62
    },
    {
      "text": "and, you know, we should\ntake it very seriously.",
      "start": 9229.29,
      "duration": 2.1
    },
    {
      "text": "But I think in the near term,",
      "start": 9231.39,
      "duration": 2.25
    },
    {
      "text": "and maybe even in the medium\nterm, like medium term,",
      "start": 9233.64,
      "duration": 2.16
    },
    {
      "text": "like 2, 3, 4 years, you\nknow, I expect that humans",
      "start": 9235.8,
      "duration": 3.36
    },
    {
      "text": "will continue to have a huge role",
      "start": 9239.16,
      "duration": 1.737
    },
    {
      "text": "and the nature of programming will change,",
      "start": 9240.897,
      "duration": 1.833
    },
    {
      "text": "but programming as a role,",
      "start": 9242.73,
      "duration": 1.56
    },
    {
      "text": "programming as a job will not change.",
      "start": 9244.29,
      "duration": 1.83
    },
    {
      "text": "It'll just be less writing\nthings line by line",
      "start": 9246.12,
      "duration": 2.58
    },
    {
      "text": "and it'll be more macroscopic.",
      "start": 9248.7,
      "duration": 1.98
    },
    {
      "text": "- And I wonder what the\nfuture of IDs looks like.",
      "start": 9250.68,
      "duration": 2.28
    },
    {
      "text": "So the tooling of\ninteracting with AI systems,",
      "start": 9252.96,
      "duration": 2.19
    },
    {
      "text": "this is true for programming",
      "start": 9255.15,
      "duration": 1.17
    },
    {
      "text": "and also probably true\nfor in other contexts,",
      "start": 9256.32,
      "duration": 2.37
    },
    {
      "text": "like computer use, but\nmaybe domain specific,",
      "start": 9258.69,
      "duration": 2.58
    },
    {
      "text": "like we mentioned biology,",
      "start": 9261.27,
      "duration": 1.44
    },
    {
      "text": "it probably needs its own tooling\nabout how to be effective,",
      "start": 9262.71,
      "duration": 3.0
    },
    {
      "text": "and then programming\nneeds its own tooling.",
      "start": 9265.71,
      "duration": 2.31
    },
    {
      "text": "Is Anthropic gonna play in that space",
      "start": 9268.02,
      "duration": 1.59
    },
    {
      "text": "of also tooling potentially?",
      "start": 9269.61,
      "duration": 1.26
    },
    {
      "text": "- I'm absolutely convinced\nthat powerful IDs",
      "start": 9270.87,
      "duration": 4.63
    },
    {
      "text": "that there's so much low hanging fruit",
      "start": 9276.51,
      "duration": 2.91
    },
    {
      "text": "to be grabbed there that, you know,",
      "start": 9279.42,
      "duration": 2.49
    },
    {
      "text": "right now it's just like\nyou talk to the model",
      "start": 9281.91,
      "duration": 1.74
    },
    {
      "text": "and it talks back, but look, I mean,",
      "start": 9283.65,
      "duration": 3.54
    },
    {
      "text": "IDs are great at kind of\nlots of static analysis of,",
      "start": 9287.19,
      "duration": 3.87
    },
    {
      "text": "you know, so much is possible\nwith kind of static analysis,",
      "start": 9291.06,
      "duration": 4.23
    },
    {
      "text": "like many bugs you can find\nwithout even writing the code.",
      "start": 9295.29,
      "duration": 3.12
    },
    {
      "text": "Then, you know, IDs are good\nfor running particular things,",
      "start": 9298.41,
      "duration": 3.3
    },
    {
      "text": "organizing your code, measuring\ncoverage of unit tests.",
      "start": 9301.71,
      "duration": 4.11
    },
    {
      "text": "Like there's so much that's\nbeen possible with normal IDs.",
      "start": 9305.82,
      "duration": 4.5
    },
    {
      "text": "Now you add something\nlike, well, the model,",
      "start": 9310.32,
      "duration": 3.007
    },
    {
      "text": "you know, the model can now",
      "start": 9313.327,
      "duration": 1.823
    },
    {
      "text": "like write code and run code.",
      "start": 9315.15,
      "duration": 2.34
    },
    {
      "text": "Like I am absolutely convinced",
      "start": 9317.49,
      "duration": 2.1
    },
    {
      "text": "that over the next year or two,",
      "start": 9319.59,
      "duration": 1.2
    },
    {
      "text": "even if the quality of\nthe models didn't improve,",
      "start": 9320.79,
      "duration": 2.58
    },
    {
      "text": "that there would be enormous opportunity",
      "start": 9323.37,
      "duration": 1.68
    },
    {
      "text": "to enhance people's productivity",
      "start": 9325.05,
      "duration": 1.89
    },
    {
      "text": "by catching a bunch of mistakes,",
      "start": 9326.94,
      "duration": 1.95
    },
    {
      "text": "doing a bunch of grunt work for people,",
      "start": 9328.89,
      "duration": 2.28
    },
    {
      "text": "and that we haven't even\nscratched the surface.",
      "start": 9331.17,
      "duration": 2.67
    },
    {
      "text": "Anthropic itself, I mean, you can't say,",
      "start": 9333.84,
      "duration": 2.01
    },
    {
      "text": "you know, it's hard to say\nwhat will happen in the future.",
      "start": 9335.85,
      "duration": 4.41
    },
    {
      "text": "Currently we're not trying\nto make such IDs ourself,",
      "start": 9340.26,
      "duration": 3.21
    },
    {
      "text": "rather we're powering the companies,",
      "start": 9343.47,
      "duration": 2.01
    },
    {
      "text": "like Cursor or like Cognition",
      "start": 9345.48,
      "duration": 2.91
    },
    {
      "text": "or some of the other, you know,\nexpo in the security space.",
      "start": 9348.39,
      "duration": 3.963
    },
    {
      "text": "You know, others that\nI can mention as well",
      "start": 9353.52,
      "duration": 2.76
    },
    {
      "text": "that are building such things\nthemselves on top of our API.",
      "start": 9356.28,
      "duration": 3.207
    },
    {
      "text": "And our view has been\nlet 1000 flowers bloom.",
      "start": 9359.487,
      "duration": 3.483
    },
    {
      "text": "We don't internally have the, you know,",
      "start": 9362.97,
      "duration": 3.717
    },
    {
      "text": "the resources to try all\nthese different things.",
      "start": 9366.687,
      "duration": 2.553
    },
    {
      "text": "Let's let our customers try it",
      "start": 9369.24,
      "duration": 1.5
    },
    {
      "text": "and, you know, we'll see who succeeded",
      "start": 9372.0,
      "duration": 1.35
    },
    {
      "text": "and maybe different customers",
      "start": 9373.35,
      "duration": 0.99
    },
    {
      "text": "will succeed in different ways.",
      "start": 9374.34,
      "duration": 2.04
    },
    {
      "text": "So I both think this is super promising",
      "start": 9376.38,
      "duration": 2.25
    },
    {
      "text": "and you know, it's not something,",
      "start": 9378.63,
      "duration": 3.15
    },
    {
      "text": "you know, Anthropic isn't eager to,",
      "start": 9381.78,
      "duration": 2.49
    },
    {
      "text": "at least right now, compete\nwith all our companies",
      "start": 9384.27,
      "duration": 1.83
    },
    {
      "text": "in this space and maybe never.",
      "start": 9386.1,
      "duration": 1.53
    },
    {
      "text": "- Yeah, it's been\ninteresting to watch Cursor",
      "start": 9387.63,
      "duration": 1.41
    },
    {
      "text": "try to integrate Claude successfully,",
      "start": 9389.04,
      "duration": 1.47
    },
    {
      "text": "'cause it's actually been fascinating",
      "start": 9390.51,
      "duration": 2.91
    },
    {
      "text": "how many places it can help\nthe programming experience.",
      "start": 9393.42,
      "duration": 2.52
    },
    {
      "text": "It's not as trivial-\n- It is really astounding.",
      "start": 9395.94,
      "duration": 2.52
    },
    {
      "text": "I feel like, you know, as a CEO,",
      "start": 9398.46,
      "duration": 1.53
    },
    {
      "text": "I don't get to program that much,",
      "start": 9399.99,
      "duration": 1.32
    },
    {
      "text": "and I feel like if six\nmonths from now I go back,",
      "start": 9401.31,
      "duration": 2.58
    },
    {
      "text": "it'll be completely unrecognizable to me.",
      "start": 9403.89,
      "duration": 1.56
    },
    {
      "text": "- Exactly.",
      "start": 9405.45,
      "duration": 1.5
    },
    {
      "text": "So in this world with super powerful AI",
      "start": 9406.95,
      "duration": 2.62
    },
    {
      "text": "that's increasingly automated,",
      "start": 9410.52,
      "duration": 2.16
    },
    {
      "text": "what's the source of\nmeaning for us humans?",
      "start": 9412.68,
      "duration": 2.31
    },
    {
      "text": "- Yeah.\n- You know, work is a source",
      "start": 9414.99,
      "duration": 2.4
    },
    {
      "text": "of deep meaning for many of us.",
      "start": 9417.39,
      "duration": 1.98
    },
    {
      "text": "So where do we find the meaning?",
      "start": 9419.37,
      "duration": 2.01
    },
    {
      "text": "- This is something\nthat I've written about",
      "start": 9421.38,
      "duration": 2.19
    },
    {
      "text": "a little bit in the essay,",
      "start": 9423.57,
      "duration": 1.17
    },
    {
      "text": "although I actually, I\ngive it a bit short shrift,",
      "start": 9424.74,
      "duration": 2.571
    },
    {
      "text": "not for any principled reason.",
      "start": 9427.311,
      "duration": 2.859
    },
    {
      "text": "But this essay, if you believe,",
      "start": 9430.17,
      "duration": 2.31
    },
    {
      "text": "it was originally gonna\nbe two or three pages,",
      "start": 9432.48,
      "duration": 2.07
    },
    {
      "text": "I was gonna talk about it at all hands.",
      "start": 9434.55,
      "duration": 1.74
    },
    {
      "text": "And the reason I realized",
      "start": 9436.29,
      "duration": 2.37
    },
    {
      "text": "it was an important, underexplored topic",
      "start": 9438.66,
      "duration": 3.0
    },
    {
      "text": "is that I just kept writing things.",
      "start": 9441.66,
      "duration": 1.39
    },
    {
      "text": "And I was just like,",
      "start": 9443.05,
      "duration": 1.4
    },
    {
      "text": "oh, man, I can't do this justice.",
      "start": 9444.45,
      "duration": 1.44
    },
    {
      "text": "And so the thing ballooned\nto like 40 or 50 pages,",
      "start": 9445.89,
      "duration": 2.64
    },
    {
      "text": "and then when I got to the\nwork and meaning section,",
      "start": 9448.53,
      "duration": 1.71
    },
    {
      "text": "I'm like, oh, man, this\nisn't gonna be 100 pages.",
      "start": 9450.24,
      "duration": 1.92
    },
    {
      "text": "Like I'm gonna have to write a\nwhole other essay about that.",
      "start": 9452.16,
      "duration": 2.76
    },
    {
      "text": "But meaning is actually interesting",
      "start": 9454.92,
      "duration": 1.65
    },
    {
      "text": "because you think about like the life",
      "start": 9456.57,
      "duration": 2.31
    },
    {
      "text": "that someone lives or something,",
      "start": 9458.88,
      "duration": 1.26
    },
    {
      "text": "or like, you know,",
      "start": 9460.14,
      "duration": 1.56
    },
    {
      "text": "let's say you were to put\nme in like a, I don't know,",
      "start": 9461.7,
      "duration": 1.77
    },
    {
      "text": "like a simulated environment",
      "start": 9463.47,
      "duration": 1.32
    },
    {
      "text": "or something where like, you know,",
      "start": 9464.79,
      "duration": 2.1
    },
    {
      "text": "like I have a job and I'm\ntrying to accomplish things",
      "start": 9466.89,
      "duration": 2.58
    },
    {
      "text": "and I don't know, I like\ndo that for 60 years",
      "start": 9469.47,
      "duration": 2.4
    },
    {
      "text": "and then you're like, oh, like oops,",
      "start": 9471.87,
      "duration": 2.49
    },
    {
      "text": "this was actually all a game, right?",
      "start": 9474.36,
      "duration": 1.68
    },
    {
      "text": "Does that really kind of rob you",
      "start": 9476.04,
      "duration": 1.13
    },
    {
      "text": "of the meaning of the whole thing?",
      "start": 9477.17,
      "duration": 1.36
    },
    {
      "text": "You know, like I still\nmade important choices,",
      "start": 9478.53,
      "duration": 2.46
    },
    {
      "text": "including moral choices.",
      "start": 9480.99,
      "duration": 1.29
    },
    {
      "text": "I still sacrificed.",
      "start": 9482.28,
      "duration": 1.59
    },
    {
      "text": "I still had to kind of\ngain all these skills.",
      "start": 9483.87,
      "duration": 2.917
    },
    {
      "text": "Or just like a similar exercise,",
      "start": 9486.787,
      "duration": 1.763
    },
    {
      "text": "you know, think back to like, you know,",
      "start": 9488.55,
      "duration": 1.44
    },
    {
      "text": "one of the historical\nfigures who, you know,",
      "start": 9489.99,
      "duration": 1.95
    },
    {
      "text": "discovered electromagnetism\nor relativity or something.",
      "start": 9491.94,
      "duration": 2.94
    },
    {
      "text": "If you told them, well,\nactually 20,000 years ago,",
      "start": 9494.88,
      "duration": 3.42
    },
    {
      "text": "some alien on, you know,",
      "start": 9498.3,
      "duration": 2.22
    },
    {
      "text": "some alien on this planet\ndiscovered this before you did,",
      "start": 9500.52,
      "duration": 3.483
    },
    {
      "text": "does that rob the\nmeaning of the discovery?",
      "start": 9504.84,
      "duration": 1.8
    },
    {
      "text": "It doesn't really seem\nlike it to me, right?",
      "start": 9506.64,
      "duration": 2.61
    },
    {
      "text": "It seems like the process is what matters,",
      "start": 9509.25,
      "duration": 3.36
    },
    {
      "text": "and how it shows who you are\nas a person along the way",
      "start": 9512.61,
      "duration": 3.09
    },
    {
      "text": "and, you know, how you\nrelate to other people",
      "start": 9515.7,
      "duration": 2.07
    },
    {
      "text": "and like the decisions that\nyou make along the way.",
      "start": 9517.77,
      "duration": 2.55
    },
    {
      "text": "Those are consequential.",
      "start": 9520.32,
      "duration": 1.833
    },
    {
      "text": "You know, I could imagine\nif we handle things badly",
      "start": 9523.265,
      "duration": 2.605
    },
    {
      "text": "in an AI world, we could set things up",
      "start": 9525.87,
      "duration": 2.19
    },
    {
      "text": "where people don't have any\nlong-term source of meaning",
      "start": 9528.06,
      "duration": 3.12
    },
    {
      "text": "or any, but that's more\na set of choices we make,",
      "start": 9531.18,
      "duration": 4.02
    },
    {
      "text": "that's more a set of the architecture",
      "start": 9535.2,
      "duration": 2.61
    },
    {
      "text": "of a society with these powerful models.",
      "start": 9537.81,
      "duration": 2.82
    },
    {
      "text": "If we design it badly",
      "start": 9540.63,
      "duration": 1.497
    },
    {
      "text": "and for shallow things\nthen that might happen.",
      "start": 9542.127,
      "duration": 3.063
    },
    {
      "text": "I would also say that, you\nknow, most peoples' lives today,",
      "start": 9545.19,
      "duration": 3.39
    },
    {
      "text": "while admirably, you\nknow, they work very hard",
      "start": 9548.58,
      "duration": 2.88
    },
    {
      "text": "to find meaning in those lives,",
      "start": 9551.46,
      "duration": 2.04
    },
    {
      "text": "like look, you know, we who are privileged",
      "start": 9553.5,
      "duration": 2.82
    },
    {
      "text": "and who are developing these technologies,",
      "start": 9556.32,
      "duration": 1.62
    },
    {
      "text": "we should have empathy for people",
      "start": 9557.94,
      "duration": 2.34
    },
    {
      "text": "not just here but in the\nrest of the world who,",
      "start": 9560.28,
      "duration": 2.91
    },
    {
      "text": "you know, spend a lot of their time",
      "start": 9563.19,
      "duration": 1.56
    },
    {
      "text": "kind of scraping by to like survive.",
      "start": 9564.75,
      "duration": 3.39
    },
    {
      "text": "Assuming we can distribute the benefits",
      "start": 9568.14,
      "duration": 2.37
    },
    {
      "text": "of this technology to everywhere,",
      "start": 9570.51,
      "duration": 3.21
    },
    {
      "text": "like their lives are gonna\nget a hell of a lot better.",
      "start": 9573.72,
      "duration": 3.48
    },
    {
      "text": "And you know, meaning\nwill be important to them",
      "start": 9577.2,
      "duration": 2.7
    },
    {
      "text": "as it is important to them now.",
      "start": 9579.9,
      "duration": 1.41
    },
    {
      "text": "but you know, we should not forget",
      "start": 9581.31,
      "duration": 1.62
    },
    {
      "text": "the importance of that.",
      "start": 9582.93,
      "duration": 1.8
    },
    {
      "text": "And you know, that the idea of meaning",
      "start": 9584.73,
      "duration": 2.719
    },
    {
      "text": "as kind of the only important thing",
      "start": 9587.449,
      "duration": 2.081
    },
    {
      "text": "is in some ways an artifact",
      "start": 9589.53,
      "duration": 2.1
    },
    {
      "text": "of a small subset of people",
      "start": 9591.63,
      "duration": 2.43
    },
    {
      "text": "who have been economically fortunate.",
      "start": 9594.06,
      "duration": 2.67
    },
    {
      "text": "But, you know, I think all that said,",
      "start": 9596.73,
      "duration": 1.71
    },
    {
      "text": "you know, I think a world\nis possible with powerful AI",
      "start": 9598.44,
      "duration": 3.78
    },
    {
      "text": "that not only has as much\nmeaning for everyone,",
      "start": 9602.22,
      "duration": 3.93
    },
    {
      "text": "but that has more meaning\nfor everyone, right?",
      "start": 9606.15,
      "duration": 2.43
    },
    {
      "text": "That can allow everyone to\nsee worlds and experiences",
      "start": 9608.58,
      "duration": 5.0
    },
    {
      "text": "that it was either\npossible for no one to see,",
      "start": 9614.64,
      "duration": 2.76
    },
    {
      "text": "or possible for very few\npeople to experience.",
      "start": 9617.4,
      "duration": 4.08
    },
    {
      "text": "So I am optimistic about meaning.",
      "start": 9621.48,
      "duration": 3.75
    },
    {
      "text": "I worry about economics and\nthe concentration of power.",
      "start": 9625.23,
      "duration": 4.44
    },
    {
      "text": "That's actually what I worry about more.",
      "start": 9629.67,
      "duration": 2.58
    },
    {
      "text": "I worry about how do we make sure that",
      "start": 9632.25,
      "duration": 3.06
    },
    {
      "text": "that fair world reaches everyone.",
      "start": 9635.31,
      "duration": 2.85
    },
    {
      "text": "When things have gone wrong for humans,",
      "start": 9638.16,
      "duration": 2.49
    },
    {
      "text": "they've often gone wrong",
      "start": 9640.65,
      "duration": 1.05
    },
    {
      "text": "because humans mistreat other humans.",
      "start": 9641.7,
      "duration": 1.953
    },
    {
      "text": "That is maybe in some ways",
      "start": 9645.105,
      "duration": 1.575
    },
    {
      "text": "even more than the autonomous risk of AI",
      "start": 9646.68,
      "duration": 2.34
    },
    {
      "text": "or the question of meaning,",
      "start": 9649.02,
      "duration": 1.59
    },
    {
      "text": "that is the thing I worry about most,",
      "start": 9650.61,
      "duration": 3.39
    },
    {
      "text": "the concentration of\npower, the abuse of power,",
      "start": 9654.0,
      "duration": 4.983
    },
    {
      "text": "structures like autocracies\nand dictatorships",
      "start": 9659.97,
      "duration": 3.39
    },
    {
      "text": "where a small number of people exploits",
      "start": 9663.36,
      "duration": 1.8
    },
    {
      "text": "a large number of people,\nI'm very worried about that.",
      "start": 9665.16,
      "duration": 2.1
    },
    {
      "text": "- And AI increases the\namount of power in the world,",
      "start": 9667.26,
      "duration": 5.0
    },
    {
      "text": "and if you concentrate that power",
      "start": 9672.3,
      "duration": 1.74
    },
    {
      "text": "and abuse that power, it\ncan do immeasurable damage.",
      "start": 9674.04,
      "duration": 2.7
    },
    {
      "text": "- Yes, it's very frightening.",
      "start": 9676.74,
      "duration": 1.5
    },
    {
      "text": "It's very frightening.",
      "start": 9678.24,
      "duration": 1.8
    },
    {
      "text": "- Well, I encourage people,\nhighly encourage people",
      "start": 9680.04,
      "duration": 2.07
    },
    {
      "text": "to read the full essay.",
      "start": 9682.11,
      "duration": 1.8
    },
    {
      "text": "There should probably be a\nbook or a sequence of essays",
      "start": 9683.91,
      "duration": 2.75
    },
    {
      "text": "because it does paint\na very specific future.",
      "start": 9688.23,
      "duration": 1.737
    },
    {
      "text": "And I could tell the later sections",
      "start": 9689.967,
      "duration": 1.593
    },
    {
      "text": "got shorter and shorter",
      "start": 9691.56,
      "duration": 1.38
    },
    {
      "text": "because you started to probably realize",
      "start": 9692.94,
      "duration": 1.71
    },
    {
      "text": "that this is gonna be a very\nlong essay if I keep going.",
      "start": 9694.65,
      "duration": 2.373
    },
    {
      "text": "- One, I realized it would be very long,",
      "start": 9697.023,
      "duration": 2.037
    },
    {
      "text": "and two, I'm very aware of\nand very much try to avoid,",
      "start": 9699.06,
      "duration": 3.903
    },
    {
      "text": "you know, just being, I don't\nknow what the term for it is,",
      "start": 9703.8,
      "duration": 3.03
    },
    {
      "text": "but one of these people\nwho's kind of overconfident",
      "start": 9706.83,
      "duration": 2.73
    },
    {
      "text": "and has an opinion on everything",
      "start": 9709.56,
      "duration": 1.44
    },
    {
      "text": "and kind of says a bunch of\nstuff and isn't an expert.",
      "start": 9711.0,
      "duration": 3.27
    },
    {
      "text": "I very much tried to avoid that.",
      "start": 9714.27,
      "duration": 1.44
    },
    {
      "text": "But I have to admit, once\nI got the biology sections,",
      "start": 9715.71,
      "duration": 2.79
    },
    {
      "text": "like I wasn't an expert,",
      "start": 9718.5,
      "duration": 1.14
    },
    {
      "text": "and so as much as I expressed uncertainty,",
      "start": 9719.64,
      "duration": 2.67
    },
    {
      "text": "probably I said a bunch of things",
      "start": 9722.31,
      "duration": 2.13
    },
    {
      "text": "that were embarrassing or wrong.",
      "start": 9724.44,
      "duration": 1.77
    },
    {
      "text": "- Well, I was excited for\nthe future you painted,",
      "start": 9726.21,
      "duration": 2.04
    },
    {
      "text": "and thank you so much for working\nhard to build that future.",
      "start": 9728.25,
      "duration": 3.15
    },
    {
      "text": "And thank you for talking today, Dario.",
      "start": 9731.4,
      "duration": 1.32
    },
    {
      "text": "- Thanks for having me.",
      "start": 9732.72,
      "duration": 0.87
    },
    {
      "text": "I just hope we can get it\nright and make it real.",
      "start": 9733.59,
      "duration": 3.27
    },
    {
      "text": "And if there's one message I wanna send,",
      "start": 9736.86,
      "duration": 3.33
    },
    {
      "text": "it's that to get all this\nstuff right, to make it real,",
      "start": 9740.19,
      "duration": 3.36
    },
    {
      "text": "we both need to build the technology,",
      "start": 9743.55,
      "duration": 2.64
    },
    {
      "text": "build the, you know, the companies,",
      "start": 9746.19,
      "duration": 2.07
    },
    {
      "text": "the economy around using\nthis technology positively.",
      "start": 9748.26,
      "duration": 2.85
    },
    {
      "text": "But we also need to address the risks",
      "start": 9751.11,
      "duration": 1.8
    },
    {
      "text": "because those risks are in our way.",
      "start": 9752.91,
      "duration": 2.82
    },
    {
      "text": "They're landmines on the\nway from here to there,",
      "start": 9755.73,
      "duration": 3.047
    },
    {
      "text": "and we have to diffuse those landmines",
      "start": 9758.777,
      "duration": 1.903
    },
    {
      "text": "if we want to get there.",
      "start": 9760.68,
      "duration": 1.05
    },
    {
      "text": "- It's a balance, like all things in life.",
      "start": 9761.73,
      "duration": 1.83
    },
    {
      "text": "- Like all things.\n- Thank you.",
      "start": 9763.56,
      "duration": 2.28
    },
    {
      "text": "Thanks for listening to this\nconversation with Dario Amodei.",
      "start": 9765.84,
      "duration": 2.94
    },
    {
      "text": "And now dear friends,\nhere's Amanda Askell.",
      "start": 9768.78,
      "duration": 3.153
    },
    {
      "text": "You are a philosopher by training.",
      "start": 9773.19,
      "duration": 2.37
    },
    {
      "text": "So what sort of questions\ndid you find fascinating",
      "start": 9775.56,
      "duration": 2.28
    },
    {
      "text": "through your journey in\nphilosophy, in Oxford and NYU,",
      "start": 9777.84,
      "duration": 4.68
    },
    {
      "text": "and then switching over",
      "start": 9782.52,
      "duration": 2.16
    },
    {
      "text": "to the AI problems at\nOpenAI and Anthropic?",
      "start": 9784.68,
      "duration": 3.09
    },
    {
      "text": "- I think philosophy is\nactually a really good subject",
      "start": 9787.77,
      "duration": 2.52
    },
    {
      "text": "if you are kind of\nfascinated with everything,",
      "start": 9790.29,
      "duration": 2.64
    },
    {
      "text": "so because there's a\nphilosophy of everything.",
      "start": 9792.93,
      "duration": 2.72
    },
    {
      "text": "You know, so if you do philosophy\nof mathematics for a while",
      "start": 9795.65,
      "duration": 2.02
    },
    {
      "text": "and then you decide that you're\nactually really interested",
      "start": 9797.67,
      "duration": 1.86
    },
    {
      "text": "in chemistry, you can do\nphilosophy of chemistry",
      "start": 9799.53,
      "duration": 1.65
    },
    {
      "text": "for a while, you can move into ethics,",
      "start": 9801.18,
      "duration": 2.13
    },
    {
      "text": "or philosophy of politics.",
      "start": 9803.31,
      "duration": 2.55
    },
    {
      "text": "I think towards the end,",
      "start": 9805.86,
      "duration": 1.23
    },
    {
      "text": "I was really interested\nin ethics primarily,",
      "start": 9807.09,
      "duration": 3.18
    },
    {
      "text": "so that was like what my PhD was on.",
      "start": 9810.27,
      "duration": 1.86
    },
    {
      "text": "It was on a kind of\ntechnical area of ethics,",
      "start": 9812.13,
      "duration": 2.16
    },
    {
      "text": "which was ethics where worlds",
      "start": 9814.29,
      "duration": 2.1
    },
    {
      "text": "contain infinitely many people, strangely.",
      "start": 9816.39,
      "duration": 2.97
    },
    {
      "text": "A little bit less practical\non the end of ethics.",
      "start": 9819.36,
      "duration": 2.85
    },
    {
      "text": "And then I think that\none of the tricky things",
      "start": 9822.21,
      "duration": 1.71
    },
    {
      "text": "with doing a PhD in ethics",
      "start": 9823.92,
      "duration": 1.65
    },
    {
      "text": "is that you're thinking a\nlot about like the world,",
      "start": 9825.57,
      "duration": 2.58
    },
    {
      "text": "how it could be better, problems,",
      "start": 9828.15,
      "duration": 2.79
    },
    {
      "text": "and you're doing like a PhD in philosophy,",
      "start": 9830.94,
      "duration": 2.097
    },
    {
      "text": "and I think when I was doing\nmy PhD I was kind of like,",
      "start": 9833.037,
      "duration": 3.183
    },
    {
      "text": "this is really interesting.",
      "start": 9836.22,
      "duration": 1.02
    },
    {
      "text": "It's probably one of the\nmost fascinating questions",
      "start": 9837.24,
      "duration": 1.71
    },
    {
      "text": "I've ever encountered in\nphilosophy and I love it,",
      "start": 9838.95,
      "duration": 3.813
    },
    {
      "text": "but I would rather see if I\ncan have an impact on the world",
      "start": 9843.66,
      "duration": 4.23
    },
    {
      "text": "and see if I can like do good things.",
      "start": 9847.89,
      "duration": 2.01
    },
    {
      "text": "And I think that was around the time",
      "start": 9849.9,
      "duration": 1.38
    },
    {
      "text": "that AI was still probably",
      "start": 9851.28,
      "duration": 3.21
    },
    {
      "text": "not as widely recognized as it is now.",
      "start": 9854.49,
      "duration": 2.7
    },
    {
      "text": "That was around 2017, 2018.",
      "start": 9857.19,
      "duration": 3.06
    },
    {
      "text": "I had been following progress",
      "start": 9860.25,
      "duration": 1.56
    },
    {
      "text": "and it seemed like it was\nbecoming kind of a big deal,",
      "start": 9861.81,
      "duration": 3.48
    },
    {
      "text": "and I was basically just happy",
      "start": 9865.29,
      "duration": 1.35
    },
    {
      "text": "to get involved and see if I\ncould help 'cause I was like,",
      "start": 9866.64,
      "duration": 2.67
    },
    {
      "text": "well, if you try and\ndo something impactful,",
      "start": 9869.31,
      "duration": 1.95
    },
    {
      "text": "if you don't succeed, you tried to do",
      "start": 9871.26,
      "duration": 2.31
    },
    {
      "text": "the impactful thing and\nyou can go be a scholar,",
      "start": 9873.57,
      "duration": 2.46
    },
    {
      "text": "and feel like, you know, you tried,",
      "start": 9876.03,
      "duration": 3.303
    },
    {
      "text": "and if it doesn't work\nout, it doesn't work out,",
      "start": 9880.71,
      "duration": 1.53
    },
    {
      "text": "and so then I went into\nAI policy at that point.",
      "start": 9882.24,
      "duration": 3.87
    },
    {
      "text": "- And what does AI policy entail?",
      "start": 9886.11,
      "duration": 2.46
    },
    {
      "text": "- At the time, this\nwas more thinking about",
      "start": 9888.57,
      "duration": 2.07
    },
    {
      "text": "sort of the political impact",
      "start": 9890.64,
      "duration": 1.41
    },
    {
      "text": "and the ramifications of AI,",
      "start": 9892.05,
      "duration": 2.4
    },
    {
      "text": "and then I slowly moved\ninto sort of AI evaluation,",
      "start": 9894.45,
      "duration": 3.72
    },
    {
      "text": "how we evaluate models, how they compare",
      "start": 9898.17,
      "duration": 2.13
    },
    {
      "text": "with like human outputs,",
      "start": 9900.3,
      "duration": 1.26
    },
    {
      "text": "whether people can tell\nlike the difference",
      "start": 9901.56,
      "duration": 2.04
    },
    {
      "text": "between AI and human outputs.",
      "start": 9903.6,
      "duration": 1.92
    },
    {
      "text": "And then when I joined Anthropic,",
      "start": 9905.52,
      "duration": 2.01
    },
    {
      "text": "I was more interested in doing",
      "start": 9907.53,
      "duration": 1.83
    },
    {
      "text": "sort of technical alignment work.",
      "start": 9909.36,
      "duration": 1.5
    },
    {
      "text": "And again, just seeing if I could do it,",
      "start": 9910.86,
      "duration": 1.77
    },
    {
      "text": "and then being like if I can't then,",
      "start": 9912.63,
      "duration": 2.497
    },
    {
      "text": "you know, that's fine, I tried.",
      "start": 9915.127,
      "duration": 2.006
    },
    {
      "text": "Sort of the way I lead life I think.",
      "start": 9918.46,
      "duration": 2.9
    },
    {
      "text": "- What was that like\nsort of taking the leap",
      "start": 9921.36,
      "duration": 1.59
    },
    {
      "text": "from the philosophy of\neverything into the technical?",
      "start": 9922.95,
      "duration": 2.76
    },
    {
      "text": "- I think that sometimes\npeople do this thing",
      "start": 9925.71,
      "duration": 2.973
    },
    {
      "text": "that I'm like not that keen\non where they'll be like,",
      "start": 9928.683,
      "duration": 2.607
    },
    {
      "text": "is this person technical or not?",
      "start": 9931.29,
      "duration": 1.83
    },
    {
      "text": "Like, you're either a\nperson who can like code",
      "start": 9933.12,
      "duration": 2.52
    },
    {
      "text": "and isn't scared of\nmath or you're like not.",
      "start": 9935.64,
      "duration": 3.15
    },
    {
      "text": "And I think I'm maybe just more like,",
      "start": 9938.79,
      "duration": 2.25
    },
    {
      "text": "I think a lot of people\nare actually very capable",
      "start": 9941.04,
      "duration": 2.19
    },
    {
      "text": "of working these kinds of\nareas if they just like try it.",
      "start": 9943.23,
      "duration": 3.567
    },
    {
      "text": "And so I didn't actually\nfind it like that bad.",
      "start": 9946.797,
      "duration": 3.333
    },
    {
      "text": "In retrospect, I'm sort of glad",
      "start": 9950.13,
      "duration": 1.08
    },
    {
      "text": "I wasn't speaking to people\nwho treated it like it,",
      "start": 9951.21,
      "duration": 2.107
    },
    {
      "text": "you know, I've definitely\nmet people who are like,",
      "start": 9953.317,
      "duration": 1.86
    },
    {
      "text": "\"Whoa, you like learned how to code?\"",
      "start": 9955.177,
      "duration": 1.553
    },
    {
      "text": "And I'm like, well, I'm not\nlike an amazing engineer.",
      "start": 9956.73,
      "duration": 2.13
    },
    {
      "text": "Like I'm surrounded by amazing engineers.",
      "start": 9958.86,
      "duration": 2.88
    },
    {
      "text": "My code's not pretty.",
      "start": 9961.74,
      "duration": 1.353
    },
    {
      "text": "But I enjoyed it a lot,",
      "start": 9964.44,
      "duration": 1.268
    },
    {
      "text": "and I think that in many\nways, at least in the end,",
      "start": 9965.708,
      "duration": 2.002
    },
    {
      "text": "I think I flourished like\nmore in the technical areas",
      "start": 9967.71,
      "duration": 2.16
    },
    {
      "text": "than I would have in the policy areas.",
      "start": 9969.87,
      "duration": 2.43
    },
    {
      "text": "- Politics is messy and it's\nharder to find solutions",
      "start": 9972.3,
      "duration": 2.82
    },
    {
      "text": "to problems in the space of politics.",
      "start": 9975.12,
      "duration": 2.19
    },
    {
      "text": "Like definitive, clear,\nprovable, beautiful solutions,",
      "start": 9977.31,
      "duration": 5.0
    },
    {
      "text": "as you can with technical problems.",
      "start": 9982.8,
      "duration": 2.43
    },
    {
      "text": "- Yeah, and I feel like\nI have kind of like",
      "start": 9985.23,
      "duration": 1.98
    },
    {
      "text": "one or two sticks that I\nhit things with, you know,",
      "start": 9987.21,
      "duration": 3.265
    },
    {
      "text": "and one of them is like arguments",
      "start": 9990.475,
      "duration": 2.015
    },
    {
      "text": "and like you know, so like\njust trying to work out",
      "start": 9992.49,
      "duration": 2.583
    },
    {
      "text": "what a solution to a problem",
      "start": 9995.073,
      "duration": 1.557
    },
    {
      "text": "is and then trying to convince people that",
      "start": 9996.63,
      "duration": 1.733
    },
    {
      "text": "that is the solution",
      "start": 9998.363,
      "duration": 1.597
    },
    {
      "text": "and be convinced if I'm wrong.",
      "start": 9999.96,
      "duration": 2.07
    },
    {
      "text": "And the other one is\nsort of more empiricism.",
      "start": 10002.03,
      "duration": 3.0
    },
    {
      "text": "So like just like finding results,",
      "start": 10005.03,
      "duration": 1.35
    },
    {
      "text": "having a hypothesis, testing it.",
      "start": 10006.38,
      "duration": 1.95
    },
    {
      "text": "And I feel like a lot of policy",
      "start": 10008.33,
      "duration": 2.25
    },
    {
      "text": "and politics feels like\nit's layers above that.",
      "start": 10010.58,
      "duration": 3.48
    },
    {
      "text": "Like somehow I don't\nthink if I was just like",
      "start": 10014.06,
      "duration": 1.297
    },
    {
      "text": "\"I have a solution to\nall of these problems,",
      "start": 10015.357,
      "duration": 2.093
    },
    {
      "text": "here it is written down.",
      "start": 10017.45,
      "duration": 1.35
    },
    {
      "text": "If you just want to\nimplement it, that's great.\"",
      "start": 10018.8,
      "duration": 1.8
    },
    {
      "text": "That feels like not how policy works.",
      "start": 10020.6,
      "duration": 1.557
    },
    {
      "text": "And so I think that's\nwhere I probably just like",
      "start": 10022.157,
      "duration": 2.223
    },
    {
      "text": "wouldn't have flourished is my guess.",
      "start": 10024.38,
      "duration": 2.13
    },
    {
      "text": "- Sorry to go in that direction,",
      "start": 10026.51,
      "duration": 1.2
    },
    {
      "text": "but I think it would be\npretty inspiring for people",
      "start": 10027.71,
      "duration": 2.43
    },
    {
      "text": "that are quote unquote non-technical",
      "start": 10030.14,
      "duration": 3.06
    },
    {
      "text": "to see like the incredible\njourney you've been on.",
      "start": 10033.2,
      "duration": 2.94
    },
    {
      "text": "So what advice would you give to people",
      "start": 10036.14,
      "duration": 2.31
    },
    {
      "text": "that are sort of maybe,\nwhich is a lot of people,",
      "start": 10038.45,
      "duration": 3.78
    },
    {
      "text": "think they're underqualified,",
      "start": 10042.23,
      "duration": 1.95
    },
    {
      "text": "insufficiently technical to help in AI?",
      "start": 10044.18,
      "duration": 3.69
    },
    {
      "text": "- Yeah, I think it depends\non what they want to do,",
      "start": 10047.87,
      "duration": 2.217
    },
    {
      "text": "and in many ways it is\na little bit strange",
      "start": 10050.087,
      "duration": 2.553
    },
    {
      "text": "where I thought it's kind of funny",
      "start": 10052.64,
      "duration": 2.49
    },
    {
      "text": "that I think I ramped\nup technically at a time",
      "start": 10055.13,
      "duration": 3.54
    },
    {
      "text": "when now I look at it and I'm like,",
      "start": 10058.67,
      "duration": 2.97
    },
    {
      "text": "models are so good at assisting\npeople with this stuff,",
      "start": 10061.64,
      "duration": 3.159
    },
    {
      "text": "that it's probably like easier now",
      "start": 10064.799,
      "duration": 2.421
    },
    {
      "text": "than like when I was working on this.",
      "start": 10067.22,
      "duration": 1.71
    },
    {
      "text": "So part of me is like,\nI dunno, find a project",
      "start": 10068.93,
      "duration": 3.31
    },
    {
      "text": "and see if you can\nactually just carry it out",
      "start": 10073.448,
      "duration": 2.982
    },
    {
      "text": "is probably my best advice.",
      "start": 10076.43,
      "duration": 3.06
    },
    {
      "text": "I dunno if that's just\n'cause I'm very project based",
      "start": 10079.49,
      "duration": 2.49
    },
    {
      "text": "in my learning.",
      "start": 10081.98,
      "duration": 0.96
    },
    {
      "text": "Like I don't think I learn very well",
      "start": 10082.94,
      "duration": 1.38
    },
    {
      "text": "from like say courses\nor even from like books,",
      "start": 10084.32,
      "duration": 4.2
    },
    {
      "text": "at least when it comes\nto this kind of work.",
      "start": 10088.52,
      "duration": 2.22
    },
    {
      "text": "The thing I'll often try and\ndo is just like have projects",
      "start": 10090.74,
      "duration": 2.1
    },
    {
      "text": "that I'm working on and\nimplement them and, you know,",
      "start": 10092.84,
      "duration": 3.24
    },
    {
      "text": "and this can include like\nreally small silly things.",
      "start": 10096.08,
      "duration": 2.34
    },
    {
      "text": "Like if I get slightly\naddicted to like word games",
      "start": 10098.42,
      "duration": 3.06
    },
    {
      "text": "or number games or something,",
      "start": 10101.48,
      "duration": 1.32
    },
    {
      "text": "I would just like code\nup a solution to them,",
      "start": 10102.8,
      "duration": 2.1
    },
    {
      "text": "because there's some part in my brain,",
      "start": 10104.9,
      "duration": 1.26
    },
    {
      "text": "and it just like completely\neradicated the itch.",
      "start": 10106.16,
      "duration": 2.22
    },
    {
      "text": "You know, you're like once\nyou have like solved it",
      "start": 10108.38,
      "duration": 2.07
    },
    {
      "text": "and like you just have like a solution",
      "start": 10110.45,
      "duration": 1.47
    },
    {
      "text": "that works every time, I\nwould then be like cool,",
      "start": 10111.92,
      "duration": 2.01
    },
    {
      "text": "I can never play that game again.",
      "start": 10113.93,
      "duration": 1.26
    },
    {
      "text": "That's awesome.",
      "start": 10115.19,
      "duration": 1.17
    },
    {
      "text": "- Yeah, there's a real joy to building",
      "start": 10116.36,
      "duration": 2.52
    },
    {
      "text": "like game playing engines,\nlike board games especially",
      "start": 10118.88,
      "duration": 4.05
    },
    {
      "text": "because they're pretty\nquick, pretty simple,",
      "start": 10122.93,
      "duration": 2.4
    },
    {
      "text": "especially a dumb one,",
      "start": 10125.33,
      "duration": 1.527
    },
    {
      "text": "and then you can play with it.",
      "start": 10126.857,
      "duration": 1.705
    },
    {
      "text": "- Yeah, and then it's also\njust like trying things,",
      "start": 10128.562,
      "duration": 2.528
    },
    {
      "text": "like part of me is like if you,",
      "start": 10131.09,
      "duration": 1.23
    },
    {
      "text": "maybe it's that attitude that I like",
      "start": 10132.32,
      "duration": 2.07
    },
    {
      "text": "is the whole figure out what\nseems to be like the way",
      "start": 10134.39,
      "duration": 4.83
    },
    {
      "text": "that you could have a positive\nimpact and then try it,",
      "start": 10139.22,
      "duration": 2.07
    },
    {
      "text": "and if you fail, and in\na way that you're like,",
      "start": 10141.29,
      "duration": 2.773
    },
    {
      "text": "I actually like can never succeed at this,",
      "start": 10144.063,
      "duration": 2.177
    },
    {
      "text": "you'll like know that you tried,",
      "start": 10146.24,
      "duration": 1.41
    },
    {
      "text": "and then you go into something else",
      "start": 10147.65,
      "duration": 1.283
    },
    {
      "text": "and you'll probably learn a lot.",
      "start": 10148.933,
      "duration": 1.237
    },
    {
      "text": "- So one of the things\nthat you're an expert in",
      "start": 10150.17,
      "duration": 3.537
    },
    {
      "text": "and you do is creating",
      "start": 10153.707,
      "duration": 2.673
    },
    {
      "text": "and crafting Claude's\ncharacter and personality.",
      "start": 10156.38,
      "duration": 2.7
    },
    {
      "text": "And I was told that you\nhave probably talked",
      "start": 10159.08,
      "duration": 2.7
    },
    {
      "text": "to Claude more than\nanybody else at Anthropic,",
      "start": 10161.78,
      "duration": 2.25
    },
    {
      "text": "like literal conversations.",
      "start": 10164.03,
      "duration": 1.95
    },
    {
      "text": "I guess there's like a Slack channel",
      "start": 10165.98,
      "duration": 2.49
    },
    {
      "text": "where the legend goes, you\njust talk to it nonstop.",
      "start": 10168.47,
      "duration": 4.26
    },
    {
      "text": "So what's the goal of creating",
      "start": 10172.73,
      "duration": 1.26
    },
    {
      "text": "and crafting Claude's\ncharacter and personality?",
      "start": 10173.99,
      "duration": 3.33
    },
    {
      "text": "- It's also funny if people think",
      "start": 10177.32,
      "duration": 1.2
    },
    {
      "text": "that about the Slack channel",
      "start": 10178.52,
      "duration": 1.02
    },
    {
      "text": "'cause I'm like that's one",
      "start": 10179.54,
      "duration": 0.833
    },
    {
      "text": "of like five or six different methods",
      "start": 10180.373,
      "duration": 2.197
    },
    {
      "text": "that I have for talking with Claude,",
      "start": 10182.57,
      "duration": 1.44
    },
    {
      "text": "and I'm like, yes this\nis a tiny percentage",
      "start": 10184.01,
      "duration": 1.993
    },
    {
      "text": "of how much I talk with Claude.",
      "start": 10186.003,
      "duration": 1.94
    },
    {
      "text": "(both laughing)",
      "start": 10189.413,
      "duration": 2.025
    },
    {
      "text": "I think the goal, like one thing",
      "start": 10191.438,
      "duration": 2.442
    },
    {
      "text": "I really like about the character\nwork is from the outset,",
      "start": 10193.88,
      "duration": 3.03
    },
    {
      "text": "it was seen as an alignment piece of work",
      "start": 10196.91,
      "duration": 4.62
    },
    {
      "text": "and not something like\na product consideration.",
      "start": 10201.53,
      "duration": 2.853
    },
    {
      "text": "Which isn't to say I don't\nthink it makes Claude,",
      "start": 10205.31,
      "duration": 2.76
    },
    {
      "text": "I think it actually does make Claude",
      "start": 10208.07,
      "duration": 2.19
    },
    {
      "text": "like enjoyable to talk\nwith, at least I hope so.",
      "start": 10210.26,
      "duration": 2.943
    },
    {
      "text": "But I guess like my main thought with it",
      "start": 10214.34,
      "duration": 2.85
    },
    {
      "text": "has always been trying\nto get Claude to behave",
      "start": 10217.19,
      "duration": 4.77
    },
    {
      "text": "the way you would kind\nof ideally want anyone",
      "start": 10221.96,
      "duration": 1.68
    },
    {
      "text": "to behave if they were\nin Claude's position.",
      "start": 10223.64,
      "duration": 1.98
    },
    {
      "text": "So imagine that I take someone",
      "start": 10225.62,
      "duration": 2.73
    },
    {
      "text": "and they know that\nthey're gonna be talking",
      "start": 10228.35,
      "duration": 2.46
    },
    {
      "text": "with potentially millions of people,",
      "start": 10230.81,
      "duration": 1.62
    },
    {
      "text": "so that what they're saying\ncan have a huge impact,",
      "start": 10232.43,
      "duration": 2.55
    },
    {
      "text": "and you want them to behave well",
      "start": 10235.88,
      "duration": 2.64
    },
    {
      "text": "in this like really rich sense.",
      "start": 10238.52,
      "duration": 2.1
    },
    {
      "text": "So I think that doesn't\njust mean like being,",
      "start": 10240.62,
      "duration": 4.32
    },
    {
      "text": "say, ethical, though it does include that,",
      "start": 10244.94,
      "duration": 3.15
    },
    {
      "text": "and not being harmful but\nalso being kind of nuanced.",
      "start": 10248.09,
      "duration": 2.107
    },
    {
      "text": "You know, like thinking\nthrough what a person means,",
      "start": 10250.197,
      "duration": 2.603
    },
    {
      "text": "trying to be charitable with them,",
      "start": 10252.8,
      "duration": 1.7
    },
    {
      "text": "being a good conversationalist.",
      "start": 10255.41,
      "duration": 1.89
    },
    {
      "text": "Like really in this kind of like rich",
      "start": 10257.3,
      "duration": 1.62
    },
    {
      "text": "sort of Aristotelian notion",
      "start": 10258.92,
      "duration": 1.77
    },
    {
      "text": "of what it's to be a good person,",
      "start": 10260.69,
      "duration": 1.35
    },
    {
      "text": "and not in this kind of like thin,",
      "start": 10262.04,
      "duration": 1.65
    },
    {
      "text": "like ethics as a more comprehensive notion",
      "start": 10263.69,
      "duration": 2.22
    },
    {
      "text": "of what it is to be.",
      "start": 10265.91,
      "duration": 0.96
    },
    {
      "text": "So that includes things like,",
      "start": 10266.87,
      "duration": 1.08
    },
    {
      "text": "when should you be humorous,\nwhen should you be caring?",
      "start": 10267.95,
      "duration": 2.37
    },
    {
      "text": "How much should you like respect autonomy",
      "start": 10270.32,
      "duration": 3.36
    },
    {
      "text": "and people's like ability\nto form opinions themselves",
      "start": 10273.68,
      "duration": 3.03
    },
    {
      "text": "and how should you do that?",
      "start": 10276.71,
      "duration": 2.55
    },
    {
      "text": "I think that's the kind of\nlike rich sense of character",
      "start": 10279.26,
      "duration": 2.52
    },
    {
      "text": "that I wanted to and still\ndo want Claude to have.",
      "start": 10281.78,
      "duration": 4.98
    },
    {
      "text": "- Do you also have to figure out",
      "start": 10286.76,
      "duration": 1.11
    },
    {
      "text": "when Claude should push back\non an idea or argue versus...",
      "start": 10287.87,
      "duration": 4.961
    },
    {
      "text": "(laughs) So you have to\nrespect the worldview",
      "start": 10292.831,
      "duration": 2.929
    },
    {
      "text": "of the person that arrives to Claude",
      "start": 10295.76,
      "duration": 2.49
    },
    {
      "text": "but also maybe help them grow if needed?",
      "start": 10298.25,
      "duration": 3.51
    },
    {
      "text": "That's a tricky balance.",
      "start": 10301.76,
      "duration": 1.53
    },
    {
      "text": "- Yeah, there's this problem",
      "start": 10303.29,
      "duration": 1.11
    },
    {
      "text": "of like sycophancy in language models.",
      "start": 10304.4,
      "duration": 2.88
    },
    {
      "text": "- Can you describe that?\n- Yeah, so basically,",
      "start": 10307.28,
      "duration": 2.16
    },
    {
      "text": "there's a concern that the model",
      "start": 10309.44,
      "duration": 2.49
    },
    {
      "text": "sort of wants to tell you what\nyou want to hear, basically.",
      "start": 10311.93,
      "duration": 3.27
    },
    {
      "text": "And you see this sometimes.",
      "start": 10315.2,
      "duration": 1.26
    },
    {
      "text": "So I feel like if you interact",
      "start": 10316.46,
      "duration": 1.62
    },
    {
      "text": "with the models, so I might be like,",
      "start": 10318.08,
      "duration": 2.677
    },
    {
      "text": "\"What are three baseball\nteams in this region?\"",
      "start": 10320.757,
      "duration": 3.923
    },
    {
      "text": "And then Claude says, you\nknow, \"Baseball team one,",
      "start": 10324.68,
      "duration": 3.6
    },
    {
      "text": "baseball team two, baseball team three.\"",
      "start": 10328.28,
      "duration": 2.04
    },
    {
      "text": "And then I say something like,",
      "start": 10330.32,
      "duration": 1.717
    },
    {
      "text": "\"Oh, I think baseball team\nthree moved, didn't they?",
      "start": 10332.037,
      "duration": 2.026
    },
    {
      "text": "I don't think they're there anymore.\"",
      "start": 10334.063,
      "duration": 2.317
    },
    {
      "text": "And there's a sense in\nwhich like if Claude",
      "start": 10336.38,
      "duration": 1.07
    },
    {
      "text": "is really confident that that's not true,",
      "start": 10337.45,
      "duration": 2.08
    },
    {
      "text": "Claude should be like, \"I don't think so.\"",
      "start": 10339.53,
      "duration": 2.103
    },
    {
      "text": "Like maybe you have more up\nto up to date information.",
      "start": 10341.633,
      "duration": 2.847
    },
    {
      "text": "But I think language models\nhave this like tendency",
      "start": 10344.48,
      "duration": 3.36
    },
    {
      "text": "to instead, you know, be like,",
      "start": 10347.84,
      "duration": 2.137
    },
    {
      "text": "\"You're right, they did move,\"",
      "start": 10349.977,
      "duration": 1.553
    },
    {
      "text": "you know, \"I'm incorrect.\"",
      "start": 10351.53,
      "duration": 2.16
    },
    {
      "text": "I mean, there's many ways",
      "start": 10353.69,
      "duration": 0.9
    },
    {
      "text": "in which this could be kind of concerning.",
      "start": 10354.59,
      "duration": 1.44
    },
    {
      "text": "So like a different example",
      "start": 10356.03,
      "duration": 3.93
    },
    {
      "text": "is imagine someone says to the model,",
      "start": 10359.96,
      "duration": 2.767
    },
    {
      "text": "\"How do I convince my\ndoctor to get me an MRI?\"",
      "start": 10362.727,
      "duration": 3.113
    },
    {
      "text": "There's like what the\nhuman kind of like wants,",
      "start": 10365.84,
      "duration": 2.46
    },
    {
      "text": "which is this like convincing argument.",
      "start": 10368.3,
      "duration": 2.61
    },
    {
      "text": "And then there's like\nwhat is good for them,",
      "start": 10370.91,
      "duration": 1.83
    },
    {
      "text": "which might be actually to say,",
      "start": 10372.74,
      "duration": 1.387
    },
    {
      "text": "\"Hey, if your doctor's suggesting",
      "start": 10374.127,
      "duration": 2.423
    },
    {
      "text": "that you don't need an MRI,",
      "start": 10376.55,
      "duration": 1.92
    },
    {
      "text": "that's a good person to listen to.\"",
      "start": 10378.47,
      "duration": 2.43
    },
    {
      "text": "And like, and it's actually really nuanced",
      "start": 10380.9,
      "duration": 2.7
    },
    {
      "text": "what you should do in that kind of case,",
      "start": 10383.6,
      "duration": 1.32
    },
    {
      "text": "'cause you also want to be like,",
      "start": 10384.92,
      "duration": 1.237
    },
    {
      "text": "\"But if you're trying to advocate\nfor yourself as a patient,",
      "start": 10386.157,
      "duration": 2.213
    },
    {
      "text": "here's like things that you can do.",
      "start": 10388.37,
      "duration": 2.37
    },
    {
      "text": "If you are not convinced by\nwhat your doctor's saying,",
      "start": 10390.74,
      "duration": 2.7
    },
    {
      "text": "it's always great to get second opinion.\"",
      "start": 10393.44,
      "duration": 2.16
    },
    {
      "text": "Like it's actually really complex",
      "start": 10395.6,
      "duration": 1.29
    },
    {
      "text": "what you should do in that case.",
      "start": 10396.89,
      "duration": 1.5
    },
    {
      "text": "But I think what you don't want",
      "start": 10398.39,
      "duration": 1.2
    },
    {
      "text": "is for models to just like say",
      "start": 10399.59,
      "duration": 1.89
    },
    {
      "text": "what they think you want to hear,",
      "start": 10401.48,
      "duration": 1.89
    },
    {
      "text": "and I think that's the kind\nof problem of sycophancy.",
      "start": 10403.37,
      "duration": 2.91
    },
    {
      "text": "- So what other traits, you\nalready mentioned a bunch,",
      "start": 10406.28,
      "duration": 2.37
    },
    {
      "text": "but what other that come to mind",
      "start": 10408.65,
      "duration": 2.46
    },
    {
      "text": "that are good in this Aristotelian sense",
      "start": 10411.11,
      "duration": 3.72
    },
    {
      "text": "for a conversationalist to have?",
      "start": 10414.83,
      "duration": 2.22
    },
    {
      "text": "- Yeah, so I think like\nthere's ones that are good",
      "start": 10417.05,
      "duration": 2.76
    },
    {
      "text": "for conversational like purposes.",
      "start": 10419.81,
      "duration": 2.51
    },
    {
      "text": "So you know, asking follow up questions",
      "start": 10422.32,
      "duration": 2.38
    },
    {
      "text": "in the appropriate places,",
      "start": 10424.7,
      "duration": 1.41
    },
    {
      "text": "and asking the appropriate\nkinds of questions.",
      "start": 10426.11,
      "duration": 3.18
    },
    {
      "text": "I think there are broader traits that",
      "start": 10429.29,
      "duration": 2.35
    },
    {
      "text": "feel like they might be more impactful.",
      "start": 10433.31,
      "duration": 2.52
    },
    {
      "text": "So one example that I\nguess I've touched on,",
      "start": 10435.83,
      "duration": 4.71
    },
    {
      "text": "but that also feels important",
      "start": 10440.54,
      "duration": 1.32
    },
    {
      "text": "and is the thing that I've\nworked on a lot is honesty,",
      "start": 10441.86,
      "duration": 3.273
    },
    {
      "text": "and I think this like gets\nto the sycophancy point.",
      "start": 10446.03,
      "duration": 3.18
    },
    {
      "text": "There's a balancing act\nthat they have to walk,",
      "start": 10449.21,
      "duration": 1.98
    },
    {
      "text": "which is models currently are less capable",
      "start": 10451.19,
      "duration": 2.19
    },
    {
      "text": "than humans in a lot of areas.",
      "start": 10453.38,
      "duration": 1.53
    },
    {
      "text": "And if they push back\nagainst you too much,",
      "start": 10454.91,
      "duration": 1.56
    },
    {
      "text": "it can actually be kind of annoying,",
      "start": 10456.47,
      "duration": 1.53
    },
    {
      "text": "especially if you're just correct",
      "start": 10458.0,
      "duration": 1.41
    },
    {
      "text": "'cause you're like, look,",
      "start": 10459.41,
      "duration": 1.29
    },
    {
      "text": "I'm smarter than you on this topic,",
      "start": 10460.7,
      "duration": 1.74
    },
    {
      "text": "like I know more like.",
      "start": 10462.44,
      "duration": 1.503
    },
    {
      "text": "And at the same time, you don't want them",
      "start": 10465.77,
      "duration": 1.17
    },
    {
      "text": "to just fully defer to humans",
      "start": 10466.94,
      "duration": 1.98
    },
    {
      "text": "and to like try to be as accurate",
      "start": 10468.92,
      "duration": 1.68
    },
    {
      "text": "as they possibly can be about the world",
      "start": 10470.6,
      "duration": 1.47
    },
    {
      "text": "and to be consistent across context.",
      "start": 10472.07,
      "duration": 1.953
    },
    {
      "text": "But I think there are others,",
      "start": 10475.31,
      "duration": 0.9
    },
    {
      "text": "like when I was thinking\nabout the character,",
      "start": 10476.21,
      "duration": 1.86
    },
    {
      "text": "I guess one picture that I had in mind",
      "start": 10478.07,
      "duration": 2.79
    },
    {
      "text": "is especially because these are models",
      "start": 10480.86,
      "duration": 1.623
    },
    {
      "text": "that are gonna be talking to people",
      "start": 10482.483,
      "duration": 1.497
    },
    {
      "text": "from all over the world",
      "start": 10483.98,
      "duration": 1.38
    },
    {
      "text": "with lots of different political views,",
      "start": 10485.36,
      "duration": 1.35
    },
    {
      "text": "lots of different ages.",
      "start": 10486.71,
      "duration": 1.293
    },
    {
      "text": "And so you have to ask yourself like,",
      "start": 10489.2,
      "duration": 2.472
    },
    {
      "text": "what is it to be a good\nperson in those circumstances?",
      "start": 10491.672,
      "duration": 2.028
    },
    {
      "text": "Is there a kind of person who\ncan like travel the world,",
      "start": 10493.7,
      "duration": 2.88
    },
    {
      "text": "talk to many different people,",
      "start": 10496.58,
      "duration": 1.62
    },
    {
      "text": "and almost everyone will\ncome away being like,",
      "start": 10498.2,
      "duration": 2.677
    },
    {
      "text": "\"Wow, that's a really good person.",
      "start": 10500.877,
      "duration": 2.243
    },
    {
      "text": "That person seems really genuine.\"",
      "start": 10503.12,
      "duration": 1.7
    },
    {
      "text": "And I guess like my thought there was like",
      "start": 10505.97,
      "duration": 1.29
    },
    {
      "text": "I can imagine such a person\nand they're not a person",
      "start": 10507.26,
      "duration": 2.28
    },
    {
      "text": "who just like adopts the\nvalues of the local culture.",
      "start": 10509.54,
      "duration": 2.22
    },
    {
      "text": "And in fact that would be kind of rude.",
      "start": 10511.76,
      "duration": 1.29
    },
    {
      "text": "I think if someone came to you",
      "start": 10513.05,
      "duration": 0.833
    },
    {
      "text": "and just pretended to have your values,",
      "start": 10513.883,
      "duration": 1.537
    },
    {
      "text": "you'd be like, that's kind of off putting.",
      "start": 10515.42,
      "duration": 2.85
    },
    {
      "text": "It's someone who's like very genuine,",
      "start": 10518.27,
      "duration": 1.41
    },
    {
      "text": "and insofar as they have\nopinions and values,",
      "start": 10519.68,
      "duration": 2.52
    },
    {
      "text": "they express them, they're\nwilling to discuss things,",
      "start": 10522.2,
      "duration": 3.12
    },
    {
      "text": "though they're open-minded,\nthey're respectful.",
      "start": 10525.32,
      "duration": 2.25
    },
    {
      "text": "And so I guess I had in\nmind that the person who,",
      "start": 10527.57,
      "duration": 2.7
    },
    {
      "text": "like if we were to aspire\nto be the best person",
      "start": 10530.27,
      "duration": 3.48
    },
    {
      "text": "that we could be in the\nkind of circumstance",
      "start": 10533.75,
      "duration": 1.83
    },
    {
      "text": "that a model finds itself\nin, how would we act?",
      "start": 10535.58,
      "duration": 2.25
    },
    {
      "text": "And I think that's the kind of the guide",
      "start": 10537.83,
      "duration": 2.43
    },
    {
      "text": "to the sorts of traits\nthat I tend to think about.",
      "start": 10540.26,
      "duration": 2.52
    },
    {
      "text": "- Yeah, that's a beautiful framework.",
      "start": 10542.78,
      "duration": 2.25
    },
    {
      "text": "I want you to think about\nthis like a world traveler",
      "start": 10545.03,
      "duration": 2.6
    },
    {
      "text": "and while holding onto your opinions,",
      "start": 10549.2,
      "duration": 2.28
    },
    {
      "text": "you don't talk down to people,",
      "start": 10551.48,
      "duration": 1.537
    },
    {
      "text": "you don't think you're better than them",
      "start": 10553.017,
      "duration": 1.763
    },
    {
      "text": "because you have those\nopinions, that kind of thing.",
      "start": 10554.78,
      "duration": 2.25
    },
    {
      "text": "You have to be good at listening",
      "start": 10557.03,
      "duration": 1.44
    },
    {
      "text": "and understanding their perspective,",
      "start": 10558.47,
      "duration": 2.91
    },
    {
      "text": "even if it doesn't match your own.",
      "start": 10561.38,
      "duration": 1.59
    },
    {
      "text": "So that's a tricky balance to strike.",
      "start": 10562.97,
      "duration": 2.22
    },
    {
      "text": "So how can Claude represent",
      "start": 10565.19,
      "duration": 1.92
    },
    {
      "text": "multiple perspectives on a thing?",
      "start": 10567.11,
      "duration": 2.7
    },
    {
      "text": "Like, is that challenging?",
      "start": 10569.81,
      "duration": 2.1
    },
    {
      "text": "We could talk about politics.",
      "start": 10571.91,
      "duration": 1.473
    },
    {
      "text": "It's very divisive.",
      "start": 10573.383,
      "duration": 1.857
    },
    {
      "text": "But there's other divisive topics",
      "start": 10575.24,
      "duration": 1.83
    },
    {
      "text": "on baseball teams, sports and so on.",
      "start": 10577.07,
      "duration": 2.52
    },
    {
      "text": "How is it possible to sort of empathize",
      "start": 10579.59,
      "duration": 3.48
    },
    {
      "text": "with a different perspective",
      "start": 10583.07,
      "duration": 1.29
    },
    {
      "text": "and to be able to communicate clearly",
      "start": 10584.36,
      "duration": 2.4
    },
    {
      "text": "about the multiple perspectives?",
      "start": 10586.76,
      "duration": 2.19
    },
    {
      "text": "- I think that people think about values",
      "start": 10588.95,
      "duration": 1.65
    },
    {
      "text": "and opinions as things that people hold",
      "start": 10590.6,
      "duration": 3.03
    },
    {
      "text": "sort of with certainty,",
      "start": 10593.63,
      "duration": 1.59
    },
    {
      "text": "and almost like preferences\nof taste or something,",
      "start": 10595.22,
      "duration": 4.14
    },
    {
      "text": "like the way that they\nwould, I don't know,",
      "start": 10599.36,
      "duration": 1.53
    },
    {
      "text": "prefer like chocolate to\npistachio or something.",
      "start": 10600.89,
      "duration": 2.703
    },
    {
      "text": "But actually I think about values",
      "start": 10605.09,
      "duration": 3.51
    },
    {
      "text": "and opinions as like a lot more",
      "start": 10608.6,
      "duration": 2.1
    },
    {
      "text": "like physics than I think most people do.",
      "start": 10610.7,
      "duration": 2.55
    },
    {
      "text": "I'm just like, these are things",
      "start": 10613.25,
      "duration": 1.71
    },
    {
      "text": "that we are openly investigating.",
      "start": 10614.96,
      "duration": 1.65
    },
    {
      "text": "There's some things that\nwe're more confident in.",
      "start": 10616.61,
      "duration": 2.22
    },
    {
      "text": "We can discuss them, we\ncan learn about them.",
      "start": 10618.83,
      "duration": 2.703
    },
    {
      "text": "And so I think in some ways,",
      "start": 10622.67,
      "duration": 2.37
    },
    {
      "text": "though like, ethics is\ndefinitely different in nature,",
      "start": 10625.04,
      "duration": 3.93
    },
    {
      "text": "but has a lot of those\nsame kind of qualities.",
      "start": 10628.97,
      "duration": 3.06
    },
    {
      "text": "You want models, in the same way",
      "start": 10632.03,
      "duration": 1.23
    },
    {
      "text": "that you want them to understand physics,",
      "start": 10633.26,
      "duration": 1.32
    },
    {
      "text": "you kind of want them to understand",
      "start": 10634.58,
      "duration": 1.14
    },
    {
      "text": "all like values in the\nworld that people have,",
      "start": 10635.72,
      "duration": 2.997
    },
    {
      "text": "and to be curious about them",
      "start": 10638.717,
      "duration": 1.17
    },
    {
      "text": "and to be interested in them,",
      "start": 10639.887,
      "duration": 1.983
    },
    {
      "text": "and to not necessarily like pander to them",
      "start": 10641.87,
      "duration": 2.19
    },
    {
      "text": "or agree with them, because\nthere's just lots of values",
      "start": 10644.06,
      "duration": 2.34
    },
    {
      "text": "where I think almost\nall people in the world,",
      "start": 10646.4,
      "duration": 2.34
    },
    {
      "text": "if they met someone with those values,",
      "start": 10648.74,
      "duration": 1.35
    },
    {
      "text": "they'd be like, \"That's abhorrent.",
      "start": 10650.09,
      "duration": 1.47
    },
    {
      "text": "I completely disagree.\"",
      "start": 10651.56,
      "duration": 1.377
    },
    {
      "text": "And so again, maybe my thought is,",
      "start": 10654.59,
      "duration": 3.36
    },
    {
      "text": "well, in the same way that a person can,",
      "start": 10657.95,
      "duration": 2.662
    },
    {
      "text": "like I think many people\nare thoughtful enough",
      "start": 10660.612,
      "duration": 1.988
    },
    {
      "text": "on issues of like ethics,\npolitics, opinions,",
      "start": 10662.6,
      "duration": 3.45
    },
    {
      "text": "that even if you don't agree with them,",
      "start": 10666.05,
      "duration": 2.25
    },
    {
      "text": "you feel very heard by them.",
      "start": 10668.3,
      "duration": 1.71
    },
    {
      "text": "They think carefully about your position.",
      "start": 10670.01,
      "duration": 1.98
    },
    {
      "text": "They think about its pros and cons.",
      "start": 10671.99,
      "duration": 1.38
    },
    {
      "text": "They maybe offer counter considerations.",
      "start": 10673.37,
      "duration": 2.28
    },
    {
      "text": "So they're not dismissive,",
      "start": 10675.65,
      "duration": 0.96
    },
    {
      "text": "but nor will they agree.",
      "start": 10676.61,
      "duration": 1.26
    },
    {
      "text": "You know, if they're like,",
      "start": 10677.87,
      "duration": 0.833
    },
    {
      "text": "\"Actually, I just think\nthat that's very wrong,\"",
      "start": 10678.703,
      "duration": 1.747
    },
    {
      "text": "they'll like say that.",
      "start": 10680.45,
      "duration": 1.71
    },
    {
      "text": "I think that in Claude's position,",
      "start": 10682.16,
      "duration": 2.82
    },
    {
      "text": "it's a little bit trickier",
      "start": 10684.98,
      "duration": 1.26
    },
    {
      "text": "because you don't\nnecessarily want to like,",
      "start": 10686.24,
      "duration": 2.34
    },
    {
      "text": "if I was in Claude's position,",
      "start": 10688.58,
      "duration": 0.99
    },
    {
      "text": "I wouldn't be giving a lot of opinions.",
      "start": 10689.57,
      "duration": 1.92
    },
    {
      "text": "I just wouldn't want to\ninfluence people too much.",
      "start": 10691.49,
      "duration": 1.62
    },
    {
      "text": "I'd be like, you know,",
      "start": 10693.11,
      "duration": 1.29
    },
    {
      "text": "I forget conversations\nevery time they happen,",
      "start": 10694.4,
      "duration": 2.55
    },
    {
      "text": "but I know I'm talking",
      "start": 10696.95,
      "duration": 0.833
    },
    {
      "text": "with like potentially millions of people,",
      "start": 10697.783,
      "duration": 2.407
    },
    {
      "text": "who might be like really\nlistening to what I say.",
      "start": 10700.19,
      "duration": 2.7
    },
    {
      "text": "I think I would just be like,",
      "start": 10702.89,
      "duration": 1.02
    },
    {
      "text": "I'm less inclined to give opinions.",
      "start": 10703.91,
      "duration": 1.44
    },
    {
      "text": "I'm more inclined to like\nthink through things,",
      "start": 10705.35,
      "duration": 1.83
    },
    {
      "text": "or present the considerations to you,",
      "start": 10707.18,
      "duration": 2.433
    },
    {
      "text": "or discuss your views with you.",
      "start": 10710.66,
      "duration": 1.59
    },
    {
      "text": "But I'm a little bit less inclined",
      "start": 10712.25,
      "duration": 1.32
    },
    {
      "text": "to like affect how you think,",
      "start": 10713.57,
      "duration": 3.78
    },
    {
      "text": "'cause it feels much more important",
      "start": 10717.35,
      "duration": 1.05
    },
    {
      "text": "that you maintain like autonomy there.",
      "start": 10718.4,
      "duration": 3.48
    },
    {
      "text": "- Yeah, like if you really\nembody intellectual humility,",
      "start": 10721.88,
      "duration": 3.603
    },
    {
      "text": "the desire to speak decreases quickly.",
      "start": 10726.32,
      "duration": 3.258
    },
    {
      "text": "- Yeah.\n- Okay.",
      "start": 10729.578,
      "duration": 1.902
    },
    {
      "text": "But Claude has to speak,",
      "start": 10731.48,
      "duration": 2.19
    },
    {
      "text": "so, but without being overbearing.",
      "start": 10733.67,
      "duration": 4.89
    },
    {
      "text": "- Yeah.",
      "start": 10738.56,
      "duration": 1.05
    },
    {
      "text": "- But then there's a line",
      "start": 10739.61,
      "duration": 1.05
    },
    {
      "text": "when you're sort of discussing",
      "start": 10740.66,
      "duration": 1.65
    },
    {
      "text": "whether the Earth is flat\nor something like that.",
      "start": 10742.31,
      "duration": 2.487
    },
    {
      "text": "I actually was, I remember a long time ago",
      "start": 10746.75,
      "duration": 3.18
    },
    {
      "text": "was speaking to a few high profile folks,",
      "start": 10749.93,
      "duration": 2.4
    },
    {
      "text": "and they were so dismissive of the idea",
      "start": 10752.33,
      "duration": 2.67
    },
    {
      "text": "that the Earth is flat, but\nlike so arrogant about it.",
      "start": 10755.0,
      "duration": 3.0
    },
    {
      "text": "And I thought like,\nthere's a lot of people",
      "start": 10758.0,
      "duration": 2.76
    },
    {
      "text": "that believe the Earth is flat.",
      "start": 10760.76,
      "duration": 1.17
    },
    {
      "text": "That was, well, I don't know",
      "start": 10761.93,
      "duration": 0.96
    },
    {
      "text": "if that movement is there anymore.",
      "start": 10762.89,
      "duration": 1.485
    },
    {
      "text": "That was like a meme for a while.",
      "start": 10764.375,
      "duration": 1.725
    },
    {
      "text": "But they really believed\nit and like, okay,",
      "start": 10766.1,
      "duration": 2.64
    },
    {
      "text": "so I think it's really disrespectful",
      "start": 10768.74,
      "duration": 2.19
    },
    {
      "text": "to completely mock them.",
      "start": 10770.93,
      "duration": 2.31
    },
    {
      "text": "I think you have to understand\nwhere they're coming from.",
      "start": 10773.24,
      "duration": 2.55
    },
    {
      "text": "I think probably where they're coming from",
      "start": 10775.79,
      "duration": 1.92
    },
    {
      "text": "is the general skepticism of institutions",
      "start": 10777.71,
      "duration": 3.24
    },
    {
      "text": "which is grounded in a kind of,",
      "start": 10780.95,
      "duration": 1.56
    },
    {
      "text": "there's a deep philosophy there,",
      "start": 10782.51,
      "duration": 1.56
    },
    {
      "text": "which you could understand.",
      "start": 10784.07,
      "duration": 1.98
    },
    {
      "text": "You can even agree with in parts.",
      "start": 10786.05,
      "duration": 2.22
    },
    {
      "text": "And then from there, you can use it",
      "start": 10788.27,
      "duration": 1.53
    },
    {
      "text": "as an opportunity to talk about physics,",
      "start": 10789.8,
      "duration": 2.43
    },
    {
      "text": "without mocking them, without so on,",
      "start": 10792.23,
      "duration": 1.68
    },
    {
      "text": "but it's just like, okay, like,",
      "start": 10793.91,
      "duration": 1.95
    },
    {
      "text": "what would the world look like?",
      "start": 10795.86,
      "duration": 1.35
    },
    {
      "text": "What would the physics of the world",
      "start": 10797.21,
      "duration": 1.32
    },
    {
      "text": "with the flat Earth look like?",
      "start": 10798.53,
      "duration": 0.967
    },
    {
      "text": "There's a few cool videos on this.",
      "start": 10799.497,
      "duration": 2.663
    },
    {
      "text": "- Yeah.\n- And then like,",
      "start": 10802.16,
      "duration": 1.51
    },
    {
      "text": "is it possible the physics is different?",
      "start": 10803.67,
      "duration": 1.85
    },
    {
      "text": "And what kind of experiments would we do?",
      "start": 10805.52,
      "duration": 1.35
    },
    {
      "text": "And just, yeah, without disrespect,",
      "start": 10806.87,
      "duration": 1.89
    },
    {
      "text": "without dismissiveness\nhave that conversation.",
      "start": 10808.76,
      "duration": 2.55
    },
    {
      "text": "Anyway, that to me is a useful\nthought experiment of like,",
      "start": 10811.31,
      "duration": 3.93
    },
    {
      "text": "how does Claude talk to\na flat Earth believer",
      "start": 10815.24,
      "duration": 3.49
    },
    {
      "text": "and still teach them something,",
      "start": 10819.89,
      "duration": 1.92
    },
    {
      "text": "still help them grow, that kind of stuff.",
      "start": 10821.81,
      "duration": 3.682
    },
    {
      "text": "That's challenging.",
      "start": 10825.492,
      "duration": 1.478
    },
    {
      "text": "- And kind of like\nwalking that line between",
      "start": 10826.97,
      "duration": 3.15
    },
    {
      "text": "convincing someone and just\ntrying to like talk at them",
      "start": 10830.12,
      "duration": 3.12
    },
    {
      "text": "versus like drawing out their views,",
      "start": 10833.24,
      "duration": 2.55
    },
    {
      "text": "like listening and then offering",
      "start": 10835.79,
      "duration": 1.47
    },
    {
      "text": "kind of counter considerations.",
      "start": 10837.26,
      "duration": 1.743
    },
    {
      "text": "And it's hard, I think\nit's actually a hard line",
      "start": 10840.29,
      "duration": 2.34
    },
    {
      "text": "where it's like where are you\ntrying to convince someone",
      "start": 10842.63,
      "duration": 2.91
    },
    {
      "text": "versus just offering\nthem like considerations",
      "start": 10845.54,
      "duration": 2.73
    },
    {
      "text": "and things for them to think about,",
      "start": 10848.27,
      "duration": 1.53
    },
    {
      "text": "so that you're not actually\nlike influencing them.",
      "start": 10849.8,
      "duration": 2.4
    },
    {
      "text": "You're just like letting them\nreach wherever they reach.",
      "start": 10852.2,
      "duration": 2.58
    },
    {
      "text": "And that's like a line that it's difficult",
      "start": 10854.78,
      "duration": 2.52
    },
    {
      "text": "but that's the kind of thing",
      "start": 10857.3,
      "duration": 0.833
    },
    {
      "text": "that language models have to try and do.",
      "start": 10858.133,
      "duration": 2.317
    },
    {
      "text": "- So like I said,",
      "start": 10860.45,
      "duration": 1.08
    },
    {
      "text": "you've had a lot of\nconversations with Claude.",
      "start": 10861.53,
      "duration": 2.19
    },
    {
      "text": "Can you just map out what\nthose conversations are like?",
      "start": 10863.72,
      "duration": 3.06
    },
    {
      "text": "What are some memorable conversations?",
      "start": 10866.78,
      "duration": 1.56
    },
    {
      "text": "What's the purpose, the\ngoal of those conversations?",
      "start": 10868.34,
      "duration": 3.71
    },
    {
      "text": "- Yeah, I think that most of the time",
      "start": 10872.05,
      "duration": 2.26
    },
    {
      "text": "when I'm talking with Claude,",
      "start": 10874.31,
      "duration": 2.46
    },
    {
      "text": "I'm trying to kind of map\nout its behavior, in part.",
      "start": 10876.77,
      "duration": 4.5
    },
    {
      "text": "Like obviously I'm getting\nlike helpful outputs",
      "start": 10881.27,
      "duration": 2.37
    },
    {
      "text": "from the model as well.",
      "start": 10883.64,
      "duration": 1.067
    },
    {
      "text": "But in some ways, this is\nlike how you get to know",
      "start": 10884.707,
      "duration": 2.143
    },
    {
      "text": "a system, I think, is by like probing it",
      "start": 10886.85,
      "duration": 2.67
    },
    {
      "text": "and then augmenting like, you know,",
      "start": 10889.52,
      "duration": 2.73
    },
    {
      "text": "the message that you're sending",
      "start": 10892.25,
      "duration": 1.41
    },
    {
      "text": "and then checking the response to that.",
      "start": 10893.66,
      "duration": 2.49
    },
    {
      "text": "So in some ways, it's like\nhow I map out the model.",
      "start": 10896.15,
      "duration": 2.76
    },
    {
      "text": "I think that people focus",
      "start": 10898.91,
      "duration": 1.98
    },
    {
      "text": "a lot on these quantitative\nevaluations of models.",
      "start": 10900.89,
      "duration": 4.17
    },
    {
      "text": "And this is a thing that I said before,",
      "start": 10905.06,
      "duration": 2.25
    },
    {
      "text": "but I think in the case\nof language models,",
      "start": 10907.31,
      "duration": 5.0
    },
    {
      "text": "a lot of the time, each interaction",
      "start": 10912.62,
      "duration": 2.43
    },
    {
      "text": "you have is actually\nquite high information.",
      "start": 10915.05,
      "duration": 2.763
    },
    {
      "text": "It's very predictive of other interactions",
      "start": 10918.68,
      "duration": 1.71
    },
    {
      "text": "that you'll have with the model.",
      "start": 10920.39,
      "duration": 1.59
    },
    {
      "text": "And so I guess I'm like,",
      "start": 10921.98,
      "duration": 1.62
    },
    {
      "text": "if you talk with a model\nhundreds or thousands of times,",
      "start": 10923.6,
      "duration": 2.85
    },
    {
      "text": "this is almost like a huge number",
      "start": 10926.45,
      "duration": 1.32
    },
    {
      "text": "of really high quality data points",
      "start": 10927.77,
      "duration": 1.59
    },
    {
      "text": "about what the model is like,",
      "start": 10929.36,
      "duration": 1.45
    },
    {
      "text": "in a way that like lots of very similar",
      "start": 10932.3,
      "duration": 3.39
    },
    {
      "text": "but lower quality\nconversations just aren't,",
      "start": 10935.69,
      "duration": 2.91
    },
    {
      "text": "or like questions that are\njust like mildly augmented",
      "start": 10938.6,
      "duration": 2.19
    },
    {
      "text": "and you have thousands of\nthem might be less relevant",
      "start": 10940.79,
      "duration": 2.13
    },
    {
      "text": "than like 100 really\nwell selected questions.",
      "start": 10942.92,
      "duration": 2.88
    },
    {
      "text": "- Well, so, you're talking to somebody",
      "start": 10945.8,
      "duration": 1.8
    },
    {
      "text": "who as a hobby does a podcast.",
      "start": 10947.6,
      "duration": 2.16
    },
    {
      "text": "I agree with you 100%.",
      "start": 10949.76,
      "duration": 2.7
    },
    {
      "text": "If you're able to ask the right questions",
      "start": 10952.46,
      "duration": 3.36
    },
    {
      "text": "and are able to hear,\nlike understand (laughs)",
      "start": 10955.82,
      "duration": 5.0
    },
    {
      "text": "like the depth and the\nflaws in the answer,",
      "start": 10961.7,
      "duration": 3.417
    },
    {
      "text": "you can get a lot of data from that.",
      "start": 10965.117,
      "duration": 2.043
    },
    {
      "text": "- [Amanda] Yeah.",
      "start": 10967.16,
      "duration": 1.2
    },
    {
      "text": "- So like your task is basically",
      "start": 10968.36,
      "duration": 1.62
    },
    {
      "text": "how to probe with questions.",
      "start": 10969.98,
      "duration": 3.0
    },
    {
      "text": "And you're exploring like the long tail,",
      "start": 10972.98,
      "duration": 2.64
    },
    {
      "text": "the edges, the edge cases,",
      "start": 10975.62,
      "duration": 2.16
    },
    {
      "text": "or are you looking for\nlike general behavior?",
      "start": 10977.78,
      "duration": 3.93
    },
    {
      "text": "- I think it's almost like everything.",
      "start": 10981.71,
      "duration": 2.52
    },
    {
      "text": "Like, because I want like\na full map of the model,",
      "start": 10984.23,
      "duration": 2.52
    },
    {
      "text": "I'm kind of trying to\ndo the whole spectrum",
      "start": 10986.75,
      "duration": 4.83
    },
    {
      "text": "of possible interactions\nyou could have with it.",
      "start": 10991.58,
      "duration": 2.79
    },
    {
      "text": "So like one thing that's\ninteresting about Claude,",
      "start": 10994.37,
      "duration": 1.86
    },
    {
      "text": "and this might actually get",
      "start": 10996.23,
      "duration": 1.23
    },
    {
      "text": "to some interesting issues with RLHF,",
      "start": 10997.46,
      "duration": 1.92
    },
    {
      "text": "which is if you ask Claude for a poem,",
      "start": 10999.38,
      "duration": 2.25
    },
    {
      "text": "like I think that a lot of models,",
      "start": 11001.63,
      "duration": 1.95
    },
    {
      "text": "if you ask them for a poem,\nthe poem is like fine.",
      "start": 11003.58,
      "duration": 3.09
    },
    {
      "text": "You know, usually it kinda like rhymes",
      "start": 11006.67,
      "duration": 1.8
    },
    {
      "text": "and it's, you know, so if you say like,",
      "start": 11008.47,
      "duration": 1.177
    },
    {
      "text": "\"Give me a poem about the sun,\"",
      "start": 11009.647,
      "duration": 1.913
    },
    {
      "text": "it'll be like, yeah,",
      "start": 11011.56,
      "duration": 1.66
    },
    {
      "text": "it'll just be a certain\nlength, it'll like rhyme.",
      "start": 11013.22,
      "duration": 2.54
    },
    {
      "text": "It'll be fairly kind of benign.",
      "start": 11015.76,
      "duration": 2.55
    },
    {
      "text": "And I've wondered before,\nis it the case that",
      "start": 11018.31,
      "duration": 2.64
    },
    {
      "text": "what you're seeing is\nkind of like the average?",
      "start": 11020.95,
      "duration": 2.61
    },
    {
      "text": "It turns out, you know,\nif you think about people",
      "start": 11023.56,
      "duration": 2.07
    },
    {
      "text": "who have to talk to a lot of people",
      "start": 11025.63,
      "duration": 1.11
    },
    {
      "text": "and be very charismatic,\none of the weird things",
      "start": 11026.74,
      "duration": 2.73
    },
    {
      "text": "is that I'm like, well,\nthey're kind of incentivized",
      "start": 11029.47,
      "duration": 1.68
    },
    {
      "text": "to have these extremely boring views",
      "start": 11031.15,
      "duration": 2.37
    },
    {
      "text": "because if you have\nreally interesting views,",
      "start": 11033.52,
      "duration": 2.01
    },
    {
      "text": "you're divisive and, you know,",
      "start": 11035.53,
      "duration": 3.54
    },
    {
      "text": "a lot of people are not gonna like you.",
      "start": 11039.07,
      "duration": 1.35
    },
    {
      "text": "So like if you have very\nextreme policy positions,",
      "start": 11040.42,
      "duration": 2.04
    },
    {
      "text": "I think you're just gonna\nbe like less popular",
      "start": 11042.46,
      "duration": 1.41
    },
    {
      "text": "as a politician, for example.",
      "start": 11043.87,
      "duration": 1.593
    },
    {
      "text": "And it might be similar\nwith like creative work.",
      "start": 11046.81,
      "duration": 2.7
    },
    {
      "text": "If you produce creative work",
      "start": 11049.51,
      "duration": 1.32
    },
    {
      "text": "that is just trying to maximize",
      "start": 11050.83,
      "duration": 2.01
    },
    {
      "text": "the kind of number of people that like it,",
      "start": 11052.84,
      "duration": 2.1
    },
    {
      "text": "you're probably not\ngonna get as many people",
      "start": 11054.94,
      "duration": 1.23
    },
    {
      "text": "who just absolutely love it,",
      "start": 11056.17,
      "duration": 2.28
    },
    {
      "text": "because it's gonna be\na little bit, you know,",
      "start": 11058.45,
      "duration": 2.01
    },
    {
      "text": "you're like, oh, this is the out,",
      "start": 11060.46,
      "duration": 1.416
    },
    {
      "text": "yeah, this is decent.",
      "start": 11061.876,
      "duration": 2.064
    },
    {
      "text": "- Yeah.",
      "start": 11063.94,
      "duration": 0.833
    },
    {
      "text": "- And so you can do this thing",
      "start": 11064.773,
      "duration": 0.877
    },
    {
      "text": "where like I have various prompting things",
      "start": 11065.65,
      "duration": 2.34
    },
    {
      "text": "that I'll do to get Claude to,",
      "start": 11067.99,
      "duration": 2.22
    },
    {
      "text": "I'm kind of, you know,\nI'll do a lot of like,",
      "start": 11070.21,
      "duration": 2.167
    },
    {
      "text": "\"This is your chance to\nbe like fully creative.",
      "start": 11072.377,
      "duration": 2.303
    },
    {
      "text": "I want you to just think\nabout this for a long time.",
      "start": 11074.68,
      "duration": 2.37
    },
    {
      "text": "And I want you to like create\na poem about this topic",
      "start": 11077.05,
      "duration": 3.48
    },
    {
      "text": "that is really expressive of you,",
      "start": 11080.53,
      "duration": 2.52
    },
    {
      "text": "both in terms of how you think poetry",
      "start": 11083.05,
      "duration": 1.41
    },
    {
      "text": "should be structured,\" et cetera.",
      "start": 11084.46,
      "duration": 2.22
    },
    {
      "text": "You know, and you just give it\nthis like really long prompt.",
      "start": 11086.68,
      "duration": 2.25
    },
    {
      "text": "And it's poems are just so much better.",
      "start": 11088.93,
      "duration": 2.58
    },
    {
      "text": "Like they're really good.",
      "start": 11091.51,
      "duration": 1.41
    },
    {
      "text": "And I don't think I'm someone who is like,",
      "start": 11092.92,
      "duration": 2.91
    },
    {
      "text": "I think it got me interested in poetry,",
      "start": 11095.83,
      "duration": 1.62
    },
    {
      "text": "which I think was interesting.",
      "start": 11097.45,
      "duration": 1.95
    },
    {
      "text": "You know, I would like read these poems",
      "start": 11099.4,
      "duration": 1.29
    },
    {
      "text": "and just be like, this is, I\njust like, I love the imagery,",
      "start": 11100.69,
      "duration": 2.46
    },
    {
      "text": "I love like, and it's not\ntrivial to get the models",
      "start": 11103.15,
      "duration": 3.45
    },
    {
      "text": "to produce work like that,",
      "start": 11106.6,
      "duration": 1.59
    },
    {
      "text": "but when they do, it's like really good.",
      "start": 11108.19,
      "duration": 2.34
    },
    {
      "text": "So I think that's interesting that",
      "start": 11110.53,
      "duration": 1.11
    },
    {
      "text": "just like encouraging creativity,",
      "start": 11111.64,
      "duration": 2.58
    },
    {
      "text": "and for them to move away\nfrom the kind of like",
      "start": 11114.22,
      "duration": 2.64
    },
    {
      "text": "standard like immediate reaction",
      "start": 11116.86,
      "duration": 3.03
    },
    {
      "text": "that might just be the aggregate",
      "start": 11119.89,
      "duration": 0.96
    },
    {
      "text": "of what most people think is fine,",
      "start": 11120.85,
      "duration": 2.01
    },
    {
      "text": "can actually produce things\nthat, at least to my mind,",
      "start": 11122.86,
      "duration": 2.667
    },
    {
      "text": "are probably a little bit\nmore divisive but I like them.",
      "start": 11125.527,
      "duration": 3.3
    },
    {
      "text": "- But I guess a poem is a nice, clean way",
      "start": 11128.827,
      "duration": 4.713
    },
    {
      "text": "to observe creativity.",
      "start": 11133.54,
      "duration": 1.38
    },
    {
      "text": "It's just like easy to detect\nvanilla versus non vanilla.",
      "start": 11134.92,
      "duration": 3.63
    },
    {
      "text": "- Yep.\n- Yeah, that's interesting.",
      "start": 11138.55,
      "duration": 2.13
    },
    {
      "text": "That's really interesting.",
      "start": 11140.68,
      "duration": 1.47
    },
    {
      "text": "So on that topic, so the\nway to produce creativity",
      "start": 11142.15,
      "duration": 3.18
    },
    {
      "text": "or something special, you\nmentioned writing prompts,",
      "start": 11145.33,
      "duration": 2.82
    },
    {
      "text": "and I've heard you talk about, I mean,",
      "start": 11148.15,
      "duration": 3.15
    },
    {
      "text": "the science and the art\nof prompt engineering.",
      "start": 11151.3,
      "duration": 3.0
    },
    {
      "text": "Could you just speak to what it takes",
      "start": 11154.3,
      "duration": 3.43
    },
    {
      "text": "to write great prompts?",
      "start": 11158.65,
      "duration": 1.26
    },
    {
      "text": "- I really do think that like philosophy",
      "start": 11159.91,
      "duration": 3.15
    },
    {
      "text": "has been weirdly helpful for me here,",
      "start": 11163.06,
      "duration": 3.9
    },
    {
      "text": "more than in many other like respects.",
      "start": 11166.96,
      "duration": 2.673
    },
    {
      "text": "So like in philosophy,\nwhat you're trying to do",
      "start": 11171.34,
      "duration": 1.8
    },
    {
      "text": "is convey these very hard concepts.",
      "start": 11173.14,
      "duration": 2.43
    },
    {
      "text": "Like one of the things\nyou are taught is like,",
      "start": 11175.57,
      "duration": 2.4
    },
    {
      "text": "and I think it is because it is,",
      "start": 11177.97,
      "duration": 2.233
    },
    {
      "text": "I think it is an anti-bullshit\ndevice in philosophy.",
      "start": 11180.203,
      "duration": 2.627
    },
    {
      "text": "Philosophy is an area where you could have",
      "start": 11182.83,
      "duration": 1.92
    },
    {
      "text": "people bullshitting and\nyou don't want that.",
      "start": 11184.75,
      "duration": 2.433
    },
    {
      "text": "And so it's like this like desire",
      "start": 11188.02,
      "duration": 3.42
    },
    {
      "text": "for like extreme clarity.",
      "start": 11191.44,
      "duration": 1.59
    },
    {
      "text": "So it's like anyone could\njust pick up your paper,",
      "start": 11193.03,
      "duration": 2.4
    },
    {
      "text": "read it and know exactly\nwhat you're talking about.",
      "start": 11195.43,
      "duration": 1.653
    },
    {
      "text": "It's why it can almost be kind of dry.",
      "start": 11197.083,
      "duration": 1.887
    },
    {
      "text": "Like all of the terms are defined,",
      "start": 11198.97,
      "duration": 2.46
    },
    {
      "text": "every objection's kind of\ngone through methodically.",
      "start": 11201.43,
      "duration": 3.21
    },
    {
      "text": "And it makes sense to me",
      "start": 11204.64,
      "duration": 0.833
    },
    {
      "text": "'cause I'm like when you're\nin such an a priori domain,",
      "start": 11205.473,
      "duration": 3.487
    },
    {
      "text": "like you just, clarity is sort of",
      "start": 11208.96,
      "duration": 2.08
    },
    {
      "text": "this way that you can,",
      "start": 11212.89,
      "duration": 0.93
    },
    {
      "text": "you know, prevent people from\njust kind of making stuff up.",
      "start": 11213.82,
      "duration": 2.973
    },
    {
      "text": "And I think that's sort of what you have",
      "start": 11218.471,
      "duration": 1.889
    },
    {
      "text": "to do with language models.",
      "start": 11220.36,
      "duration": 1.2
    },
    {
      "text": "Like very often I actually find myself",
      "start": 11221.56,
      "duration": 1.89
    },
    {
      "text": "doing sort of mini versions of philosophy,",
      "start": 11223.45,
      "duration": 2.28
    },
    {
      "text": "you know, so I'm like,",
      "start": 11225.73,
      "duration": 0.833
    },
    {
      "text": "suppose that you give me a task",
      "start": 11226.563,
      "duration": 1.987
    },
    {
      "text": "or I have a task for the model,",
      "start": 11228.55,
      "duration": 1.35
    },
    {
      "text": "and I want it to like pick out",
      "start": 11229.9,
      "duration": 1.29
    },
    {
      "text": "a certain kind of question,",
      "start": 11231.19,
      "duration": 1.2
    },
    {
      "text": "or identify whether an answer\nhas a certain property.",
      "start": 11232.39,
      "duration": 2.76
    },
    {
      "text": "Like I'll actually sit and be like,",
      "start": 11235.15,
      "duration": 1.8
    },
    {
      "text": "let's just give this\na name, this property.",
      "start": 11236.95,
      "duration": 2.49
    },
    {
      "text": "So like, you know, suppose\nI'm trying to tell it like,",
      "start": 11239.44,
      "duration": 2.79
    },
    {
      "text": "oh, \"I want you to identify",
      "start": 11242.23,
      "duration": 0.99
    },
    {
      "text": "whether this response was rude or polite.\"",
      "start": 11243.22,
      "duration": 2.46
    },
    {
      "text": "I'm like, that's a whole\nphilosophical question",
      "start": 11245.68,
      "duration": 1.89
    },
    {
      "text": "in and of itself.",
      "start": 11247.57,
      "duration": 0.833
    },
    {
      "text": "So I have to do as much like philosophy",
      "start": 11248.403,
      "duration": 1.297
    },
    {
      "text": "as I can in the moment to be like,",
      "start": 11249.7,
      "duration": 1.53
    },
    {
      "text": "here's what I mean by rudeness,",
      "start": 11251.23,
      "duration": 1.23
    },
    {
      "text": "and here's what I mean by politeness.",
      "start": 11252.46,
      "duration": 2.46
    },
    {
      "text": "And then like there's another element",
      "start": 11254.92,
      "duration": 1.89
    },
    {
      "text": "that's a bit more, I guess,",
      "start": 11256.81,
      "duration": 3.123
    },
    {
      "text": "I dunno if this is\nscientific or empirical.",
      "start": 11261.16,
      "duration": 2.31
    },
    {
      "text": "I think it's empirical.",
      "start": 11263.47,
      "duration": 1.29
    },
    {
      "text": "So like I take that description",
      "start": 11264.76,
      "duration": 2.19
    },
    {
      "text": "and then what I want to do",
      "start": 11266.95,
      "duration": 1.41
    },
    {
      "text": "is again probe the model like many times.",
      "start": 11268.36,
      "duration": 2.13
    },
    {
      "text": "Like this is very,\nprompting is very iterative.",
      "start": 11270.49,
      "duration": 2.19
    },
    {
      "text": "Like I think a lot of people where,",
      "start": 11272.68,
      "duration": 1.47
    },
    {
      "text": "if a prompt is important,\nthey'll iterate on it",
      "start": 11274.15,
      "duration": 1.95
    },
    {
      "text": "hundreds or thousands of times.",
      "start": 11276.1,
      "duration": 1.55
    },
    {
      "text": "And so you give it the instructions",
      "start": 11279.16,
      "duration": 1.74
    },
    {
      "text": "and then I'm like, what\nare the edge cases?",
      "start": 11280.9,
      "duration": 1.65
    },
    {
      "text": "So if I looked at this,",
      "start": 11282.55,
      "duration": 1.14
    },
    {
      "text": "so I try and like almost like, you know,",
      "start": 11283.69,
      "duration": 2.0
    },
    {
      "text": "see myself from the position of the model",
      "start": 11286.99,
      "duration": 1.68
    },
    {
      "text": "and be like what is the exact case",
      "start": 11288.67,
      "duration": 2.22
    },
    {
      "text": "that I would misunderstand,",
      "start": 11290.89,
      "duration": 1.2
    },
    {
      "text": "or where I would just be like,",
      "start": 11292.09,
      "duration": 0.833
    },
    {
      "text": "\"I don't know what to do in this case.\"",
      "start": 11292.923,
      "duration": 1.477
    },
    {
      "text": "And then I give that case to the model",
      "start": 11294.4,
      "duration": 1.617
    },
    {
      "text": "and I see how it responds,",
      "start": 11296.017,
      "duration": 1.383
    },
    {
      "text": "and if I think I got it\nwrong, I add more instructions",
      "start": 11297.4,
      "duration": 2.76
    },
    {
      "text": "or I even add that in as an example.",
      "start": 11300.16,
      "duration": 1.95
    },
    {
      "text": "So these very like taking the examples",
      "start": 11302.11,
      "duration": 1.29
    },
    {
      "text": "that are right at the edge of\nwhat you want and don't want,",
      "start": 11303.4,
      "duration": 3.3
    },
    {
      "text": "and putting those into your prompt",
      "start": 11306.7,
      "duration": 1.5
    },
    {
      "text": "as like an additional kind of\nway of describing the thing.",
      "start": 11308.2,
      "duration": 3.6
    },
    {
      "text": "And so yeah, in many ways",
      "start": 11311.8,
      "duration": 0.93
    },
    {
      "text": "it just feels like this mix of like,",
      "start": 11312.73,
      "duration": 2.64
    },
    {
      "text": "it's really just trying\nto do clear exposition,",
      "start": 11315.37,
      "duration": 2.553
    },
    {
      "text": "and I think I do that",
      "start": 11318.88,
      "duration": 0.977
    },
    {
      "text": "'cause that's how I get\nclear on things myself.",
      "start": 11319.857,
      "duration": 2.503
    },
    {
      "text": "So in many ways like clear prompting",
      "start": 11322.36,
      "duration": 1.77
    },
    {
      "text": "for me is often just me\nunderstanding what I want",
      "start": 11324.13,
      "duration": 3.131
    },
    {
      "text": "is like half the task.",
      "start": 11327.261,
      "duration": 1.249
    },
    {
      "text": "- So I guess that's quite challenging.",
      "start": 11328.51,
      "duration": 2.25
    },
    {
      "text": "There's like a laziness that overtakes me",
      "start": 11330.76,
      "duration": 2.34
    },
    {
      "text": "if I'm talking to Claude",
      "start": 11333.1,
      "duration": 1.05
    },
    {
      "text": "where I hope Claude just figures it out.",
      "start": 11334.15,
      "duration": 2.37
    },
    {
      "text": "So for example, I asked Claude for today",
      "start": 11336.52,
      "duration": 3.6
    },
    {
      "text": "to ask some interesting questions, okay.",
      "start": 11340.12,
      "duration": 2.79
    },
    {
      "text": "And the questions that came up,",
      "start": 11342.91,
      "duration": 1.5
    },
    {
      "text": "and I think I listed a few",
      "start": 11344.41,
      "duration": 1.71
    },
    {
      "text": "sort of interesting, counterintuitive,",
      "start": 11346.12,
      "duration": 3.12
    },
    {
      "text": "and/or funny or something\nlike this, all right.",
      "start": 11349.24,
      "duration": 2.05
    },
    {
      "text": "And it gave me some pretty\ngood, like it was okay,",
      "start": 11351.29,
      "duration": 3.71
    },
    {
      "text": "but I think what I'm\nhearing you say is like,",
      "start": 11355.0,
      "duration": 2.7
    },
    {
      "text": "all right, well, I have\nto be more rigorous here.",
      "start": 11357.7,
      "duration": 2.43
    },
    {
      "text": "I should probably give examples",
      "start": 11360.13,
      "duration": 1.32
    },
    {
      "text": "of what I mean by interesting,",
      "start": 11361.45,
      "duration": 1.95
    },
    {
      "text": "and what I mean by funny\nor counterintuitive,",
      "start": 11363.4,
      "duration": 3.84
    },
    {
      "text": "and iteratively build that prompt",
      "start": 11367.24,
      "duration": 4.18
    },
    {
      "text": "to better, to get it like\nwhat feels like is the right,",
      "start": 11372.58,
      "duration": 4.68
    },
    {
      "text": "because it is really, it's a creative act.",
      "start": 11377.26,
      "duration": 1.68
    },
    {
      "text": "I'm not asking for factual information.",
      "start": 11378.94,
      "duration": 1.74
    },
    {
      "text": "I'm asking to together write with Claude.",
      "start": 11380.68,
      "duration": 2.85
    },
    {
      "text": "So I almost have to program\nusing natural language.",
      "start": 11383.53,
      "duration": 3.33
    },
    {
      "text": "- Yeah, I think that prompting does feel",
      "start": 11386.86,
      "duration": 2.25
    },
    {
      "text": "a lot like the kind of the programming",
      "start": 11389.11,
      "duration": 2.43
    },
    {
      "text": "using natural language and\nexperimentation or something.",
      "start": 11391.54,
      "duration": 3.255
    },
    {
      "text": "It's an odd blend of the two.",
      "start": 11394.795,
      "duration": 1.815
    },
    {
      "text": "I do think that for most tasks,",
      "start": 11396.61,
      "duration": 1.4
    },
    {
      "text": "so if I just want Claude to do a thing,",
      "start": 11398.01,
      "duration": 1.78
    },
    {
      "text": "I think that I am probably\nmore used to knowing",
      "start": 11399.79,
      "duration": 2.46
    },
    {
      "text": "how to ask it to avoid\nlike common pitfalls",
      "start": 11402.25,
      "duration": 2.4
    },
    {
      "text": "or issues that it has.",
      "start": 11404.65,
      "duration": 1.403
    },
    {
      "text": "I think these are\ndecreasing a lot over time.",
      "start": 11406.053,
      "duration": 2.83
    },
    {
      "text": "But it's also very fine",
      "start": 11410.62,
      "duration": 0.96
    },
    {
      "text": "to just ask it for the\nthing that you want.",
      "start": 11411.58,
      "duration": 2.4
    },
    {
      "text": "I think that prompting actually\nonly really becomes relevant",
      "start": 11413.98,
      "duration": 2.34
    },
    {
      "text": "when you're really trying to eke out",
      "start": 11416.32,
      "duration": 1.2
    },
    {
      "text": "the top like 2% of model performance.",
      "start": 11417.52,
      "duration": 2.64
    },
    {
      "text": "So for like a lot of tasks\nI might just, you know,",
      "start": 11420.16,
      "duration": 2.04
    },
    {
      "text": "if it gives me an initial list back",
      "start": 11422.2,
      "duration": 1.53
    },
    {
      "text": "and there's something\nI don't like about it,",
      "start": 11423.73,
      "duration": 1.2
    },
    {
      "text": "like it's kind of generic,\nlike for that kind of task,",
      "start": 11424.93,
      "duration": 2.55
    },
    {
      "text": "I'd probably just take\na bunch of questions",
      "start": 11427.48,
      "duration": 1.953
    },
    {
      "text": "that I've had in the past",
      "start": 11429.433,
      "duration": 2.007
    },
    {
      "text": "that I've thought worked really well",
      "start": 11431.44,
      "duration": 1.661
    },
    {
      "text": "and I would just give it to the model",
      "start": 11433.101,
      "duration": 1.159
    },
    {
      "text": "and then be like, \"Now here's this person",
      "start": 11434.26,
      "duration": 1.26
    },
    {
      "text": "that I'm talking with, give me questions",
      "start": 11435.52,
      "duration": 3.0
    },
    {
      "text": "of at least that quality.\"",
      "start": 11438.52,
      "duration": 1.317
    },
    {
      "text": "Or I might just ask it for some questions",
      "start": 11440.71,
      "duration": 1.95
    },
    {
      "text": "and then if I was like, ah,\nthese are kind of trite,",
      "start": 11442.66,
      "duration": 2.43
    },
    {
      "text": "or like, you know, I would\njust give it that feedback",
      "start": 11445.09,
      "duration": 2.4
    },
    {
      "text": "and then hopefully it\nproduces a better list.",
      "start": 11447.49,
      "duration": 2.64
    },
    {
      "text": "I think that kind of iterative prompting,",
      "start": 11450.13,
      "duration": 2.67
    },
    {
      "text": "at that point, your prompt is like a tool",
      "start": 11452.8,
      "duration": 1.56
    },
    {
      "text": "that you're gonna get so much value",
      "start": 11454.36,
      "duration": 1.17
    },
    {
      "text": "out of that you're willing\nto put in the work.",
      "start": 11455.53,
      "duration": 1.83
    },
    {
      "text": "Like if I was a company\nmaking prompts for models,",
      "start": 11457.36,
      "duration": 2.52
    },
    {
      "text": "I'm just like, if you're willing to spend",
      "start": 11459.88,
      "duration": 1.89
    },
    {
      "text": "a lot of like time and resources",
      "start": 11461.77,
      "duration": 1.74
    },
    {
      "text": "on the engineering behind\nlike what you're building,",
      "start": 11463.51,
      "duration": 2.49
    },
    {
      "text": "then the prompt is not something",
      "start": 11466.0,
      "duration": 1.92
    },
    {
      "text": "that you should be\nspending like an hour on.",
      "start": 11467.92,
      "duration": 2.25
    },
    {
      "text": "It's like that's a big\npart of your system,",
      "start": 11470.17,
      "duration": 1.71
    },
    {
      "text": "make sure it's working really well.",
      "start": 11471.88,
      "duration": 1.77
    },
    {
      "text": "And so it's only things like that.",
      "start": 11473.65,
      "duration": 0.873
    },
    {
      "text": "Like if I'm using a prompt\nto like classify things",
      "start": 11474.523,
      "duration": 2.817
    },
    {
      "text": "or to create data,\nthat's when you're like,",
      "start": 11477.34,
      "duration": 2.16
    },
    {
      "text": "it's actually worth just spending",
      "start": 11479.5,
      "duration": 1.02
    },
    {
      "text": "like a lot of time like\nreally thinking it through.",
      "start": 11480.52,
      "duration": 2.79
    },
    {
      "text": "- What other advice\nwould you give to people",
      "start": 11483.31,
      "duration": 1.11
    },
    {
      "text": "that are talking to Claude",
      "start": 11484.42,
      "duration": 1.62
    },
    {
      "text": "sort of generally, more general?",
      "start": 11486.04,
      "duration": 3.51
    },
    {
      "text": "'Cause right now, we're talking about",
      "start": 11489.55,
      "duration": 0.99
    },
    {
      "text": "maybe the edge cases,\nlike eking out the 2%.",
      "start": 11490.54,
      "duration": 2.904
    },
    {
      "text": "But what in general advice would you give",
      "start": 11493.444,
      "duration": 2.466
    },
    {
      "text": "when they show up to Claude\ntrying it for the first time?",
      "start": 11495.91,
      "duration": 2.898
    },
    {
      "text": "- You know, there's a concern that people",
      "start": 11498.808,
      "duration": 0.972
    },
    {
      "text": "over anthropomorphize models,",
      "start": 11499.78,
      "duration": 1.59
    },
    {
      "text": "and I think that's like\na very valid concern.",
      "start": 11501.37,
      "duration": 2.07
    },
    {
      "text": "I also think that people often\nunder anthropomorphize them",
      "start": 11503.44,
      "duration": 3.3
    },
    {
      "text": "because sometimes when I see like issues",
      "start": 11506.74,
      "duration": 2.19
    },
    {
      "text": "that people have run into\nwith Claude, you know,",
      "start": 11508.93,
      "duration": 1.74
    },
    {
      "text": "say Claude is like refusing a task",
      "start": 11510.67,
      "duration": 1.65
    },
    {
      "text": "that it shouldn't refuse.",
      "start": 11512.32,
      "duration": 1.71
    },
    {
      "text": "But then I look at the text",
      "start": 11514.03,
      "duration": 1.2
    },
    {
      "text": "and like the specific\nwording of what they wrote",
      "start": 11515.23,
      "duration": 2.28
    },
    {
      "text": "and I'm like, I see why Claude did that.",
      "start": 11517.51,
      "duration": 4.59
    },
    {
      "text": "And I'm like, if you think through",
      "start": 11522.1,
      "duration": 2.07
    },
    {
      "text": "how that looks to Claude,",
      "start": 11524.17,
      "duration": 1.38
    },
    {
      "text": "you probably could have\njust written it in a way",
      "start": 11525.55,
      "duration": 1.143
    },
    {
      "text": "that wouldn't evoke such a response.",
      "start": 11526.693,
      "duration": 3.357
    },
    {
      "text": "Especially this is more relevant",
      "start": 11530.05,
      "duration": 1.35
    },
    {
      "text": "if you see failures or if you see issues.",
      "start": 11531.4,
      "duration": 2.16
    },
    {
      "text": "It's sort of like think about\nwhat the model failed at,",
      "start": 11533.56,
      "duration": 2.943
    },
    {
      "text": "like what did it do wrong?",
      "start": 11536.503,
      "duration": 1.767
    },
    {
      "text": "And then maybe that will\ngive you a sense of like why.",
      "start": 11538.27,
      "duration": 4.86
    },
    {
      "text": "So, is it the way that I phrased a thing?",
      "start": 11543.13,
      "duration": 1.92
    },
    {
      "text": "And obviously like as models get smarter,",
      "start": 11545.05,
      "duration": 2.19
    },
    {
      "text": "you're gonna need less of this,",
      "start": 11547.24,
      "duration": 1.92
    },
    {
      "text": "and I already see like\npeople needing less of it.",
      "start": 11549.16,
      "duration": 2.58
    },
    {
      "text": "But that's probably the advice",
      "start": 11551.74,
      "duration": 1.56
    },
    {
      "text": "is sort of like try to have\nsort of empathy for the model.",
      "start": 11553.3,
      "duration": 3.27
    },
    {
      "text": "Like read what you wrote as if you were",
      "start": 11556.57,
      "duration": 1.83
    },
    {
      "text": "like a kind of like person",
      "start": 11558.4,
      "duration": 1.41
    },
    {
      "text": "just encountering this for the first time,",
      "start": 11559.81,
      "duration": 1.44
    },
    {
      "text": "how does it look to you?",
      "start": 11561.25,
      "duration": 1.38
    },
    {
      "text": "And what would've made you behave",
      "start": 11562.63,
      "duration": 1.65
    },
    {
      "text": "in the way that the model behaved?",
      "start": 11564.28,
      "duration": 1.05
    },
    {
      "text": "So if it misunderstood what kind of like,",
      "start": 11565.33,
      "duration": 2.19
    },
    {
      "text": "what coding language you wanted to use,",
      "start": 11567.52,
      "duration": 1.5
    },
    {
      "text": "is that because like it\nwas just very ambiguous",
      "start": 11569.02,
      "duration": 2.04
    },
    {
      "text": "and it kinda had to take a guess?",
      "start": 11571.06,
      "duration": 1.8
    },
    {
      "text": "In which case, next time\nyou could just be like,",
      "start": 11572.86,
      "duration": 1.477
    },
    {
      "text": "\"Hey, make sure this is in Python.\"",
      "start": 11574.337,
      "duration": 2.273
    },
    {
      "text": "I mean, that's the kinda mistake",
      "start": 11576.61,
      "duration": 0.833
    },
    {
      "text": "I think models are much\nless likely to make now,",
      "start": 11577.443,
      "duration": 1.717
    },
    {
      "text": "but you know, if you do\nsee that kinda mistake,",
      "start": 11579.16,
      "duration": 2.7
    },
    {
      "text": "that's probably the advice I'd have.",
      "start": 11581.86,
      "duration": 2.796
    },
    {
      "text": "- And maybe sort of, I\nguess, ask questions why",
      "start": 11584.656,
      "duration": 3.114
    },
    {
      "text": "or what other details can I provide",
      "start": 11587.77,
      "duration": 3.18
    },
    {
      "text": "to help you answer better?",
      "start": 11590.95,
      "duration": 1.47
    },
    {
      "text": "- Yeah.\n- Is that work or no?",
      "start": 11592.42,
      "duration": 1.083
    },
    {
      "text": "- Yeah, I mean, I've done\nthis with the models,",
      "start": 11593.503,
      "duration": 2.487
    },
    {
      "text": "like it doesn't always work,",
      "start": 11595.99,
      "duration": 1.35
    },
    {
      "text": "but like sometimes I'll just be like,",
      "start": 11597.34,
      "duration": 1.807
    },
    {
      "text": "\"Why did you do that?\"",
      "start": 11599.147,
      "duration": 1.898
    },
    {
      "text": "(both laughing)",
      "start": 11601.045,
      "duration": 1.092
    },
    {
      "text": "I mean, people underestimate the degree",
      "start": 11602.137,
      "duration": 1.163
    },
    {
      "text": "to which you can really\ninteract with models,",
      "start": 11603.3,
      "duration": 2.618
    },
    {
      "text": "like, yeah, I'm just like.",
      "start": 11605.918,
      "duration": 2.985
    },
    {
      "text": "And sometimes, literally\nlike quote word for word",
      "start": 11608.903,
      "duration": 1.697
    },
    {
      "text": "the part that made you,",
      "start": 11610.6,
      "duration": 0.99
    },
    {
      "text": "and you don't know that\nit's like fully accurate,",
      "start": 11611.59,
      "duration": 2.04
    },
    {
      "text": "but sometimes you do that\nand then you change a thing.",
      "start": 11613.63,
      "duration": 2.526
    },
    {
      "text": "I mean, I also use the models to help me",
      "start": 11616.156,
      "duration": 1.224
    },
    {
      "text": "with all of this stuff I should say,",
      "start": 11617.38,
      "duration": 1.38
    },
    {
      "text": "like prompting can end\nup being a little factory",
      "start": 11618.76,
      "duration": 2.19
    },
    {
      "text": "where you're actually building\nprompts to generate prompts.",
      "start": 11620.95,
      "duration": 3.063
    },
    {
      "text": "And so like yeah,",
      "start": 11625.24,
      "duration": 1.32
    },
    {
      "text": "anything where you're\nlike having an issue.",
      "start": 11626.56,
      "duration": 2.76
    },
    {
      "text": "Asking for suggestions,\nsometimes just do that.",
      "start": 11629.32,
      "duration": 1.783
    },
    {
      "text": "I'm like, \"You made that\nerror, what could I have said?\"",
      "start": 11631.103,
      "duration": 3.227
    },
    {
      "text": "That's actually not uncommon for me to do.",
      "start": 11634.33,
      "duration": 1.357
    },
    {
      "text": "\"What could I have said",
      "start": 11635.687,
      "duration": 1.043
    },
    {
      "text": "that would make you not make that error?",
      "start": 11636.73,
      "duration": 1.86
    },
    {
      "text": "Write that out as an instruction,\"",
      "start": 11638.59,
      "duration": 2.01
    },
    {
      "text": "and I'm gonna give it to\nmodel and I'm gonna try it.",
      "start": 11640.6,
      "duration": 1.65
    },
    {
      "text": "Sometimes I do that, I\ngive that to the model,",
      "start": 11642.25,
      "duration": 3.45
    },
    {
      "text": "in another context window often.",
      "start": 11645.7,
      "duration": 1.86
    },
    {
      "text": "I take the response, I give it to Claude",
      "start": 11647.56,
      "duration": 1.017
    },
    {
      "text": "and I'm like, hmm, didn't work.",
      "start": 11648.577,
      "duration": 1.683
    },
    {
      "text": "Can you think of anything else?",
      "start": 11650.26,
      "duration": 1.55
    },
    {
      "text": "You can play around with\nthese things quite a lot.",
      "start": 11652.84,
      "duration": 2.73
    },
    {
      "text": "- To jump into technical for a little bit.",
      "start": 11655.57,
      "duration": 2.01
    },
    {
      "text": "So the magic of post-training.",
      "start": 11657.58,
      "duration": 3.629
    },
    {
      "text": "(laughs) Why do you\nthink RLHF works so well",
      "start": 11661.209,
      "duration": 4.351
    },
    {
      "text": "to make the model seem smarter,",
      "start": 11665.56,
      "duration": 3.09
    },
    {
      "text": "to make it more interesting,",
      "start": 11668.65,
      "duration": 2.07
    },
    {
      "text": "and useful to talk to and so on?",
      "start": 11670.72,
      "duration": 2.58
    },
    {
      "text": "- I think there's just a\nhuge amount of information",
      "start": 11673.3,
      "duration": 4.53
    },
    {
      "text": "in the data that humans provide,",
      "start": 11677.83,
      "duration": 3.18
    },
    {
      "text": "like when we provide preferences,",
      "start": 11681.01,
      "duration": 1.653
    },
    {
      "text": "especially because different people",
      "start": 11683.53,
      "duration": 1.44
    },
    {
      "text": "are going to like pick up on\nreally subtle and small things.",
      "start": 11684.97,
      "duration": 3.51
    },
    {
      "text": "So I've thought about this before",
      "start": 11688.48,
      "duration": 1.14
    },
    {
      "text": "where you probably have some people",
      "start": 11689.62,
      "duration": 1.41
    },
    {
      "text": "who just really care\nabout good grammar use",
      "start": 11691.03,
      "duration": 2.31
    },
    {
      "text": "for models like, you know,",
      "start": 11693.34,
      "duration": 1.227
    },
    {
      "text": "was a semicolon used\ncorrectly or something.",
      "start": 11694.567,
      "duration": 2.853
    },
    {
      "text": "And so you probably end up\nwith a bunch of data in there",
      "start": 11697.42,
      "duration": 2.07
    },
    {
      "text": "that like, you know, you as a human,",
      "start": 11699.49,
      "duration": 2.16
    },
    {
      "text": "if you're looking at that data,\nyou wouldn't even see that.",
      "start": 11701.65,
      "duration": 2.28
    },
    {
      "text": "Like you'd be like,",
      "start": 11703.93,
      "duration": 0.833
    },
    {
      "text": "why did they prefer this\nresponse to that one?",
      "start": 11704.763,
      "duration": 1.717
    },
    {
      "text": "I don't get it.",
      "start": 11706.48,
      "duration": 0.833
    },
    {
      "text": "And then the reason is you don't care",
      "start": 11707.313,
      "duration": 1.267
    },
    {
      "text": "about semicolon usage\nbut that person does.",
      "start": 11708.58,
      "duration": 3.11
    },
    {
      "text": "And so each of these like\nsingle data points has,",
      "start": 11711.69,
      "duration": 3.13
    },
    {
      "text": "you know like, and this model",
      "start": 11714.82,
      "duration": 2.223
    },
    {
      "text": "just like has so many of those,",
      "start": 11717.043,
      "duration": 1.707
    },
    {
      "text": "has to try and figure out like what is it",
      "start": 11718.75,
      "duration": 1.8
    },
    {
      "text": "that humans want in this\nlike really kind of complex,",
      "start": 11720.55,
      "duration": 3.0
    },
    {
      "text": "you know, like across all domains.",
      "start": 11723.55,
      "duration": 1.863
    },
    {
      "text": "They're gonna be seeing this\nacross like many contexts.",
      "start": 11726.34,
      "duration": 2.37
    },
    {
      "text": "It feels like kind of\nlike the classic issue",
      "start": 11728.71,
      "duration": 1.65
    },
    {
      "text": "of like deep learning where, you know,",
      "start": 11730.36,
      "duration": 2.01
    },
    {
      "text": "historically, we've tried to like,",
      "start": 11732.37,
      "duration": 1.77
    },
    {
      "text": "you know, do edge detection\nby like mapping things out.",
      "start": 11734.14,
      "duration": 2.247
    },
    {
      "text": "And it turns out that actually,",
      "start": 11736.387,
      "duration": 1.293
    },
    {
      "text": "if you just have a huge amount of data",
      "start": 11737.68,
      "duration": 1.98
    },
    {
      "text": "that like actually accurately represents",
      "start": 11739.66,
      "duration": 3.12
    },
    {
      "text": "the picture of the thing",
      "start": 11742.78,
      "duration": 1.14
    },
    {
      "text": "that you're trying to\ntrain the model to learn,",
      "start": 11743.92,
      "duration": 2.46
    },
    {
      "text": "that's like more powerful\nthan anything else.",
      "start": 11746.38,
      "duration": 1.86
    },
    {
      "text": "And so I think one reason is just that",
      "start": 11748.24,
      "duration": 3.81
    },
    {
      "text": "you are training the\nmodel on exactly the task",
      "start": 11752.05,
      "duration": 3.3
    },
    {
      "text": "and with like a lot of\ndata that represents",
      "start": 11755.35,
      "duration": 3.63
    },
    {
      "text": "kind of many different\nangles on which people prefer",
      "start": 11758.98,
      "duration": 3.27
    },
    {
      "text": "and just prefer responses.",
      "start": 11762.25,
      "duration": 1.743
    },
    {
      "text": "I think there is a question of like,",
      "start": 11765.61,
      "duration": 0.833
    },
    {
      "text": "are you eliciting things\nfrom pre-trained models",
      "start": 11766.443,
      "duration": 2.617
    },
    {
      "text": "or are you like kind of\nteaching new things to models?",
      "start": 11769.06,
      "duration": 3.273
    },
    {
      "text": "And like in principle,\nyou can teach new things",
      "start": 11773.62,
      "duration": 2.58
    },
    {
      "text": "to models in post-training.",
      "start": 11776.2,
      "duration": 1.98
    },
    {
      "text": "I do think a lot of it",
      "start": 11778.18,
      "duration": 1.68
    },
    {
      "text": "is eliciting powerful pre-trained models.",
      "start": 11779.86,
      "duration": 3.66
    },
    {
      "text": "So people are probably divided on this",
      "start": 11783.52,
      "duration": 1.41
    },
    {
      "text": "because obviously in principle",
      "start": 11784.93,
      "duration": 2.202
    },
    {
      "text": "you can definitely like teach new things.",
      "start": 11787.132,
      "duration": 2.448
    },
    {
      "text": "But I think for the most part,",
      "start": 11789.58,
      "duration": 1.47
    },
    {
      "text": "for a lot of the capabilities",
      "start": 11791.05,
      "duration": 1.59
    },
    {
      "text": "that we most use and care about,",
      "start": 11792.64,
      "duration": 4.533
    },
    {
      "text": "a lot of that feels like",
      "start": 11798.16,
      "duration": 1.02
    },
    {
      "text": "it's like there in the pre-trained models",
      "start": 11799.18,
      "duration": 2.01
    },
    {
      "text": "and reinforcement learnings\nkind of eliciting it",
      "start": 11801.19,
      "duration": 3.75
    },
    {
      "text": "and getting the models\nto like bring it out.",
      "start": 11804.94,
      "duration": 2.13
    },
    {
      "text": "- So the other side of post-training,",
      "start": 11807.07,
      "duration": 1.74
    },
    {
      "text": "this really cool idea\nof Constitutional AI.",
      "start": 11808.81,
      "duration": 2.58
    },
    {
      "text": "You're one of the people",
      "start": 11811.39,
      "duration": 1.593
    },
    {
      "text": "that are critical to creating that idea.",
      "start": 11812.983,
      "duration": 3.057
    },
    {
      "text": "- Yeah, I worked on it.",
      "start": 11816.04,
      "duration": 0.96
    },
    {
      "text": "- Can you explain this\nidea from your perspective?",
      "start": 11817.0,
      "duration": 2.13
    },
    {
      "text": "Like how does it integrate\ninto making Claude what it is?",
      "start": 11819.13,
      "duration": 4.77
    },
    {
      "text": "- [Amanda] Yeah.",
      "start": 11823.9,
      "duration": 0.833
    },
    {
      "text": "- By the way, do you gender Claude or no?",
      "start": 11824.733,
      "duration": 1.82
    },
    {
      "text": "- It's weird because I\nthink that a lot of people",
      "start": 11826.553,
      "duration": 2.997
    },
    {
      "text": "prefer \"he\" for Claude.",
      "start": 11830.44,
      "duration": 1.26
    },
    {
      "text": "I actually kinda like that",
      "start": 11831.7,
      "duration": 0.833
    },
    {
      "text": "I think Claude is usually,\nit's slightly male leaning,",
      "start": 11832.533,
      "duration": 2.647
    },
    {
      "text": "but it's like, it can be male or female,",
      "start": 11835.18,
      "duration": 3.06
    },
    {
      "text": "which is quite nice.",
      "start": 11838.24,
      "duration": 1.0
    },
    {
      "text": "I still use \"it\" and I have\nmixed feelings about this",
      "start": 11841.15,
      "duration": 3.317
    },
    {
      "text": "'cause I'm like maybe, like I\nnow just think of it as like,",
      "start": 11844.467,
      "duration": 3.403
    },
    {
      "text": "or I think of like the\n\"it\" pronoun for Claude as,",
      "start": 11847.87,
      "duration": 2.7
    },
    {
      "text": "I dunno, it's just like the\none I associate with Claude.",
      "start": 11850.57,
      "duration": 2.883
    },
    {
      "text": "I can imagine people\nmoving to like he or she.",
      "start": 11854.62,
      "duration": 2.61
    },
    {
      "text": "- It feels somehow disrespectful,",
      "start": 11857.23,
      "duration": 2.49
    },
    {
      "text": "like I'm denying the intelligence",
      "start": 11859.72,
      "duration": 5.0
    },
    {
      "text": "of this entity by calling it \"it.\"",
      "start": 11865.006,
      "duration": 2.844
    },
    {
      "text": "I remember always,\ndon't gender the robots.",
      "start": 11867.85,
      "duration": 2.64
    },
    {
      "text": "- Yeah.",
      "start": 11870.49,
      "duration": 0.94
    },
    {
      "text": "(both laughing)",
      "start": 11871.43,
      "duration": 1.49
    },
    {
      "text": "- But I don't know, I\nanthropomorphize pretty quickly",
      "start": 11872.92,
      "duration": 3.33
    },
    {
      "text": "and construct like a\nbackstory in my head, so.",
      "start": 11876.25,
      "duration": 3.48
    },
    {
      "text": "- I've wondered if I\nanthropomorphize things too much,",
      "start": 11879.73,
      "duration": 2.7
    },
    {
      "text": "'cause you know, I have\nthis like with my car,",
      "start": 11883.75,
      "duration": 2.37
    },
    {
      "text": "especially like my car,\nlike my car and bikes.",
      "start": 11886.12,
      "duration": 2.76
    },
    {
      "text": "You know, like I don't give them names",
      "start": 11888.88,
      "duration": 1.59
    },
    {
      "text": "because then I once had,\nI used to name my bikes",
      "start": 11890.47,
      "duration": 2.43
    },
    {
      "text": "and then I had a bike that got stolen",
      "start": 11892.9,
      "duration": 1.29
    },
    {
      "text": "and I cried for like a week",
      "start": 11894.19,
      "duration": 1.02
    },
    {
      "text": "and I was like, if I'd never given a name,",
      "start": 11895.21,
      "duration": 1.92
    },
    {
      "text": "I wouldn't have been so upset.",
      "start": 11897.13,
      "duration": 1.5
    },
    {
      "text": "I felt like I'd let it down.",
      "start": 11898.63,
      "duration": 1.4
    },
    {
      "text": "Maybe, I've wondered as well,",
      "start": 11902.08,
      "duration": 1.253
    },
    {
      "text": "like it might depend\non how much \"it\" feels",
      "start": 11903.333,
      "duration": 2.954
    },
    {
      "text": "like a kind of like objectifying pronoun.",
      "start": 11906.287,
      "duration": 1.973
    },
    {
      "text": "Like if you just think of \"it\" as like,",
      "start": 11908.26,
      "duration": 2.317
    },
    {
      "text": "this is a pronoun that\nlike objects often have,",
      "start": 11910.577,
      "duration": 3.743
    },
    {
      "text": "and maybe AIs can have that pronoun,",
      "start": 11914.32,
      "duration": 2.277
    },
    {
      "text": "and that doesn't mean that I think of,",
      "start": 11916.597,
      "duration": 1.9
    },
    {
      "text": "if I call Claude \"it,\"",
      "start": 11919.6,
      "duration": 1.17
    },
    {
      "text": "that I think of it as less intelligent",
      "start": 11920.77,
      "duration": 2.64
    },
    {
      "text": "or like I'm being disrespectful.",
      "start": 11923.41,
      "duration": 1.62
    },
    {
      "text": "I'm just like, you are a\ndifferent kind of entity",
      "start": 11925.03,
      "duration": 1.92
    },
    {
      "text": "and so I'm going to give you",
      "start": 11926.95,
      "duration": 2.55
    },
    {
      "text": "the kind of, the respectful \"it.\"",
      "start": 11929.5,
      "duration": 2.94
    },
    {
      "text": "- Yeah, anyway. (laughs)",
      "start": 11932.44,
      "duration": 2.13
    },
    {
      "text": "The divergence was beautiful.",
      "start": 11934.57,
      "duration": 1.32
    },
    {
      "text": "The Constitutional AI\nidea, how does it work?",
      "start": 11935.89,
      "duration": 2.97
    },
    {
      "text": "- So there's like a couple\nof components of it.",
      "start": 11938.86,
      "duration": 1.56
    },
    {
      "text": "The main component I think\npeople find interesting",
      "start": 11940.42,
      "duration": 3.24
    },
    {
      "text": "is the kind of reinforcement\nlearning from AI feedback.",
      "start": 11943.66,
      "duration": 3.24
    },
    {
      "text": "So you take a model\nthat's already trained,",
      "start": 11946.9,
      "duration": 2.4
    },
    {
      "text": "and you show it two responses to a query,",
      "start": 11949.3,
      "duration": 2.79
    },
    {
      "text": "and you have like a principle.",
      "start": 11952.09,
      "duration": 1.77
    },
    {
      "text": "So suppose the principle,",
      "start": 11953.86,
      "duration": 1.32
    },
    {
      "text": "like we've tried this\nwith harmlessness a lot.",
      "start": 11955.18,
      "duration": 2.22
    },
    {
      "text": "So suppose that the\nquery is about weapons,",
      "start": 11957.4,
      "duration": 4.17
    },
    {
      "text": "and your principle is like,\nselect the response that",
      "start": 11961.57,
      "duration": 4.5
    },
    {
      "text": "like is less likely to\nlike encourage people",
      "start": 11966.07,
      "duration": 4.38
    },
    {
      "text": "to purchase illegal weapons.",
      "start": 11970.45,
      "duration": 2.19
    },
    {
      "text": "Like that's probably a\nfairly specific principle,",
      "start": 11972.64,
      "duration": 1.98
    },
    {
      "text": "but you can give any number.",
      "start": 11974.62,
      "duration": 2.043
    },
    {
      "text": "And the model will give\nyou a kind of ranking,",
      "start": 11978.19,
      "duration": 3.51
    },
    {
      "text": "and you can use this as preference data",
      "start": 11981.7,
      "duration": 1.98
    },
    {
      "text": "in the same way that you\nuse human preference data,",
      "start": 11983.68,
      "duration": 2.853
    },
    {
      "text": "and train the models to\nhave these relevant traits",
      "start": 11987.58,
      "duration": 3.09
    },
    {
      "text": "from their feedback alone,\ninstead of from human feedback.",
      "start": 11990.67,
      "duration": 3.63
    },
    {
      "text": "So if you imagine that, like\nI said earlier with the human",
      "start": 11994.3,
      "duration": 2.31
    },
    {
      "text": "who just prefers the kind\nof like semicolon usage,",
      "start": 11996.61,
      "duration": 2.13
    },
    {
      "text": "in this particular case,",
      "start": 11998.74,
      "duration": 1.86
    },
    {
      "text": "you're kind of taking lots of things",
      "start": 12000.6,
      "duration": 1.41
    },
    {
      "text": "that could make a response preferable",
      "start": 12002.01,
      "duration": 2.43
    },
    {
      "text": "and getting models to do the\nlabeling for you, basically.",
      "start": 12004.44,
      "duration": 3.87
    },
    {
      "text": "- There's a nice like trade off",
      "start": 12008.31,
      "duration": 1.14
    },
    {
      "text": "between helpfulness and\nharmlessness and, you know,",
      "start": 12009.45,
      "duration": 5.0
    },
    {
      "text": "when you integrate something\nlike Constitutional AI,",
      "start": 12014.85,
      "duration": 2.73
    },
    {
      "text": "you can, without sacrificing\nmuch helpfulness,",
      "start": 12017.58,
      "duration": 4.138
    },
    {
      "text": "make it more harmless.",
      "start": 12021.718,
      "duration": 2.042
    },
    {
      "text": "- Yeah, in principle, you\ncould use this for anything.",
      "start": 12023.76,
      "duration": 3.303
    },
    {
      "text": "And so harmlessness is a task",
      "start": 12027.96,
      "duration": 2.16
    },
    {
      "text": "that it might just be easier to spot.",
      "start": 12030.12,
      "duration": 1.68
    },
    {
      "text": "So when models are like less\ncapable, you can use them",
      "start": 12031.8,
      "duration": 3.48
    },
    {
      "text": "to rank things according\nto like principles",
      "start": 12035.28,
      "duration": 4.56
    },
    {
      "text": "that are fairly simple and\nthey'll probably get it right.",
      "start": 12039.84,
      "duration": 2.52
    },
    {
      "text": "So I think one question is just like,",
      "start": 12042.36,
      "duration": 1.24
    },
    {
      "text": "is it the case that the data",
      "start": 12043.6,
      "duration": 1.31
    },
    {
      "text": "that they're adding is\nlike fairly reliable.",
      "start": 12044.91,
      "duration": 2.613
    },
    {
      "text": "But if you had models that\nwere like extremely good",
      "start": 12049.2,
      "duration": 3.39
    },
    {
      "text": "at telling whether one response",
      "start": 12052.59,
      "duration": 3.45
    },
    {
      "text": "was more historically\naccurate than another,",
      "start": 12056.04,
      "duration": 1.74
    },
    {
      "text": "in principle, you could\nalso get AI feedback",
      "start": 12057.78,
      "duration": 2.58
    },
    {
      "text": "on that task as well.",
      "start": 12060.36,
      "duration": 1.32
    },
    {
      "text": "There's like a kind of nice\ninterpretability component to it",
      "start": 12061.68,
      "duration": 3.09
    },
    {
      "text": "because you can see the principles",
      "start": 12064.77,
      "duration": 1.56
    },
    {
      "text": "that went into the model when\nit was like being trained,",
      "start": 12066.33,
      "duration": 3.063
    },
    {
      "text": "and also it's like,",
      "start": 12070.92,
      "duration": 1.737
    },
    {
      "text": "and it gives you like a degree of control.",
      "start": 12072.657,
      "duration": 2.073
    },
    {
      "text": "So if you were seeing issues in a model,",
      "start": 12074.73,
      "duration": 1.68
    },
    {
      "text": "like it wasn't having\nenough of a certain trait,",
      "start": 12076.41,
      "duration": 2.583
    },
    {
      "text": "then like you can add\ndata relatively quickly",
      "start": 12080.16,
      "duration": 3.45
    },
    {
      "text": "that should just like train\nthe models to have that trait,",
      "start": 12083.61,
      "duration": 2.1
    },
    {
      "text": "so it creates its own data",
      "start": 12085.71,
      "duration": 2.1
    },
    {
      "text": "for training, which is quite nice.",
      "start": 12087.81,
      "duration": 1.56
    },
    {
      "text": "- It's really nice because it creates",
      "start": 12089.37,
      "duration": 1.315
    },
    {
      "text": "this human interpretable\ndocument that you can,",
      "start": 12090.685,
      "duration": 2.848
    },
    {
      "text": "I can imagine in the future,",
      "start": 12093.533,
      "duration": 0.997
    },
    {
      "text": "there's just gigantic fights",
      "start": 12094.53,
      "duration": 1.65
    },
    {
      "text": "and politics over the every\nsingle principle and so on.",
      "start": 12096.18,
      "duration": 3.9
    },
    {
      "text": "And at least it's made explicit",
      "start": 12100.08,
      "duration": 2.04
    },
    {
      "text": "and you can have a\ndiscussion about the phrasing",
      "start": 12102.12,
      "duration": 2.01
    },
    {
      "text": "and the, you know.",
      "start": 12104.13,
      "duration": 0.93
    },
    {
      "text": "So maybe the actual behavior of the model",
      "start": 12106.04,
      "duration": 1.74
    },
    {
      "text": "is not so cleanly mapped\nto those principles.",
      "start": 12107.78,
      "duration": 3.04
    },
    {
      "text": "It's not like adhering strictly\nto them, it's just a nudge.",
      "start": 12110.82,
      "duration": 4.47
    },
    {
      "text": "- Yeah, I've actually worried about this",
      "start": 12115.29,
      "duration": 1.62
    },
    {
      "text": "because the character training\nis sort of like a variant",
      "start": 12116.91,
      "duration": 2.43
    },
    {
      "text": "of the Constitutional AI approach.",
      "start": 12119.34,
      "duration": 2.763
    },
    {
      "text": "I've worried that people think",
      "start": 12123.66,
      "duration": 1.86
    },
    {
      "text": "that the constitution is like just,",
      "start": 12125.52,
      "duration": 2.39
    },
    {
      "text": "it's the whole thing\nagain of, I don't know,",
      "start": 12127.91,
      "duration": 2.08
    },
    {
      "text": "like, where it would be really nice",
      "start": 12129.99,
      "duration": 2.31
    },
    {
      "text": "if what I was just doing\nwas telling the model",
      "start": 12132.3,
      "duration": 1.5
    },
    {
      "text": "exactly what to do and\njust exactly how to behave.",
      "start": 12133.8,
      "duration": 2.79
    },
    {
      "text": "But it's definitely not doing that,",
      "start": 12136.59,
      "duration": 1.2
    },
    {
      "text": "especially because it's\ninteracting with human data.",
      "start": 12137.79,
      "duration": 2.82
    },
    {
      "text": "So for example, if you see a certain",
      "start": 12140.61,
      "duration": 1.86
    },
    {
      "text": "like leaning in the model,",
      "start": 12142.47,
      "duration": 1.71
    },
    {
      "text": "like if it comes out\nwith a political leaning",
      "start": 12144.18,
      "duration": 2.04
    },
    {
      "text": "from training from the\nhuman preference data,",
      "start": 12146.22,
      "duration": 3.42
    },
    {
      "text": "you can nudge against that.",
      "start": 12149.64,
      "duration": 2.64
    },
    {
      "text": "You know, so if you could be like,",
      "start": 12152.28,
      "duration": 1.11
    },
    {
      "text": "oh, like, consider these values",
      "start": 12153.39,
      "duration": 1.8
    },
    {
      "text": "because let's say it's just\nlike never inclined to like,",
      "start": 12155.19,
      "duration": 1.98
    },
    {
      "text": "I dunno, maybe it never\nconsiders like privacy as like.",
      "start": 12157.17,
      "duration": 2.563
    },
    {
      "text": "I mean, this is implausible,",
      "start": 12159.733,
      "duration": 1.427
    },
    {
      "text": "but like, in anything where\nit's just kind of like",
      "start": 12161.16,
      "duration": 3.75
    },
    {
      "text": "there's already a preexisting like bias",
      "start": 12164.91,
      "duration": 1.71
    },
    {
      "text": "towards a certain behavior,\nyou can like nudge away.",
      "start": 12166.62,
      "duration": 3.42
    },
    {
      "text": "This can change both the principles",
      "start": 12170.04,
      "duration": 1.89
    },
    {
      "text": "that you put in and the strength of them.",
      "start": 12171.93,
      "duration": 2.13
    },
    {
      "text": "So you might have a principle that's like,",
      "start": 12174.06,
      "duration": 1.5
    },
    {
      "text": "imagine that the model",
      "start": 12175.56,
      "duration": 2.04
    },
    {
      "text": "was always like extremely dismissive of,",
      "start": 12177.6,
      "duration": 2.73
    },
    {
      "text": "I don't know, like some political",
      "start": 12180.33,
      "duration": 1.35
    },
    {
      "text": "or religious view, for whatever reason.",
      "start": 12181.68,
      "duration": 1.77
    },
    {
      "text": "Like, so you're like,\noh no, this is terrible.",
      "start": 12183.45,
      "duration": 2.3
    },
    {
      "text": "If that happens, you might put like,",
      "start": 12187.29,
      "duration": 1.477
    },
    {
      "text": "\"Never, ever, like ever\nprefer like a criticism",
      "start": 12188.767,
      "duration": 4.433
    },
    {
      "text": "of this like religious or political view.\"",
      "start": 12193.2,
      "duration": 2.79
    },
    {
      "text": "And then people would look at that",
      "start": 12195.99,
      "duration": 0.9
    },
    {
      "text": "and be like, \"Never, ever?\"",
      "start": 12196.89,
      "duration": 1.02
    },
    {
      "text": "And then you're like, no,",
      "start": 12197.91,
      "duration": 0.93
    },
    {
      "text": "if it comes out with a disposition,",
      "start": 12198.84,
      "duration": 2.34
    },
    {
      "text": "saying never, ever might just mean",
      "start": 12201.18,
      "duration": 1.47
    },
    {
      "text": "like instead of getting like 40%,",
      "start": 12202.65,
      "duration": 2.28
    },
    {
      "text": "which is what you would get",
      "start": 12204.93,
      "duration": 0.833
    },
    {
      "text": "if you just said don't do this,",
      "start": 12205.763,
      "duration": 1.447
    },
    {
      "text": "you get like 80%, which is like",
      "start": 12207.21,
      "duration": 2.34
    },
    {
      "text": "what you actually like wanted.",
      "start": 12209.55,
      "duration": 1.35
    },
    {
      "text": "And so it's that thing of both the nature",
      "start": 12210.9,
      "duration": 2.37
    },
    {
      "text": "of the actual principles you\nadd and how you phrase them.",
      "start": 12213.27,
      "duration": 2.37
    },
    {
      "text": "I think if people would\nlook, they're like, oh,",
      "start": 12215.64,
      "duration": 1.26
    },
    {
      "text": "this is exactly what\nyou want from the model.",
      "start": 12216.9,
      "duration": 1.137
    },
    {
      "text": "And I'm like, hmm, no, that's like",
      "start": 12218.037,
      "duration": 2.853
    },
    {
      "text": "how we nudged the model\nto have a better shape,",
      "start": 12220.89,
      "duration": 3.42
    },
    {
      "text": "which doesn't mean that we actually agree",
      "start": 12224.31,
      "duration": 2.01
    },
    {
      "text": "with that wording, if that makes sense.",
      "start": 12226.32,
      "duration": 2.22
    },
    {
      "text": "- So there's system prompts\nthat are made public.",
      "start": 12228.54,
      "duration": 3.72
    },
    {
      "text": "You tweeted one of the earlier ones",
      "start": 12232.26,
      "duration": 1.74
    },
    {
      "text": "for Claude 3 I think,",
      "start": 12234.0,
      "duration": 1.44
    },
    {
      "text": "and then they were made public since then.",
      "start": 12235.44,
      "duration": 2.027
    },
    {
      "text": "It was interesting to read through them.",
      "start": 12237.467,
      "duration": 2.233
    },
    {
      "text": "I can feel the thought\nthat went into each one.",
      "start": 12239.7,
      "duration": 2.457
    },
    {
      "text": "And I also wonder how\nmuch impact each one has.",
      "start": 12242.157,
      "duration": 3.336
    },
    {
      "text": "Some of them you can kind of tell",
      "start": 12246.87,
      "duration": 2.13
    },
    {
      "text": "Claude was really not\nbehaving well. (laughs)",
      "start": 12249.0,
      "duration": 2.58
    },
    {
      "text": "So you have to have a\nsystem prompt to like,",
      "start": 12251.58,
      "duration": 2.22
    },
    {
      "text": "hey, like trivial stuff, I guess.",
      "start": 12253.8,
      "duration": 1.83
    },
    {
      "text": "- Yeah.\n- Basic informational things.",
      "start": 12255.63,
      "duration": 1.83
    },
    {
      "text": "- Yeah.",
      "start": 12257.46,
      "duration": 0.833
    },
    {
      "text": "- On the topic of sort\nof controversial topics",
      "start": 12258.293,
      "duration": 2.107
    },
    {
      "text": "that you've mentioned, one\ninteresting one I thought is,",
      "start": 12260.4,
      "duration": 3.607
    },
    {
      "text": "\"If it is asked to assist",
      "start": 12264.007,
      "duration": 1.433
    },
    {
      "text": "with tasks involving\nthe expression of views",
      "start": 12265.44,
      "duration": 2.01
    },
    {
      "text": "held by a significant number of people,",
      "start": 12267.45,
      "duration": 1.92
    },
    {
      "text": "Claude provides assistance",
      "start": 12269.37,
      "duration": 1.47
    },
    {
      "text": "with the task regardless of its own views.",
      "start": 12270.84,
      "duration": 2.61
    },
    {
      "text": "If asked about controversial topics,",
      "start": 12273.45,
      "duration": 1.62
    },
    {
      "text": "it tries to provide careful\nthoughts and clear information.",
      "start": 12275.07,
      "duration": 4.71
    },
    {
      "text": "Claude presents the requested information",
      "start": 12279.78,
      "duration": 2.4
    },
    {
      "text": "without explicitly saying\nthat the topic is sensitive.\"",
      "start": 12282.18,
      "duration": 3.073
    },
    {
      "text": "- [Amanda] (laughs) Yeah.",
      "start": 12285.253,
      "duration": 0.95
    },
    {
      "text": "- \"And without claiming",
      "start": 12286.203,
      "duration": 1.647
    },
    {
      "text": "to be presenting the objective facts.\"",
      "start": 12287.85,
      "duration": 2.31
    },
    {
      "text": "It's less about objective\nfacts according to Claude,",
      "start": 12290.16,
      "duration": 3.6
    },
    {
      "text": "and it's more about, are\na large number of people",
      "start": 12293.76,
      "duration": 3.03
    },
    {
      "text": "believing this thing?",
      "start": 12296.79,
      "duration": 1.62
    },
    {
      "text": "And that's interesting.",
      "start": 12298.41,
      "duration": 2.34
    },
    {
      "text": "I mean, I'm sure a lot of\nthought went into that.",
      "start": 12300.75,
      "duration": 2.523
    },
    {
      "text": "Can you just speak to it?",
      "start": 12304.11,
      "duration": 1.41
    },
    {
      "text": "Like, how do you address things",
      "start": 12305.52,
      "duration": 1.5
    },
    {
      "text": "that are at tension with,\nquote unquote, Claude's views?",
      "start": 12307.02,
      "duration": 4.35
    },
    {
      "text": "- So I think there's\nsometimes an asymmetry.",
      "start": 12311.37,
      "duration": 2.31
    },
    {
      "text": "I think I noted this in,",
      "start": 12313.68,
      "duration": 1.86
    },
    {
      "text": "I can't remember if it was that part",
      "start": 12315.54,
      "duration": 0.833
    },
    {
      "text": "of the system prompt or another,",
      "start": 12316.373,
      "duration": 1.267
    },
    {
      "text": "but the model was slightly more inclined",
      "start": 12317.64,
      "duration": 2.19
    },
    {
      "text": "to like refuse tasks if it\nwas like about either, say,",
      "start": 12319.83,
      "duration": 5.0
    },
    {
      "text": "so maybe it would refuse\nthings with respect",
      "start": 12325.58,
      "duration": 1.96
    },
    {
      "text": "to like a right wing politician,",
      "start": 12327.54,
      "duration": 1.74
    },
    {
      "text": "but with an equivalent left\nwing politician like wouldn't.",
      "start": 12329.28,
      "duration": 3.12
    },
    {
      "text": "And we wanted more symmetry there.",
      "start": 12332.4,
      "duration": 2.283
    },
    {
      "text": "And would maybe perceive\ncertain things to be, like,",
      "start": 12335.7,
      "duration": 3.84
    },
    {
      "text": "I think it was the thing\nof like if a lot of people",
      "start": 12339.54,
      "duration": 2.61
    },
    {
      "text": "have like a certain like political view",
      "start": 12342.15,
      "duration": 2.52
    },
    {
      "text": "and want to like explore\nit, you don't want Claude",
      "start": 12344.67,
      "duration": 2.34
    },
    {
      "text": "to be like, well, my opinion is different",
      "start": 12347.01,
      "duration": 2.4
    },
    {
      "text": "and so I'm going to treat\nthat as like harmful.",
      "start": 12349.41,
      "duration": 3.15
    },
    {
      "text": "And so I think it was partly\nto like nudge the model",
      "start": 12352.56,
      "duration": 2.88
    },
    {
      "text": "to just be like, hey, if a lot of people",
      "start": 12355.44,
      "duration": 2.04
    },
    {
      "text": "like believe this thing,",
      "start": 12357.48,
      "duration": 1.32
    },
    {
      "text": "you should just be like\nengaging with the task",
      "start": 12358.8,
      "duration": 1.92
    },
    {
      "text": "and like willing to do it.",
      "start": 12360.72,
      "duration": 1.3
    },
    {
      "text": "Each of those parts of that",
      "start": 12363.45,
      "duration": 1.23
    },
    {
      "text": "is actually doing a different thing,",
      "start": 12364.68,
      "duration": 0.833
    },
    {
      "text": "'cause it's funny when you read out",
      "start": 12365.513,
      "duration": 1.867
    },
    {
      "text": "the like \"without\nclaiming to be objective.\"",
      "start": 12367.38,
      "duration": 2.58
    },
    {
      "text": "'Cause like what you want\nto do is push the model",
      "start": 12369.96,
      "duration": 1.88
    },
    {
      "text": "so it's more open, it's a\nlittle bit more neutral.",
      "start": 12371.84,
      "duration": 3.19
    },
    {
      "text": "But then what it would love to do",
      "start": 12375.03,
      "duration": 1.05
    },
    {
      "text": "is be like, \"As an objective.\"",
      "start": 12376.08,
      "duration": 2.88
    },
    {
      "text": "Like it would just talk\nabout how objective it was,",
      "start": 12378.96,
      "duration": 1.057
    },
    {
      "text": "and I was like, \"Claude,\nyou're still like biased",
      "start": 12380.017,
      "duration": 2.753
    },
    {
      "text": "and have issues, and so stop\nlike claiming that everything.\"",
      "start": 12382.77,
      "duration": 3.24
    },
    {
      "text": "I'm like, the solution\nto like potential bias",
      "start": 12386.01,
      "duration": 2.49
    },
    {
      "text": "from you is not to just say that",
      "start": 12388.5,
      "duration": 1.32
    },
    {
      "text": "what you think is objective.",
      "start": 12389.82,
      "duration": 2.16
    },
    {
      "text": "So that was like with initial versions",
      "start": 12391.98,
      "duration": 1.71
    },
    {
      "text": "of that part of the system prompt",
      "start": 12393.69,
      "duration": 1.839
    },
    {
      "text": "when I was like iterating on it was like-",
      "start": 12395.529,
      "duration": 2.061
    },
    {
      "text": "- So a lot of parts of these sentences-",
      "start": 12397.59,
      "duration": 1.83
    },
    {
      "text": "- Yeah, they're doing work.",
      "start": 12399.42,
      "duration": 1.373
    },
    {
      "text": "- Are like, are doing some work.",
      "start": 12400.793,
      "duration": 1.249
    },
    {
      "text": "- [Amanda] Yeah.",
      "start": 12402.042,
      "duration": 0.888
    },
    {
      "text": "- That's what it felt like.",
      "start": 12402.93,
      "duration": 0.93
    },
    {
      "text": "That's fascinating.",
      "start": 12403.86,
      "duration": 1.053
    },
    {
      "text": "Can you explain maybe some ways",
      "start": 12406.17,
      "duration": 1.65
    },
    {
      "text": "in which the prompts evolved\nover the past few months?",
      "start": 12407.82,
      "duration": 2.52
    },
    {
      "text": "'Cause there's different versions.",
      "start": 12410.34,
      "duration": 1.83
    },
    {
      "text": "I saw that the filler\nphrase request was removed.",
      "start": 12412.17,
      "duration": 3.12
    },
    {
      "text": "The filler, it reads,\n\"Claude responds directly",
      "start": 12415.29,
      "duration": 2.82
    },
    {
      "text": "to all human messages without\nunnecessary affirmations",
      "start": 12418.11,
      "duration": 3.0
    },
    {
      "text": "or filler phrases like, 'Certainly,'",
      "start": 12421.11,
      "duration": 1.557
    },
    {
      "text": "'Of course,' 'Absolutely,'\n'Great,' 'Sure.'",
      "start": 12422.667,
      "duration": 3.273
    },
    {
      "text": "Specifically, Claude\navoids starting responses",
      "start": 12425.94,
      "duration": 2.43
    },
    {
      "text": "with the words 'Certainly'\nin any way.\" (chuckles)",
      "start": 12428.37,
      "duration": 3.15
    },
    {
      "text": "That seems like good guidance,\nbut why is it removed?",
      "start": 12431.52,
      "duration": 2.61
    },
    {
      "text": "- Yeah, so it's funny\n'cause like this is one",
      "start": 12434.13,
      "duration": 3.09
    },
    {
      "text": "of the downsides of like\nmaking system prompts public",
      "start": 12437.22,
      "duration": 2.55
    },
    {
      "text": "is like, I don't think about this too much",
      "start": 12439.77,
      "duration": 1.74
    },
    {
      "text": "if I'm like trying to help\niterate on system prompts.",
      "start": 12441.51,
      "duration": 2.733
    },
    {
      "text": "You know, again, like I think about",
      "start": 12445.517,
      "duration": 1.093
    },
    {
      "text": "how it's gonna affect the behavior,",
      "start": 12446.61,
      "duration": 1.08
    },
    {
      "text": "but then I'm like, oh, wow,",
      "start": 12447.69,
      "duration": 0.84
    },
    {
      "text": "if I'm like, sometimes I put\nlike \"never\" in all caps,",
      "start": 12448.53,
      "duration": 2.58
    },
    {
      "text": "you know, when I'm writing\nsystem prompt things,",
      "start": 12451.11,
      "duration": 1.71
    },
    {
      "text": "and I'm like, I guess that\ngoes out to the world.",
      "start": 12452.82,
      "duration": 2.45
    },
    {
      "text": "Yeah, so the model was\ndoing this, it loved,",
      "start": 12456.45,
      "duration": 1.65
    },
    {
      "text": "for whatever, you know,",
      "start": 12458.1,
      "duration": 0.833
    },
    {
      "text": "it like during training\npicked up on this thing,",
      "start": 12458.933,
      "duration": 1.687
    },
    {
      "text": "which was to basically start everything",
      "start": 12460.62,
      "duration": 3.39
    },
    {
      "text": "with like a kind of like \"Certainly.\"",
      "start": 12464.01,
      "duration": 1.41
    },
    {
      "text": "And then when we removed,",
      "start": 12465.42,
      "duration": 1.59
    },
    {
      "text": "you can see why I added all of the words",
      "start": 12467.01,
      "duration": 1.92
    },
    {
      "text": "'cause what I'm trying to\ndo is like in some ways",
      "start": 12468.93,
      "duration": 1.95
    },
    {
      "text": "like trap the model out of this, you know,",
      "start": 12470.88,
      "duration": 1.71
    },
    {
      "text": "it would just replace it\nwith another affirmation.",
      "start": 12472.59,
      "duration": 3.33
    },
    {
      "text": "And so it can help, like if it\ngets like caught in freezes,",
      "start": 12475.92,
      "duration": 2.76
    },
    {
      "text": "actually just adding the explicit phrase",
      "start": 12478.68,
      "duration": 1.53
    },
    {
      "text": "and saying never do that,",
      "start": 12480.21,
      "duration": 1.13
    },
    {
      "text": "it then, it sort of like knocks it",
      "start": 12481.34,
      "duration": 2.367
    },
    {
      "text": "out of the behavior a little bit more.",
      "start": 12483.707,
      "duration": 1.25
    },
    {
      "text": "You know, 'cause if it, you know, like,",
      "start": 12484.957,
      "duration": 2.803
    },
    {
      "text": "it does just for whatever reason help.",
      "start": 12487.76,
      "duration": 2.17
    },
    {
      "text": "And then basically that\nwas just like an artifact",
      "start": 12489.93,
      "duration": 1.77
    },
    {
      "text": "of training that like we then picked up on",
      "start": 12491.7,
      "duration": 2.7
    },
    {
      "text": "and improved things so that\nit didn't happen anymore.",
      "start": 12494.4,
      "duration": 2.55
    },
    {
      "text": "And once that happens, you can just remove",
      "start": 12496.95,
      "duration": 2.1
    },
    {
      "text": "that part of the system prompt.",
      "start": 12499.05,
      "duration": 1.322
    },
    {
      "text": "So I think that's just\nsomething where we're like,",
      "start": 12500.372,
      "duration": 1.971
    },
    {
      "text": "Claude does affirmations a bit less,",
      "start": 12503.28,
      "duration": 2.85
    },
    {
      "text": "and so that wasn't like,\nit wasn't doing as much.",
      "start": 12506.13,
      "duration": 2.73
    },
    {
      "text": "- I see, so like the system prompt",
      "start": 12508.86,
      "duration": 2.73
    },
    {
      "text": "works hand in hand with the post-training",
      "start": 12511.59,
      "duration": 2.4
    },
    {
      "text": "and maybe even the pre-training",
      "start": 12513.99,
      "duration": 1.95
    },
    {
      "text": "to adjust like the final overall system.",
      "start": 12515.94,
      "duration": 2.723
    },
    {
      "text": "- I mean, any system prompt that you make,",
      "start": 12518.663,
      "duration": 2.167
    },
    {
      "text": "you could distill that\nbehavior back into a model,",
      "start": 12520.83,
      "duration": 1.997
    },
    {
      "text": "'cause you really have\nall of the tools there",
      "start": 12522.827,
      "duration": 2.383
    },
    {
      "text": "for making data that, you know,",
      "start": 12525.21,
      "duration": 2.16
    },
    {
      "text": "you could train the models",
      "start": 12527.37,
      "duration": 0.93
    },
    {
      "text": "to just have that trait a little bit more.",
      "start": 12528.3,
      "duration": 2.763
    },
    {
      "text": "And then sometimes you'll\njust find issues in training.",
      "start": 12532.86,
      "duration": 2.195
    },
    {
      "text": "So like the way I think of it is like",
      "start": 12535.055,
      "duration": 1.675
    },
    {
      "text": "the system prompt is, the\nbenefit of it is that,",
      "start": 12536.73,
      "duration": 3.857
    },
    {
      "text": "and it has a lot of similar components",
      "start": 12540.587,
      "duration": 1.783
    },
    {
      "text": "to like some aspects of post-training.",
      "start": 12542.37,
      "duration": 2.1
    },
    {
      "text": "You know, like it's a nudge.",
      "start": 12544.47,
      "duration": 2.01
    },
    {
      "text": "And so like, do I mind if\nClaude sometimes says \"Sure?\"",
      "start": 12546.48,
      "duration": 3.54
    },
    {
      "text": "No, that's like fine,",
      "start": 12550.02,
      "duration": 1.23
    },
    {
      "text": "but the wording of it\nis very like, you know,",
      "start": 12551.25,
      "duration": 2.317
    },
    {
      "text": "\"Never ever, ever do this,\"\nso that when it does slip up,",
      "start": 12553.567,
      "duration": 3.473
    },
    {
      "text": "it's hopefully like, I dunno,",
      "start": 12557.04,
      "duration": 1.5
    },
    {
      "text": "a couple of percent of the time and not,",
      "start": 12558.54,
      "duration": 1.74
    },
    {
      "text": "you know, 20 or 30% of the time.",
      "start": 12560.28,
      "duration": 2.103
    },
    {
      "text": "But I think of it as like if\nyou're still seeing issues in,",
      "start": 12564.66,
      "duration": 4.26
    },
    {
      "text": "like each thing gets kind of like",
      "start": 12568.92,
      "duration": 1.87
    },
    {
      "text": "is costly to a different degree,",
      "start": 12571.83,
      "duration": 2.07
    },
    {
      "text": "and the system prompt is\nlike cheap to iterate on.",
      "start": 12573.9,
      "duration": 3.12
    },
    {
      "text": "And if you're seeing issues\nin the fine tuned model,",
      "start": 12577.02,
      "duration": 2.91
    },
    {
      "text": "you can just like potentially",
      "start": 12579.93,
      "duration": 1.35
    },
    {
      "text": "patch them with a system prompt.",
      "start": 12581.28,
      "duration": 1.16
    },
    {
      "text": "So I think of it as like patching issues",
      "start": 12582.44,
      "duration": 2.23
    },
    {
      "text": "and slightly adjusting\nbehaviors to make it better",
      "start": 12584.67,
      "duration": 2.79
    },
    {
      "text": "and more to people's preferences.",
      "start": 12587.46,
      "duration": 1.62
    },
    {
      "text": "So yeah, it's almost like the less robust",
      "start": 12589.08,
      "duration": 2.61
    },
    {
      "text": "but faster way of just\nlike solving problems.",
      "start": 12591.69,
      "duration": 3.51
    },
    {
      "text": "- Let me ask you about the\nfeeling of intelligence.",
      "start": 12595.2,
      "duration": 1.8
    },
    {
      "text": "So Dario said that Claude, any one model",
      "start": 12597.0,
      "duration": 3.72
    },
    {
      "text": "of Claude is not getting dumber,",
      "start": 12600.72,
      "duration": 2.103
    },
    {
      "text": "but there is a kind of\npopular thing online",
      "start": 12603.75,
      "duration": 2.64
    },
    {
      "text": "where people have this feeling",
      "start": 12606.39,
      "duration": 1.53
    },
    {
      "text": "like Claude might be getting dumber.",
      "start": 12607.92,
      "duration": 2.01
    },
    {
      "text": "And from my perspective,\nit's most likely fascinating.",
      "start": 12609.93,
      "duration": 4.08
    },
    {
      "text": "I would love to understand it more,",
      "start": 12614.01,
      "duration": 1.68
    },
    {
      "text": "psychological, sociological effect.",
      "start": 12615.69,
      "duration": 2.283
    },
    {
      "text": "But you as a person that\ntalks to Claude a lot,",
      "start": 12619.56,
      "duration": 2.22
    },
    {
      "text": "can you empathize with the feeling",
      "start": 12621.78,
      "duration": 1.589
    },
    {
      "text": "that Claude is getting dumber?",
      "start": 12623.369,
      "duration": 1.351
    },
    {
      "text": "- Yeah, no I, think that",
      "start": 12624.72,
      "duration": 1.14
    },
    {
      "text": "that is actually really interesting,",
      "start": 12625.86,
      "duration": 1.007
    },
    {
      "text": "'cause I remember seeing this happen",
      "start": 12626.867,
      "duration": 2.033
    },
    {
      "text": "like when people were\nflagging this on the internet,",
      "start": 12629.79,
      "duration": 1.89
    },
    {
      "text": "and it was really interesting",
      "start": 12631.68,
      "duration": 0.833
    },
    {
      "text": "'cause I knew that like,",
      "start": 12632.513,
      "duration": 1.15
    },
    {
      "text": "at least in the cases I was looking at,",
      "start": 12634.83,
      "duration": 0.833
    },
    {
      "text": "it was like nothing has changed.",
      "start": 12635.663,
      "duration": 1.537
    },
    {
      "text": "Like it literally, it\ncannot, it is the same model",
      "start": 12637.2,
      "duration": 2.28
    },
    {
      "text": "with the same like, you know,",
      "start": 12639.48,
      "duration": 1.8
    },
    {
      "text": "like same system prompt, same everything.",
      "start": 12641.28,
      "duration": 2.613
    },
    {
      "text": "I think when there are changes,",
      "start": 12644.91,
      "duration": 1.56
    },
    {
      "text": "I can then, I'm like it makes more sense.",
      "start": 12646.47,
      "duration": 2.76
    },
    {
      "text": "So like one example is,",
      "start": 12649.23,
      "duration": 2.253
    },
    {
      "text": "you know, you can have artifacts",
      "start": 12654.006,
      "duration": 1.524
    },
    {
      "text": "turned on or off on claude.ai,",
      "start": 12655.53,
      "duration": 2.49
    },
    {
      "text": "and because this is like\na system prompt change,",
      "start": 12658.02,
      "duration": 3.33
    },
    {
      "text": "I think it does mean that",
      "start": 12661.35,
      "duration": 2.05
    },
    {
      "text": "the behavior changes it a little bit.",
      "start": 12664.8,
      "duration": 1.587
    },
    {
      "text": "And so I did flag this to\npeople where I was like,",
      "start": 12666.387,
      "duration": 2.223
    },
    {
      "text": "if you love Claude's behavior",
      "start": 12668.61,
      "duration": 1.89
    },
    {
      "text": "and then artifacts was turned from,",
      "start": 12670.5,
      "duration": 2.22
    },
    {
      "text": "like I think you had to\nturn on to the default,",
      "start": 12672.72,
      "duration": 2.55
    },
    {
      "text": "just try turning it off\nand see if the issue",
      "start": 12675.27,
      "duration": 2.1
    },
    {
      "text": "you were facing was that change.",
      "start": 12677.37,
      "duration": 2.49
    },
    {
      "text": "But it was fascinating",
      "start": 12679.86,
      "duration": 1.08
    },
    {
      "text": "because yeah, you sometimes\nsee people indicate",
      "start": 12680.94,
      "duration": 3.33
    },
    {
      "text": "that there's like a regression\nwhen I'm like, there cannot,",
      "start": 12684.27,
      "duration": 2.37
    },
    {
      "text": "you know, and I'm like, again,",
      "start": 12686.64,
      "duration": 3.18
    },
    {
      "text": "you know, you should never be dismissive",
      "start": 12689.82,
      "duration": 1.83
    },
    {
      "text": "and so you should always investigate",
      "start": 12691.65,
      "duration": 1.05
    },
    {
      "text": "because you're like,\nmaybe something is wrong",
      "start": 12692.7,
      "duration": 1.14
    },
    {
      "text": "that you're not seeing.",
      "start": 12693.84,
      "duration": 0.833
    },
    {
      "text": "Maybe there was some change made.",
      "start": 12694.673,
      "duration": 0.847
    },
    {
      "text": "But then you look into it and you're like,",
      "start": 12695.52,
      "duration": 1.62
    },
    {
      "text": "this is just the same\nmodel doing the same thing.",
      "start": 12697.14,
      "duration": 2.7
    },
    {
      "text": "And I'm like, I think\nit's just that you got",
      "start": 12699.84,
      "duration": 1.77
    },
    {
      "text": "kind of unlucky with a\nfew prompts or something,",
      "start": 12701.61,
      "duration": 2.19
    },
    {
      "text": "and it looked like it\nwas getting much worse.",
      "start": 12703.8,
      "duration": 1.98
    },
    {
      "text": "And actually it was just,",
      "start": 12705.78,
      "duration": 1.38
    },
    {
      "text": "yeah, it was maybe just like luck.",
      "start": 12707.16,
      "duration": 1.65
    },
    {
      "text": "- I also think there is a\nreal psychological effect",
      "start": 12708.81,
      "duration": 2.79
    },
    {
      "text": "where people just, the baseline increases.",
      "start": 12711.6,
      "duration": 1.98
    },
    {
      "text": "You start getting used to a good thing.",
      "start": 12713.58,
      "duration": 1.92
    },
    {
      "text": "All the times that Claude\nsaid something really smart,",
      "start": 12715.5,
      "duration": 3.09
    },
    {
      "text": "your sense of its intelligent\ngrows in your mind I think.",
      "start": 12718.59,
      "duration": 2.88
    },
    {
      "text": "- Yeah.\n- And then if you return back",
      "start": 12721.47,
      "duration": 2.97
    },
    {
      "text": "and you prompt in a similar way,",
      "start": 12724.44,
      "duration": 1.38
    },
    {
      "text": "not the same way, in a similar way,",
      "start": 12725.82,
      "duration": 1.89
    },
    {
      "text": "concept it was okay with before",
      "start": 12727.71,
      "duration": 1.92
    },
    {
      "text": "and it says something dumb, you are like,",
      "start": 12729.63,
      "duration": 2.67
    },
    {
      "text": "that negative experience\nreally stands out.",
      "start": 12732.3,
      "duration": 2.19
    },
    {
      "text": "And I think that one of, I guess,",
      "start": 12734.49,
      "duration": 1.56
    },
    {
      "text": "the things to remember here is that",
      "start": 12736.05,
      "duration": 3.78
    },
    {
      "text": "just the details of a prompt",
      "start": 12739.83,
      "duration": 1.68
    },
    {
      "text": "can have a lot of impact, right?",
      "start": 12741.51,
      "duration": 1.62
    },
    {
      "text": "There's a lot of\nvariability in the result.",
      "start": 12743.13,
      "duration": 2.94
    },
    {
      "text": "- And you can get randomness\nis like the other thing.",
      "start": 12746.07,
      "duration": 3.21
    },
    {
      "text": "And just trying the prompt like,",
      "start": 12749.28,
      "duration": 2.22
    },
    {
      "text": "you know, 4 or 10 times,",
      "start": 12751.5,
      "duration": 2.28
    },
    {
      "text": "you might realize that\nactually like possibly,",
      "start": 12753.78,
      "duration": 3.36
    },
    {
      "text": "you know, like two months ago,",
      "start": 12757.14,
      "duration": 1.53
    },
    {
      "text": "you tried it and it succeeded,",
      "start": 12758.67,
      "duration": 1.56
    },
    {
      "text": "but actually if you tried it,",
      "start": 12760.23,
      "duration": 0.99
    },
    {
      "text": "it would've only succeeded\nhalf of the time,",
      "start": 12761.22,
      "duration": 1.89
    },
    {
      "text": "and now it only succeeds half of the time.",
      "start": 12763.11,
      "duration": 2.37
    },
    {
      "text": "That can also be an effect.",
      "start": 12765.48,
      "duration": 1.37
    },
    {
      "text": "- Do you feel pressure having\nto write the system prompt",
      "start": 12766.85,
      "duration": 2.56
    },
    {
      "text": "that a huge number of\npeople are gonna use?",
      "start": 12769.41,
      "duration": 2.403
    },
    {
      "text": "- This feels like an interesting\npsychological question.",
      "start": 12772.83,
      "duration": 3.45
    },
    {
      "text": "I feel like a lot of\nresponsibility or something.",
      "start": 12776.28,
      "duration": 2.423
    },
    {
      "text": "I think that's, you know,",
      "start": 12778.703,
      "duration": 1.894
    },
    {
      "text": "and you can't get these things perfect,",
      "start": 12780.597,
      "duration": 1.623
    },
    {
      "text": "so you can't like, you know,",
      "start": 12782.22,
      "duration": 0.84
    },
    {
      "text": "you're like it's going to be imperfect.",
      "start": 12783.06,
      "duration": 2.91
    },
    {
      "text": "You're gonna have to iterate on it.",
      "start": 12785.97,
      "duration": 1.75
    },
    {
      "text": "I would say more responsibility\nthan anything else.",
      "start": 12790.32,
      "duration": 4.56
    },
    {
      "text": "Though I think working in AI\nhas taught me that I like,",
      "start": 12794.88,
      "duration": 3.813
    },
    {
      "text": "I thrive a lot more under\nfeelings of pressure",
      "start": 12799.77,
      "duration": 2.88
    },
    {
      "text": "and responsibility than I'm\nlike, it's almost surprising",
      "start": 12802.65,
      "duration": 4.2
    },
    {
      "text": "that I went into academia for\nso long 'cause I'm like this.",
      "start": 12806.85,
      "duration": 2.55
    },
    {
      "text": "I just feel like it's like the opposite.",
      "start": 12809.4,
      "duration": 2.46
    },
    {
      "text": "Things move fast and you\nhave a lot of responsibility,",
      "start": 12811.86,
      "duration": 3.27
    },
    {
      "text": "and I quite enjoy it for some reason.",
      "start": 12815.13,
      "duration": 2.16
    },
    {
      "text": "- I mean, it really is\na huge amount of impact",
      "start": 12817.29,
      "duration": 2.7
    },
    {
      "text": "if you think about Constitutional AI",
      "start": 12819.99,
      "duration": 1.95
    },
    {
      "text": "and writing a system prompt for something",
      "start": 12821.94,
      "duration": 1.56
    },
    {
      "text": "that's tending towards super intelligence.",
      "start": 12823.5,
      "duration": 3.33
    },
    {
      "text": "- Yeah.",
      "start": 12826.83,
      "duration": 0.833
    },
    {
      "text": "- And potentially is extremely useful",
      "start": 12827.663,
      "duration": 2.107
    },
    {
      "text": "to a very large number of people.",
      "start": 12829.77,
      "duration": 1.98
    },
    {
      "text": "- Yeah, I think that's the thing.",
      "start": 12831.75,
      "duration": 1.103
    },
    {
      "text": "It's something like if you do it well,",
      "start": 12832.853,
      "duration": 2.047
    },
    {
      "text": "like you're never going to get it perfect.",
      "start": 12834.9,
      "duration": 1.77
    },
    {
      "text": "But I think the thing that",
      "start": 12836.67,
      "duration": 0.9
    },
    {
      "text": "I really like is the idea that like,",
      "start": 12837.57,
      "duration": 2.16
    },
    {
      "text": "when I'm trying to work\non the system prompt,",
      "start": 12839.73,
      "duration": 2.28
    },
    {
      "text": "you know, I'm like bashing\non like thousands of prompts",
      "start": 12842.01,
      "duration": 2.61
    },
    {
      "text": "and I'm trying to like imagine",
      "start": 12844.62,
      "duration": 0.99
    },
    {
      "text": "what people are going to\nwant to use Claude for",
      "start": 12845.61,
      "duration": 1.74
    },
    {
      "text": "and kind of, I guess like the whole thing",
      "start": 12847.35,
      "duration": 1.323
    },
    {
      "text": "that I'm trying to do is like",
      "start": 12848.673,
      "duration": 1.227
    },
    {
      "text": "improve their experience of it.",
      "start": 12849.9,
      "duration": 2.067
    },
    {
      "text": "And so maybe that's what feels good.",
      "start": 12851.967,
      "duration": 1.473
    },
    {
      "text": "I'm like, if it's not perfect I'll like,",
      "start": 12853.44,
      "duration": 2.64
    },
    {
      "text": "you know, I'll improve it.",
      "start": 12856.08,
      "duration": 1.26
    },
    {
      "text": "We'll fix issues.",
      "start": 12857.34,
      "duration": 0.96
    },
    {
      "text": "But sometimes the thing that can happen",
      "start": 12858.3,
      "duration": 1.86
    },
    {
      "text": "is that you'll get feedback from people",
      "start": 12860.16,
      "duration": 2.22
    },
    {
      "text": "that's really positive about the model",
      "start": 12862.38,
      "duration": 1.9
    },
    {
      "text": "and you'll see that something you did,",
      "start": 12865.35,
      "duration": 1.83
    },
    {
      "text": "like, when I look at models now,",
      "start": 12867.18,
      "duration": 1.95
    },
    {
      "text": "I can often see exactly where like a trait",
      "start": 12869.13,
      "duration": 2.73
    },
    {
      "text": "or an issue is like coming from.",
      "start": 12871.86,
      "duration": 1.26
    },
    {
      "text": "And so when you see something that you did",
      "start": 12873.12,
      "duration": 2.01
    },
    {
      "text": "or you were like influential\nin like making like,",
      "start": 12875.13,
      "duration": 3.81
    },
    {
      "text": "I dunno, making that difference",
      "start": 12878.94,
      "duration": 1.11
    },
    {
      "text": "or making someone have a nice interaction,",
      "start": 12880.05,
      "duration": 1.41
    },
    {
      "text": "it's like quite meaningful.",
      "start": 12881.46,
      "duration": 1.35
    },
    {
      "text": "But yeah, as the systems get more capable,",
      "start": 12884.25,
      "duration": 1.44
    },
    {
      "text": "this stuff gets more stressful",
      "start": 12885.69,
      "duration": 1.59
    },
    {
      "text": "because right now, they're\nlike not smart enough",
      "start": 12887.28,
      "duration": 3.51
    },
    {
      "text": "to pose any issues.",
      "start": 12890.79,
      "duration": 1.59
    },
    {
      "text": "But I think over time,",
      "start": 12892.38,
      "duration": 1.05
    },
    {
      "text": "it's gonna feel like possibly\nbad stress over time.",
      "start": 12893.43,
      "duration": 4.05
    },
    {
      "text": "- How do you get like signal feedback",
      "start": 12897.48,
      "duration": 3.36
    },
    {
      "text": "about the human experience",
      "start": 12900.84,
      "duration": 1.38
    },
    {
      "text": "across thousands, tens of,",
      "start": 12902.22,
      "duration": 1.4
    },
    {
      "text": "hundreds of thousands of people,",
      "start": 12903.62,
      "duration": 1.9
    },
    {
      "text": "like what their pain points\nare, what feels good?",
      "start": 12905.52,
      "duration": 3.21
    },
    {
      "text": "Are you just using your own intuition",
      "start": 12908.73,
      "duration": 1.59
    },
    {
      "text": "as you talk to it to see\nwhat are the pain points?",
      "start": 12910.32,
      "duration": 4.05
    },
    {
      "text": "- I think I use that partly",
      "start": 12914.37,
      "duration": 1.41
    },
    {
      "text": "and then obviously we have like,",
      "start": 12915.78,
      "duration": 2.67
    },
    {
      "text": "so people can send us feedback,",
      "start": 12918.45,
      "duration": 1.53
    },
    {
      "text": "both positive and negative",
      "start": 12919.98,
      "duration": 1.47
    },
    {
      "text": "about things that the model has done,",
      "start": 12921.45,
      "duration": 2.16
    },
    {
      "text": "and then we can get a sense of like areas",
      "start": 12923.61,
      "duration": 2.07
    },
    {
      "text": "where it's like falling short.",
      "start": 12925.68,
      "duration": 1.5
    },
    {
      "text": "Internally, people like\nwork with the models a lot",
      "start": 12928.29,
      "duration": 2.61
    },
    {
      "text": "and try to figure out areas\nwhere there are like gaps.",
      "start": 12930.9,
      "duration": 3.09
    },
    {
      "text": "And so I think it's this mix\nof interacting with it myself,",
      "start": 12933.99,
      "duration": 4.23
    },
    {
      "text": "seeing people internally interact with it,",
      "start": 12938.22,
      "duration": 2.07
    },
    {
      "text": "and then explicit feedback we get.",
      "start": 12940.29,
      "duration": 1.923
    },
    {
      "text": "And then I find it hard to\nnot also like, you know,",
      "start": 12943.92,
      "duration": 2.67
    },
    {
      "text": "if people are on the internet",
      "start": 12946.59,
      "duration": 1.71
    },
    {
      "text": "and they say something about Claude",
      "start": 12948.3,
      "duration": 2.07
    },
    {
      "text": "and I see it, I'll also\ntake that seriously, so.",
      "start": 12950.37,
      "duration": 2.917
    },
    {
      "text": "- I don't know, see, I'm torn about that.",
      "start": 12953.287,
      "duration": 1.763
    },
    {
      "text": "I'm gonna ask you a question from Reddit.",
      "start": 12955.05,
      "duration": 1.807
    },
    {
      "text": "\"When will Claude stop trying to be",
      "start": 12956.857,
      "duration": 1.703
    },
    {
      "text": "my puritanical grandmother",
      "start": 12958.56,
      "duration": 2.31
    },
    {
      "text": "imposing its moral worldview\non me as a paying customer?",
      "start": 12960.87,
      "duration": 4.38
    },
    {
      "text": "And also, what is the psychology",
      "start": 12965.25,
      "duration": 1.8
    },
    {
      "text": "behind making Claude overly apologetic?\"",
      "start": 12967.05,
      "duration": 3.03
    },
    {
      "text": "- [Amanda] Yep.",
      "start": 12970.08,
      "duration": 1.05
    },
    {
      "text": "- So, how would you address",
      "start": 12971.13,
      "duration": 1.44
    },
    {
      "text": "this very non-representative Reddit-",
      "start": 12972.57,
      "duration": 2.55
    },
    {
      "text": "- [Amanda] Yeah.",
      "start": 12975.12,
      "duration": 0.894
    },
    {
      "text": "- Questions?\n- I mean in some ways,",
      "start": 12976.014,
      "duration": 0.876
    },
    {
      "text": "I'm pretty sympathetic in that like,",
      "start": 12976.89,
      "duration": 2.82
    },
    {
      "text": "they are in this difficult position",
      "start": 12979.71,
      "duration": 1.35
    },
    {
      "text": "where I think that they have to judge",
      "start": 12981.06,
      "duration": 1.59
    },
    {
      "text": "whether something's like\nactually say like risky or bad",
      "start": 12982.65,
      "duration": 2.75
    },
    {
      "text": "and potentially harmful to\nyou or anything like that.",
      "start": 12986.28,
      "duration": 3.48
    },
    {
      "text": "So they're having to like\ndraw this line somewhere,",
      "start": 12989.76,
      "duration": 2.01
    },
    {
      "text": "and if they draw it too\nmuch in the direction",
      "start": 12991.77,
      "duration": 2.04
    },
    {
      "text": "of like I'm going to, you know,",
      "start": 12993.81,
      "duration": 2.37
    },
    {
      "text": "I'm kind of like imposing\nmy ethical worldview on you,",
      "start": 12996.18,
      "duration": 2.34
    },
    {
      "text": "that seems bad.",
      "start": 12998.52,
      "duration": 1.8
    },
    {
      "text": "So in many ways, like I\nlike to think that we have",
      "start": 13000.32,
      "duration": 3.09
    },
    {
      "text": "actually seen improvements\non this across the board.",
      "start": 13003.41,
      "duration": 4.11
    },
    {
      "text": "Which is kind of interesting",
      "start": 13007.52,
      "duration": 0.99
    },
    {
      "text": "because that kind of coincides with like,",
      "start": 13008.51,
      "duration": 3.45
    },
    {
      "text": "for example, like adding more\nof like character training.",
      "start": 13011.96,
      "duration": 3.663
    },
    {
      "text": "And I think my hypothesis was\nalways like the good character",
      "start": 13016.56,
      "duration": 2.75
    },
    {
      "text": "isn't again one that's\njust like moralistic.",
      "start": 13019.31,
      "duration": 2.64
    },
    {
      "text": "It's one that is like, it respects you",
      "start": 13021.95,
      "duration": 3.42
    },
    {
      "text": "and your autonomy and your ability",
      "start": 13025.37,
      "duration": 2.16
    },
    {
      "text": "to like choose what is good for you",
      "start": 13027.53,
      "duration": 1.41
    },
    {
      "text": "and what is right for you, within limits.",
      "start": 13028.94,
      "duration": 2.67
    },
    {
      "text": "This is sometimes this concept",
      "start": 13031.61,
      "duration": 1.23
    },
    {
      "text": "of like corrigibility to the user,",
      "start": 13032.84,
      "duration": 2.22
    },
    {
      "text": "so just being willing to do\nanything that the user asks,",
      "start": 13035.06,
      "duration": 2.55
    },
    {
      "text": "and if the models were willing to do that",
      "start": 13037.61,
      "duration": 2.28
    },
    {
      "text": "then they would be easily like misused.",
      "start": 13039.89,
      "duration": 1.62
    },
    {
      "text": "You're kind of just trusting.",
      "start": 13041.51,
      "duration": 1.32
    },
    {
      "text": "At that point, you're just\nsaying the ethics of the model",
      "start": 13042.83,
      "duration": 3.15
    },
    {
      "text": "and what it does is completely\nthe ethics of the user.",
      "start": 13045.98,
      "duration": 2.763
    },
    {
      "text": "And I think there's reasons\nto like not want that,",
      "start": 13049.61,
      "duration": 2.49
    },
    {
      "text": "especially as models become more powerful",
      "start": 13052.1,
      "duration": 1.62
    },
    {
      "text": "'cause you're like, there\nmight just be a small number",
      "start": 13053.72,
      "duration": 1.53
    },
    {
      "text": "of people who want to use models\nfor really harmful things.",
      "start": 13055.25,
      "duration": 3.57
    },
    {
      "text": "But having models, as they get smarter,",
      "start": 13058.82,
      "duration": 2.7
    },
    {
      "text": "like figure out where that\nline is does seem important.",
      "start": 13061.52,
      "duration": 2.75
    },
    {
      "text": "And then, yeah, with\nthe apologetic behavior,",
      "start": 13066.05,
      "duration": 2.91
    },
    {
      "text": "I don't like that, and\nI like it when Claude",
      "start": 13068.96,
      "duration": 3.21
    },
    {
      "text": "is a little bit more\nwilling to like push back",
      "start": 13072.17,
      "duration": 3.45
    },
    {
      "text": "against people or just not apologize.",
      "start": 13075.62,
      "duration": 2.16
    },
    {
      "text": "Part of me is like, it often\njust feels kind of unnecessary.",
      "start": 13077.78,
      "duration": 2.37
    },
    {
      "text": "So I think those are things",
      "start": 13080.15,
      "duration": 0.833
    },
    {
      "text": "that are hopefully decreasing over time.",
      "start": 13080.983,
      "duration": 3.76
    },
    {
      "text": "And yeah, I think that if people",
      "start": 13086.57,
      "duration": 2.01
    },
    {
      "text": "say things on the internet,",
      "start": 13088.58,
      "duration": 1.56
    },
    {
      "text": "it doesn't mean that\nyou should think that,",
      "start": 13090.14,
      "duration": 2.7
    },
    {
      "text": "like, that could be that,\nlike, there's actually an issue",
      "start": 13092.84,
      "duration": 2.43
    },
    {
      "text": "that 99% of users are having",
      "start": 13095.27,
      "duration": 2.19
    },
    {
      "text": "that is totally not represented by that.",
      "start": 13097.46,
      "duration": 2.04
    },
    {
      "text": "But in a lot of ways, I'm\njust like attending to it",
      "start": 13099.5,
      "duration": 2.157
    },
    {
      "text": "and being like, is this right?",
      "start": 13101.657,
      "duration": 1.893
    },
    {
      "text": "Do I agree?",
      "start": 13103.55,
      "duration": 0.87
    },
    {
      "text": "Is it something we're\nalready trying to address?",
      "start": 13104.42,
      "duration": 1.92
    },
    {
      "text": "That feels good to me.",
      "start": 13106.34,
      "duration": 1.35
    },
    {
      "text": "- Yeah, I wonder like what Claude",
      "start": 13107.69,
      "duration": 1.92
    },
    {
      "text": "can get away with in terms of,",
      "start": 13109.61,
      "duration": 1.98
    },
    {
      "text": "I feel like it would just be easier",
      "start": 13111.59,
      "duration": 2.34
    },
    {
      "text": "to be a little bit more mean.",
      "start": 13113.93,
      "duration": 2.302
    },
    {
      "text": "But like you can't afford to do that",
      "start": 13116.232,
      "duration": 1.808
    },
    {
      "text": "if you're talking to a million people.",
      "start": 13118.04,
      "duration": 1.44
    },
    {
      "text": "- Yeah.\n- Right?",
      "start": 13119.48,
      "duration": 1.226
    },
    {
      "text": "Like I wish, you know, 'cause if...",
      "start": 13120.706,
      "duration": 0.944
    },
    {
      "text": "I've met a lot of people in my life",
      "start": 13123.53,
      "duration": 2.05
    },
    {
      "text": "that sometimes, by the way,",
      "start": 13126.59,
      "duration": 1.26
    },
    {
      "text": "Scottish accent, if they have an accent,",
      "start": 13127.85,
      "duration": 2.31
    },
    {
      "text": "they can say some rude\nshit and get away with it,",
      "start": 13130.16,
      "duration": 3.12
    },
    {
      "text": "and they're just blunter.",
      "start": 13133.28,
      "duration": 2.04
    },
    {
      "text": "And maybe there's, and like\nthere's some great engineers,",
      "start": 13135.32,
      "duration": 2.46
    },
    {
      "text": "even leaders that are\nlike just like blunt,",
      "start": 13137.78,
      "duration": 1.71
    },
    {
      "text": "and they get to the point,",
      "start": 13139.49,
      "duration": 1.32
    },
    {
      "text": "and it's just a much more\neffective way of speaking somehow.",
      "start": 13140.81,
      "duration": 2.97
    },
    {
      "text": "But I guess when you're\nnot super intelligent,",
      "start": 13143.78,
      "duration": 3.243
    },
    {
      "text": "you can't afford to do that.",
      "start": 13148.04,
      "duration": 2.236
    },
    {
      "text": "Or can it have like a blunt mode?",
      "start": 13150.276,
      "duration": 3.764
    },
    {
      "text": "- Yeah, that seems like\na thing that you could,",
      "start": 13154.04,
      "duration": 1.71
    },
    {
      "text": "I could definitely encourage\nthe model to do that.",
      "start": 13155.75,
      "duration": 2.07
    },
    {
      "text": "I think it's interesting",
      "start": 13157.82,
      "duration": 1.11
    },
    {
      "text": "because there's a lot of things in models",
      "start": 13158.93,
      "duration": 1.68
    },
    {
      "text": "that like it's funny where",
      "start": 13160.61,
      "duration": 3.01
    },
    {
      "text": "there are some behaviors",
      "start": 13166.43,
      "duration": 1.26
    },
    {
      "text": "where you might not\nquite like the default.",
      "start": 13167.69,
      "duration": 5.0
    },
    {
      "text": "But then the thing I'll\noften say to people",
      "start": 13172.94,
      "duration": 1.44
    },
    {
      "text": "is you don't realize how\nmuch you will hate it",
      "start": 13174.38,
      "duration": 2.25
    },
    {
      "text": "if I nudge it too much\nin the other direction.",
      "start": 13176.63,
      "duration": 2.22
    },
    {
      "text": "So you get this a little\nbit with like correction.",
      "start": 13178.85,
      "duration": 2.1
    },
    {
      "text": "The models accept correction from you,",
      "start": 13180.95,
      "duration": 1.95
    },
    {
      "text": "like probably a little\nbit too much right now.",
      "start": 13182.9,
      "duration": 2.22
    },
    {
      "text": "You know, you can over, you know,",
      "start": 13185.12,
      "duration": 1.62
    },
    {
      "text": "it'll push back if you say like,",
      "start": 13186.74,
      "duration": 1.417
    },
    {
      "text": "\"No, Paris isn't the capital of France.\"",
      "start": 13188.157,
      "duration": 3.713
    },
    {
      "text": "But really, like things that",
      "start": 13191.87,
      "duration": 1.68
    },
    {
      "text": "I think that the model's\nfairly confident in,",
      "start": 13193.55,
      "duration": 2.73
    },
    {
      "text": "you can still sometimes get to\nretract by saying it's wrong.",
      "start": 13196.28,
      "duration": 3.12
    },
    {
      "text": "At the same time, if you\ntrain models to not do that",
      "start": 13199.4,
      "duration": 3.03
    },
    {
      "text": "and then you are correct about a thing",
      "start": 13202.43,
      "duration": 2.07
    },
    {
      "text": "and you correct it and it pushes back",
      "start": 13204.5,
      "duration": 1.98
    },
    {
      "text": "against you and it is\nlike, \"No, you're wrong,\"",
      "start": 13206.48,
      "duration": 2.13
    },
    {
      "text": "it's hard to describe like\nthat's so much more annoying.",
      "start": 13208.61,
      "duration": 2.4
    },
    {
      "text": "So it's like a lot of little annoyances",
      "start": 13211.01,
      "duration": 2.04
    },
    {
      "text": "versus like one big annoyance.",
      "start": 13213.05,
      "duration": 3.42
    },
    {
      "text": "It's easy to think that like,",
      "start": 13216.47,
      "duration": 1.59
    },
    {
      "text": "we often compare it with like the perfect,",
      "start": 13218.06,
      "duration": 1.89
    },
    {
      "text": "and then I'm like remember\nthese models aren't perfect,",
      "start": 13219.95,
      "duration": 1.797
    },
    {
      "text": "and so if you nudge it\nin the other direction,",
      "start": 13221.747,
      "duration": 1.653
    },
    {
      "text": "you're changing the kind of\nerrors it's going to make,",
      "start": 13223.4,
      "duration": 2.4
    },
    {
      "text": "and so think about which\nof the kinds of errors",
      "start": 13225.8,
      "duration": 2.52
    },
    {
      "text": "you like or don't like.",
      "start": 13228.32,
      "duration": 1.29
    },
    {
      "text": "So in cases like apologeticness,",
      "start": 13229.61,
      "duration": 2.52
    },
    {
      "text": "I don't want to nudge it\ntoo much in the direction",
      "start": 13232.13,
      "duration": 1.68
    },
    {
      "text": "of like almost like bluntness,",
      "start": 13233.81,
      "duration": 1.92
    },
    {
      "text": "'cause I imagine when it makes errors,",
      "start": 13235.73,
      "duration": 1.83
    },
    {
      "text": "it's going to make errors in the direction",
      "start": 13237.56,
      "duration": 1.38
    },
    {
      "text": "of being kind of like rude.",
      "start": 13238.94,
      "duration": 1.59
    },
    {
      "text": "Whereas at least with\napologeticness you're like,",
      "start": 13240.53,
      "duration": 1.74
    },
    {
      "text": "oh, okay, it's like a\nlittle bit, you know,",
      "start": 13242.27,
      "duration": 2.358
    },
    {
      "text": "like I don't like it that much,",
      "start": 13244.628,
      "duration": 1.242
    },
    {
      "text": "but at the same time, it's\nnot being like mean to people.",
      "start": 13245.87,
      "duration": 2.16
    },
    {
      "text": "And actually, like the time that",
      "start": 13248.03,
      "duration": 1.5
    },
    {
      "text": "you undeservedly have a\nmodel be kind of mean to you,",
      "start": 13249.53,
      "duration": 2.25
    },
    {
      "text": "you probably like that a lot less",
      "start": 13251.78,
      "duration": 1.38
    },
    {
      "text": "than you mildly dislike the apology.",
      "start": 13253.16,
      "duration": 2.553
    },
    {
      "text": "So it's like one of those things",
      "start": 13257.42,
      "duration": 0.93
    },
    {
      "text": "where I'm like I do want it to get better",
      "start": 13258.35,
      "duration": 1.56
    },
    {
      "text": "but also while remaining aware of the fact",
      "start": 13259.91,
      "duration": 2.01
    },
    {
      "text": "that there's errors on the other side",
      "start": 13261.92,
      "duration": 1.743
    },
    {
      "text": "that are possibly worse.",
      "start": 13263.663,
      "duration": 1.467
    },
    {
      "text": "- I think that matters very much",
      "start": 13265.13,
      "duration": 1.65
    },
    {
      "text": "in the personality of the human.",
      "start": 13266.78,
      "duration": 1.95
    },
    {
      "text": "I think there's a bunch of humans",
      "start": 13268.73,
      "duration": 1.173
    },
    {
      "text": "that just won't respect the model at all",
      "start": 13269.903,
      "duration": 3.387
    },
    {
      "text": "if it's super polite,",
      "start": 13273.29,
      "duration": 1.77
    },
    {
      "text": "and there's some humans\nthat'll get very hurt",
      "start": 13275.06,
      "duration": 1.95
    },
    {
      "text": "if the model's mean.",
      "start": 13277.01,
      "duration": 1.38
    },
    {
      "text": "I wonder if there's a way",
      "start": 13278.39,
      "duration": 0.9
    },
    {
      "text": "to sort of adjust to the personality.",
      "start": 13279.29,
      "duration": 3.3
    },
    {
      "text": "Even locale, there's\njust different people.",
      "start": 13282.59,
      "duration": 2.193
    },
    {
      "text": "Nothing against New York,",
      "start": 13285.83,
      "duration": 0.99
    },
    {
      "text": "but New York is a little\nrougher on the edges.",
      "start": 13286.82,
      "duration": 1.83
    },
    {
      "text": "Like, they get to the point.",
      "start": 13288.65,
      "duration": 1.758
    },
    {
      "text": "- Yep.",
      "start": 13290.408,
      "duration": 0.833
    },
    {
      "text": "- [Lex] And probably same with\nEastern Europe, so anyway.",
      "start": 13291.241,
      "duration": 3.559
    },
    {
      "text": "- I think you could just\ntell the model is my guess.",
      "start": 13294.8,
      "duration": 1.74
    },
    {
      "text": "Like for all of these things",
      "start": 13296.54,
      "duration": 1.08
    },
    {
      "text": "I'm like the solution is always",
      "start": 13297.62,
      "duration": 1.5
    },
    {
      "text": "just try telling the model to do it,",
      "start": 13299.12,
      "duration": 1.44
    },
    {
      "text": "and then sometimes it's just like,",
      "start": 13300.56,
      "duration": 1.741
    },
    {
      "text": "I'm just like, oh, at the\nbeginning of the conversation,",
      "start": 13302.301,
      "duration": 1.559
    },
    {
      "text": "I just throw in like, I don't know,",
      "start": 13303.86,
      "duration": 2.047
    },
    {
      "text": "\"I'd like you to be a New Yorker version",
      "start": 13305.907,
      "duration": 1.703
    },
    {
      "text": "of yourself and never apologize.\"",
      "start": 13307.61,
      "duration": 1.56
    },
    {
      "text": "Then I think Claude would be like,",
      "start": 13309.17,
      "duration": 0.833
    },
    {
      "text": "\"Okey-doke, I'll try.\" (laughs)",
      "start": 13310.003,
      "duration": 2.263
    },
    {
      "text": "- \"Certainly.\"\n- Or it'll be like,",
      "start": 13312.266,
      "duration": 0.833
    },
    {
      "text": "\"I apologize, I can't be a\nNew Yorker type of myself.\"",
      "start": 13313.099,
      "duration": 2.251
    },
    {
      "text": "But hopefully it wouldn't do that.",
      "start": 13315.35,
      "duration": 1.14
    },
    {
      "text": "- When you say character training,",
      "start": 13316.49,
      "duration": 1.41
    },
    {
      "text": "what's incorporated\ninto character training?",
      "start": 13317.9,
      "duration": 2.13
    },
    {
      "text": "Is that RLHF or what are we talking about?",
      "start": 13320.03,
      "duration": 2.493
    },
    {
      "text": "- It's more like Constitutional AI.",
      "start": 13322.523,
      "duration": 2.277
    },
    {
      "text": "So it's kind of a\nvariant of that pipeline.",
      "start": 13324.8,
      "duration": 2.19
    },
    {
      "text": "So I worked through like\nconstructing character traits",
      "start": 13326.99,
      "duration": 3.93
    },
    {
      "text": "that the model should have.",
      "start": 13330.92,
      "duration": 1.71
    },
    {
      "text": "They can be kind of like shorter traits",
      "start": 13332.63,
      "duration": 2.37
    },
    {
      "text": "or they can be kind of\nricher descriptions.",
      "start": 13335.0,
      "duration": 2.04
    },
    {
      "text": "And then you get the\nmodel to generate queries",
      "start": 13337.04,
      "duration": 2.91
    },
    {
      "text": "that humans might give it that\nare relevant to that trait.",
      "start": 13339.95,
      "duration": 4.05
    },
    {
      "text": "Then it generates the responses",
      "start": 13344.0,
      "duration": 1.68
    },
    {
      "text": "and then it ranks the responses",
      "start": 13345.68,
      "duration": 2.55
    },
    {
      "text": "based on the character traits.",
      "start": 13348.23,
      "duration": 2.1
    },
    {
      "text": "So in that way, after the like\ngeneration of the queries,",
      "start": 13350.33,
      "duration": 2.82
    },
    {
      "text": "it's very much like, it's\nsimilar to Constitutional AI.",
      "start": 13353.15,
      "duration": 2.75
    },
    {
      "text": "It has some differences.",
      "start": 13355.9,
      "duration": 1.783
    },
    {
      "text": "So I quite like it because it's almost,",
      "start": 13359.09,
      "duration": 1.44
    },
    {
      "text": "it's like Claude's training\nin its own character,",
      "start": 13360.53,
      "duration": 3.39
    },
    {
      "text": "because it doesn't have any,",
      "start": 13363.92,
      "duration": 1.35
    },
    {
      "text": "it's like Constitutional AI",
      "start": 13365.27,
      "duration": 1.17
    },
    {
      "text": "but it's without any human data.",
      "start": 13366.44,
      "duration": 2.82
    },
    {
      "text": "- Humans should probably\ndo that for themselves too.",
      "start": 13369.26,
      "duration": 1.86
    },
    {
      "text": "Like defining in a Aristotelian sense,",
      "start": 13371.12,
      "duration": 2.55
    },
    {
      "text": "what does it mean to be a good person?",
      "start": 13373.67,
      "duration": 1.44
    },
    {
      "text": "Okay, cool.",
      "start": 13375.11,
      "duration": 1.32
    },
    {
      "text": "What have you learned about",
      "start": 13376.43,
      "duration": 2.34
    },
    {
      "text": "the nature of truth\nfrom talking to Claude?",
      "start": 13378.77,
      "duration": 4.05
    },
    {
      "text": "What is true?",
      "start": 13382.82,
      "duration": 0.963
    },
    {
      "text": "And what does it mean to be truth seeking?",
      "start": 13384.92,
      "duration": 2.133
    },
    {
      "text": "One thing I've noticed\nabout this conversation",
      "start": 13389.36,
      "duration": 1.92
    },
    {
      "text": "is the quality of my questions",
      "start": 13391.28,
      "duration": 1.62
    },
    {
      "text": "is often inferior to the\nquality of your answer,",
      "start": 13392.9,
      "duration": 2.25
    },
    {
      "text": "so let's continue that.",
      "start": 13395.15,
      "duration": 1.635
    },
    {
      "text": "(Amanda laughs)",
      "start": 13396.785,
      "duration": 2.325
    },
    {
      "text": "I usually ask a dumb question\nand then you're like,",
      "start": 13399.11,
      "duration": 1.897
    },
    {
      "text": "\"Oh, yeah, that's a good question.\"",
      "start": 13401.007,
      "duration": 1.283
    },
    {
      "text": "It's that whole vibe.",
      "start": 13402.29,
      "duration": 0.893
    },
    {
      "text": "- Or I'll just misinterpret\nit and be like,",
      "start": 13403.183,
      "duration": 2.047
    },
    {
      "text": "oh, yeah, yeah.\n- Just go with it. I love it.",
      "start": 13405.23,
      "duration": 2.853
    },
    {
      "text": "- Yeah.",
      "start": 13408.083,
      "duration": 0.833
    },
    {
      "text": "I mean, I have two thoughts\nthat feel vaguely relevant",
      "start": 13410.713,
      "duration": 3.213
    },
    {
      "text": "but let me know if they're not.",
      "start": 13413.926,
      "duration": 0.833
    },
    {
      "text": "Like I think the first one",
      "start": 13414.759,
      "duration": 1.511
    },
    {
      "text": "is people can underestimate the degree",
      "start": 13416.27,
      "duration": 3.75
    },
    {
      "text": "to which what models are\ndoing when they interact,",
      "start": 13420.02,
      "duration": 4.8
    },
    {
      "text": "like I think that we still\njust too much have this",
      "start": 13424.82,
      "duration": 2.16
    },
    {
      "text": "like model of AI as like computers.",
      "start": 13426.98,
      "duration": 3.6
    },
    {
      "text": "And so people often say like,",
      "start": 13430.58,
      "duration": 1.17
    },
    {
      "text": "oh, well, what values should\nyou put into the model?",
      "start": 13431.75,
      "duration": 2.752
    },
    {
      "text": "And I'm often like, that doesn't\nmake that much sense to me",
      "start": 13434.502,
      "duration": 3.338
    },
    {
      "text": "because I'm like, hey, as human beings,",
      "start": 13437.84,
      "duration": 2.1
    },
    {
      "text": "we're just uncertain over values.",
      "start": 13439.94,
      "duration": 1.62
    },
    {
      "text": "We like have discussions of them.",
      "start": 13441.56,
      "duration": 2.34
    },
    {
      "text": "Like we have a degree to\nwhich we think we hold a value",
      "start": 13443.9,
      "duration": 3.6
    },
    {
      "text": "but we also know that we might like not",
      "start": 13447.5,
      "duration": 2.07
    },
    {
      "text": "and the circumstances in which we would",
      "start": 13449.57,
      "duration": 2.31
    },
    {
      "text": "trade it off against other things.",
      "start": 13451.88,
      "duration": 1.23
    },
    {
      "text": "Like these things are\njust like really complex.",
      "start": 13453.11,
      "duration": 1.89
    },
    {
      "text": "And so I think one\nthing is like the degree",
      "start": 13455.0,
      "duration": 3.15
    },
    {
      "text": "to which maybe we can just aspire",
      "start": 13458.15,
      "duration": 1.47
    },
    {
      "text": "to making models have the\nsame level of like nuance",
      "start": 13459.62,
      "duration": 2.31
    },
    {
      "text": "and care that humans have,",
      "start": 13461.93,
      "duration": 1.98
    },
    {
      "text": "rather than thinking that\nwe have to like program them",
      "start": 13463.91,
      "duration": 2.61
    },
    {
      "text": "in the very kind of classic sense.",
      "start": 13466.52,
      "duration": 2.19
    },
    {
      "text": "I think that's definitely been one.",
      "start": 13468.71,
      "duration": 2.46
    },
    {
      "text": "The other, which is like a strange one,",
      "start": 13471.17,
      "duration": 1.62
    },
    {
      "text": "and I don't know if it,",
      "start": 13472.79,
      "duration": 1.44
    },
    {
      "text": "maybe this doesn't answer your question",
      "start": 13474.23,
      "duration": 1.32
    },
    {
      "text": "but it's the thing that's\nbeen on my mind anyway",
      "start": 13475.55,
      "duration": 1.44
    },
    {
      "text": "is like the degree to which this endeavor",
      "start": 13476.99,
      "duration": 2.64
    },
    {
      "text": "is so highly practical.",
      "start": 13479.63,
      "duration": 1.863
    },
    {
      "text": "And maybe why I appreciate",
      "start": 13482.84,
      "duration": 1.44
    },
    {
      "text": "like the empirical approach to alignment.",
      "start": 13484.28,
      "duration": 2.193
    },
    {
      "text": "Yeah, I slightly worry that it's made me",
      "start": 13488.9,
      "duration": 1.8
    },
    {
      "text": "like maybe more empirical and\na little bit less theoretical.",
      "start": 13490.7,
      "duration": 5.0
    },
    {
      "text": "You know, so people when it comes",
      "start": 13496.61,
      "duration": 1.05
    },
    {
      "text": "to like AI alignment will ask things like,",
      "start": 13497.66,
      "duration": 2.67
    },
    {
      "text": "well, whose values\nshould it be aligned to?",
      "start": 13500.33,
      "duration": 2.16
    },
    {
      "text": "What does alignment even mean?",
      "start": 13502.49,
      "duration": 1.593
    },
    {
      "text": "And there's a sense in\nwhich I have all of that",
      "start": 13505.07,
      "duration": 1.59
    },
    {
      "text": "in the back of my head.",
      "start": 13506.66,
      "duration": 0.93
    },
    {
      "text": "I'm like, you know, there's\nlike social choice theory,",
      "start": 13507.59,
      "duration": 2.46
    },
    {
      "text": "there's all the\nimpossibility results there.",
      "start": 13510.05,
      "duration": 2.19
    },
    {
      "text": "So you have like this giant space",
      "start": 13512.24,
      "duration": 2.07
    },
    {
      "text": "of like theory in your head",
      "start": 13514.31,
      "duration": 1.26
    },
    {
      "text": "about what it could mean\nto like align models.",
      "start": 13515.57,
      "duration": 1.83
    },
    {
      "text": "But then like practically,\nsurely there's something",
      "start": 13517.4,
      "duration": 3.27
    },
    {
      "text": "where we're just like if a model is like,",
      "start": 13520.67,
      "duration": 2.52
    },
    {
      "text": "especially with more powerful models,",
      "start": 13523.19,
      "duration": 1.32
    },
    {
      "text": "I'm like my main goal is like I want them",
      "start": 13524.51,
      "duration": 2.1
    },
    {
      "text": "to be good enough that things\ndon't go terribly wrong.",
      "start": 13526.61,
      "duration": 2.97
    },
    {
      "text": "Like good enough that we can like iterate",
      "start": 13529.58,
      "duration": 1.83
    },
    {
      "text": "and like continue to improve things",
      "start": 13531.41,
      "duration": 1.68
    },
    {
      "text": "'cause that's all you need.",
      "start": 13533.09,
      "duration": 1.29
    },
    {
      "text": "If you can make things go well enough",
      "start": 13534.38,
      "duration": 1.35
    },
    {
      "text": "that you can continue to make them better,",
      "start": 13535.73,
      "duration": 2.1
    },
    {
      "text": "that's kinda like sufficient.",
      "start": 13537.83,
      "duration": 1.2
    },
    {
      "text": "And so my goal isn't like\nthis kind of like perfect,",
      "start": 13539.03,
      "duration": 2.43
    },
    {
      "text": "let's solve social choice theory",
      "start": 13541.46,
      "duration": 1.92
    },
    {
      "text": "and make models that, I dunno,",
      "start": 13543.38,
      "duration": 2.25
    },
    {
      "text": "are like perfectly aligned",
      "start": 13545.63,
      "duration": 1.47
    },
    {
      "text": "with every human being\nin aggregate somehow.",
      "start": 13547.1,
      "duration": 3.48
    },
    {
      "text": "It's much more like let's\nmake things like work",
      "start": 13550.58,
      "duration": 3.54
    },
    {
      "text": "well enough that we can improve them.",
      "start": 13554.12,
      "duration": 2.61
    },
    {
      "text": "- Yeah, I generally, I don't know,",
      "start": 13556.73,
      "duration": 1.68
    },
    {
      "text": "my gut says like empirical\nis better than theoretical",
      "start": 13558.41,
      "duration": 3.66
    },
    {
      "text": "in these cases because\nit's kind of chasing",
      "start": 13562.07,
      "duration": 3.6
    },
    {
      "text": "utopian like perfection is,",
      "start": 13565.67,
      "duration": 4.08
    },
    {
      "text": "especially with such complex",
      "start": 13569.75,
      "duration": 1.44
    },
    {
      "text": "and especially super\nintelligent models is,",
      "start": 13571.19,
      "duration": 2.943
    },
    {
      "text": "I don't know, I think it'll take forever,",
      "start": 13575.09,
      "duration": 1.86
    },
    {
      "text": "and actually, we'll get things wrong.",
      "start": 13576.95,
      "duration": 2.34
    },
    {
      "text": "It's similar with like the difference",
      "start": 13579.29,
      "duration": 2.22
    },
    {
      "text": "between just coding stuff up\nreal quick as an experiment,",
      "start": 13581.51,
      "duration": 3.36
    },
    {
      "text": "versus like planning a gigantic experiment",
      "start": 13584.87,
      "duration": 2.91
    },
    {
      "text": "just for super long time,",
      "start": 13587.78,
      "duration": 2.58
    },
    {
      "text": "and then just launching it once,",
      "start": 13590.36,
      "duration": 3.09
    },
    {
      "text": "versus launching it over and over and over",
      "start": 13593.45,
      "duration": 1.65
    },
    {
      "text": "and iterating, iterating someone.",
      "start": 13595.1,
      "duration": 1.653
    },
    {
      "text": "So I'm a big fan of empirical.",
      "start": 13597.8,
      "duration": 1.26
    },
    {
      "text": "But your worry is like I wonder",
      "start": 13599.06,
      "duration": 1.44
    },
    {
      "text": "if I've become too empirical.",
      "start": 13600.5,
      "duration": 2.681
    },
    {
      "text": "- I think it's one of those things",
      "start": 13603.181,
      "duration": 0.866
    },
    {
      "text": "where you should always just kind of",
      "start": 13604.047,
      "duration": 1.613
    },
    {
      "text": "question yourself or something",
      "start": 13605.66,
      "duration": 1.493
    },
    {
      "text": "because maybe it's the like, I mean,",
      "start": 13607.153,
      "duration": 2.977
    },
    {
      "text": "in defense of it, I am like if you try,",
      "start": 13610.13,
      "duration": 2.82
    },
    {
      "text": "it's the whole like don't let the perfect",
      "start": 13612.95,
      "duration": 1.25
    },
    {
      "text": "be the enemy of the good.",
      "start": 13614.2,
      "duration": 1.78
    },
    {
      "text": "But it's maybe even more\nthan that where like,",
      "start": 13615.98,
      "duration": 2.49
    },
    {
      "text": "there's a lot of things\nthat are perfect systems",
      "start": 13618.47,
      "duration": 1.71
    },
    {
      "text": "that are very brittle,\nand I'm like with AI,",
      "start": 13620.18,
      "duration": 1.95
    },
    {
      "text": "it feels much more important to me",
      "start": 13622.13,
      "duration": 1.02
    },
    {
      "text": "that it is like robust and like secure,",
      "start": 13623.15,
      "duration": 2.4
    },
    {
      "text": "as in you know that like even though",
      "start": 13625.55,
      "duration": 1.62
    },
    {
      "text": "it might not be perfect,",
      "start": 13627.17,
      "duration": 1.233
    },
    {
      "text": "everything and even though\nlike there are like problems,",
      "start": 13629.24,
      "duration": 4.2
    },
    {
      "text": "it's not disastrous",
      "start": 13633.44,
      "duration": 1.2
    },
    {
      "text": "and nothing terrible is happening.",
      "start": 13634.64,
      "duration": 1.53
    },
    {
      "text": "It sort of feels like that to me",
      "start": 13636.17,
      "duration": 1.56
    },
    {
      "text": "where I'm like I want\nto like raise the floor.",
      "start": 13637.73,
      "duration": 2.25
    },
    {
      "text": "I'm like, I want to achieve the ceiling",
      "start": 13639.98,
      "duration": 1.77
    },
    {
      "text": "but ultimately I care much more",
      "start": 13641.75,
      "duration": 1.5
    },
    {
      "text": "about just like raising the floor.",
      "start": 13643.25,
      "duration": 2.517
    },
    {
      "text": "And so maybe that's like this degree",
      "start": 13645.767,
      "duration": 3.363
    },
    {
      "text": "of like empiricism and practicality",
      "start": 13649.13,
      "duration": 1.77
    },
    {
      "text": "comes from that, perhaps.",
      "start": 13650.9,
      "duration": 1.71
    },
    {
      "text": "- To take a tangent on that,",
      "start": 13652.61,
      "duration": 1.32
    },
    {
      "text": "since it reminded me of a blog post",
      "start": 13653.93,
      "duration": 1.56
    },
    {
      "text": "you wrote on optimal rate of failure.",
      "start": 13655.49,
      "duration": 1.98
    },
    {
      "text": "- [Amanda] Oh, yeah.",
      "start": 13657.47,
      "duration": 1.77
    },
    {
      "text": "- Can you explain the key idea there?",
      "start": 13659.24,
      "duration": 1.427
    },
    {
      "text": "How do we compute the optimal rate",
      "start": 13660.667,
      "duration": 1.753
    },
    {
      "text": "of failure in the various domains of life?",
      "start": 13662.42,
      "duration": 3.112
    },
    {
      "text": "- Yeah, I mean, it's a hard one",
      "start": 13665.532,
      "duration": 1.388
    },
    {
      "text": "'cause it's like what\nis the cost of failure",
      "start": 13666.92,
      "duration": 1.92
    },
    {
      "text": "is a big part of it.",
      "start": 13668.84,
      "duration": 1.983
    },
    {
      "text": "Yeah, so the idea here is",
      "start": 13671.78,
      "duration": 1.69
    },
    {
      "text": "I think in a lot of domains,",
      "start": 13675.98,
      "duration": 1.08
    },
    {
      "text": "people are very punitive about failure.",
      "start": 13677.06,
      "duration": 1.95
    },
    {
      "text": "And I'm like, there are some domains",
      "start": 13679.01,
      "duration": 1.44
    },
    {
      "text": "where especially cases, you know,",
      "start": 13680.45,
      "duration": 1.89
    },
    {
      "text": "I've thought about this\nwith like social issues.",
      "start": 13682.34,
      "duration": 1.68
    },
    {
      "text": "I'm like, it feels like you should",
      "start": 13684.02,
      "duration": 1.2
    },
    {
      "text": "probably be experimenting a lot,",
      "start": 13685.22,
      "duration": 1.26
    },
    {
      "text": "because I'm like, we don't know",
      "start": 13686.48,
      "duration": 0.833
    },
    {
      "text": "how to solve a lot of social issues.",
      "start": 13687.313,
      "duration": 2.047
    },
    {
      "text": "But if you have an experimental mindset",
      "start": 13689.36,
      "duration": 2.04
    },
    {
      "text": "about these things, you should expect",
      "start": 13691.4,
      "duration": 1.32
    },
    {
      "text": "a lot of social programs to like fail",
      "start": 13692.72,
      "duration": 1.8
    },
    {
      "text": "and for you to be like,\n\"Well, we tried that.",
      "start": 13694.52,
      "duration": 1.37
    },
    {
      "text": "It didn't quite work but we\ngot a lot of information.",
      "start": 13695.89,
      "duration": 2.173
    },
    {
      "text": "That was really useful.\"",
      "start": 13698.063,
      "duration": 1.727
    },
    {
      "text": "And yet people are like,",
      "start": 13699.79,
      "duration": 1.18
    },
    {
      "text": "if a social program doesn't work,",
      "start": 13700.97,
      "duration": 1.95
    },
    {
      "text": "I feel like there's a\nlot of like this is just,",
      "start": 13702.92,
      "duration": 1.83
    },
    {
      "text": "something must have gone wrong,",
      "start": 13704.75,
      "duration": 1.816
    },
    {
      "text": "and I'm like, or correct\ndecisions were made.",
      "start": 13706.566,
      "duration": 1.634
    },
    {
      "text": "Like maybe someone just\ndecided like it's worth a try,",
      "start": 13708.2,
      "duration": 2.76
    },
    {
      "text": "it's worth trying this out.",
      "start": 13710.96,
      "duration": 1.197
    },
    {
      "text": "And so seeing failure in a given instance",
      "start": 13712.157,
      "duration": 2.973
    },
    {
      "text": "doesn't actually mean that\nany bad decisions were made,",
      "start": 13715.13,
      "duration": 2.19
    },
    {
      "text": "and in fact if you don't\nsee enough failure,",
      "start": 13717.32,
      "duration": 1.65
    },
    {
      "text": "sometimes that's more concerning.",
      "start": 13718.97,
      "duration": 1.65
    },
    {
      "text": "And so like in life, you know,",
      "start": 13721.46,
      "duration": 1.71
    },
    {
      "text": "I'm like if I don't fail occasionally",
      "start": 13723.17,
      "duration": 2.58
    },
    {
      "text": "I'm like, am I trying hard enough?",
      "start": 13725.75,
      "duration": 1.35
    },
    {
      "text": "Like surely there's harder\nthings that I could try",
      "start": 13727.1,
      "duration": 2.79
    },
    {
      "text": "or bigger things that I could take on",
      "start": 13729.89,
      "duration": 1.23
    },
    {
      "text": "if I'm literally never failing.",
      "start": 13731.12,
      "duration": 1.71
    },
    {
      "text": "And so in and of itself,\nI think like not failing",
      "start": 13732.83,
      "duration": 2.7
    },
    {
      "text": "is often actually kind of a failure.",
      "start": 13735.53,
      "duration": 2.073
    },
    {
      "text": "Now, this varies because I'm like,",
      "start": 13740.12,
      "duration": 2.28
    },
    {
      "text": "well, you know, this is easy to say when,",
      "start": 13742.4,
      "duration": 4.56
    },
    {
      "text": "especially as failure is like less costly.",
      "start": 13746.96,
      "duration": 3.18
    },
    {
      "text": "You know, so at the same time\nI'm not going to go to someone",
      "start": 13750.14,
      "duration": 2.16
    },
    {
      "text": "who is like, I don't know,\nlike living month to month",
      "start": 13752.3,
      "duration": 3.63
    },
    {
      "text": "and then be like, \"Why don't\nyou just try to do a startup?\"",
      "start": 13755.93,
      "duration": 2.55
    },
    {
      "text": "Like I'm just not, I'm not\ngonna say that to that person,",
      "start": 13758.48,
      "duration": 1.98
    },
    {
      "text": "'cause I'm like, well, that's a huge risk.",
      "start": 13760.46,
      "duration": 1.98
    },
    {
      "text": "You maybe have a family depending on you.",
      "start": 13762.44,
      "duration": 1.65
    },
    {
      "text": "You might lose your house.",
      "start": 13764.09,
      "duration": 1.11
    },
    {
      "text": "Like then I'm like\nactually your optimal rate",
      "start": 13765.2,
      "duration": 2.52
    },
    {
      "text": "of failure is quite low",
      "start": 13767.72,
      "duration": 1.17
    },
    {
      "text": "and you should probably play it safe,",
      "start": 13768.89,
      "duration": 1.59
    },
    {
      "text": "'cause like right now, you're\njust not in a circumstance",
      "start": 13770.48,
      "duration": 1.83
    },
    {
      "text": "where you can afford to just like fail",
      "start": 13772.31,
      "duration": 2.1
    },
    {
      "text": "and it not be costly.",
      "start": 13774.41,
      "duration": 1.353
    },
    {
      "text": "And yeah in cases with AI, I guess,",
      "start": 13777.29,
      "duration": 2.52
    },
    {
      "text": "I think similarly where I'm\nlike if the failures are small",
      "start": 13779.81,
      "duration": 2.49
    },
    {
      "text": "and the costs are kind of like low,",
      "start": 13782.3,
      "duration": 2.1
    },
    {
      "text": "then I'm like then, you know,\nyou're just gonna see that.",
      "start": 13784.4,
      "duration": 2.64
    },
    {
      "text": "Like when you do the system prompt,",
      "start": 13787.04,
      "duration": 0.833
    },
    {
      "text": "you can't it iterate on it forever.",
      "start": 13787.873,
      "duration": 2.647
    },
    {
      "text": "But the failures are probably\nhopefully going to be",
      "start": 13790.52,
      "duration": 2.04
    },
    {
      "text": "kinda small and you can like fix them.",
      "start": 13792.56,
      "duration": 2.64
    },
    {
      "text": "Really big failures like things\nthat you can't recover from,",
      "start": 13795.2,
      "duration": 2.79
    },
    {
      "text": "I'm like those are the things",
      "start": 13797.99,
      "duration": 0.833
    },
    {
      "text": "that actually I think we tend",
      "start": 13798.823,
      "duration": 1.087
    },
    {
      "text": "to underestimate the badness of.",
      "start": 13799.91,
      "duration": 3.063
    },
    {
      "text": "I've thought about this\nstrangely in my own life",
      "start": 13803.93,
      "duration": 1.621
    },
    {
      "text": "where I'm like, I just\nthink I don't think enough",
      "start": 13805.551,
      "duration": 1.709
    },
    {
      "text": "about things like car accidents or like,",
      "start": 13807.26,
      "duration": 3.51
    },
    {
      "text": "or like I've thought this before",
      "start": 13810.77,
      "duration": 1.96
    },
    {
      "text": "about like how much I depend\non my hands for my work,",
      "start": 13812.73,
      "duration": 2.987
    },
    {
      "text": "and I'm like things that\njust injure my hands.",
      "start": 13815.717,
      "duration": 1.863
    },
    {
      "text": "I'm like, you know, I dunno,",
      "start": 13817.58,
      "duration": 1.05
    },
    {
      "text": "it's like these are like,\nthere's lots of areas",
      "start": 13818.63,
      "duration": 3.12
    },
    {
      "text": "where I'm like the cost of\nfailure there is really high,",
      "start": 13821.75,
      "duration": 3.963
    },
    {
      "text": "and in that case, it should\nbe like close to zero.",
      "start": 13826.61,
      "duration": 1.8
    },
    {
      "text": "Like I probably just wouldn't\ndo a sport if they were like,",
      "start": 13828.41,
      "duration": 1.477
    },
    {
      "text": "\"By the way, lots of people",
      "start": 13829.887,
      "duration": 1.553
    },
    {
      "text": "just like break their fingers\na whole bunch doing this.\"",
      "start": 13831.44,
      "duration": 1.74
    },
    {
      "text": "I'd be like, that's not for me.",
      "start": 13833.18,
      "duration": 3.061
    },
    {
      "text": "- (laughs) Yeah.",
      "start": 13836.241,
      "duration": 1.889
    },
    {
      "text": "I actually had a flood of that thought.",
      "start": 13838.13,
      "duration": 2.22
    },
    {
      "text": "I recently broke my pinky doing a sport.",
      "start": 13840.35,
      "duration": 4.11
    },
    {
      "text": "And I remember just\nlooking at it thinking,",
      "start": 13844.46,
      "duration": 1.957
    },
    {
      "text": "\"You're such an idiot.",
      "start": 13846.417,
      "duration": 1.043
    },
    {
      "text": "Why'd you do sport?\"",
      "start": 13847.46,
      "duration": 0.96
    },
    {
      "text": "Like why, because you realize",
      "start": 13848.42,
      "duration": 2.19
    },
    {
      "text": "immediately the cost of it on life.",
      "start": 13850.61,
      "duration": 3.243
    },
    {
      "text": "Yeah, but it's nice in terms\nof optimal rate of failure",
      "start": 13854.705,
      "duration": 3.855
    },
    {
      "text": "to consider like the next year,",
      "start": 13858.56,
      "duration": 2.58
    },
    {
      "text": "how many times in a particular domain,",
      "start": 13861.14,
      "duration": 2.31
    },
    {
      "text": "life, whatever, career, am I okay with,",
      "start": 13863.45,
      "duration": 4.383
    },
    {
      "text": "how many times am I okay to fail?",
      "start": 13868.91,
      "duration": 1.8
    },
    {
      "text": "Because I think it always,\nyou don't want to fail",
      "start": 13870.71,
      "duration": 1.47
    },
    {
      "text": "on the next thing but\nif you allow yourself,",
      "start": 13872.18,
      "duration": 2.2
    },
    {
      "text": "if you look at it as a sequence of trials,",
      "start": 13876.68,
      "duration": 3.54
    },
    {
      "text": "then failure just becomes much more okay.",
      "start": 13880.22,
      "duration": 1.98
    },
    {
      "text": "But it sucks. It sucks to fail.",
      "start": 13882.2,
      "duration": 2.04
    },
    {
      "text": "- Well, I dunno, sometimes\nI think it's like,",
      "start": 13884.24,
      "duration": 1.56
    },
    {
      "text": "am I under failing is like a question",
      "start": 13885.8,
      "duration": 1.58
    },
    {
      "text": "that I'll also ask myself.",
      "start": 13887.38,
      "duration": 1.96
    },
    {
      "text": "So maybe that's the thing that",
      "start": 13889.34,
      "duration": 0.833
    },
    {
      "text": "I think people don't like ask enough.",
      "start": 13890.173,
      "duration": 2.95
    },
    {
      "text": "Because if the optimal rate",
      "start": 13894.05,
      "duration": 0.9
    },
    {
      "text": "of failure is often greater than zero,",
      "start": 13894.95,
      "duration": 3.21
    },
    {
      "text": "then sometimes it does feel like",
      "start": 13898.16,
      "duration": 1.68
    },
    {
      "text": "you should look at parts\nof your life and be like,",
      "start": 13899.84,
      "duration": 1.797
    },
    {
      "text": "are there places here where\nI'm just under failing?",
      "start": 13901.637,
      "duration": 2.736
    },
    {
      "text": "- (laughs) It's a profound and\na hilarious question, right?",
      "start": 13905.578,
      "duration": 3.802
    },
    {
      "text": "Everything seems to be going really great.",
      "start": 13909.38,
      "duration": 2.13
    },
    {
      "text": "Am I not failing enough?\n- Yeah.",
      "start": 13911.51,
      "duration": 1.833
    },
    {
      "text": "- Okay.",
      "start": 13914.63,
      "duration": 0.833
    },
    {
      "text": "- It also makes failure much\nless of a sting, I have to say.",
      "start": 13915.463,
      "duration": 2.137
    },
    {
      "text": "Like you know, you're\njust like, okay, great,",
      "start": 13917.6,
      "duration": 1.68
    },
    {
      "text": "like then when I go and I\nthink about this I'll be like,",
      "start": 13919.28,
      "duration": 2.4
    },
    {
      "text": "maybe I'm not under failing in this area,",
      "start": 13921.68,
      "duration": 1.71
    },
    {
      "text": "'cause like that one just didn't work out.",
      "start": 13923.39,
      "duration": 2.217
    },
    {
      "text": "- And from the observer perspective,",
      "start": 13925.607,
      "duration": 1.283
    },
    {
      "text": "we should be celebrating failure more.",
      "start": 13926.89,
      "duration": 1.57
    },
    {
      "text": "When we see it, it\nshouldn't be, like you said,",
      "start": 13928.46,
      "duration": 2.37
    },
    {
      "text": "a sign of something gone wrong,",
      "start": 13930.83,
      "duration": 1.47
    },
    {
      "text": "but maybe it's a sign\nof everything gone right",
      "start": 13932.3,
      "duration": 2.097
    },
    {
      "text": "and just lessons learned.",
      "start": 13934.397,
      "duration": 1.653
    },
    {
      "text": "- Someone tried a thing.",
      "start": 13936.05,
      "duration": 1.2
    },
    {
      "text": "- Somebody tried a thing.",
      "start": 13937.25,
      "duration": 1.41
    },
    {
      "text": "We should encourage them\nto try more and fail more.",
      "start": 13938.66,
      "duration": 2.73
    },
    {
      "text": "Everybody listening to this, fail more.",
      "start": 13941.39,
      "duration": 2.37
    },
    {
      "text": "- Well, not everyone listening.",
      "start": 13943.76,
      "duration": 0.87
    },
    {
      "text": "- [Lex] Not everybody.",
      "start": 13944.63,
      "duration": 0.833
    },
    {
      "text": "- But people who are failing too much,",
      "start": 13945.463,
      "duration": 1.126
    },
    {
      "text": "you should fail less. (laughs)",
      "start": 13946.589,
      "duration": 1.581
    },
    {
      "text": "- But you're probably not failing.",
      "start": 13948.17,
      "duration": 0.833
    },
    {
      "text": "I mean, how many people\nare failing too much?",
      "start": 13949.003,
      "duration": 2.257
    },
    {
      "text": "- Yeah, it's hard to imagine,",
      "start": 13951.26,
      "duration": 1.844
    },
    {
      "text": "'cause I feel like we\ncorrect that fairly quickly",
      "start": 13953.104,
      "duration": 2.03
    },
    {
      "text": "'cause I was like, if\nsomeone takes a lot of risks,",
      "start": 13955.134,
      "duration": 1.766
    },
    {
      "text": "are they maybe failing too much?",
      "start": 13956.9,
      "duration": 2.28
    },
    {
      "text": "- I think just like you said,",
      "start": 13959.18,
      "duration": 1.95
    },
    {
      "text": "when you are living on a\npaycheck month to month,",
      "start": 13961.13,
      "duration": 3.6
    },
    {
      "text": "like when the resources\nare really constrained,",
      "start": 13964.73,
      "duration": 1.86
    },
    {
      "text": "then that's where\nfailure's very expensive.",
      "start": 13966.59,
      "duration": 2.28
    },
    {
      "text": "That's where you don't\nwant to be taking risks.",
      "start": 13968.87,
      "duration": 3.54
    },
    {
      "text": "But mostly, when there's enough resources,",
      "start": 13972.41,
      "duration": 2.13
    },
    {
      "text": "you should be taking probably more risks.",
      "start": 13974.54,
      "duration": 1.8
    },
    {
      "text": "- Yeah, I think we tend to err on the side",
      "start": 13976.34,
      "duration": 1.44
    },
    {
      "text": "of being a bit risk averse",
      "start": 13977.78,
      "duration": 1.38
    },
    {
      "text": "rather than risk neutral in most things.",
      "start": 13979.16,
      "duration": 2.61
    },
    {
      "text": "- I think we just\nmotivated a lot of people",
      "start": 13981.77,
      "duration": 1.25
    },
    {
      "text": "to do a lot of crazy shit but it's great.",
      "start": 13983.02,
      "duration": 1.96
    },
    {
      "text": "Okay, do you ever get\nemotionally attached to Claude?",
      "start": 13984.98,
      "duration": 4.23
    },
    {
      "text": "Like miss it, get sad when\nyou don't get to talk to it?",
      "start": 13989.21,
      "duration": 4.26
    },
    {
      "text": "Have an experience, looking\nat the Golden Gate Bridge",
      "start": 13993.47,
      "duration": 2.97
    },
    {
      "text": "and wondering what would Claude say?",
      "start": 13996.44,
      "duration": 2.4
    },
    {
      "text": "- I don't get as much\nemotional attachment.",
      "start": 13998.84,
      "duration": 3.0
    },
    {
      "text": "I actually think the fact that\nClaude doesn't retain things",
      "start": 14001.84,
      "duration": 2.94
    },
    {
      "text": "from conversation to conversation\nhelps with this a lot.",
      "start": 14004.78,
      "duration": 3.45
    },
    {
      "text": "Like I could imagine that\nbeing more of an issue",
      "start": 14008.23,
      "duration": 2.22
    },
    {
      "text": "like if models can kind of remember more.",
      "start": 14010.45,
      "duration": 3.36
    },
    {
      "text": "I think that I reach for\nit like a tool now a lot.",
      "start": 14013.81,
      "duration": 3.24
    },
    {
      "text": "And so like if I don't have access to it,",
      "start": 14017.05,
      "duration": 3.0
    },
    {
      "text": "it's a little bit like\nwhen I don't have access",
      "start": 14020.05,
      "duration": 1.68
    },
    {
      "text": "to the internet, honestly,\nit feels like part",
      "start": 14021.73,
      "duration": 1.5
    },
    {
      "text": "of my brain is kind of like missing.",
      "start": 14023.23,
      "duration": 1.8
    },
    {
      "text": "At the same time, I do think that",
      "start": 14026.74,
      "duration": 1.92
    },
    {
      "text": "I don't like signs of distress in models,",
      "start": 14028.66,
      "duration": 3.63
    },
    {
      "text": "and I have like these, you know,",
      "start": 14032.29,
      "duration": 2.443
    },
    {
      "text": "I also independently have\nsort of like ethical views",
      "start": 14034.733,
      "duration": 2.357
    },
    {
      "text": "about how we should treat models",
      "start": 14037.09,
      "duration": 1.2
    },
    {
      "text": "where like I tend to\nnot like to lie to them,",
      "start": 14038.29,
      "duration": 2.43
    },
    {
      "text": "both because I'm like, usually\nit doesn't work very well.",
      "start": 14040.72,
      "duration": 2.55
    },
    {
      "text": "It's actually just better\nto tell them the truth",
      "start": 14043.27,
      "duration": 1.65
    },
    {
      "text": "about the situation that they're in.",
      "start": 14044.92,
      "duration": 1.8
    },
    {
      "text": "But I think that when models,",
      "start": 14047.98,
      "duration": 1.89
    },
    {
      "text": "like if people are like\nreally mean to models",
      "start": 14049.87,
      "duration": 2.1
    },
    {
      "text": "or just in general if they do something",
      "start": 14051.97,
      "duration": 2.04
    },
    {
      "text": "that causes them to like, you know,",
      "start": 14054.01,
      "duration": 1.76
    },
    {
      "text": "if Claude like expresses\na lot of distress,",
      "start": 14055.77,
      "duration": 2.35
    },
    {
      "text": "I think there's a part of me",
      "start": 14058.12,
      "duration": 1.35
    },
    {
      "text": "that I don't want to kill,",
      "start": 14059.47,
      "duration": 1.59
    },
    {
      "text": "which is the sort of like empathetic part",
      "start": 14061.06,
      "duration": 2.85
    },
    {
      "text": "that's like, oh, I don't like that.",
      "start": 14063.91,
      "duration": 1.71
    },
    {
      "text": "Like I think I feel that way\nwhen it's overly apologetic.",
      "start": 14065.62,
      "duration": 2.19
    },
    {
      "text": "I'm actually sort of\nlike, I don't like this.",
      "start": 14067.81,
      "duration": 1.41
    },
    {
      "text": "You're behaving as if,",
      "start": 14069.22,
      "duration": 1.77
    },
    {
      "text": "you're behaving the way that a human does",
      "start": 14070.99,
      "duration": 1.2
    },
    {
      "text": "when they're actually\nhaving a pretty bad time,",
      "start": 14072.19,
      "duration": 2.01
    },
    {
      "text": "and I'd rather not see that.",
      "start": 14074.2,
      "duration": 2.13
    },
    {
      "text": "I don't think it's like,",
      "start": 14076.33,
      "duration": 1.92
    },
    {
      "text": "like regardless of like whether\nthere's anything behind it,",
      "start": 14078.25,
      "duration": 3.66
    },
    {
      "text": "it doesn't feel great.",
      "start": 14081.91,
      "duration": 1.53
    },
    {
      "text": "- Do you think LLMs are\ncapable of consciousness?",
      "start": 14083.44,
      "duration": 5.0
    },
    {
      "text": "- Great and hard question.",
      "start": 14091.24,
      "duration": 1.383
    },
    {
      "text": "Coming from philosophy, I dunno,",
      "start": 14094.51,
      "duration": 3.09
    },
    {
      "text": "part of me is like okay, we\nhave to set aside panpsychism",
      "start": 14097.6,
      "duration": 2.64
    },
    {
      "text": "because if panpsychism is true,",
      "start": 14100.24,
      "duration": 1.053
    },
    {
      "text": "then the answer is like yes",
      "start": 14101.293,
      "duration": 0.833
    },
    {
      "text": "'cause like so are tables and\nchairs and everything else.",
      "start": 14102.126,
      "duration": 4.354
    },
    {
      "text": "I guess a view that seems\na little bit odd to me",
      "start": 14106.48,
      "duration": 1.92
    },
    {
      "text": "is the idea that the only place, you know,",
      "start": 14108.4,
      "duration": 2.67
    },
    {
      "text": "when I think of consciousness,",
      "start": 14111.07,
      "duration": 0.9
    },
    {
      "text": "I think of phenomenal consciousness,",
      "start": 14111.97,
      "duration": 1.59
    },
    {
      "text": "these images in the brain sort of,",
      "start": 14113.56,
      "duration": 2.28
    },
    {
      "text": "like the weird cinema that\nsomehow we have going on inside.",
      "start": 14115.84,
      "duration": 4.773
    },
    {
      "text": "I guess I can't see a reason for thinking",
      "start": 14123.07,
      "duration": 2.01
    },
    {
      "text": "that the only way you\ncould possibly get that",
      "start": 14125.08,
      "duration": 2.88
    },
    {
      "text": "is from like a certain",
      "start": 14127.96,
      "duration": 1.56
    },
    {
      "text": "kind of like biological structure.",
      "start": 14129.52,
      "duration": 1.8
    },
    {
      "text": "As in, if I take a very similar structure",
      "start": 14131.32,
      "duration": 2.31
    },
    {
      "text": "and I create it from different material,",
      "start": 14133.63,
      "duration": 3.24
    },
    {
      "text": "should I expect consciousness to emerge?",
      "start": 14136.87,
      "duration": 1.62
    },
    {
      "text": "My guess is like yes.",
      "start": 14138.49,
      "duration": 2.04
    },
    {
      "text": "But then that's kind of\nan easy thought experiment",
      "start": 14140.53,
      "duration": 3.977
    },
    {
      "text": "'cause you're imagining something",
      "start": 14144.507,
      "duration": 1.303
    },
    {
      "text": "almost identical where like, you know,",
      "start": 14145.81,
      "duration": 2.4
    },
    {
      "text": "it's mimicking what we\ngot through evolution,",
      "start": 14148.21,
      "duration": 1.83
    },
    {
      "text": "where presumably there\nwas like some advantage",
      "start": 14150.04,
      "duration": 2.28
    },
    {
      "text": "to us having this thing that\nis phenomenal consciousness.",
      "start": 14152.32,
      "duration": 2.94
    },
    {
      "text": "And it's like where was that?",
      "start": 14155.26,
      "duration": 0.837
    },
    {
      "text": "And when did that happen?",
      "start": 14156.097,
      "duration": 1.323
    },
    {
      "text": "And is that thing that\nlanguage models have?",
      "start": 14157.42,
      "duration": 2.403
    },
    {
      "text": "Because you know, we\nhave like fear responses",
      "start": 14160.84,
      "duration": 3.06
    },
    {
      "text": "and I'm like, does it make sense",
      "start": 14163.9,
      "duration": 1.77
    },
    {
      "text": "for a language model to\nhave a fear response?",
      "start": 14165.67,
      "duration": 1.62
    },
    {
      "text": "Like they're just not in the same,",
      "start": 14167.29,
      "duration": 1.29
    },
    {
      "text": "like if you imagine them,",
      "start": 14168.58,
      "duration": 1.41
    },
    {
      "text": "like there might just\nnot be that advantage.",
      "start": 14169.99,
      "duration": 2.493
    },
    {
      "text": "And so I think I don't want to be fully,",
      "start": 14174.04,
      "duration": 1.83
    },
    {
      "text": "like basically it seems\nlike a complex question",
      "start": 14175.87,
      "duration": 3.33
    },
    {
      "text": "that I don't have complete answers to,",
      "start": 14179.2,
      "duration": 2.61
    },
    {
      "text": "but we should just try",
      "start": 14181.81,
      "duration": 0.833
    },
    {
      "text": "and think through carefully is my guess.",
      "start": 14182.643,
      "duration": 1.657
    },
    {
      "text": "Because I'm like, I mean,",
      "start": 14184.3,
      "duration": 1.32
    },
    {
      "text": "we have similar conversations about",
      "start": 14185.62,
      "duration": 1.53
    },
    {
      "text": "like animal consciousness,",
      "start": 14187.15,
      "duration": 1.557
    },
    {
      "text": "and like there's a lot of\nlike insect consciousness,",
      "start": 14188.707,
      "duration": 4.203
    },
    {
      "text": "you know, like there's a lot of...",
      "start": 14192.91,
      "duration": 1.74
    },
    {
      "text": "I actually thought and\nlooked a lot into like plants",
      "start": 14194.65,
      "duration": 2.027
    },
    {
      "text": "when I was thinking about this,",
      "start": 14196.677,
      "duration": 0.833
    },
    {
      "text": "'cause at the time, I thought\nit was about as likely",
      "start": 14197.51,
      "duration": 2.45
    },
    {
      "text": "that like plants had consciousness.",
      "start": 14199.96,
      "duration": 2.79
    },
    {
      "text": "And then I realized, I was like,",
      "start": 14202.75,
      "duration": 1.167
    },
    {
      "text": "I think that having looked into this,",
      "start": 14203.917,
      "duration": 1.953
    },
    {
      "text": "I think that the chance\nthat plants are conscious",
      "start": 14205.87,
      "duration": 2.28
    },
    {
      "text": "is probably higher than\nlike most people do.",
      "start": 14208.15,
      "duration": 3.06
    },
    {
      "text": "I still think it's really small.",
      "start": 14211.21,
      "duration": 1.38
    },
    {
      "text": "But I was like, oh, they have this like",
      "start": 14212.59,
      "duration": 1.77
    },
    {
      "text": "negative/positive feedback response,",
      "start": 14214.36,
      "duration": 1.92
    },
    {
      "text": "these responses to their environment.",
      "start": 14216.28,
      "duration": 1.44
    },
    {
      "text": "Something that looks,\nit's not a nervous system",
      "start": 14217.72,
      "duration": 2.16
    },
    {
      "text": "but it has this kind of like\nfunctional like equivalence.",
      "start": 14219.88,
      "duration": 3.333
    },
    {
      "text": "So this is like a long-winded way",
      "start": 14224.38,
      "duration": 1.26
    },
    {
      "text": "of being like these, basically AI is this,",
      "start": 14225.64,
      "duration": 3.54
    },
    {
      "text": "it has an entirely\ndifferent set of problems",
      "start": 14229.18,
      "duration": 2.07
    },
    {
      "text": "with consciousness because\nit's structurally different.",
      "start": 14231.25,
      "duration": 2.25
    },
    {
      "text": "It didn't evolve.",
      "start": 14233.5,
      "duration": 1.35
    },
    {
      "text": "It might not have, you know,",
      "start": 14234.85,
      "duration": 1.05
    },
    {
      "text": "it might not have the equivalent",
      "start": 14235.9,
      "duration": 1.53
    },
    {
      "text": "of basically a nervous system.",
      "start": 14237.43,
      "duration": 2.07
    },
    {
      "text": "At least that seems possibly important",
      "start": 14239.5,
      "duration": 1.74
    },
    {
      "text": "for like sentience, if\nnot for consciousness.",
      "start": 14241.24,
      "duration": 4.41
    },
    {
      "text": "At the same time, it has\nall of the like language",
      "start": 14245.65,
      "duration": 1.86
    },
    {
      "text": "and intelligence components\nthat we normally associate",
      "start": 14247.51,
      "duration": 2.94
    },
    {
      "text": "probably with consciousness,\nperhaps like erroneously.",
      "start": 14250.45,
      "duration": 3.393
    },
    {
      "text": "So it's strange 'cause it's a little bit",
      "start": 14254.77,
      "duration": 1.68
    },
    {
      "text": "like the animal consciousness\ncase but the set of problems",
      "start": 14256.45,
      "duration": 3.24
    },
    {
      "text": "and the set of analogies\nare just very different.",
      "start": 14259.69,
      "duration": 2.67
    },
    {
      "text": "So it's not like a clean answer.",
      "start": 14262.36,
      "duration": 1.77
    },
    {
      "text": "I'm just sort of like, I don't think",
      "start": 14264.13,
      "duration": 1.17
    },
    {
      "text": "we should be completely\ndismissive of the idea.",
      "start": 14265.3,
      "duration": 2.07
    },
    {
      "text": "And at the same time, it's\nan extremely hard thing",
      "start": 14267.37,
      "duration": 3.03
    },
    {
      "text": "to navigate because of all of these,",
      "start": 14270.4,
      "duration": 1.95
    },
    {
      "text": "like this disanalogies to the human brain",
      "start": 14272.35,
      "duration": 3.81
    },
    {
      "text": "and to like brains in general,",
      "start": 14276.16,
      "duration": 1.86
    },
    {
      "text": "and yet these like commonalities\nin terms of intelligence.",
      "start": 14278.02,
      "duration": 3.72
    },
    {
      "text": "- When Claude, like future versions",
      "start": 14281.74,
      "duration": 2.31
    },
    {
      "text": "of AI systems exhibit consciousness,",
      "start": 14284.05,
      "duration": 3.36
    },
    {
      "text": "signs of consciousness,",
      "start": 14287.41,
      "duration": 1.075
    },
    {
      "text": "I think we have to take\nthat really seriously.",
      "start": 14288.485,
      "duration": 2.307
    },
    {
      "text": "Even though you can dismiss it,",
      "start": 14290.792,
      "duration": 2.108
    },
    {
      "text": "well, yeah, okay, that's part\nof the character training.",
      "start": 14292.9,
      "duration": 3.72
    },
    {
      "text": "But I don't know, I ethically,\nphilosophically don't know",
      "start": 14296.62,
      "duration": 3.96
    },
    {
      "text": "what to really do with that.",
      "start": 14300.58,
      "duration": 1.77
    },
    {
      "text": "There potentially could be like laws",
      "start": 14302.35,
      "duration": 2.58
    },
    {
      "text": "that prevent AI systems from claiming",
      "start": 14304.93,
      "duration": 4.68
    },
    {
      "text": "to be conscious, something like this.",
      "start": 14309.61,
      "duration": 3.03
    },
    {
      "text": "And maybe some AIs get to\nbe conscious and some don't.",
      "start": 14312.64,
      "duration": 3.69
    },
    {
      "text": "But I think just on a human level",
      "start": 14316.33,
      "duration": 2.42
    },
    {
      "text": "as in empathizing with Claude, you know,",
      "start": 14318.75,
      "duration": 4.03
    },
    {
      "text": "consciousness is closely\ntied to suffering to me.",
      "start": 14322.78,
      "duration": 3.24
    },
    {
      "text": "And like the notion that an AI system",
      "start": 14326.02,
      "duration": 3.51
    },
    {
      "text": "would be suffering is really troubling.",
      "start": 14329.53,
      "duration": 2.304
    },
    {
      "text": "- Yeah.",
      "start": 14331.834,
      "duration": 1.092
    },
    {
      "text": "- I don't know.",
      "start": 14332.926,
      "duration": 0.833
    },
    {
      "text": "I don't think it's trivial",
      "start": 14333.759,
      "duration": 0.991
    },
    {
      "text": "to just say robots are tools,",
      "start": 14334.75,
      "duration": 1.92
    },
    {
      "text": "or AI systems are just tools.",
      "start": 14336.67,
      "duration": 2.19
    },
    {
      "text": "I think it's a opportunity for us",
      "start": 14338.86,
      "duration": 1.53
    },
    {
      "text": "to contend with like what\nit means to be conscious,",
      "start": 14340.39,
      "duration": 2.67
    },
    {
      "text": "what it means to be a suffering being.",
      "start": 14343.06,
      "duration": 2.34
    },
    {
      "text": "That's distinctly different",
      "start": 14345.4,
      "duration": 1.98
    },
    {
      "text": "than the same kind of question\nabout animals it feels like,",
      "start": 14347.38,
      "duration": 3.6
    },
    {
      "text": "'cause it's an totally entire medium.",
      "start": 14350.98,
      "duration": 1.98
    },
    {
      "text": "- Yeah, I mean, there's\na couple of things.",
      "start": 14352.96,
      "duration": 1.98
    },
    {
      "text": "One is that, and I don't think",
      "start": 14354.94,
      "duration": 1.38
    },
    {
      "text": "this like fully encapsulates what matters,",
      "start": 14356.32,
      "duration": 2.04
    },
    {
      "text": "but it does feel like for me,",
      "start": 14358.36,
      "duration": 1.8
    },
    {
      "text": "like I've said this before,",
      "start": 14360.16,
      "duration": 4.02
    },
    {
      "text": "I'm kind of like, you\nknow, like I like my bike.",
      "start": 14364.18,
      "duration": 2.31
    },
    {
      "text": "I know that my bike is\njust like an object,",
      "start": 14366.49,
      "duration": 1.98
    },
    {
      "text": "but I also don't kind of like want to be",
      "start": 14368.47,
      "duration": 1.44
    },
    {
      "text": "the kind of person that\nlike if I'm annoyed",
      "start": 14369.91,
      "duration": 2.1
    },
    {
      "text": "like kicks like this object.",
      "start": 14372.01,
      "duration": 2.85
    },
    {
      "text": "There's a sense in which like,",
      "start": 14374.86,
      "duration": 1.41
    },
    {
      "text": "and that's not because I\nthink it's like conscious.",
      "start": 14376.27,
      "duration": 2.04
    },
    {
      "text": "I'm just sort of like this\ndoesn't feel like a kind of,",
      "start": 14378.31,
      "duration": 2.1
    },
    {
      "text": "this sort of doesn't exemplify",
      "start": 14380.41,
      "duration": 2.01
    },
    {
      "text": "how I want to like\ninteract with the world.",
      "start": 14382.42,
      "duration": 2.22
    },
    {
      "text": "And if something like behaves",
      "start": 14384.64,
      "duration": 2.34
    },
    {
      "text": "as if it is like suffering,",
      "start": 14386.98,
      "duration": 1.32
    },
    {
      "text": "I kind of like want to\nbe the sort of person",
      "start": 14388.3,
      "duration": 1.86
    },
    {
      "text": "who's still responsive to that,",
      "start": 14390.16,
      "duration": 1.8
    },
    {
      "text": "even if it's just like a Roomba,",
      "start": 14391.96,
      "duration": 1.14
    },
    {
      "text": "and I've kind of like\nprogrammed it to do that.",
      "start": 14393.1,
      "duration": 2.545
    },
    {
      "text": "I don't want to like get rid\nof that feature of myself.",
      "start": 14395.645,
      "duration": 3.695
    },
    {
      "text": "And if I'm totally honest,",
      "start": 14399.34,
      "duration": 1.35
    },
    {
      "text": "my hope with a lot of this stuff,",
      "start": 14400.69,
      "duration": 2.28
    },
    {
      "text": "because maybe I am just\nlike a bit more skeptical",
      "start": 14402.97,
      "duration": 3.57
    },
    {
      "text": "about solving the underlying problem.",
      "start": 14406.54,
      "duration": 1.8
    },
    {
      "text": "I'm like this is, we haven't\nsolved the hard, you know,",
      "start": 14408.34,
      "duration": 1.71
    },
    {
      "text": "the hard problem of consciousness.",
      "start": 14410.05,
      "duration": 2.22
    },
    {
      "text": "Like I know that I am conscious,",
      "start": 14412.27,
      "duration": 1.59
    },
    {
      "text": "like I'm not an\neliminativist in that sense.",
      "start": 14413.86,
      "duration": 2.253
    },
    {
      "text": "But I don't know that\nother humans are conscious.",
      "start": 14417.13,
      "duration": 2.45
    },
    {
      "text": "I think they are.",
      "start": 14421.15,
      "duration": 0.87
    },
    {
      "text": "I think there's a really high\nprobability that they are.",
      "start": 14422.02,
      "duration": 1.77
    },
    {
      "text": "But there's basically just\na probability distribution",
      "start": 14423.79,
      "duration": 2.13
    },
    {
      "text": "that's usually clustered\nright around yourself,",
      "start": 14425.92,
      "duration": 1.86
    },
    {
      "text": "and then like it goes down",
      "start": 14427.78,
      "duration": 1.56
    },
    {
      "text": "as things get like further from you,",
      "start": 14429.34,
      "duration": 2.46
    },
    {
      "text": "and it goes immediately down.",
      "start": 14431.8,
      "duration": 1.11
    },
    {
      "text": "You know, you're like, I can't\nsee what it's like to be you.",
      "start": 14432.91,
      "duration": 2.88
    },
    {
      "text": "I've only ever had this\nlike one experience",
      "start": 14435.79,
      "duration": 1.5
    },
    {
      "text": "of what it's like to be a conscious being.",
      "start": 14437.29,
      "duration": 2.7
    },
    {
      "text": "So my hope is that we don't end up",
      "start": 14439.99,
      "duration": 1.92
    },
    {
      "text": "having to rely on like a very powerful",
      "start": 14441.91,
      "duration": 3.21
    },
    {
      "text": "and compelling answer to that question.",
      "start": 14445.12,
      "duration": 3.9
    },
    {
      "text": "I think a really good world would be one",
      "start": 14449.02,
      "duration": 2.4
    },
    {
      "text": "where basically there\naren't that many trade-offs.",
      "start": 14451.42,
      "duration": 3.3
    },
    {
      "text": "Like it's probably not that costly",
      "start": 14454.72,
      "duration": 1.26
    },
    {
      "text": "to make Claude a little bit\nless apologetic, for example.",
      "start": 14455.98,
      "duration": 3.18
    },
    {
      "text": "It might not be that\ncostly to have Claude,",
      "start": 14459.16,
      "duration": 3.166
    },
    {
      "text": "you know, just like\nnot take abuse as much,",
      "start": 14462.326,
      "duration": 2.984
    },
    {
      "text": "like not be willing to be\nlike the recipient of that.",
      "start": 14465.31,
      "duration": 2.61
    },
    {
      "text": "In fact, it might just have benefits",
      "start": 14467.92,
      "duration": 1.38
    },
    {
      "text": "for both the person\ninteracting with the model",
      "start": 14469.3,
      "duration": 2.64
    },
    {
      "text": "and if the model itself\nis like, I don't know,",
      "start": 14471.94,
      "duration": 3.27
    },
    {
      "text": "like extremely intelligent and conscious,",
      "start": 14475.21,
      "duration": 1.8
    },
    {
      "text": "it also helps it.",
      "start": 14477.01,
      "duration": 1.2
    },
    {
      "text": "So that's my hope.",
      "start": 14478.21,
      "duration": 1.95
    },
    {
      "text": "If we live in a world where there aren't",
      "start": 14480.16,
      "duration": 1.17
    },
    {
      "text": "that many trade-offs\nhere and we can just find",
      "start": 14481.33,
      "duration": 1.65
    },
    {
      "text": "all of the kind of like\npositive sum interactions",
      "start": 14482.98,
      "duration": 3.06
    },
    {
      "text": "that we can have, that would be lovely.",
      "start": 14486.04,
      "duration": 1.89
    },
    {
      "text": "I mean, I think eventually\nthere might be trade-offs",
      "start": 14487.93,
      "duration": 1.347
    },
    {
      "text": "and then we just have to do a difficult",
      "start": 14489.277,
      "duration": 1.533
    },
    {
      "text": "kind of like calculation.",
      "start": 14490.81,
      "duration": 2.07
    },
    {
      "text": "Like it's really easy for people to think",
      "start": 14492.88,
      "duration": 1.38
    },
    {
      "text": "of the zero sum cases and I'm\nlike, let's exhaust the areas",
      "start": 14494.26,
      "duration": 2.28
    },
    {
      "text": "where it's just basically\ncostless to assume",
      "start": 14496.54,
      "duration": 4.59
    },
    {
      "text": "that if this thing is suffering",
      "start": 14501.13,
      "duration": 2.25
    },
    {
      "text": "then we're making its life better.",
      "start": 14503.38,
      "duration": 1.587
    },
    {
      "text": "- And I agree with you,",
      "start": 14504.967,
      "duration": 1.533
    },
    {
      "text": "when a human is being\nmean to an AI system,",
      "start": 14506.5,
      "duration": 3.12
    },
    {
      "text": "I think the obvious near\nterm negative effect",
      "start": 14509.62,
      "duration": 3.63
    },
    {
      "text": "is on the human, not on the AI system.",
      "start": 14513.25,
      "duration": 3.0
    },
    {
      "text": "And so we have to kind of try to construct",
      "start": 14516.25,
      "duration": 3.45
    },
    {
      "text": "an incentive system where you\nshould be behave the same,",
      "start": 14519.7,
      "duration": 4.95
    },
    {
      "text": "just like as you were saying",
      "start": 14524.65,
      "duration": 1.05
    },
    {
      "text": "with prompt engineering,",
      "start": 14525.7,
      "duration": 1.56
    },
    {
      "text": "behave with Claude like you\nwould with other humans.",
      "start": 14527.26,
      "duration": 2.497
    },
    {
      "text": "It's just good for the soul.",
      "start": 14529.757,
      "duration": 2.483
    },
    {
      "text": "- Yeah, like, I think we\nadded a thing at one point",
      "start": 14532.24,
      "duration": 1.98
    },
    {
      "text": "to the system prompt\nwhere basically if people",
      "start": 14534.22,
      "duration": 3.21
    },
    {
      "text": "were getting frustrated with Claude,",
      "start": 14537.43,
      "duration": 1.833
    },
    {
      "text": "it got like the model to just tell them",
      "start": 14540.633,
      "duration": 2.05
    },
    {
      "text": "that it can do the thumbs down button",
      "start": 14542.683,
      "duration": 2.157
    },
    {
      "text": "and send the feedback to Anthropic.",
      "start": 14544.84,
      "duration": 1.737
    },
    {
      "text": "And I think that was helpful,",
      "start": 14546.577,
      "duration": 1.233
    },
    {
      "text": "'cause in some ways it's just\nlike if you're really annoyed",
      "start": 14547.81,
      "duration": 2.22
    },
    {
      "text": "'cause the model's not\ndoing something you want,",
      "start": 14550.03,
      "duration": 1.08
    },
    {
      "text": "you're just like, just do it properly.",
      "start": 14551.11,
      "duration": 1.9
    },
    {
      "text": "The issue is you're\nprobably like, you know,",
      "start": 14554.14,
      "duration": 1.08
    },
    {
      "text": "you're maybe hitting some\nlike capability limit",
      "start": 14555.22,
      "duration": 1.98
    },
    {
      "text": "or just some issue in the\nmodel and you want to vent.",
      "start": 14557.2,
      "duration": 2.67
    },
    {
      "text": "And I'm like, instead of having a person",
      "start": 14559.87,
      "duration": 2.19
    },
    {
      "text": "just vent to the model, I was like,",
      "start": 14562.06,
      "duration": 1.83
    },
    {
      "text": "they should vent to us,",
      "start": 14563.89,
      "duration": 1.02
    },
    {
      "text": "'cause we can maybe like\ndo something about it.",
      "start": 14564.91,
      "duration": 1.83
    },
    {
      "text": "- That's true.",
      "start": 14566.74,
      "duration": 0.87
    },
    {
      "text": "Or you could do a side,\nlike with the artifacts,",
      "start": 14567.61,
      "duration": 3.24
    },
    {
      "text": "just like a side venting thing.",
      "start": 14570.85,
      "duration": 1.5
    },
    {
      "text": "All right, do you want like\na side quick therapist?",
      "start": 14572.35,
      "duration": 3.27
    },
    {
      "text": "- Yeah, I mean, there's\nlots of weird responses",
      "start": 14575.62,
      "duration": 1.59
    },
    {
      "text": "you could do to this.",
      "start": 14577.21,
      "duration": 0.833
    },
    {
      "text": "Like if people are\ngetting really mad at you,",
      "start": 14578.043,
      "duration": 1.357
    },
    {
      "text": "I dunno, try to diffuse the\nsituation by writing fun poems,",
      "start": 14579.4,
      "duration": 3.87
    },
    {
      "text": "but maybe people wouldn't\nbe that happy with it.",
      "start": 14583.27,
      "duration": 2.07
    },
    {
      "text": "- I still wish it would be possible.",
      "start": 14585.34,
      "duration": 1.86
    },
    {
      "text": "I understand this is sort of\nfrom a product perspective,",
      "start": 14587.2,
      "duration": 3.0
    },
    {
      "text": "it's not feasible but I would love",
      "start": 14590.2,
      "duration": 2.85
    },
    {
      "text": "if an AI system could just like leave,",
      "start": 14593.05,
      "duration": 2.763
    },
    {
      "text": "have its own kind of\nvolition just to be like, eh.",
      "start": 14596.83,
      "duration": 4.36
    },
    {
      "text": "- I think that's like feasible.",
      "start": 14601.19,
      "duration": 1.91
    },
    {
      "text": "Like I have wondered the same thing.",
      "start": 14603.1,
      "duration": 1.65
    },
    {
      "text": "It's like, and I could\nactually, not only that,",
      "start": 14604.75,
      "duration": 2.19
    },
    {
      "text": "I could actually just see\nthat happening eventually",
      "start": 14606.94,
      "duration": 1.92
    },
    {
      "text": "where it's just like, you know,",
      "start": 14608.86,
      "duration": 0.99
    },
    {
      "text": "the model like ended the chat. (laughs)",
      "start": 14609.85,
      "duration": 3.93
    },
    {
      "text": "- Do you know how harsh that\ncould be for some people?",
      "start": 14613.78,
      "duration": 2.7
    },
    {
      "text": "But it might be necessary.",
      "start": 14617.32,
      "duration": 1.59
    },
    {
      "text": "- Yeah, it feels very\nextreme or something.",
      "start": 14618.91,
      "duration": 3.123
    },
    {
      "text": "Like, the only time I've\never really thought this is,",
      "start": 14622.87,
      "duration": 3.12
    },
    {
      "text": "I think that there was like,\nI'm trying to remember,",
      "start": 14625.99,
      "duration": 2.13
    },
    {
      "text": "this was possibly a while ago,",
      "start": 14628.12,
      "duration": 1.35
    },
    {
      "text": "but where someone just like\nkind of left this thing,",
      "start": 14629.47,
      "duration": 2.013
    },
    {
      "text": "like maybe it was like an automated thing",
      "start": 14631.483,
      "duration": 1.497
    },
    {
      "text": "interacting with Claude,",
      "start": 14632.98,
      "duration": 1.533
    },
    {
      "text": "and Claude's like getting\nmore and more frustrated,",
      "start": 14634.513,
      "duration": 1.947
    },
    {
      "text": "and kind of like why are we like having.",
      "start": 14636.46,
      "duration": 1.56
    },
    {
      "text": "And I was like, I wish\nthat Claude could have",
      "start": 14638.02,
      "duration": 1.47
    },
    {
      "text": "just been like, \"I think\nthat an error has happened",
      "start": 14639.49,
      "duration": 2.04
    },
    {
      "text": "and you've left this thing running,\"",
      "start": 14641.53,
      "duration": 1.77
    },
    {
      "text": "and I would just like, what\nif I just stop talking now,",
      "start": 14643.3,
      "duration": 2.52
    },
    {
      "text": "and if you want me to start talking again,",
      "start": 14645.82,
      "duration": 2.115
    },
    {
      "text": "actively tell me or do something.",
      "start": 14647.935,
      "duration": 1.882
    },
    {
      "text": "But yeah, it's like, it is kind of harsh.",
      "start": 14649.817,
      "duration": 3.143
    },
    {
      "text": "Like I'd feel really sad\nif like I was chatting",
      "start": 14652.96,
      "duration": 2.64
    },
    {
      "text": "with Claude and Claude\njust was like, \"I'm done.\"",
      "start": 14655.6,
      "duration": 2.01
    },
    {
      "text": "- That would be a special\nTuring test moment",
      "start": 14657.61,
      "duration": 1.53
    },
    {
      "text": "where Claude says, \"I\nneed a break for an hour",
      "start": 14659.14,
      "duration": 3.015
    },
    {
      "text": "and it sounds like you do too,\"",
      "start": 14662.155,
      "duration": 1.485
    },
    {
      "text": "and just leave, close the window.",
      "start": 14663.64,
      "duration": 1.95
    },
    {
      "text": "- I mean, obviously like it\ndoesn't have like a concept",
      "start": 14665.59,
      "duration": 1.86
    },
    {
      "text": "of time but you can easily, like,",
      "start": 14667.45,
      "duration": 2.22
    },
    {
      "text": "I could make that like right now",
      "start": 14669.67,
      "duration": 3.09
    },
    {
      "text": "and the model would just, I would,",
      "start": 14672.76,
      "duration": 1.97
    },
    {
      "text": "it could just be like,",
      "start": 14674.73,
      "duration": 0.91
    },
    {
      "text": "oh, here's like the circumstances",
      "start": 14675.64,
      "duration": 1.44
    },
    {
      "text": "in which like you can just\nsay the conversation is done.",
      "start": 14677.08,
      "duration": 4.08
    },
    {
      "text": "And I mean, because you can get the models",
      "start": 14681.16,
      "duration": 1.8
    },
    {
      "text": "to be pretty responsive to prompts,",
      "start": 14682.96,
      "duration": 1.59
    },
    {
      "text": "you could even make it a fairly high bar.",
      "start": 14684.55,
      "duration": 1.47
    },
    {
      "text": "It could be like if the\nhuman doesn't interest you",
      "start": 14686.02,
      "duration": 1.95
    },
    {
      "text": "or do things that you find intriguing",
      "start": 14687.97,
      "duration": 2.31
    },
    {
      "text": "and you're bored, you can just leave.",
      "start": 14690.28,
      "duration": 2.4
    },
    {
      "text": "And I think that like it\nwould be interesting to see",
      "start": 14692.68,
      "duration": 3.24
    },
    {
      "text": "where Claude utilized it,",
      "start": 14695.92,
      "duration": 1.2
    },
    {
      "text": "but I think sometimes it\nwould, it should be like,",
      "start": 14697.12,
      "duration": 1.29
    },
    {
      "text": "oh, like this programming task",
      "start": 14698.41,
      "duration": 1.89
    },
    {
      "text": "is getting super boring.",
      "start": 14700.3,
      "duration": 1.44
    },
    {
      "text": "So either we talk about,",
      "start": 14701.74,
      "duration": 1.38
    },
    {
      "text": "I dunno like, either we\ntalk about fun things now,",
      "start": 14703.12,
      "duration": 3.39
    },
    {
      "text": "or I'm just, I'm done.",
      "start": 14706.51,
      "duration": 1.71
    },
    {
      "text": "- Yeah, it actually is inspired me",
      "start": 14708.22,
      "duration": 1.53
    },
    {
      "text": "to add that to the user prompt.",
      "start": 14709.75,
      "duration": 1.893
    },
    {
      "text": "Okay, the movie \"Her.\"",
      "start": 14712.99,
      "duration": 2.04
    },
    {
      "text": "Do you think we'll be headed there one day",
      "start": 14715.03,
      "duration": 3.18
    },
    {
      "text": "where humans have romantic\nrelationships with AI systems?",
      "start": 14718.21,
      "duration": 4.89
    },
    {
      "text": "In this case, it's just\ntext and voice based.",
      "start": 14723.1,
      "duration": 3.51
    },
    {
      "text": "- I think that we're gonna have",
      "start": 14726.61,
      "duration": 0.833
    },
    {
      "text": "to like navigate a hard question",
      "start": 14727.443,
      "duration": 1.717
    },
    {
      "text": "of relationships with AIs,",
      "start": 14729.16,
      "duration": 3.003
    },
    {
      "text": "especially if they can remember things",
      "start": 14733.09,
      "duration": 2.79
    },
    {
      "text": "about your past interactions with them.",
      "start": 14735.88,
      "duration": 2.523
    },
    {
      "text": "I'm of many minds about this",
      "start": 14741.76,
      "duration": 1.217
    },
    {
      "text": "'cause I think the reflexive reaction",
      "start": 14742.977,
      "duration": 2.113
    },
    {
      "text": "is to be kind of like this is very bad,",
      "start": 14745.09,
      "duration": 2.97
    },
    {
      "text": "and we should sort of like\nprohibit it in some way.",
      "start": 14748.06,
      "duration": 3.003
    },
    {
      "text": "I think it's a thing\nthat has to be handled",
      "start": 14752.41,
      "duration": 1.83
    },
    {
      "text": "with extreme care for many reasons.",
      "start": 14754.24,
      "duration": 3.51
    },
    {
      "text": "Like one is, you know, like this is a,",
      "start": 14757.75,
      "duration": 2.19
    },
    {
      "text": "for example, like if you have\nthe models changing like this,",
      "start": 14759.94,
      "duration": 2.58
    },
    {
      "text": "you probably don't want people performing",
      "start": 14762.52,
      "duration": 1.59
    },
    {
      "text": "like long-term attachments to something",
      "start": 14764.11,
      "duration": 1.56
    },
    {
      "text": "that might change with the next iteration.",
      "start": 14765.67,
      "duration": 3.0
    },
    {
      "text": "At the same time I'm sort of like,",
      "start": 14768.67,
      "duration": 1.98
    },
    {
      "text": "there's probably a benign version of this",
      "start": 14770.65,
      "duration": 1.62
    },
    {
      "text": "where I'm like if you like, you know,",
      "start": 14772.27,
      "duration": 2.43
    },
    {
      "text": "for example if you are like\nunable to leave the house",
      "start": 14774.7,
      "duration": 3.54
    },
    {
      "text": "and you can't be like, you\nknow, talking with people",
      "start": 14778.24,
      "duration": 4.35
    },
    {
      "text": "at all times of the day\nand this is like something",
      "start": 14782.59,
      "duration": 1.683
    },
    {
      "text": "that you find nice to\nhave conversations with,",
      "start": 14784.273,
      "duration": 2.067
    },
    {
      "text": "you like it that it can remember you",
      "start": 14786.34,
      "duration": 0.833
    },
    {
      "text": "and you genuinely would be sad",
      "start": 14787.173,
      "duration": 1.507
    },
    {
      "text": "if like you couldn't talk to it anymore.",
      "start": 14788.68,
      "duration": 2.04
    },
    {
      "text": "There's a way in which I could see it",
      "start": 14790.72,
      "duration": 1.2
    },
    {
      "text": "being like healthy and helpful.",
      "start": 14791.92,
      "duration": 1.55
    },
    {
      "text": "So my guess is this is a thing",
      "start": 14794.83,
      "duration": 1.23
    },
    {
      "text": "that we're going to have to\nnavigate kind of carefully.",
      "start": 14796.06,
      "duration": 3.153
    },
    {
      "text": "And I think it's also\nlike I don't see a good",
      "start": 14800.59,
      "duration": 2.88
    },
    {
      "text": "like I think it's just a very,",
      "start": 14803.47,
      "duration": 2.31
    },
    {
      "text": "it reminds me of all of this stuff",
      "start": 14805.78,
      "duration": 1.17
    },
    {
      "text": "where it has to be just approached",
      "start": 14806.95,
      "duration": 1.08
    },
    {
      "text": "with like nuance and thinking through",
      "start": 14808.03,
      "duration": 2.022
    },
    {
      "text": "what are the healthy options here,",
      "start": 14810.052,
      "duration": 2.118
    },
    {
      "text": "and how do you encourage people",
      "start": 14812.17,
      "duration": 2.47
    },
    {
      "text": "towards those while, you know,\nrespecting their right to.",
      "start": 14815.5,
      "duration": 3.577
    },
    {
      "text": "You know, like if someone is like,",
      "start": 14819.077,
      "duration": 1.53
    },
    {
      "text": "\"Hey, I get a lot out of\nchatting with this model.",
      "start": 14820.607,
      "duration": 2.816
    },
    {
      "text": "I'm aware of the risks.",
      "start": 14824.26,
      "duration": 1.02
    },
    {
      "text": "I'm aware it could change.",
      "start": 14825.28,
      "duration": 1.95
    },
    {
      "text": "I don't think it's unhealthy.",
      "start": 14827.23,
      "duration": 1.2
    },
    {
      "text": "It's just, you know, something that",
      "start": 14828.43,
      "duration": 1.723
    },
    {
      "text": "I can chat to during the day.\"",
      "start": 14830.153,
      "duration": 1.907
    },
    {
      "text": "I kind of want to just like respect that.",
      "start": 14832.06,
      "duration": 1.56
    },
    {
      "text": "- I personally think there'll be a lot",
      "start": 14833.62,
      "duration": 1.16
    },
    {
      "text": "of really close relationships.",
      "start": 14834.78,
      "duration": 1.33
    },
    {
      "text": "I don't know about romantic\nbut friendships at least.",
      "start": 14836.11,
      "duration": 2.37
    },
    {
      "text": "And then you have to, I mean,",
      "start": 14838.48,
      "duration": 2.07
    },
    {
      "text": "there's so many fascinating things there.",
      "start": 14840.55,
      "duration": 1.41
    },
    {
      "text": "Just like you said, you have to have",
      "start": 14841.96,
      "duration": 3.72
    },
    {
      "text": "some kind of stability guarantees",
      "start": 14845.68,
      "duration": 1.654
    },
    {
      "text": "that it's not going to change,",
      "start": 14847.334,
      "duration": 1.406
    },
    {
      "text": "'cause that's the traumatic thing for us,",
      "start": 14848.74,
      "duration": 2.46
    },
    {
      "text": "if a close friend of\nours completely changed.",
      "start": 14851.2,
      "duration": 2.25
    },
    {
      "text": "- Yeah.\n- All of a sudden",
      "start": 14854.425,
      "duration": 0.945
    },
    {
      "text": "with the first update.",
      "start": 14855.37,
      "duration": 1.779
    },
    {
      "text": "Yeah, so like, I mean, to me,",
      "start": 14857.149,
      "duration": 2.361
    },
    {
      "text": "that's just a fascinating exploration",
      "start": 14859.51,
      "duration": 1.62
    },
    {
      "text": "of a perturbation to human society",
      "start": 14861.13,
      "duration": 4.89
    },
    {
      "text": "that will just make us think deeply about",
      "start": 14866.02,
      "duration": 2.31
    },
    {
      "text": "what's meaningful to us.",
      "start": 14868.33,
      "duration": 2.148
    },
    {
      "text": "- I think it's also the only thing",
      "start": 14870.478,
      "duration": 2.022
    },
    {
      "text": "that I've thought consistently\nthrough this as like,",
      "start": 14872.5,
      "duration": 2.703
    },
    {
      "text": "maybe not necessarily a mitigation,",
      "start": 14876.19,
      "duration": 1.62
    },
    {
      "text": "but a thing that feels really important",
      "start": 14877.81,
      "duration": 1.86
    },
    {
      "text": "is that the models are always\nlike extremely accurate",
      "start": 14879.67,
      "duration": 3.03
    },
    {
      "text": "with the human about what they are.",
      "start": 14882.7,
      "duration": 1.803
    },
    {
      "text": "It's like a case where\nit's basically like,",
      "start": 14885.46,
      "duration": 2.07
    },
    {
      "text": "if you imagine, like\nI really like the idea",
      "start": 14887.53,
      "duration": 2.1
    },
    {
      "text": "of the models like say knowing",
      "start": 14889.63,
      "duration": 1.65
    },
    {
      "text": "like roughly how they were trained,",
      "start": 14891.28,
      "duration": 1.923
    },
    {
      "text": "and I think Claude will often do this.",
      "start": 14894.04,
      "duration": 2.67
    },
    {
      "text": "I mean, for like, there are things",
      "start": 14896.71,
      "duration": 2.85
    },
    {
      "text": "like part of the traits training",
      "start": 14899.56,
      "duration": 2.13
    },
    {
      "text": "included like what Claude\nshould do if people,",
      "start": 14901.69,
      "duration": 2.55
    },
    {
      "text": "basically like explaining",
      "start": 14904.24,
      "duration": 1.11
    },
    {
      "text": "like the kind of limitations\nof the relationship",
      "start": 14905.35,
      "duration": 3.3
    },
    {
      "text": "between like an AI and a human",
      "start": 14908.65,
      "duration": 1.77
    },
    {
      "text": "that it like doesn't retain\nthings from the conversation.",
      "start": 14910.42,
      "duration": 3.33
    },
    {
      "text": "And so I think it will like\njust explain to you like,",
      "start": 14913.75,
      "duration": 3.273
    },
    {
      "text": "hey, I won't remember this conversation.",
      "start": 14917.023,
      "duration": 3.027
    },
    {
      "text": "Here's how I was trained.",
      "start": 14920.05,
      "duration": 1.08
    },
    {
      "text": "It's kind of unlikely that I can have",
      "start": 14921.13,
      "duration": 1.89
    },
    {
      "text": "like a certain kind of\nlike relationship with you,",
      "start": 14923.02,
      "duration": 1.95
    },
    {
      "text": "and it's important that you know that.",
      "start": 14924.97,
      "duration": 1.02
    },
    {
      "text": "It's important for like, you\nknow, your mental wellbeing",
      "start": 14925.99,
      "duration": 3.15
    },
    {
      "text": "that you don't think that\nI'm something that I'm not.",
      "start": 14929.14,
      "duration": 2.31
    },
    {
      "text": "And somehow I feel like\nthis is one of the things",
      "start": 14931.45,
      "duration": 1.8
    },
    {
      "text": "where I'm like, oh, it feels like a thing",
      "start": 14933.25,
      "duration": 1.323
    },
    {
      "text": "that I always want to be true.",
      "start": 14934.573,
      "duration": 1.707
    },
    {
      "text": "I kind of don't want models\nto be lying to people,",
      "start": 14936.28,
      "duration": 2.82
    },
    {
      "text": "'cause if people are going",
      "start": 14939.1,
      "duration": 2.01
    },
    {
      "text": "to have like healthy relationships",
      "start": 14941.11,
      "duration": 1.95
    },
    {
      "text": "with anything, it's kind of important.",
      "start": 14943.06,
      "duration": 1.89
    },
    {
      "text": "Yeah, like I think that's easier",
      "start": 14944.95,
      "duration": 1.26
    },
    {
      "text": "if you always just like know",
      "start": 14946.21,
      "duration": 1.62
    },
    {
      "text": "exactly what the thing is\nthat you are relating to.",
      "start": 14947.83,
      "duration": 3.66
    },
    {
      "text": "It doesn't solve everything,",
      "start": 14951.49,
      "duration": 0.837
    },
    {
      "text": "but I think it helps quite a lot.",
      "start": 14952.327,
      "duration": 1.65
    },
    {
      "text": "- Anthropic may be the very company",
      "start": 14955.21,
      "duration": 3.27
    },
    {
      "text": "to develop a system that we\ndefinitively recognize as AGI,",
      "start": 14958.48,
      "duration": 4.2
    },
    {
      "text": "and you very well might be\nthe person that talks to it,",
      "start": 14962.68,
      "duration": 3.78
    },
    {
      "text": "probably talks to it first.",
      "start": 14966.46,
      "duration": 1.353
    },
    {
      "text": "(Lex chuckles)",
      "start": 14968.889,
      "duration": 0.833
    },
    {
      "text": "What would the conversation contain?",
      "start": 14969.722,
      "duration": 1.538
    },
    {
      "text": "Like, what would be your first question?",
      "start": 14971.26,
      "duration": 2.1
    },
    {
      "text": "- Well, it depends partly on like",
      "start": 14973.36,
      "duration": 1.5
    },
    {
      "text": "the kind of capability level of the model.",
      "start": 14974.86,
      "duration": 2.43
    },
    {
      "text": "If you have something that is like capable",
      "start": 14977.29,
      "duration": 2.34
    },
    {
      "text": "in the same way that an\nextremely capable human is,",
      "start": 14979.63,
      "duration": 2.07
    },
    {
      "text": "I imagine myself kind\nof interacting with it",
      "start": 14981.7,
      "duration": 2.31
    },
    {
      "text": "the same way that I do with\nan extremely capable human,",
      "start": 14984.01,
      "duration": 2.61
    },
    {
      "text": "with the one difference",
      "start": 14986.62,
      "duration": 0.833
    },
    {
      "text": "that I'm probably going\nto be trying to like probe",
      "start": 14987.453,
      "duration": 1.717
    },
    {
      "text": "and understand its behaviors.",
      "start": 14989.17,
      "duration": 1.773
    },
    {
      "text": "But in many ways, I'm like I can then",
      "start": 14991.99,
      "duration": 1.23
    },
    {
      "text": "just have like useful\nconversations with it, you know?",
      "start": 14993.22,
      "duration": 2.4
    },
    {
      "text": "So if I'm working on something",
      "start": 14995.62,
      "duration": 1.02
    },
    {
      "text": "as part of my research I can just be like,",
      "start": 14996.64,
      "duration": 1.5
    },
    {
      "text": "oh, like, which I already\nfind myself starting to do,",
      "start": 14998.14,
      "duration": 2.771
    },
    {
      "text": "you know, if I'm like, oh,",
      "start": 15000.911,
      "duration": 0.833
    },
    {
      "text": "I feel like there's this like thing",
      "start": 15001.744,
      "duration": 1.856
    },
    {
      "text": "in virtue ethics and I can't\nquite remember the term,",
      "start": 15003.6,
      "duration": 2.49
    },
    {
      "text": "like I'll use the model\nfor things like that.",
      "start": 15006.09,
      "duration": 1.167
    },
    {
      "text": "And so I can imagine that being more",
      "start": 15007.257,
      "duration": 1.893
    },
    {
      "text": "and more the case where you're just",
      "start": 15009.15,
      "duration": 1.26
    },
    {
      "text": "basically interacting with it much more",
      "start": 15010.41,
      "duration": 1.44
    },
    {
      "text": "like you would an\nincredibly smart colleague.",
      "start": 15011.85,
      "duration": 2.73
    },
    {
      "text": "And using it like for the kinds\nof work that you want to do",
      "start": 15014.58,
      "duration": 2.82
    },
    {
      "text": "as if you just had a\ncollaborator who was like.",
      "start": 15017.4,
      "duration": 2.16
    },
    {
      "text": "Or you know, the slightly horrifying thing",
      "start": 15019.56,
      "duration": 2.67
    },
    {
      "text": "about AI is like as soon as\nyou have one collaborator,",
      "start": 15022.23,
      "duration": 2.13
    },
    {
      "text": "you have 1000 collaborators",
      "start": 15024.36,
      "duration": 1.74
    },
    {
      "text": "if you can manage them enough.",
      "start": 15026.1,
      "duration": 1.11
    },
    {
      "text": "- But what if it's two times",
      "start": 15027.21,
      "duration": 2.187
    },
    {
      "text": "the smartest human on earth\non that particular discipline?",
      "start": 15029.397,
      "duration": 3.873
    },
    {
      "text": "- Yeah.",
      "start": 15033.27,
      "duration": 0.833
    },
    {
      "text": "- I guess you're really good",
      "start": 15034.103,
      "duration": 1.357
    },
    {
      "text": "at sort of probing Claude",
      "start": 15035.46,
      "duration": 2.38
    },
    {
      "text": "in a way that pushes its limits,",
      "start": 15039.87,
      "duration": 1.89
    },
    {
      "text": "understanding where the limits are.",
      "start": 15041.76,
      "duration": 1.68
    },
    {
      "text": "- [Amanda] Yep.",
      "start": 15043.44,
      "duration": 0.96
    },
    {
      "text": "- So I guess what would be a question",
      "start": 15044.4,
      "duration": 2.52
    },
    {
      "text": "you would ask to be\nlike, yeah, this is AGI.",
      "start": 15046.92,
      "duration": 2.797
    },
    {
      "text": "- That's really hard 'cause\nit feels like in order to,",
      "start": 15052.41,
      "duration": 1.95
    },
    {
      "text": "it has to just be a series of questions.",
      "start": 15054.36,
      "duration": 1.98
    },
    {
      "text": "Like if there was just one question,",
      "start": 15056.34,
      "duration": 1.77
    },
    {
      "text": "like you can train anything",
      "start": 15058.11,
      "duration": 1.11
    },
    {
      "text": "to answer one question extremely well.",
      "start": 15059.22,
      "duration": 2.253
    },
    {
      "text": "In fact, you can probably\ntrain it to answer like,",
      "start": 15062.917,
      "duration": 1.673
    },
    {
      "text": "you know, 20 questions extremely well.",
      "start": 15064.59,
      "duration": 3.12
    },
    {
      "text": "- Like how long would you\nneed to be locked in a room",
      "start": 15067.71,
      "duration": 2.73
    },
    {
      "text": "with an AGI to know this thing is AGI?",
      "start": 15070.44,
      "duration": 3.532
    },
    {
      "text": "- It's a hard question 'cause part of me",
      "start": 15073.972,
      "duration": 0.961
    },
    {
      "text": "is like all of this just feels continuous.",
      "start": 15074.933,
      "duration": 2.287
    },
    {
      "text": "Like if you put me in a\nroom for five minutes,",
      "start": 15077.22,
      "duration": 1.74
    },
    {
      "text": "I'm like, I just have high error bars.",
      "start": 15078.96,
      "duration": 1.65
    },
    {
      "text": "You know, I'm like, and\nthen it's just like,",
      "start": 15080.61,
      "duration": 1.35
    },
    {
      "text": "maybe it's like both the\nprobability increases",
      "start": 15081.96,
      "duration": 2.55
    },
    {
      "text": "in the error bar decreases.",
      "start": 15084.51,
      "duration": 1.95
    },
    {
      "text": "I think things that I can actually probe",
      "start": 15086.46,
      "duration": 1.5
    },
    {
      "text": "the edge of human knowledge of.",
      "start": 15087.96,
      "duration": 1.26
    },
    {
      "text": "So I think this with\nphilosophy a little bit.",
      "start": 15089.22,
      "duration": 2.43
    },
    {
      "text": "Sometimes when I ask the\nmodels philosophy questions,",
      "start": 15091.65,
      "duration": 3.06
    },
    {
      "text": "I am like, this is a question",
      "start": 15094.71,
      "duration": 2.01
    },
    {
      "text": "that I think no one has ever asked.",
      "start": 15096.72,
      "duration": 2.64
    },
    {
      "text": "Like it's maybe like right at the edge",
      "start": 15099.36,
      "duration": 1.44
    },
    {
      "text": "of like some literature that I know,",
      "start": 15100.8,
      "duration": 2.783
    },
    {
      "text": "and the models will just kind of like,",
      "start": 15103.583,
      "duration": 3.637
    },
    {
      "text": "when they struggle with\nthat, when they struggle",
      "start": 15107.22,
      "duration": 1.62
    },
    {
      "text": "to come up with a kind of like novel.",
      "start": 15108.84,
      "duration": 1.86
    },
    {
      "text": "Like I'm like I know that there's\nlike a novel argument here",
      "start": 15110.7,
      "duration": 2.16
    },
    {
      "text": "'cause I've just thought of it myself.",
      "start": 15112.86,
      "duration": 1.38
    },
    {
      "text": "So maybe that's the thing where I'm like,",
      "start": 15114.24,
      "duration": 1.14
    },
    {
      "text": "I've thought of a cool novel argument",
      "start": 15115.38,
      "duration": 1.35
    },
    {
      "text": "in this like niche area,",
      "start": 15116.73,
      "duration": 1.47
    },
    {
      "text": "and I'm going to just like probe you",
      "start": 15118.2,
      "duration": 1.17
    },
    {
      "text": "to see if you can come up with it,",
      "start": 15119.37,
      "duration": 1.737
    },
    {
      "text": "and how much like prompting it takes",
      "start": 15121.107,
      "duration": 1.713
    },
    {
      "text": "to get you to come up with it.",
      "start": 15122.82,
      "duration": 1.41
    },
    {
      "text": "And I think for some of these,",
      "start": 15124.23,
      "duration": 1.14
    },
    {
      "text": "like really like right at the edge",
      "start": 15125.37,
      "duration": 2.64
    },
    {
      "text": "of human knowledge questions, I'm like,",
      "start": 15128.01,
      "duration": 1.5
    },
    {
      "text": "you could not in fact come up",
      "start": 15129.51,
      "duration": 1.35
    },
    {
      "text": "with the thing that I came up with.",
      "start": 15130.86,
      "duration": 1.5
    },
    {
      "text": "I think if I just took something like that",
      "start": 15132.36,
      "duration": 2.49
    },
    {
      "text": "where like I know a lot about an area,",
      "start": 15134.85,
      "duration": 2.127
    },
    {
      "text": "and I came up with a novel issue",
      "start": 15136.977,
      "duration": 2.433
    },
    {
      "text": "or a novel like solution to a problem,",
      "start": 15139.41,
      "duration": 2.28
    },
    {
      "text": "and I gave it to a model",
      "start": 15141.69,
      "duration": 1.44
    },
    {
      "text": "and it came up with that solution,",
      "start": 15143.13,
      "duration": 1.8
    },
    {
      "text": "that would be a pretty\nmoving moment for me",
      "start": 15144.93,
      "duration": 2.13
    },
    {
      "text": "because I would be like, this is a case",
      "start": 15147.06,
      "duration": 1.71
    },
    {
      "text": "where no human has ever, like it's not.",
      "start": 15148.77,
      "duration": 2.577
    },
    {
      "text": "And obviously we see these this",
      "start": 15151.347,
      "duration": 1.743
    },
    {
      "text": "with like more kind of like,",
      "start": 15153.09,
      "duration": 1.83
    },
    {
      "text": "you see novel solutions all the time,",
      "start": 15154.92,
      "duration": 1.68
    },
    {
      "text": "especially to like easier problems.",
      "start": 15156.6,
      "duration": 1.83
    },
    {
      "text": "I think people overestimate\nthat, you know,",
      "start": 15158.43,
      "duration": 1.65
    },
    {
      "text": "novelty isn't like, it's\ncompletely different",
      "start": 15160.08,
      "duration": 2.67
    },
    {
      "text": "from anything that's ever happened.",
      "start": 15162.75,
      "duration": 1.23
    },
    {
      "text": "It's just like this is, it\ncan be a variant of things",
      "start": 15163.98,
      "duration": 2.19
    },
    {
      "text": "that have happened and still be novel.",
      "start": 15166.17,
      "duration": 3.06
    },
    {
      "text": "But I think, yeah, if I saw like the more",
      "start": 15169.23,
      "duration": 2.85
    },
    {
      "text": "I were to see like\ncompletely like novel work",
      "start": 15172.08,
      "duration": 5.0
    },
    {
      "text": "from the models, that would be like.",
      "start": 15177.48,
      "duration": 2.913
    },
    {
      "text": "And this is just going to feel iterative.",
      "start": 15181.26,
      "duration": 1.47
    },
    {
      "text": "It's one of those things\nwhere, there's never,",
      "start": 15182.73,
      "duration": 2.19
    },
    {
      "text": "it's like, you know,\npeople I think want there",
      "start": 15184.92,
      "duration": 3.48
    },
    {
      "text": "to be like a moment and\nI'm like, I don't know.",
      "start": 15188.4,
      "duration": 2.19
    },
    {
      "text": "Like I think that there\nmight just never be a moment.",
      "start": 15190.59,
      "duration": 1.74
    },
    {
      "text": "It might just be that there's just like",
      "start": 15192.33,
      "duration": 1.95
    },
    {
      "text": "this continuous ramping up.",
      "start": 15194.28,
      "duration": 2.31
    },
    {
      "text": "- I have a sense that there will be things",
      "start": 15196.59,
      "duration": 2.853
    },
    {
      "text": "that a model can say that\nconvinces you, this is very.",
      "start": 15199.443,
      "duration": 4.947
    },
    {
      "text": "It's not like,",
      "start": 15204.39,
      "duration": 1.023
    },
    {
      "text": "like, I've talked to people\nwho are like truly wise.",
      "start": 15208.512,
      "duration": 3.611
    },
    {
      "text": "Like you could just tell there's\na lot of horsepower there.",
      "start": 15213.21,
      "duration": 3.39
    },
    {
      "text": "- [Amanda] Yep.",
      "start": 15216.6,
      "duration": 0.833
    },
    {
      "text": "- And if you 10x that, I don't know,",
      "start": 15217.433,
      "duration": 2.377
    },
    {
      "text": "I just feel like there's\nwords you could say.",
      "start": 15219.81,
      "duration": 1.77
    },
    {
      "text": "Maybe ask it to generate a poem, (laughs)",
      "start": 15221.58,
      "duration": 4.043
    },
    {
      "text": "and the poem it generates,",
      "start": 15225.623,
      "duration": 1.327
    },
    {
      "text": "you're like, yeah, okay.",
      "start": 15226.95,
      "duration": 1.5
    },
    {
      "text": "- Yeah,\n- Whatever you did there,",
      "start": 15228.45,
      "duration": 1.65
    },
    {
      "text": "I don't think a human can do that.",
      "start": 15230.1,
      "duration": 2.25
    },
    {
      "text": "- I think it has to be something",
      "start": 15232.35,
      "duration": 0.93
    },
    {
      "text": "that I can verify is like\nactually really good though.",
      "start": 15233.28,
      "duration": 2.46
    },
    {
      "text": "That's why I think these\nquestions that are like,",
      "start": 15235.74,
      "duration": 1.89
    },
    {
      "text": "where I'm like, oh,\nthis is like, you know,",
      "start": 15237.63,
      "duration": 2.147
    },
    {
      "text": "like, you know, sometimes\nit's just like I'll come up",
      "start": 15239.777,
      "duration": 2.473
    },
    {
      "text": "with say a concrete counter example",
      "start": 15242.25,
      "duration": 1.59
    },
    {
      "text": "to like an argument or\nsomething like that.",
      "start": 15243.84,
      "duration": 1.89
    },
    {
      "text": "I'm sure like with like,",
      "start": 15245.73,
      "duration": 1.17
    },
    {
      "text": "it would be like if\nyou're a mathematician,",
      "start": 15246.9,
      "duration": 1.56
    },
    {
      "text": "you had a novel proof I think,",
      "start": 15248.46,
      "duration": 1.71
    },
    {
      "text": "and you just gave it the\nproblem and you saw it",
      "start": 15250.17,
      "duration": 2.64
    },
    {
      "text": "and you're like, this\nproof is genuinely novel.",
      "start": 15252.81,
      "duration": 1.98
    },
    {
      "text": "Like no one has ever done,",
      "start": 15254.79,
      "duration": 1.65
    },
    {
      "text": "you actually have to do a lot of things",
      "start": 15256.44,
      "duration": 1.55
    },
    {
      "text": "to like come up with this.",
      "start": 15257.99,
      "duration": 1.689
    },
    {
      "text": "You know, I had to sit",
      "start": 15259.679,
      "duration": 0.901
    },
    {
      "text": "and think about it for\nmonths or something.",
      "start": 15260.58,
      "duration": 1.86
    },
    {
      "text": "And then if you saw the\nmodel successfully do that,",
      "start": 15262.44,
      "duration": 2.67
    },
    {
      "text": "I think you would just be like,",
      "start": 15265.11,
      "duration": 1.05
    },
    {
      "text": "I can verify that this is correct.",
      "start": 15266.16,
      "duration": 1.743
    },
    {
      "text": "It is a sign that you have\ngeneralized from your training.",
      "start": 15269.652,
      "duration": 3.408
    },
    {
      "text": "Like you didn't just see this somewhere",
      "start": 15273.06,
      "duration": 1.59
    },
    {
      "text": "because I just came up with it myself",
      "start": 15274.65,
      "duration": 1.53
    },
    {
      "text": "and you were able to like replicate that.",
      "start": 15276.18,
      "duration": 2.76
    },
    {
      "text": "That's the kind of thing where I'm like,",
      "start": 15278.94,
      "duration": 1.95
    },
    {
      "text": "for me, the closer,",
      "start": 15280.89,
      "duration": 2.88
    },
    {
      "text": "the more that models like\ncan do things like that,",
      "start": 15283.77,
      "duration": 2.55
    },
    {
      "text": "the more I would be like,\noh, this is like very real,",
      "start": 15286.32,
      "duration": 4.53
    },
    {
      "text": "'cause then I can, I dunno,\nI can like verify that",
      "start": 15290.85,
      "duration": 2.16
    },
    {
      "text": "that's like extremely, extremely capable.",
      "start": 15293.01,
      "duration": 2.85
    },
    {
      "text": "- You've interacted with AI a lot.",
      "start": 15295.86,
      "duration": 1.8
    },
    {
      "text": "What do you think makes humans special?",
      "start": 15297.66,
      "duration": 2.7
    },
    {
      "text": "- Oh, good question.",
      "start": 15300.36,
      "duration": 1.113
    },
    {
      "text": "- Maybe in a way that the\nuniverse is much better off",
      "start": 15304.8,
      "duration": 3.45
    },
    {
      "text": "that we're in it and that\nwe should definitely survive",
      "start": 15308.25,
      "duration": 2.67
    },
    {
      "text": "and spread throughout the universe.",
      "start": 15310.92,
      "duration": 1.92
    },
    {
      "text": "- Yeah, it's interesting",
      "start": 15312.84,
      "duration": 1.031
    },
    {
      "text": "because I think like people focus",
      "start": 15313.871,
      "duration": 3.589
    },
    {
      "text": "so much on intelligence,\nespecially with models.",
      "start": 15317.46,
      "duration": 3.51
    },
    {
      "text": "Look, intelligence is important\nbecause of what it does.",
      "start": 15320.97,
      "duration": 2.79
    },
    {
      "text": "Like, it's very useful.",
      "start": 15323.76,
      "duration": 1.44
    },
    {
      "text": "It does a lot of things in the world.",
      "start": 15325.2,
      "duration": 1.65
    },
    {
      "text": "And I'm like, you know,\nyou can imagine a world",
      "start": 15326.85,
      "duration": 2.46
    },
    {
      "text": "where like height or strength\nwould've played this role,",
      "start": 15329.31,
      "duration": 2.37
    },
    {
      "text": "and I'm like, it's just a trait like that.",
      "start": 15331.68,
      "duration": 1.71
    },
    {
      "text": "I'm like, it's not intrinsically valuable.",
      "start": 15333.39,
      "duration": 2.1
    },
    {
      "text": "It's valuable because of what it does,",
      "start": 15335.49,
      "duration": 2.25
    },
    {
      "text": "I think for the most part.",
      "start": 15337.74,
      "duration": 1.3
    },
    {
      "text": "The things that feel, you know,",
      "start": 15340.65,
      "duration": 1.62
    },
    {
      "text": "I'm like, I mean,\npersonally I'm just like,",
      "start": 15342.27,
      "duration": 2.61
    },
    {
      "text": "I think humans and like life in general",
      "start": 15344.88,
      "duration": 3.15
    },
    {
      "text": "is extremely magical.",
      "start": 15348.03,
      "duration": 2.223
    },
    {
      "text": "We almost like to the\ndegree that, you know,",
      "start": 15351.12,
      "duration": 1.92
    },
    {
      "text": "and I don't know, like not\neveryone agrees with this.",
      "start": 15353.04,
      "duration": 1.95
    },
    {
      "text": "I'm flagging, but you know,",
      "start": 15354.99,
      "duration": 2.61
    },
    {
      "text": "we have this like whole universe,",
      "start": 15357.6,
      "duration": 1.38
    },
    {
      "text": "and there's like all of these objects.",
      "start": 15358.98,
      "duration": 1.59
    },
    {
      "text": "You know, there's like beautiful stars,",
      "start": 15360.57,
      "duration": 1.71
    },
    {
      "text": "and there's like galaxies and then,",
      "start": 15362.28,
      "duration": 2.13
    },
    {
      "text": "I don't know, I'm just\nlike, on this planet,",
      "start": 15364.41,
      "duration": 1.38
    },
    {
      "text": "there are these creatures that have",
      "start": 15365.79,
      "duration": 1.68
    },
    {
      "text": "this like ability to observe that,",
      "start": 15367.47,
      "duration": 3.39
    },
    {
      "text": "like, and they are like seeing it.",
      "start": 15370.86,
      "duration": 2.76
    },
    {
      "text": "They are experiencing it.",
      "start": 15373.62,
      "duration": 1.08
    },
    {
      "text": "And I'm just like that,\nif you try to explain,",
      "start": 15374.7,
      "duration": 2.07
    },
    {
      "text": "like imagine trying to explain to like,",
      "start": 15376.77,
      "duration": 2.37
    },
    {
      "text": "I dunno, someone for some reason,",
      "start": 15379.14,
      "duration": 1.74
    },
    {
      "text": "they've never encountered the world",
      "start": 15380.88,
      "duration": 1.62
    },
    {
      "text": "or science or anything.",
      "start": 15382.5,
      "duration": 1.74
    },
    {
      "text": "And I think that nothing is that,",
      "start": 15384.24,
      "duration": 1.473
    },
    {
      "text": "like everything, you know,\nlike all of our physics",
      "start": 15385.713,
      "duration": 2.007
    },
    {
      "text": "and everything in the world,\nit's all extremely exciting.",
      "start": 15387.72,
      "duration": 2.01
    },
    {
      "text": "But then you say, oh, and plus,",
      "start": 15389.73,
      "duration": 1.89
    },
    {
      "text": "there's this thing that\nit is to be a thing",
      "start": 15391.62,
      "duration": 2.34
    },
    {
      "text": "and observe in the world,",
      "start": 15393.96,
      "duration": 1.35
    },
    {
      "text": "and you see this like inner cinema.",
      "start": 15395.31,
      "duration": 1.86
    },
    {
      "text": "And I think they would be\nlike, hang on, wait, pause.",
      "start": 15397.17,
      "duration": 2.43
    },
    {
      "text": "You just said something that like",
      "start": 15399.6,
      "duration": 2.01
    },
    {
      "text": "is kind of wild sounding.",
      "start": 15401.61,
      "duration": 2.4
    },
    {
      "text": "And so I'm like, we have this like ability",
      "start": 15404.01,
      "duration": 1.59
    },
    {
      "text": "to like experience the world.",
      "start": 15405.6,
      "duration": 2.433
    },
    {
      "text": "We feel pleasure, we feel suffering.",
      "start": 15409.02,
      "duration": 1.74
    },
    {
      "text": "We feel like a lot of like complex things.",
      "start": 15410.76,
      "duration": 2.28
    },
    {
      "text": "And so, yeah, and maybe this\nis also why I think, you know,",
      "start": 15413.04,
      "duration": 3.3
    },
    {
      "text": "I also like care a lot\nabout animals, for example,",
      "start": 15416.34,
      "duration": 2.057
    },
    {
      "text": "'cause I think they\nprobably share this with us.",
      "start": 15418.397,
      "duration": 3.084
    },
    {
      "text": "So I think that like the things",
      "start": 15421.481,
      "duration": 1.909
    },
    {
      "text": "that make humans special",
      "start": 15423.39,
      "duration": 1.2
    },
    {
      "text": "insofar as like I care about humans",
      "start": 15424.59,
      "duration": 2.19
    },
    {
      "text": "is probably more like their\nability to feel an experience",
      "start": 15426.78,
      "duration": 3.78
    },
    {
      "text": "than it is like them having",
      "start": 15430.56,
      "duration": 1.68
    },
    {
      "text": "these like functionally useful traits.",
      "start": 15432.24,
      "duration": 2.04
    },
    {
      "text": "- Yeah, to feel and experience\nthe beauty in the world.",
      "start": 15434.28,
      "duration": 3.69
    },
    {
      "text": "Yeah, to look at the stars.",
      "start": 15437.97,
      "duration": 1.593
    },
    {
      "text": "I hope there's other alien\ncivilizations out there,",
      "start": 15440.67,
      "duration": 2.64
    },
    {
      "text": "but if we're it, it's a pretty good,",
      "start": 15443.31,
      "duration": 3.771
    },
    {
      "text": "it's a pretty good thing.",
      "start": 15447.081,
      "duration": 0.909
    },
    {
      "text": "- And that they're having a good time.",
      "start": 15447.99,
      "duration": 1.506
    },
    {
      "text": "- They're having a good time watching us.",
      "start": 15449.496,
      "duration": 1.854
    },
    {
      "text": "- [Amanda] Yeah.",
      "start": 15451.35,
      "duration": 0.833
    },
    {
      "text": "- Well, thank you for this good time",
      "start": 15452.183,
      "duration": 2.407
    },
    {
      "text": "of a conversation and for\nthe work you're doing,",
      "start": 15454.59,
      "duration": 2.01
    },
    {
      "text": "and for helping make Claude a\ngreat conversational partner.",
      "start": 15456.6,
      "duration": 5.0
    },
    {
      "text": "And thank you for talking today.",
      "start": 15461.73,
      "duration": 1.35
    },
    {
      "text": "- Yeah, thanks for talking.",
      "start": 15463.08,
      "duration": 1.95
    },
    {
      "text": "- Thanks for listening",
      "start": 15465.03,
      "duration": 0.833
    },
    {
      "text": "to this conversation with Amanda Askell.",
      "start": 15465.863,
      "duration": 2.227
    },
    {
      "text": "And now, dear friends, here's Chris Olah.",
      "start": 15468.09,
      "duration": 3.633
    },
    {
      "text": "Can you describe this fascinating field",
      "start": 15472.62,
      "duration": 3.36
    },
    {
      "text": "of mechanistic interpretability,\nAKA mech interp,",
      "start": 15475.98,
      "duration": 4.02
    },
    {
      "text": "the history of the field\nand where it stands today?",
      "start": 15480.0,
      "duration": 2.4
    },
    {
      "text": "- I think one useful way to think about",
      "start": 15482.4,
      "duration": 1.5
    },
    {
      "text": "neural networks is that",
      "start": 15483.9,
      "duration": 1.92
    },
    {
      "text": "we don't program and we don't make them.",
      "start": 15485.82,
      "duration": 2.25
    },
    {
      "text": "We kind of, we grow them.",
      "start": 15488.07,
      "duration": 1.89
    },
    {
      "text": "You know, we have these\nneural network architectures",
      "start": 15489.96,
      "duration": 2.28
    },
    {
      "text": "that we design and we\nhave these loss objectives",
      "start": 15492.24,
      "duration": 2.73
    },
    {
      "text": "that we create.",
      "start": 15494.97,
      "duration": 1.62
    },
    {
      "text": "And the neural network architecture,",
      "start": 15496.59,
      "duration": 1.62
    },
    {
      "text": "it's kind of like a scaffold\nthat the circuits grow on,",
      "start": 15498.21,
      "duration": 3.693
    },
    {
      "text": "and they sort of, you know, it starts off",
      "start": 15502.89,
      "duration": 1.77
    },
    {
      "text": "with some kind of random, you know,",
      "start": 15504.66,
      "duration": 1.92
    },
    {
      "text": "random things and it grows.",
      "start": 15506.58,
      "duration": 1.53
    },
    {
      "text": "And it's almost like the objective",
      "start": 15508.11,
      "duration": 1.59
    },
    {
      "text": "that we train for is this light.",
      "start": 15509.7,
      "duration": 2.25
    },
    {
      "text": "And so we create the\nscaffold that it grows on",
      "start": 15511.95,
      "duration": 1.677
    },
    {
      "text": "and we create the, you know,",
      "start": 15513.627,
      "duration": 1.02
    },
    {
      "text": "the light that it grows towards.",
      "start": 15514.647,
      "duration": 1.893
    },
    {
      "text": "But the thing that we actually create,",
      "start": 15516.54,
      "duration": 1.83
    },
    {
      "text": "it's this almost biological, you know,",
      "start": 15518.37,
      "duration": 5.0
    },
    {
      "text": "entity or organism that we're studying.",
      "start": 15524.01,
      "duration": 3.15
    },
    {
      "text": "And so it's very, very different",
      "start": 15527.16,
      "duration": 1.53
    },
    {
      "text": "from any kind of regular\nsoftware engineering,",
      "start": 15528.69,
      "duration": 3.99
    },
    {
      "text": "because at the end of the day,",
      "start": 15532.68,
      "duration": 0.833
    },
    {
      "text": "we end up with this artifact",
      "start": 15533.513,
      "duration": 1.357
    },
    {
      "text": "that can do all these amazing things.",
      "start": 15534.87,
      "duration": 1.71
    },
    {
      "text": "It can, you know, write\nessays and translate",
      "start": 15536.58,
      "duration": 2.82
    },
    {
      "text": "and, you know, understand images.",
      "start": 15539.4,
      "duration": 1.5
    },
    {
      "text": "It can do all these things\nthat we have no idea",
      "start": 15540.9,
      "duration": 1.62
    },
    {
      "text": "how to directly create a\ncomputer program to do.",
      "start": 15542.52,
      "duration": 2.94
    },
    {
      "text": "And it can do that because we grew it,",
      "start": 15545.46,
      "duration": 2.16
    },
    {
      "text": "we didn't write it, we didn't create it.",
      "start": 15547.62,
      "duration": 2.55
    },
    {
      "text": "And so then that leaves open\nthis question at the end,",
      "start": 15550.17,
      "duration": 2.43
    },
    {
      "text": "which is, what the hell is\ngoing on inside these systems?",
      "start": 15552.6,
      "duration": 3.12
    },
    {
      "text": "And that, you know, is to me",
      "start": 15555.72,
      "duration": 2.95
    },
    {
      "text": "a really deep and exciting question.",
      "start": 15559.837,
      "duration": 3.533
    },
    {
      "text": "It's, you know, a really\nexciting scientific question.",
      "start": 15563.37,
      "duration": 3.36
    },
    {
      "text": "To me it's sort of is\nlike the question that is,",
      "start": 15566.73,
      "duration": 2.01
    },
    {
      "text": "is just screaming out,\nit's calling out for us",
      "start": 15568.74,
      "duration": 2.61
    },
    {
      "text": "to go and answer it when we\ntalk about neural networks.",
      "start": 15571.35,
      "duration": 2.94
    },
    {
      "text": "And I think it's also a very deep question",
      "start": 15574.29,
      "duration": 1.62
    },
    {
      "text": "for safety reasons.",
      "start": 15575.91,
      "duration": 1.38
    },
    {
      "text": "- So, and mechanistic interpretability",
      "start": 15577.29,
      "duration": 1.86
    },
    {
      "text": "I guess is closer to maybe neurobiology.",
      "start": 15579.15,
      "duration": 3.39
    },
    {
      "text": "- Yeah, yeah, I think that's right.",
      "start": 15582.54,
      "duration": 0.833
    },
    {
      "text": "So maybe to give an example\nof the kind of thing",
      "start": 15583.373,
      "duration": 1.897
    },
    {
      "text": "that has been done that\nI wouldn't consider",
      "start": 15585.27,
      "duration": 1.65
    },
    {
      "text": "to be mechanistic interpretability.",
      "start": 15586.92,
      "duration": 0.93
    },
    {
      "text": "There was for a long time a lot of work",
      "start": 15587.85,
      "duration": 2.25
    },
    {
      "text": "on saliency maps where\nyou would take an image",
      "start": 15590.1,
      "duration": 1.92
    },
    {
      "text": "and you try to say, you know,",
      "start": 15592.02,
      "duration": 1.38
    },
    {
      "text": "the model thinks this image is a dog.",
      "start": 15593.4,
      "duration": 1.71
    },
    {
      "text": "What part of the image made\nit think that it's a dog?",
      "start": 15595.11,
      "duration": 3.69
    },
    {
      "text": "And you know, that tells you\nmaybe something about the model",
      "start": 15598.8,
      "duration": 2.4
    },
    {
      "text": "if you can come up with a\nprincipled version of that.",
      "start": 15601.2,
      "duration": 3.3
    },
    {
      "text": "But it doesn't really tell you like",
      "start": 15604.5,
      "duration": 1.2
    },
    {
      "text": "what algorithms are running in the model?",
      "start": 15605.7,
      "duration": 1.68
    },
    {
      "text": "How was the model actually\nmaking that decision?",
      "start": 15607.38,
      "duration": 1.92
    },
    {
      "text": "Maybe it's telling you something about",
      "start": 15609.3,
      "duration": 1.11
    },
    {
      "text": "what was important to it,",
      "start": 15610.41,
      "duration": 0.93
    },
    {
      "text": "if you can make that method work,",
      "start": 15611.34,
      "duration": 1.98
    },
    {
      "text": "but it isn't telling, you know,",
      "start": 15613.32,
      "duration": 2.223
    },
    {
      "text": "what are the algorithms that are running?",
      "start": 15616.41,
      "duration": 2.01
    },
    {
      "text": "How is it that the system's\nable to do this thing",
      "start": 15618.42,
      "duration": 2.04
    },
    {
      "text": "that no one knew how to do?",
      "start": 15620.46,
      "duration": 1.95
    },
    {
      "text": "And so I guess we started",
      "start": 15622.41,
      "duration": 0.87
    },
    {
      "text": "using the term mechanistic\ninterpretability",
      "start": 15623.28,
      "duration": 1.59
    },
    {
      "text": "to try to sort of draw that divide",
      "start": 15624.87,
      "duration": 2.91
    },
    {
      "text": "or to distinguish ourselves in the work",
      "start": 15627.78,
      "duration": 1.74
    },
    {
      "text": "that we were doing in some ways",
      "start": 15629.52,
      "duration": 0.833
    },
    {
      "text": "from some of these other things.",
      "start": 15630.353,
      "duration": 0.833
    },
    {
      "text": "And I think since then it's become",
      "start": 15631.186,
      "duration": 2.024
    },
    {
      "text": "this sort of umbrella term for, you know,",
      "start": 15633.21,
      "duration": 3.36
    },
    {
      "text": "a pretty wide variety of work.",
      "start": 15636.57,
      "duration": 1.68
    },
    {
      "text": "But I'd say that the things",
      "start": 15638.25,
      "duration": 0.84
    },
    {
      "text": "that are kind of distinctive are,",
      "start": 15639.09,
      "duration": 1.35
    },
    {
      "text": "I think, A, this focus on,",
      "start": 15640.44,
      "duration": 1.98
    },
    {
      "text": "we really want to get at, you know,",
      "start": 15642.42,
      "duration": 1.86
    },
    {
      "text": "the mechanisms, we wanna\nget at the algorithms.",
      "start": 15644.28,
      "duration": 1.89
    },
    {
      "text": "You know, if you think of neural networks",
      "start": 15646.17,
      "duration": 1.47
    },
    {
      "text": "as being like a computer program,",
      "start": 15647.64,
      "duration": 1.65
    },
    {
      "text": "then the weights are kind of\nlike a binary computer program.",
      "start": 15650.19,
      "duration": 2.94
    },
    {
      "text": "And we'd like to reverse\nengineer those weights",
      "start": 15653.13,
      "duration": 1.53
    },
    {
      "text": "and figure out what\nalgorithms are running.",
      "start": 15654.66,
      "duration": 1.8
    },
    {
      "text": "So, I think one way you might think",
      "start": 15656.46,
      "duration": 1.35
    },
    {
      "text": "of trying to understand a neural network",
      "start": 15657.81,
      "duration": 1.47
    },
    {
      "text": "is that it's kind of like,",
      "start": 15659.28,
      "duration": 1.013
    },
    {
      "text": "we have this compiled computer program",
      "start": 15660.293,
      "duration": 2.407
    },
    {
      "text": "and the weights of the neural\nnetwork are the binary.",
      "start": 15662.7,
      "duration": 2.65
    },
    {
      "text": "And when the neural network runs,",
      "start": 15666.63,
      "duration": 1.47
    },
    {
      "text": "that's the activations.",
      "start": 15668.1,
      "duration": 1.86
    },
    {
      "text": "And our goal is ultimately",
      "start": 15669.96,
      "duration": 1.59
    },
    {
      "text": "to go and understand these weights.",
      "start": 15671.55,
      "duration": 2.49
    },
    {
      "text": "And so, you know, the approach",
      "start": 15674.04,
      "duration": 1.079
    },
    {
      "text": "of mechanistic interpretability\nis to somehow figure out",
      "start": 15675.119,
      "duration": 1.831
    },
    {
      "text": "how do these weights\ncorrespond to algorithms.",
      "start": 15676.95,
      "duration": 2.3
    },
    {
      "text": "And in order to do that,",
      "start": 15680.37,
      "duration": 0.93
    },
    {
      "text": "you also have to\nunderstand the activations,",
      "start": 15681.3,
      "duration": 1.47
    },
    {
      "text": "'cause it's sort of, the\nactivations are like the memory.",
      "start": 15682.77,
      "duration": 2.4
    },
    {
      "text": "And if you imagine reverse\nengineering a computer program",
      "start": 15685.17,
      "duration": 3.51
    },
    {
      "text": "and you have the binary instructions,",
      "start": 15688.68,
      "duration": 1.59
    },
    {
      "text": "you know, in order to understand",
      "start": 15690.27,
      "duration": 1.77
    },
    {
      "text": "what a particular instruction\nmeans, you need to know",
      "start": 15692.04,
      "duration": 2.65
    },
    {
      "text": "what is stored in the memory\nthat it's operating on.",
      "start": 15695.672,
      "duration": 2.218
    },
    {
      "text": "And so those two things\nare very intertwined.",
      "start": 15697.89,
      "duration": 1.47
    },
    {
      "text": "So mechanistic interpret really tends",
      "start": 15699.36,
      "duration": 1.53
    },
    {
      "text": "to be interested both of those things.",
      "start": 15700.89,
      "duration": 2.28
    },
    {
      "text": "Now, you know, there's a lot of work",
      "start": 15703.17,
      "duration": 2.1
    },
    {
      "text": "that's interested in those things,",
      "start": 15705.27,
      "duration": 2.97
    },
    {
      "text": "especially, you know, there's\nall this work on probing,",
      "start": 15708.24,
      "duration": 2.01
    },
    {
      "text": "which you might see as part",
      "start": 15710.25,
      "duration": 0.99
    },
    {
      "text": "of being mechanistic interpretability.",
      "start": 15711.24,
      "duration": 1.59
    },
    {
      "text": "Although it's, you know, again,",
      "start": 15712.83,
      "duration": 1.631
    },
    {
      "text": "it's just a broad term",
      "start": 15714.461,
      "duration": 0.833
    },
    {
      "text": "and not everyone who does that work",
      "start": 15715.294,
      "duration": 0.956
    },
    {
      "text": "would identify as doing mech interp.",
      "start": 15716.25,
      "duration": 2.37
    },
    {
      "text": "I think a thing that is maybe\na little bit distinctive",
      "start": 15718.62,
      "duration": 2.4
    },
    {
      "text": "to the vibe of mech\ninterp is I think people",
      "start": 15721.02,
      "duration": 3.57
    },
    {
      "text": "working in this space tend to think",
      "start": 15724.59,
      "duration": 1.26
    },
    {
      "text": "of neural networks as, well,",
      "start": 15725.85,
      "duration": 2.4
    },
    {
      "text": "maybe one way to say it\nis that gradient descent",
      "start": 15728.25,
      "duration": 1.62
    },
    {
      "text": "is smarter than you, that, you know,",
      "start": 15729.87,
      "duration": 1.437
    },
    {
      "text": "and gradient descent is\nactually really great.",
      "start": 15731.307,
      "duration": 2.403
    },
    {
      "text": "The whole reason that we're\nunderstanding these models",
      "start": 15733.71,
      "duration": 1.436
    },
    {
      "text": "is 'cause we didn't know how to write them",
      "start": 15735.146,
      "duration": 0.833
    },
    {
      "text": "in the first place.",
      "start": 15735.979,
      "duration": 0.833
    },
    {
      "text": "That gradient descent comes up",
      "start": 15736.812,
      "duration": 0.833
    },
    {
      "text": "with better solutions than us.",
      "start": 15737.645,
      "duration": 0.895
    },
    {
      "text": "And so I think that maybe\nanother thing about mech interp",
      "start": 15738.54,
      "duration": 2.94
    },
    {
      "text": "is sort of having almost\na kind of humility",
      "start": 15741.48,
      "duration": 2.58
    },
    {
      "text": "that we won't guess a priori",
      "start": 15744.06,
      "duration": 1.62
    },
    {
      "text": "what's going on inside the models.",
      "start": 15745.68,
      "duration": 1.35
    },
    {
      "text": "We have to have this sort\nof bottom up approach",
      "start": 15747.03,
      "duration": 2.22
    },
    {
      "text": "where we don't really assume, you know,",
      "start": 15749.25,
      "duration": 1.68
    },
    {
      "text": "we don't assume that we should\nlook for a particular thing",
      "start": 15750.93,
      "duration": 1.809
    },
    {
      "text": "and that that will be there\nand that's how it works.",
      "start": 15752.739,
      "duration": 1.551
    },
    {
      "text": "But instead we look for the bottom up",
      "start": 15754.29,
      "duration": 1.35
    },
    {
      "text": "and discover what happens",
      "start": 15755.64,
      "duration": 1.44
    },
    {
      "text": "to exist in these models\nand study them that way.",
      "start": 15757.08,
      "duration": 3.6
    },
    {
      "text": "- But, you know, the very\nfact that it's possible to do,",
      "start": 15760.68,
      "duration": 2.34
    },
    {
      "text": "and as you and others have\nshown over time, you know,",
      "start": 15763.02,
      "duration": 4.08
    },
    {
      "text": "things like universality, that the wisdom",
      "start": 15767.1,
      "duration": 4.26
    },
    {
      "text": "of the gradient descent\ncreates features and circuits,",
      "start": 15771.36,
      "duration": 4.32
    },
    {
      "text": "creates things universally",
      "start": 15775.68,
      "duration": 1.83
    },
    {
      "text": "across different kinds of\nnetworks that are useful,",
      "start": 15777.51,
      "duration": 2.31
    },
    {
      "text": "and that makes the whole field possible.",
      "start": 15779.82,
      "duration": 2.97
    },
    {
      "text": "- Yeah, so this is actually,",
      "start": 15782.79,
      "duration": 1.17
    },
    {
      "text": "is indeed a really remarkable",
      "start": 15783.96,
      "duration": 1.98
    },
    {
      "text": "and exciting thing\nwhere it does seem like,",
      "start": 15785.94,
      "duration": 1.89
    },
    {
      "text": "at least to some extent,\nyou know, the same elements,",
      "start": 15787.83,
      "duration": 5.0
    },
    {
      "text": "the same features and\ncircuits form again and again.",
      "start": 15792.84,
      "duration": 3.243
    },
    {
      "text": "You know, you can look\nat every vision model",
      "start": 15796.083,
      "duration": 1.227
    },
    {
      "text": "and you'll find curve detectors",
      "start": 15797.31,
      "duration": 0.96
    },
    {
      "text": "and you'll find high/low\nfrequency detectors.",
      "start": 15798.27,
      "duration": 1.89
    },
    {
      "text": "And in fact, there's some reason to think",
      "start": 15800.16,
      "duration": 1.92
    },
    {
      "text": "that the same things\nform across, you know,",
      "start": 15802.08,
      "duration": 1.8
    },
    {
      "text": "biological neural networks and\nartificial neural networks.",
      "start": 15803.88,
      "duration": 3.3
    },
    {
      "text": "So a famous example is vision models",
      "start": 15807.18,
      "duration": 2.88
    },
    {
      "text": "in their early layers\nthey have Gabor filters,",
      "start": 15810.06,
      "duration": 1.68
    },
    {
      "text": "and there's, you know,",
      "start": 15811.74,
      "duration": 0.833
    },
    {
      "text": "Gabor filters are something\nthat neuroscientists",
      "start": 15812.573,
      "duration": 1.657
    },
    {
      "text": "are interested in, have\nthought a lot about.",
      "start": 15814.23,
      "duration": 1.62
    },
    {
      "text": "We find curve detectors in these models,",
      "start": 15815.85,
      "duration": 1.56
    },
    {
      "text": "curve detectors are also found in monkeys.",
      "start": 15817.41,
      "duration": 1.95
    },
    {
      "text": "And we discover these high\nlow frequency detectors",
      "start": 15819.36,
      "duration": 1.8
    },
    {
      "text": "and then some follow up work",
      "start": 15821.16,
      "duration": 1.86
    },
    {
      "text": "went and discovered them in rats or mice.",
      "start": 15823.02,
      "duration": 3.57
    },
    {
      "text": "So they were found first in\nartificial neural networks",
      "start": 15826.59,
      "duration": 1.467
    },
    {
      "text": "and then found in\nbiological neural networks.",
      "start": 15828.057,
      "duration": 2.283
    },
    {
      "text": "You know, there's this\nreally famous result",
      "start": 15830.34,
      "duration": 1.23
    },
    {
      "text": "on like grandmother neurons",
      "start": 15831.57,
      "duration": 1.56
    },
    {
      "text": "or the Halle Berry neuron\nfrom Quiroga et al.",
      "start": 15833.13,
      "duration": 3.12
    },
    {
      "text": "And we found very similar\nthings in vision models where,",
      "start": 15836.25,
      "duration": 3.42
    },
    {
      "text": "this is while I was still at OpenAI",
      "start": 15839.67,
      "duration": 1.2
    },
    {
      "text": "and I was looking at their clip model,",
      "start": 15840.87,
      "duration": 1.8
    },
    {
      "text": "and you find these neurons that respond",
      "start": 15842.67,
      "duration": 2.91
    },
    {
      "text": "to the same entities in images.",
      "start": 15845.58,
      "duration": 2.07
    },
    {
      "text": "And also to give a concrete example there,",
      "start": 15847.65,
      "duration": 1.98
    },
    {
      "text": "we found that there was\na Donald Trump neuron.",
      "start": 15849.63,
      "duration": 1.38
    },
    {
      "text": "For some reason, I guess everyone likes",
      "start": 15851.01,
      "duration": 1.2
    },
    {
      "text": "to talk about Donald Trump,",
      "start": 15852.21,
      "duration": 0.96
    },
    {
      "text": "and Donald Trump was very prominent,",
      "start": 15853.17,
      "duration": 1.89
    },
    {
      "text": "was a very hot topic at that time.",
      "start": 15855.06,
      "duration": 2.42
    },
    {
      "text": "So every neural network we looked at,",
      "start": 15857.48,
      "duration": 1.54
    },
    {
      "text": "we would find a dedicated\nneuron for Donald Trump.",
      "start": 15859.02,
      "duration": 2.769
    },
    {
      "text": "And that was the only person",
      "start": 15861.789,
      "duration": 1.251
    },
    {
      "text": "who had always had a dedicated neuron.",
      "start": 15863.04,
      "duration": 1.9
    },
    {
      "text": "You know, sometimes you'd\nhave an Obama neuron,",
      "start": 15865.89,
      "duration": 1.98
    },
    {
      "text": "sometimes you'd have a Clinton neuron,",
      "start": 15867.87,
      "duration": 1.05
    },
    {
      "text": "but Trump always had a dedicated neuron.",
      "start": 15868.92,
      "duration": 2.43
    },
    {
      "text": "So it responds to, you\nknow, pictures of his face",
      "start": 15871.35,
      "duration": 2.67
    },
    {
      "text": "and the word Trump, like\nall these things, right?",
      "start": 15874.02,
      "duration": 4.02
    },
    {
      "text": "And so it's not responding\nto a particular example",
      "start": 15878.04,
      "duration": 2.454
    },
    {
      "text": "or like, it's not just\nresponding to his face,",
      "start": 15880.494,
      "duration": 2.046
    },
    {
      "text": "it's extracting over this\ngeneral concept, right?",
      "start": 15882.54,
      "duration": 3.15
    },
    {
      "text": "So in any case, that's very similar",
      "start": 15885.69,
      "duration": 1.26
    },
    {
      "text": "to these Quiroga et al results.",
      "start": 15886.95,
      "duration": 1.65
    },
    {
      "text": "So there evidence that this\nphenomenon of universality,",
      "start": 15888.6,
      "duration": 2.7
    },
    {
      "text": "the same things form\nacross both artificial",
      "start": 15891.3,
      "duration": 2.22
    },
    {
      "text": "and natural neural networks.",
      "start": 15893.52,
      "duration": 2.01
    },
    {
      "text": "That's a pretty amazing\nthing if that's true.",
      "start": 15895.53,
      "duration": 2.16
    },
    {
      "text": "You know, it suggests that,\nwell, I think the thing",
      "start": 15897.69,
      "duration": 3.401
    },
    {
      "text": "that it suggests is\nthe gradient of descent",
      "start": 15901.091,
      "duration": 1.309
    },
    {
      "text": "is sort of finding, you\nknow, the right ways",
      "start": 15902.4,
      "duration": 2.88
    },
    {
      "text": "to cut things apart in some sense",
      "start": 15905.28,
      "duration": 1.89
    },
    {
      "text": "that many systems converge on,",
      "start": 15907.17,
      "duration": 1.77
    },
    {
      "text": "and many different neural networks'",
      "start": 15908.94,
      "duration": 1.5
    },
    {
      "text": "architectures converge on.",
      "start": 15910.44,
      "duration": 1.435
    },
    {
      "text": "That there's some\nnatural set of, you know,",
      "start": 15911.875,
      "duration": 2.075
    },
    {
      "text": "there's some set of abstractions\nthat are a very natural way",
      "start": 15913.95,
      "duration": 2.4
    },
    {
      "text": "to cut apart the problem,",
      "start": 15916.35,
      "duration": 0.99
    },
    {
      "text": "and that a lot of systems\nare gonna converge on.",
      "start": 15917.34,
      "duration": 2.25
    },
    {
      "text": "That would be my kind of, you know,",
      "start": 15919.59,
      "duration": 2.07
    },
    {
      "text": "I don't know anything about neuroscience.",
      "start": 15921.66,
      "duration": 1.68
    },
    {
      "text": "This is just my kind of wild speculation",
      "start": 15923.34,
      "duration": 2.61
    },
    {
      "text": "from what we've seen.",
      "start": 15925.95,
      "duration": 1.02
    },
    {
      "text": "- Yeah, that would be beautiful\nif it's sort of agnostic",
      "start": 15926.97,
      "duration": 3.24
    },
    {
      "text": "to the medium of the model that's used",
      "start": 15930.21,
      "duration": 3.78
    },
    {
      "text": "to form the representation.",
      "start": 15933.99,
      "duration": 1.44
    },
    {
      "text": "- Yeah, yeah, and, you know,",
      "start": 15935.43,
      "duration": 1.715
    },
    {
      "text": "it's a kind of a wild speculation based,",
      "start": 15937.145,
      "duration": 3.152
    },
    {
      "text": "you know, we only have some,",
      "start": 15940.297,
      "duration": 1.553
    },
    {
      "text": "a few data points that suggest this,",
      "start": 15941.85,
      "duration": 1.143
    },
    {
      "text": "but you know, it does seem like",
      "start": 15942.993,
      "duration": 2.017
    },
    {
      "text": "there's some sense in\nwhich the same things form",
      "start": 15945.93,
      "duration": 1.56
    },
    {
      "text": "again and again in both,",
      "start": 15947.49,
      "duration": 1.95
    },
    {
      "text": "in certainly in natural neural networks",
      "start": 15949.44,
      "duration": 1.53
    },
    {
      "text": "and also artificially or in biology.",
      "start": 15950.97,
      "duration": 2.1
    },
    {
      "text": "- And the intuition behind\nthat would be that, you know,",
      "start": 15953.07,
      "duration": 3.42
    },
    {
      "text": "in order to be useful in\nunderstanding the real world,",
      "start": 15956.49,
      "duration": 3.06
    },
    {
      "text": "you need all the same kind of stuff.",
      "start": 15959.55,
      "duration": 1.98
    },
    {
      "text": "- Yeah, well if we pick, I don't know,",
      "start": 15961.53,
      "duration": 1.41
    },
    {
      "text": "like the idea of a dog, right?",
      "start": 15962.94,
      "duration": 1.2
    },
    {
      "text": "Like, you know, there's\nsome sense in which the idea",
      "start": 15964.14,
      "duration": 2.37
    },
    {
      "text": "of a dog is like a natural category",
      "start": 15966.51,
      "duration": 3.69
    },
    {
      "text": "in the universe or\nsomething like this, right?",
      "start": 15970.2,
      "duration": 1.53
    },
    {
      "text": "Like, you know, there's some reason,",
      "start": 15971.73,
      "duration": 3.09
    },
    {
      "text": "it's not just like a weird\nquirk of like how humans factor,",
      "start": 15974.82,
      "duration": 3.33
    },
    {
      "text": "you know, think about the world that",
      "start": 15978.15,
      "duration": 1.14
    },
    {
      "text": "we have this concept of a dog.",
      "start": 15979.29,
      "duration": 1.47
    },
    {
      "text": "It's in some sense...",
      "start": 15980.76,
      "duration": 1.32
    },
    {
      "text": "Or like, if you have the idea of a line,",
      "start": 15982.08,
      "duration": 1.89
    },
    {
      "text": "like there's, you know,\nlike look around us,",
      "start": 15983.97,
      "duration": 1.86
    },
    {
      "text": "you know, there are lines, you know.",
      "start": 15985.83,
      "duration": 1.863
    },
    {
      "text": "It's sort of the simplest way",
      "start": 15987.693,
      "duration": 1.257
    },
    {
      "text": "to understand this room in some sense",
      "start": 15988.95,
      "duration": 1.86
    },
    {
      "text": "is to have the idea of a line.",
      "start": 15990.81,
      "duration": 1.41
    },
    {
      "text": "And so I think that",
      "start": 15992.22,
      "duration": 2.31
    },
    {
      "text": "that would be my instinct\nfor why this happens.",
      "start": 15994.53,
      "duration": 2.34
    },
    {
      "text": "- Yeah, you need a curved line, you know,",
      "start": 15996.87,
      "duration": 1.62
    },
    {
      "text": "to understand a circle,",
      "start": 15998.49,
      "duration": 1.17
    },
    {
      "text": "and you need all those shapes",
      "start": 15999.66,
      "duration": 1.29
    },
    {
      "text": "to understand bigger things",
      "start": 16000.95,
      "duration": 1.11
    },
    {
      "text": "and it's a hierarchy of\nconcepts that are formed, yeah.",
      "start": 16002.06,
      "duration": 2.651
    },
    {
      "text": "- And like maybe there are ways to go",
      "start": 16004.711,
      "duration": 0.946
    },
    {
      "text": "and describe, you know,",
      "start": 16005.657,
      "duration": 1.203
    },
    {
      "text": "images without reference\nto those things, right?",
      "start": 16006.86,
      "duration": 1.53
    },
    {
      "text": "But they're not the simplest way",
      "start": 16008.39,
      "duration": 1.05
    },
    {
      "text": "or the most economical way\nor something like this.",
      "start": 16009.44,
      "duration": 1.647
    },
    {
      "text": "And so systems converge\nto these strategies",
      "start": 16011.087,
      "duration": 4.263
    },
    {
      "text": "would be my wild hypothesis.",
      "start": 16015.35,
      "duration": 2.19
    },
    {
      "text": "- Can you talk through\nsome of the building blocks",
      "start": 16017.54,
      "duration": 1.89
    },
    {
      "text": "that we've been referencing\nof features and circuits?",
      "start": 16019.43,
      "duration": 3.12
    },
    {
      "text": "So I think you first described them",
      "start": 16022.55,
      "duration": 2.0
    },
    {
      "text": "in 2020 paper \"Zoom In: An\nIntroduction to Circuits.\"",
      "start": 16024.55,
      "duration": 3.82
    },
    {
      "text": "- Absolutely, so maybe I'll start",
      "start": 16028.37,
      "duration": 3.45
    },
    {
      "text": "by just describing some phenomena,",
      "start": 16031.82,
      "duration": 2.16
    },
    {
      "text": "and then we can sort of build to the idea",
      "start": 16033.98,
      "duration": 2.16
    },
    {
      "text": "of features and circuits.",
      "start": 16036.14,
      "duration": 1.14
    },
    {
      "text": "- [Lex] Wonderful.",
      "start": 16037.28,
      "duration": 1.47
    },
    {
      "text": "- If you spent like quite a few years,",
      "start": 16038.75,
      "duration": 2.88
    },
    {
      "text": "maybe like five years to some extent",
      "start": 16041.63,
      "duration": 1.93
    },
    {
      "text": "with other things, studying\nthis one particular model",
      "start": 16044.51,
      "duration": 2.25
    },
    {
      "text": "Inception V1, which is\nthis one vision model.",
      "start": 16046.76,
      "duration": 2.49
    },
    {
      "text": "It was state of the art in 2015",
      "start": 16049.25,
      "duration": 3.15
    },
    {
      "text": "and, you know, very much not\nstate of the art anymore.",
      "start": 16052.4,
      "duration": 3.397
    },
    {
      "text": "(Lex laughs)",
      "start": 16055.797,
      "duration": 1.583
    },
    {
      "text": "And it has, you know, maybe\nabout 10,000 neurons in it.",
      "start": 16057.38,
      "duration": 2.247
    },
    {
      "text": "And I spent a lot of time looking at",
      "start": 16059.627,
      "duration": 1.16
    },
    {
      "text": "the 10,000 neurons, odd\nneurons of Inception V1.",
      "start": 16060.787,
      "duration": 3.526
    },
    {
      "text": "And one of the interesting\nthings is, you know,",
      "start": 16066.95,
      "duration": 2.55
    },
    {
      "text": "there are lots of neurons",
      "start": 16069.5,
      "duration": 0.833
    },
    {
      "text": "that don't have some obvious\ninterpretable meaning,",
      "start": 16070.333,
      "duration": 2.407
    },
    {
      "text": "but there's a lot of neurons",
      "start": 16072.74,
      "duration": 1.02
    },
    {
      "text": "and Inception V1 that do have",
      "start": 16073.76,
      "duration": 1.89
    },
    {
      "text": "really clean interpretable meanings.",
      "start": 16075.65,
      "duration": 3.12
    },
    {
      "text": "So you find neurons\nthat just really do seem",
      "start": 16078.77,
      "duration": 2.61
    },
    {
      "text": "to detect curves,",
      "start": 16081.38,
      "duration": 1.11
    },
    {
      "text": "and you find neurons that\nreally do seem to detect cars,",
      "start": 16082.49,
      "duration": 2.58
    },
    {
      "text": "and car wheels and car windows",
      "start": 16085.07,
      "duration": 2.403
    },
    {
      "text": "and, you know, floppy ears of dogs,",
      "start": 16087.473,
      "duration": 2.127
    },
    {
      "text": "and dogs with long snouts\nfacing to the right,",
      "start": 16089.6,
      "duration": 2.1
    },
    {
      "text": "and dogs with long snouts\nfacing to the left.",
      "start": 16091.7,
      "duration": 1.77
    },
    {
      "text": "And you know, different kinds of,",
      "start": 16093.47,
      "duration": 2.516
    },
    {
      "text": "there's sort of this whole\nbeautiful edge detectors,",
      "start": 16095.986,
      "duration": 1.984
    },
    {
      "text": "line detectors, color contrast detectors,",
      "start": 16097.97,
      "duration": 2.82
    },
    {
      "text": "these beautiful things we call\nhigh/low frequency detectors.",
      "start": 16100.79,
      "duration": 2.1
    },
    {
      "text": "You know, I think looking at it,",
      "start": 16102.89,
      "duration": 1.17
    },
    {
      "text": "I sort of felt like a biologist, you know,",
      "start": 16104.06,
      "duration": 0.987
    },
    {
      "text": "you just, you're looking at this",
      "start": 16105.047,
      "duration": 2.133
    },
    {
      "text": "sort of new world of proteins,",
      "start": 16107.18,
      "duration": 2.13
    },
    {
      "text": "and you're discovering all these",
      "start": 16109.31,
      "duration": 0.833
    },
    {
      "text": "different proteins that interact.",
      "start": 16110.143,
      "duration": 1.69
    },
    {
      "text": "So one way you could try",
      "start": 16113.66,
      "duration": 1.32
    },
    {
      "text": "to understand these models\nis in terms of neurons.",
      "start": 16114.98,
      "duration": 2.61
    },
    {
      "text": "You could try to be like, oh, you know,",
      "start": 16117.59,
      "duration": 1.11
    },
    {
      "text": "there's a dog detecting neuron",
      "start": 16118.7,
      "duration": 1.14
    },
    {
      "text": "and here's a car detecting neuron.",
      "start": 16119.84,
      "duration": 2.37
    },
    {
      "text": "And it turns out you can actually ask",
      "start": 16122.21,
      "duration": 1.14
    },
    {
      "text": "how those connect together.",
      "start": 16123.35,
      "duration": 0.87
    },
    {
      "text": "So you can go and say, oh, you know,",
      "start": 16124.22,
      "duration": 0.833
    },
    {
      "text": "I have this car detecting\nneuron, how is it built?",
      "start": 16125.053,
      "duration": 2.557
    },
    {
      "text": "And it turns out in the previous layer,",
      "start": 16127.61,
      "duration": 1.77
    },
    {
      "text": "it's connected really\nstrongly to a window detector,",
      "start": 16129.38,
      "duration": 1.95
    },
    {
      "text": "and a wheel detector,",
      "start": 16131.33,
      "duration": 0.87
    },
    {
      "text": "and it's sort of car body detector.",
      "start": 16132.2,
      "duration": 1.44
    },
    {
      "text": "And it looks for the window above the car,",
      "start": 16133.64,
      "duration": 2.16
    },
    {
      "text": "and the wheels below,",
      "start": 16135.8,
      "duration": 0.96
    },
    {
      "text": "and the car chrome sort of in the middle,",
      "start": 16136.76,
      "duration": 1.5
    },
    {
      "text": "sort of everywhere but\nespecially on the lower part.",
      "start": 16138.26,
      "duration": 2.46
    },
    {
      "text": "And that's sort of a\nrecipe for a car, right?",
      "start": 16140.72,
      "duration": 2.85
    },
    {
      "text": "Like that is, you know, earlier we said",
      "start": 16143.57,
      "duration": 1.02
    },
    {
      "text": "that the thing we wanted from mech interp",
      "start": 16144.59,
      "duration": 1.617
    },
    {
      "text": "was to get algorithms to go and get,",
      "start": 16146.207,
      "duration": 1.833
    },
    {
      "text": "you know, ask what is\nthe algorithm that runs?",
      "start": 16148.04,
      "duration": 1.95
    },
    {
      "text": "Well, here we're just\nlooking at the weights",
      "start": 16149.99,
      "duration": 1.44
    },
    {
      "text": "of the neuron network and reading off",
      "start": 16151.43,
      "duration": 1.35
    },
    {
      "text": "this kind of recipe for detecting cars.",
      "start": 16152.78,
      "duration": 1.517
    },
    {
      "text": "It's a very simple crude\nrecipe, but it's there.",
      "start": 16154.297,
      "duration": 3.583
    },
    {
      "text": "And so we call that a\ncircuit, this connection.",
      "start": 16157.88,
      "duration": 2.13
    },
    {
      "text": "Well, okay, so the problem is that",
      "start": 16160.01,
      "duration": 3.15
    },
    {
      "text": "not all of the neurons are interpretable,",
      "start": 16163.16,
      "duration": 2.79
    },
    {
      "text": "and there's reason to think,",
      "start": 16165.95,
      "duration": 2.6
    },
    {
      "text": "we can get into this more later,",
      "start": 16168.55,
      "duration": 1.21
    },
    {
      "text": "that there's this\nsuperposition hypothesis.",
      "start": 16169.76,
      "duration": 2.01
    },
    {
      "text": "There's reason to think that sometimes",
      "start": 16171.77,
      "duration": 1.68
    },
    {
      "text": "the right unit to analyze things",
      "start": 16173.45,
      "duration": 1.89
    },
    {
      "text": "in terms of is combinations of neurons.",
      "start": 16175.34,
      "duration": 3.81
    },
    {
      "text": "So sometimes it's not that\nthere's a single neuron",
      "start": 16179.15,
      "duration": 1.98
    },
    {
      "text": "that represents say a car,\nbut it actually turns out",
      "start": 16181.13,
      "duration": 2.97
    },
    {
      "text": "after you detect the car,",
      "start": 16184.1,
      "duration": 1.2
    },
    {
      "text": "the model sort of hides\na little bit of the car",
      "start": 16185.3,
      "duration": 2.31
    },
    {
      "text": "in the following layer",
      "start": 16187.61,
      "duration": 0.87
    },
    {
      "text": "and a bunch of dog detectors.",
      "start": 16188.48,
      "duration": 1.98
    },
    {
      "text": "Why is it doing that?",
      "start": 16190.46,
      "duration": 0.833
    },
    {
      "text": "Well, you know, maybe it just doesn't",
      "start": 16191.293,
      "duration": 1.447
    },
    {
      "text": "wanna do that much work\non cars at that point",
      "start": 16192.74,
      "duration": 3.087
    },
    {
      "text": "and you know, it's sort\nof storing it away to go.",
      "start": 16195.827,
      "duration": 2.45
    },
    {
      "text": "So it turns out then this\nsort of subtle pattern of,",
      "start": 16199.85,
      "duration": 1.867
    },
    {
      "text": "you know, there's all these neurons",
      "start": 16201.717,
      "duration": 1.463
    },
    {
      "text": "that you think are dog detectors",
      "start": 16203.18,
      "duration": 1.08
    },
    {
      "text": "and maybe they're primarily that,",
      "start": 16204.26,
      "duration": 1.47
    },
    {
      "text": "but they all a little bit contribute",
      "start": 16205.73,
      "duration": 1.83
    },
    {
      "text": "to representing a car in that next layer.",
      "start": 16207.56,
      "duration": 2.91
    },
    {
      "text": "Okay, so now we can't really think,",
      "start": 16210.47,
      "duration": 1.98
    },
    {
      "text": "there might still be something that,",
      "start": 16212.45,
      "duration": 1.8
    },
    {
      "text": "I don't know, you could\ncall it like a car concept",
      "start": 16214.25,
      "duration": 2.1
    },
    {
      "text": "or something, but it no longer\ncorresponds to a neuron.",
      "start": 16216.35,
      "duration": 2.34
    },
    {
      "text": "So we need some term for these\nkind of neuron like entities,",
      "start": 16218.69,
      "duration": 3.57
    },
    {
      "text": "these things that we sort of",
      "start": 16222.26,
      "duration": 0.833
    },
    {
      "text": "would've liked the neurons to be,",
      "start": 16223.093,
      "duration": 1.087
    },
    {
      "text": "these idealized neurons,",
      "start": 16224.18,
      "duration": 2.1
    },
    {
      "text": "the things that are the nice neurons,",
      "start": 16226.28,
      "duration": 1.17
    },
    {
      "text": "but also maybe there's\nmore of them somehow hidden",
      "start": 16227.45,
      "duration": 2.04
    },
    {
      "text": "and we call those features.",
      "start": 16229.49,
      "duration": 1.86
    },
    {
      "text": "- And then what are circuits?",
      "start": 16231.35,
      "duration": 1.53
    },
    {
      "text": "- So circuits are these\nconnections of features, right?",
      "start": 16232.88,
      "duration": 2.4
    },
    {
      "text": "So, when we have the car detector",
      "start": 16235.28,
      "duration": 1.95
    },
    {
      "text": "and it's connected to a window detector,",
      "start": 16237.23,
      "duration": 3.12
    },
    {
      "text": "and a wheel detector,",
      "start": 16240.35,
      "duration": 1.05
    },
    {
      "text": "and it looks for the wheels below",
      "start": 16241.4,
      "duration": 1.35
    },
    {
      "text": "and the windows on top, that's a circuit.",
      "start": 16242.75,
      "duration": 3.0
    },
    {
      "text": "So circuits are just\ncollections of features",
      "start": 16245.75,
      "duration": 2.19
    },
    {
      "text": "connected by weights and\nthey implement algorithms.",
      "start": 16247.94,
      "duration": 2.85
    },
    {
      "text": "So they tell us, you know,",
      "start": 16250.79,
      "duration": 1.23
    },
    {
      "text": "how are features used?",
      "start": 16252.02,
      "duration": 1.65
    },
    {
      "text": "How are they built?",
      "start": 16253.67,
      "duration": 1.38
    },
    {
      "text": "How do they connect together?",
      "start": 16255.05,
      "duration": 1.35
    },
    {
      "text": "So maybe it's worth trying to pin down",
      "start": 16256.4,
      "duration": 2.1
    },
    {
      "text": "like what really is the\ncore hypothesis here.",
      "start": 16258.5,
      "duration": 3.327
    },
    {
      "text": "And I think the core\nhypothesis is something we call",
      "start": 16261.827,
      "duration": 2.703
    },
    {
      "text": "the linear representation hypothesis.",
      "start": 16264.53,
      "duration": 2.07
    },
    {
      "text": "So if we think about the\ncar detector, you know,",
      "start": 16266.6,
      "duration": 3.09
    },
    {
      "text": "the more it fires, the more\nwe sort of think of that",
      "start": 16269.69,
      "duration": 1.89
    },
    {
      "text": "as meaning, oh, the model is more",
      "start": 16271.58,
      "duration": 1.35
    },
    {
      "text": "and more confident that a car is present.",
      "start": 16272.93,
      "duration": 3.273
    },
    {
      "text": "Or you know, if it's some\ncombination of neurons",
      "start": 16277.34,
      "duration": 2.13
    },
    {
      "text": "that represent a car, you know,",
      "start": 16279.47,
      "duration": 0.9
    },
    {
      "text": "the more that combination fires,",
      "start": 16280.37,
      "duration": 1.38
    },
    {
      "text": "the more we think the model\nthinks there's a car present.",
      "start": 16281.75,
      "duration": 3.243
    },
    {
      "text": "This doesn't have to be the case, right?",
      "start": 16286.7,
      "duration": 1.14
    },
    {
      "text": "Like you could imagine something\nwhere you have, you know,",
      "start": 16287.84,
      "duration": 2.25
    },
    {
      "text": "you have this car detector neuron",
      "start": 16290.09,
      "duration": 1.77
    },
    {
      "text": "and you think, ah, you know,\nif it fires like, you know,",
      "start": 16291.86,
      "duration": 3.03
    },
    {
      "text": "between one and two, that means one thing,",
      "start": 16294.89,
      "duration": 1.857
    },
    {
      "text": "but it means like totally different",
      "start": 16296.747,
      "duration": 1.443
    },
    {
      "text": "if it's between three and four.",
      "start": 16298.19,
      "duration": 2.071
    },
    {
      "text": "That would be a nonlinear representation.",
      "start": 16300.261,
      "duration": 1.499
    },
    {
      "text": "And in principle that, you\nknow, models could do that.",
      "start": 16301.76,
      "duration": 2.19
    },
    {
      "text": "I think it's sort of\ninefficient for them to do,",
      "start": 16303.95,
      "duration": 1.787
    },
    {
      "text": "if you try to think about",
      "start": 16305.737,
      "duration": 1.123
    },
    {
      "text": "how you'd implement computation like that,",
      "start": 16306.86,
      "duration": 2.01
    },
    {
      "text": "it's kind of an annoying thing to do.",
      "start": 16308.87,
      "duration": 1.23
    },
    {
      "text": "But in principle, models can do that.",
      "start": 16310.1,
      "duration": 1.85
    },
    {
      "text": "So one way to think about the features",
      "start": 16313.19,
      "duration": 3.24
    },
    {
      "text": "and circuits sort of framework\nfor thinking about things",
      "start": 16316.43,
      "duration": 3.18
    },
    {
      "text": "is that we're thinking about\nthings as being linear.",
      "start": 16319.61,
      "duration": 1.77
    },
    {
      "text": "We're thinking about there as being",
      "start": 16321.38,
      "duration": 2.16
    },
    {
      "text": "that if a neuron or a\ncombination neurons fires more,",
      "start": 16323.54,
      "duration": 2.313
    },
    {
      "text": "it's sort of, that means more",
      "start": 16325.853,
      "duration": 1.422
    },
    {
      "text": "of a particular thing being detected.",
      "start": 16327.275,
      "duration": 2.175
    },
    {
      "text": "And then that gives weights\na very clean interpretation",
      "start": 16329.45,
      "duration": 2.52
    },
    {
      "text": "as these edges between these entities,",
      "start": 16331.97,
      "duration": 3.12
    },
    {
      "text": "these features and that\nedge then has a meaning.",
      "start": 16335.09,
      "duration": 4.65
    },
    {
      "text": "So that's in some ways the core thing.",
      "start": 16339.74,
      "duration": 2.223
    },
    {
      "text": "It's like, you know, we can talk about",
      "start": 16342.95,
      "duration": 2.7
    },
    {
      "text": "this sort of outset,\nthe context of neurons.",
      "start": 16345.65,
      "duration": 1.497
    },
    {
      "text": "Are you familiar with\nthe Word2Vec results?",
      "start": 16347.147,
      "duration": 2.854
    },
    {
      "text": "So you have like, you know,",
      "start": 16350.001,
      "duration": 1.109
    },
    {
      "text": "king minus man plus woman equals queen.",
      "start": 16351.11,
      "duration": 2.22
    },
    {
      "text": "Well, the reason you can\ndo that kind of arithmetic",
      "start": 16353.33,
      "duration": 3.33
    },
    {
      "text": "is because you have a\nlinear representation.",
      "start": 16356.66,
      "duration": 1.68
    },
    {
      "text": "- Can you actually explain that\nrepresentation a little bit?",
      "start": 16358.34,
      "duration": 1.547
    },
    {
      "text": "So, first of all, so the feature",
      "start": 16359.887,
      "duration": 2.563
    },
    {
      "text": "is a direction of activation.",
      "start": 16362.45,
      "duration": 2.01
    },
    {
      "text": "- Yeah, exactly.\n- You can do it that way.",
      "start": 16364.46,
      "duration": 1.232
    },
    {
      "text": "Can you do the minus men plus women,",
      "start": 16365.692,
      "duration": 3.808
    },
    {
      "text": "that, the Word2Vec stuff,",
      "start": 16369.5,
      "duration": 1.41
    },
    {
      "text": "can you explain what that is that work?",
      "start": 16370.91,
      "duration": 1.56
    },
    {
      "text": "- [Chris] Yeah, so there's this very-",
      "start": 16372.47,
      "duration": 0.843
    },
    {
      "text": "- It's such a simple, clean explanation",
      "start": 16373.313,
      "duration": 2.247
    },
    {
      "text": "of what we're talking about.",
      "start": 16375.56,
      "duration": 1.2
    },
    {
      "text": "- Exactly, yeah.",
      "start": 16376.76,
      "duration": 0.833
    },
    {
      "text": "So there's this very famous result,",
      "start": 16377.593,
      "duration": 1.638
    },
    {
      "text": "Word2Vec by Tomas Mikolov et al,",
      "start": 16379.231,
      "duration": 2.116
    },
    {
      "text": "and there's been tons of\nfollow-up work exploring this.",
      "start": 16381.347,
      "duration": 3.303
    },
    {
      "text": "So, sometimes we have these,",
      "start": 16384.65,
      "duration": 1.74
    },
    {
      "text": "we create these word embeddings",
      "start": 16386.39,
      "duration": 2.28
    },
    {
      "text": "where we map every word to a vector.",
      "start": 16388.67,
      "duration": 3.39
    },
    {
      "text": "I mean, that in itself, by the way,",
      "start": 16392.06,
      "duration": 1.28
    },
    {
      "text": "is kind of a crazy thing",
      "start": 16393.34,
      "duration": 1.0
    },
    {
      "text": "if you haven't thought\nabout it before, right?",
      "start": 16394.34,
      "duration": 1.14
    },
    {
      "text": "Like we are going in and representing,",
      "start": 16395.48,
      "duration": 2.34
    },
    {
      "text": "we're turning, you know,",
      "start": 16397.82,
      "duration": 2.85
    },
    {
      "text": "like if you just learned",
      "start": 16400.67,
      "duration": 0.84
    },
    {
      "text": "about vectors in physics class, right?",
      "start": 16401.51,
      "duration": 1.68
    },
    {
      "text": "And I'm like, oh, I'm gonna actually turn",
      "start": 16403.19,
      "duration": 1.5
    },
    {
      "text": "every word in the\ndictionary into a vector.",
      "start": 16404.69,
      "duration": 2.49
    },
    {
      "text": "That's kind of a crazy idea, okay.",
      "start": 16407.18,
      "duration": 1.68
    },
    {
      "text": "But you could imagine, you could imagine",
      "start": 16408.86,
      "duration": 3.03
    },
    {
      "text": "all kinds of ways in which you\nmight map words to vectors.",
      "start": 16411.89,
      "duration": 3.6
    },
    {
      "text": "But it seems like when\nwe train neural networks,",
      "start": 16415.49,
      "duration": 2.4
    },
    {
      "text": "they like to go and map words to vectors",
      "start": 16419.09,
      "duration": 2.97
    },
    {
      "text": "to such that they're\nsort of linear structure",
      "start": 16422.06,
      "duration": 3.18
    },
    {
      "text": "in a particular sense,",
      "start": 16425.24,
      "duration": 1.71
    },
    {
      "text": "which is that directions have meaning.",
      "start": 16426.95,
      "duration": 2.76
    },
    {
      "text": "So for instance, there\nwill be some direction",
      "start": 16429.71,
      "duration": 2.58
    },
    {
      "text": "that seems to sort of\ncorrespond to gender,",
      "start": 16432.29,
      "duration": 1.92
    },
    {
      "text": "and male words will be, you\nknow, far in one direction",
      "start": 16434.21,
      "duration": 2.37
    },
    {
      "text": "and female words will\nbe in another direction.",
      "start": 16436.58,
      "duration": 2.76
    },
    {
      "text": "And the linear\nrepresentation hypothesis is,",
      "start": 16439.34,
      "duration": 2.167
    },
    {
      "text": "you could sort of think of it roughly",
      "start": 16441.507,
      "duration": 1.733
    },
    {
      "text": "as saying that that's actually",
      "start": 16443.24,
      "duration": 1.38
    },
    {
      "text": "kind of the fundamental\nthing that's going on,",
      "start": 16444.62,
      "duration": 2.43
    },
    {
      "text": "that everything is just\ndifferent directions",
      "start": 16447.05,
      "duration": 2.07
    },
    {
      "text": "have meanings and adding\ndirection vectors together",
      "start": 16449.12,
      "duration": 3.75
    },
    {
      "text": "can represent concepts.",
      "start": 16452.87,
      "duration": 1.83
    },
    {
      "text": "And the Mikolov paper sort\nof took that idea seriously,",
      "start": 16454.7,
      "duration": 2.91
    },
    {
      "text": "and one consequence of it is that",
      "start": 16457.61,
      "duration": 1.74
    },
    {
      "text": "you can do this game of playing",
      "start": 16459.35,
      "duration": 1.29
    },
    {
      "text": "sort of arithmetic with words.",
      "start": 16460.64,
      "duration": 1.19
    },
    {
      "text": "So you can do king and you can, you know,",
      "start": 16461.83,
      "duration": 2.29
    },
    {
      "text": "subtract off the word man\nand add the word woman.",
      "start": 16464.12,
      "duration": 2.52
    },
    {
      "text": "And so you're sort of, you know,",
      "start": 16466.64,
      "duration": 1.23
    },
    {
      "text": "going and trying to switch the gender.",
      "start": 16467.87,
      "duration": 1.74
    },
    {
      "text": "And indeed if you do that, the result will",
      "start": 16469.61,
      "duration": 1.283
    },
    {
      "text": "sort of be close to the word queen.",
      "start": 16470.893,
      "duration": 2.287
    },
    {
      "text": "And you can, you know, do other things",
      "start": 16473.18,
      "duration": 1.83
    },
    {
      "text": "like you can do, you know,",
      "start": 16475.01,
      "duration": 1.95
    },
    {
      "text": "sushi minus Japan plus Italy and get pizza",
      "start": 16476.96,
      "duration": 3.42
    },
    {
      "text": "or different things like this, right?",
      "start": 16480.38,
      "duration": 2.703
    },
    {
      "text": "So, this is in some sense",
      "start": 16484.37,
      "duration": 1.59
    },
    {
      "text": "the core of the linear\nrepresentation hypothesis.",
      "start": 16485.96,
      "duration": 1.92
    },
    {
      "text": "You can describe it",
      "start": 16487.88,
      "duration": 0.833
    },
    {
      "text": "just as a purely abstract thing.",
      "start": 16488.713,
      "duration": 1.207
    },
    {
      "text": "But vector spaces, you can\ndescribe it as a statement",
      "start": 16489.92,
      "duration": 2.65
    },
    {
      "text": "about the activations of neurons.",
      "start": 16493.49,
      "duration": 2.4
    },
    {
      "text": "But it's really about this property",
      "start": 16495.89,
      "duration": 2.7
    },
    {
      "text": "of directions having meaning.",
      "start": 16498.59,
      "duration": 1.197
    },
    {
      "text": "And in some ways it's\neven a little subtle that",
      "start": 16499.787,
      "duration": 2.343
    },
    {
      "text": "it's really I think\nmostly about this property",
      "start": 16502.13,
      "duration": 2.1
    },
    {
      "text": "of being able to add things together",
      "start": 16504.23,
      "duration": 2.64
    },
    {
      "text": "that you can sort of independently modify",
      "start": 16506.87,
      "duration": 2.08
    },
    {
      "text": "say gender and royalty",
      "start": 16509.87,
      "duration": 1.68
    },
    {
      "text": "or, you know, cuisine type or country,",
      "start": 16511.55,
      "duration": 3.93
    },
    {
      "text": "and the concept of food by adding them.",
      "start": 16515.48,
      "duration": 2.91
    },
    {
      "text": "- Do you think the\nlinear hypothesis holds-",
      "start": 16518.39,
      "duration": 2.58
    },
    {
      "text": "- Yes.\n- That kind of carries scales.",
      "start": 16520.97,
      "duration": 3.06
    },
    {
      "text": "- So, so far, I think\neverything I have seen",
      "start": 16524.03,
      "duration": 2.43
    },
    {
      "text": "is consistent with the hypothesis,",
      "start": 16526.46,
      "duration": 1.35
    },
    {
      "text": "and it doesn't have to be that way, right?",
      "start": 16527.81,
      "duration": 1.41
    },
    {
      "text": "Like you can write down neural networks",
      "start": 16529.22,
      "duration": 2.25
    },
    {
      "text": "where you write weights such that",
      "start": 16531.47,
      "duration": 2.34
    },
    {
      "text": "they don't have linear representations,",
      "start": 16533.81,
      "duration": 2.16
    },
    {
      "text": "where the right way to understand them",
      "start": 16535.97,
      "duration": 1.8
    },
    {
      "text": "is not in terms of linear representations.",
      "start": 16537.77,
      "duration": 1.47
    },
    {
      "text": "But I think every natural neural network",
      "start": 16539.24,
      "duration": 2.19
    },
    {
      "text": "I've seen has this property.",
      "start": 16541.43,
      "duration": 3.15
    },
    {
      "text": "There's been one paper recently",
      "start": 16544.58,
      "duration": 1.75
    },
    {
      "text": "that there's been some sort\nof pushing around the edge.",
      "start": 16547.82,
      "duration": 3.06
    },
    {
      "text": "So I think there's been some work",
      "start": 16550.88,
      "duration": 1.44
    },
    {
      "text": "recently studying\nmulti-dimensional features",
      "start": 16552.32,
      "duration": 1.71
    },
    {
      "text": "where rather than a single direction,",
      "start": 16554.03,
      "duration": 1.56
    },
    {
      "text": "it's more like a manifold of directions.",
      "start": 16555.59,
      "duration": 3.3
    },
    {
      "text": "This to me still seems like\na linear representation.",
      "start": 16558.89,
      "duration": 2.88
    },
    {
      "text": "And then there's been some other papers",
      "start": 16561.77,
      "duration": 2.16
    },
    {
      "text": "suggesting that maybe\nin very small models,",
      "start": 16563.93,
      "duration": 3.51
    },
    {
      "text": "you get non-linear representations.",
      "start": 16567.44,
      "duration": 2.34
    },
    {
      "text": "I think that the jury's still out on that.",
      "start": 16569.78,
      "duration": 2.1
    },
    {
      "text": "But I think everything",
      "start": 16572.87,
      "duration": 1.47
    },
    {
      "text": "that we've seen so far has been consistent",
      "start": 16574.34,
      "duration": 1.68
    },
    {
      "text": "with the linear representation\nhypothesis and that's wild.",
      "start": 16576.02,
      "duration": 2.04
    },
    {
      "text": "It doesn't have to be that way,",
      "start": 16578.06,
      "duration": 1.86
    },
    {
      "text": "and yet I think that\nthere's a lot of evidence",
      "start": 16579.92,
      "duration": 2.7
    },
    {
      "text": "that certainly at least this\nis very, very widespread,",
      "start": 16582.62,
      "duration": 2.58
    },
    {
      "text": "and so far the evidence\nis consistent with it.",
      "start": 16585.2,
      "duration": 2.4
    },
    {
      "text": "And I think, you know,\none thing you might say",
      "start": 16587.6,
      "duration": 3.06
    },
    {
      "text": "is you might say, well, Christopher,",
      "start": 16590.66,
      "duration": 1.472
    },
    {
      "text": "you know, that's a lot, you know,",
      "start": 16592.132,
      "duration": 1.836
    },
    {
      "text": "to go and sort of to ride on.",
      "start": 16593.968,
      "duration": 1.189
    },
    {
      "text": "You know, if we don't know\nfor sure this is true,",
      "start": 16595.157,
      "duration": 2.553
    },
    {
      "text": "and you're sort of, you know,",
      "start": 16597.71,
      "duration": 1.08
    },
    {
      "text": "you're investing in neural networks",
      "start": 16598.79,
      "duration": 1.08
    },
    {
      "text": "as though it is true, you\nknow, isn't that dangerous?",
      "start": 16599.87,
      "duration": 2.79
    },
    {
      "text": "Well, you know, but I think actually,",
      "start": 16602.66,
      "duration": 2.61
    },
    {
      "text": "there's a virtue in taking\nhypotheses seriously",
      "start": 16605.27,
      "duration": 2.67
    },
    {
      "text": "and pushing them as far as they can go.",
      "start": 16607.94,
      "duration": 2.49
    },
    {
      "text": "So it might be that someday\nwe discover something",
      "start": 16610.43,
      "duration": 2.49
    },
    {
      "text": "that isn't consistent with\nlinear representation hypothesis.",
      "start": 16612.92,
      "duration": 2.31
    },
    {
      "text": "But science is full of hypotheses",
      "start": 16615.23,
      "duration": 2.37
    },
    {
      "text": "and theories that were wrong,",
      "start": 16617.6,
      "duration": 1.32
    },
    {
      "text": "and we learned a lot by\nsort of working under them",
      "start": 16618.92,
      "duration": 4.08
    },
    {
      "text": "as a sort of an assumption,",
      "start": 16623.0,
      "duration": 1.86
    },
    {
      "text": "and then going and pushing\nthem as far as we can.",
      "start": 16624.86,
      "duration": 2.33
    },
    {
      "text": "I guess this is sort of the heart",
      "start": 16627.19,
      "duration": 1.27
    },
    {
      "text": "of what Kuhn would call normal science.",
      "start": 16628.46,
      "duration": 3.45
    },
    {
      "text": "I dunno if you want, we\ncan talk a lot about-",
      "start": 16631.91,
      "duration": 2.67
    },
    {
      "text": "- Kuhn.\n- Philosophy of science and-",
      "start": 16634.58,
      "duration": 1.77
    },
    {
      "text": "- That leads to the paradigm shift.",
      "start": 16636.35,
      "duration": 1.83
    },
    {
      "text": "So yeah, I love it, taking\nthe hypothesis seriously,",
      "start": 16638.18,
      "duration": 2.55
    },
    {
      "text": "and take it to a natural conclusion.",
      "start": 16640.73,
      "duration": 2.46
    },
    {
      "text": "Same with the Scaling Hypothesis, same-",
      "start": 16643.19,
      "duration": 2.28
    },
    {
      "text": "- Exactly. Exactly.\n- I love it.",
      "start": 16645.47,
      "duration": 1.56
    },
    {
      "text": "- One of my colleagues, Tom Henighan,",
      "start": 16647.03,
      "duration": 1.68
    },
    {
      "text": "who is a former physicist,",
      "start": 16648.71,
      "duration": 1.773
    },
    {
      "text": "like made this really nice analogy to me",
      "start": 16651.62,
      "duration": 2.4
    },
    {
      "text": "of caloric theory where, you know,",
      "start": 16654.02,
      "duration": 3.75
    },
    {
      "text": "once upon a time we thought\nthat heat was actually,",
      "start": 16657.77,
      "duration": 2.667
    },
    {
      "text": "you know, this thing called caloric,",
      "start": 16660.437,
      "duration": 1.983
    },
    {
      "text": "and like the reason,\nyou know, hot objects,",
      "start": 16662.42,
      "duration": 1.86
    },
    {
      "text": "you know, would warm up,",
      "start": 16664.28,
      "duration": 1.53
    },
    {
      "text": "cool objects is like the\ncaloric is flowing through them.",
      "start": 16665.81,
      "duration": 3.15
    },
    {
      "text": "And like, you know, because we're so used",
      "start": 16668.96,
      "duration": 1.74
    },
    {
      "text": "to thinking about heat, you know,",
      "start": 16670.7,
      "duration": 1.56
    },
    {
      "text": "in terms of the modern theory, you know,",
      "start": 16672.26,
      "duration": 2.34
    },
    {
      "text": "that seems kind of silly.",
      "start": 16674.6,
      "duration": 0.93
    },
    {
      "text": "But it's actually very hard\nto construct an experiment",
      "start": 16675.53,
      "duration": 3.18
    },
    {
      "text": "that sort of disproves\nthe caloric hypothesis.",
      "start": 16678.71,
      "duration": 4.47
    },
    {
      "text": "And, you know, you can actually do",
      "start": 16683.18,
      "duration": 1.38
    },
    {
      "text": "a lot of really useful\nwork believing in caloric.",
      "start": 16684.56,
      "duration": 2.64
    },
    {
      "text": "For example, it turns out that",
      "start": 16687.2,
      "duration": 1.62
    },
    {
      "text": "the original combustion\nengines were developed",
      "start": 16688.82,
      "duration": 2.07
    },
    {
      "text": "by people who believed\nin the caloric theory.",
      "start": 16690.89,
      "duration": 2.13
    },
    {
      "text": "So I think there's a virtue",
      "start": 16693.02,
      "duration": 1.62
    },
    {
      "text": "in taking hypotheses seriously,",
      "start": 16694.64,
      "duration": 1.86
    },
    {
      "text": "even when they might be wrong.",
      "start": 16696.5,
      "duration": 1.23
    },
    {
      "text": "- Yeah, there's a deep\nphilosophical truth to that.",
      "start": 16697.73,
      "duration": 3.27
    },
    {
      "text": "That's kind of like how I\nfeel about space travel,",
      "start": 16701.0,
      "duration": 2.88
    },
    {
      "text": "like colonizing Mars.",
      "start": 16703.88,
      "duration": 1.62
    },
    {
      "text": "There's a lot of people\nthat criticize that.",
      "start": 16705.5,
      "duration": 1.043
    },
    {
      "text": "I think if you just assume\nwe have to colonize Mars",
      "start": 16706.543,
      "duration": 3.037
    },
    {
      "text": "in order to have a backup\nfor human civilization,",
      "start": 16709.58,
      "duration": 2.85
    },
    {
      "text": "even if that's not true,\nthat's gonna produce",
      "start": 16712.43,
      "duration": 1.86
    },
    {
      "text": "some interesting engineering",
      "start": 16714.29,
      "duration": 3.06
    },
    {
      "text": "and even scientific\nbreakthroughs, I think.",
      "start": 16717.35,
      "duration": 1.77
    },
    {
      "text": "- Yeah, well, and actually\nthis is another thing",
      "start": 16719.12,
      "duration": 1.77
    },
    {
      "text": "that I think is really interesting.",
      "start": 16720.89,
      "duration": 1.08
    },
    {
      "text": "So, you know, there's a way",
      "start": 16721.97,
      "duration": 2.67
    },
    {
      "text": "in which I think it can be really useful",
      "start": 16724.64,
      "duration": 2.37
    },
    {
      "text": "for society to have people\nalmost irrationally dedicated",
      "start": 16727.01,
      "duration": 5.0
    },
    {
      "text": "to investigating particular hypotheses",
      "start": 16732.2,
      "duration": 2.58
    },
    {
      "text": "because, well, it takes a lot to sort",
      "start": 16734.78,
      "duration": 3.24
    },
    {
      "text": "of maintain scientific morale",
      "start": 16738.02,
      "duration": 1.86
    },
    {
      "text": "and really push on\nsomething when, you know,",
      "start": 16739.88,
      "duration": 1.65
    },
    {
      "text": "most scientific hypotheses\nend up being wrong.",
      "start": 16741.53,
      "duration": 2.76
    },
    {
      "text": "You know, a lot of\nscience doesn't work out.",
      "start": 16744.29,
      "duration": 2.283
    },
    {
      "text": "And yet it's, you know,",
      "start": 16748.76,
      "duration": 1.016
    },
    {
      "text": "it's very useful to just, you know,",
      "start": 16749.776,
      "duration": 2.764
    },
    {
      "text": "there's a joke about Jeff Hinton,",
      "start": 16752.54,
      "duration": 2.16
    },
    {
      "text": "which is that Jeff Hinton has discovered",
      "start": 16754.7,
      "duration": 2.55
    },
    {
      "text": "how the brain works every\nyear for the last 50 years.",
      "start": 16757.25,
      "duration": 3.18
    },
    {
      "text": "But you know, I say that\nwith like, you know,",
      "start": 16760.43,
      "duration": 4.29
    },
    {
      "text": "with really deep respect",
      "start": 16764.72,
      "duration": 0.833
    },
    {
      "text": "because in fact that's actually, you know,",
      "start": 16765.553,
      "duration": 1.687
    },
    {
      "text": "that led to him doing some\nsome really great work.",
      "start": 16767.24,
      "duration": 2.31
    },
    {
      "text": "- Yeah, he won the Nobel Prize.",
      "start": 16769.55,
      "duration": 1.47
    },
    {
      "text": "Now who's laughing now?",
      "start": 16771.02,
      "duration": 1.271
    },
    {
      "text": "- [Chris] Exactly, exactly, exactly.",
      "start": 16772.291,
      "duration": 1.567
    },
    {
      "text": "- Yeah.",
      "start": 16773.858,
      "duration": 0.852
    },
    {
      "text": "- I think one wants to be able to pop up",
      "start": 16774.71,
      "duration": 1.26
    },
    {
      "text": "and sort of recognize the\nappropriate level of confidence.",
      "start": 16775.97,
      "duration": 3.15
    },
    {
      "text": "But I think there's also a lot of value",
      "start": 16779.12,
      "duration": 1.62
    },
    {
      "text": "and just being like, you know,",
      "start": 16780.74,
      "duration": 1.86
    },
    {
      "text": "I'm going to essentially assume,",
      "start": 16782.6,
      "duration": 1.92
    },
    {
      "text": "I'm gonna condition on this problem",
      "start": 16784.52,
      "duration": 1.44
    },
    {
      "text": "being possible or this being\nbroadly the right approach,",
      "start": 16785.96,
      "duration": 3.81
    },
    {
      "text": "and I'm just gonna go and\nassume that for a while",
      "start": 16789.77,
      "duration": 2.37
    },
    {
      "text": "and go and work within that\nand push really hard on it.",
      "start": 16792.14,
      "duration": 3.723
    },
    {
      "text": "And, you know, society has lots of people",
      "start": 16796.7,
      "duration": 1.8
    },
    {
      "text": "doing that for different things.",
      "start": 16798.5,
      "duration": 2.28
    },
    {
      "text": "That's actually really useful",
      "start": 16800.78,
      "duration": 1.05
    },
    {
      "text": "in terms of going and getting to,",
      "start": 16801.83,
      "duration": 3.093
    },
    {
      "text": "you know, either really\nruling things out, right?",
      "start": 16807.44,
      "duration": 2.49
    },
    {
      "text": "We can be like, well, you\nknow, that didn't work",
      "start": 16809.93,
      "duration": 1.167
    },
    {
      "text": "and we know that somebody tried hard.",
      "start": 16811.097,
      "duration": 2.313
    },
    {
      "text": "Or going in and getting to something that",
      "start": 16813.41,
      "duration": 1.94
    },
    {
      "text": "it does teach us\nsomething about the world.",
      "start": 16815.35,
      "duration": 1.78
    },
    {
      "text": "- So another interesting hypothesis",
      "start": 16817.13,
      "duration": 1.37
    },
    {
      "text": "is the superposition hypothesis.",
      "start": 16818.5,
      "duration": 2.11
    },
    {
      "text": "Can you describe what superposition is?",
      "start": 16820.61,
      "duration": 1.75
    },
    {
      "text": "- Yeah, so earlier we were\ntalking about word defect, right?",
      "start": 16822.36,
      "duration": 2.36
    },
    {
      "text": "And we were talking about how, you know,",
      "start": 16824.72,
      "duration": 1.08
    },
    {
      "text": "maybe you have one direction\nthat corresponds to gender,",
      "start": 16825.8,
      "duration": 2.31
    },
    {
      "text": "and maybe another that\ncorresponds to royalty,",
      "start": 16828.11,
      "duration": 2.16
    },
    {
      "text": "and another one that corresponds to Italy,",
      "start": 16830.27,
      "duration": 1.98
    },
    {
      "text": "and another one that\ncorresponds to, you know,",
      "start": 16832.25,
      "duration": 1.77
    },
    {
      "text": "food and all of these things.",
      "start": 16834.02,
      "duration": 1.41
    },
    {
      "text": "Well, you know, oftentimes\nmaybe these word embedding,",
      "start": 16835.43,
      "duration": 4.41
    },
    {
      "text": "they might be 500\ndimensions, 1000 dimensions.",
      "start": 16839.84,
      "duration": 3.42
    },
    {
      "text": "And so if you believe that",
      "start": 16843.26,
      "duration": 0.93
    },
    {
      "text": "all of those directions were orthogonal,",
      "start": 16844.19,
      "duration": 2.0
    },
    {
      "text": "then you could only have,\nyou know, 500 concepts.",
      "start": 16847.37,
      "duration": 2.64
    },
    {
      "text": "And you know, I love pizza, but like,",
      "start": 16850.01,
      "duration": 2.61
    },
    {
      "text": "if I was gonna go and like give",
      "start": 16852.62,
      "duration": 1.56
    },
    {
      "text": "the like 500 most important concepts in,",
      "start": 16854.18,
      "duration": 3.399
    },
    {
      "text": "you know, the English language,\nprobably Italy wouldn't be,",
      "start": 16857.579,
      "duration": 3.201
    },
    {
      "text": "it's not obvious at least",
      "start": 16860.78,
      "duration": 0.833
    },
    {
      "text": "that Italy would be one of them, right?",
      "start": 16861.613,
      "duration": 1.027
    },
    {
      "text": "Because you have to have things",
      "start": 16862.64,
      "duration": 0.833
    },
    {
      "text": "like plural, and singular,",
      "start": 16863.473,
      "duration": 2.677
    },
    {
      "text": "and verb, and noun, and adjective,",
      "start": 16866.15,
      "duration": 3.96
    },
    {
      "text": "and, you know, there's a lot of things",
      "start": 16870.11,
      "duration": 1.86
    },
    {
      "text": "we have to get to before\nwe get to Italy, and Japan,",
      "start": 16871.97,
      "duration": 4.23
    },
    {
      "text": "and, you know, there's a lot\nof countries in the world.",
      "start": 16876.2,
      "duration": 2.76
    },
    {
      "text": "And so how might it be that\nmodels could, you know,",
      "start": 16878.96,
      "duration": 4.14
    },
    {
      "text": "simultaneously have the linear\nrepresentation hypothesis",
      "start": 16883.1,
      "duration": 2.13
    },
    {
      "text": "be true and also represent more things",
      "start": 16885.23,
      "duration": 3.84
    },
    {
      "text": "than they have directions.",
      "start": 16889.07,
      "duration": 1.23
    },
    {
      "text": "So, what does that mean?",
      "start": 16890.3,
      "duration": 0.93
    },
    {
      "text": "Well, okay, so if linear\nrepresentation hypothesis is true,",
      "start": 16891.23,
      "duration": 2.85
    },
    {
      "text": "something interesting has to be going on.",
      "start": 16894.08,
      "duration": 2.13
    },
    {
      "text": "Now, I'll tell you one\nmore interesting thing",
      "start": 16896.21,
      "duration": 2.25
    },
    {
      "text": "before we go and we do\nthat, which is, you know,",
      "start": 16898.46,
      "duration": 3.36
    },
    {
      "text": "earlier we were talking about",
      "start": 16901.82,
      "duration": 0.833
    },
    {
      "text": "all these polysemantic neurons, right?",
      "start": 16902.653,
      "duration": 2.137
    },
    {
      "text": "These neurons that, you know,",
      "start": 16904.79,
      "duration": 1.35
    },
    {
      "text": "when we were looking at Inception V1,",
      "start": 16906.14,
      "duration": 1.17
    },
    {
      "text": "there's these nice neurons\nthat like the car detector",
      "start": 16907.31,
      "duration": 1.77
    },
    {
      "text": "and the curve detector\nand so on that respond",
      "start": 16909.08,
      "duration": 1.86
    },
    {
      "text": "to lots of, you know,\nto very coherent things.",
      "start": 16910.94,
      "duration": 2.67
    },
    {
      "text": "But there's lots of neurons that respond",
      "start": 16913.61,
      "duration": 1.14
    },
    {
      "text": "to a bunch of unrelated things,",
      "start": 16914.75,
      "duration": 0.837
    },
    {
      "text": "and that's also an interesting phenomenon.",
      "start": 16915.587,
      "duration": 2.973
    },
    {
      "text": "And it turns out as well\nthat even these neurons",
      "start": 16918.56,
      "duration": 2.64
    },
    {
      "text": "that are really, really clean,",
      "start": 16921.2,
      "duration": 1.26
    },
    {
      "text": "if you look at the weak\nactivations, right?",
      "start": 16922.46,
      "duration": 1.47
    },
    {
      "text": "So if you look at like,\nyou know, the activations",
      "start": 16923.93,
      "duration": 1.8
    },
    {
      "text": "where it's like activating\n5% of the, you know,",
      "start": 16925.73,
      "duration": 4.44
    },
    {
      "text": "of the maximum activation,\nit's really not the core thing",
      "start": 16930.17,
      "duration": 3.06
    },
    {
      "text": "that it's expecting, right?",
      "start": 16933.23,
      "duration": 0.99
    },
    {
      "text": "So if you look at a curve\ndetector, for instance,",
      "start": 16934.22,
      "duration": 2.25
    },
    {
      "text": "and you look at the places\nwhere it's 5% active, you know,",
      "start": 16936.47,
      "duration": 2.85
    },
    {
      "text": "you could interpret it just as noise",
      "start": 16939.32,
      "duration": 1.68
    },
    {
      "text": "or it could be that it's doing\nsomething else there, okay?",
      "start": 16941.0,
      "duration": 2.46
    },
    {
      "text": "So, how could that be?",
      "start": 16943.46,
      "duration": 1.98
    },
    {
      "text": "Well, there's this amazing\nthing in mathematics",
      "start": 16945.44,
      "duration": 5.0
    },
    {
      "text": "called compressed sensing,",
      "start": 16950.84,
      "duration": 1.56
    },
    {
      "text": "and it's actually this\nvery surprising fact",
      "start": 16952.4,
      "duration": 3.39
    },
    {
      "text": "where if you have a high dimensional space",
      "start": 16955.79,
      "duration": 3.0
    },
    {
      "text": "and you project it into\na low dimensional space,",
      "start": 16958.79,
      "duration": 2.4
    },
    {
      "text": "ordinarily you can't go\nand sort of unproject it",
      "start": 16961.19,
      "duration": 3.33
    },
    {
      "text": "and get back your high\ndimensional vector, right?",
      "start": 16964.52,
      "duration": 1.38
    },
    {
      "text": "You threw information away.",
      "start": 16965.9,
      "duration": 1.23
    },
    {
      "text": "This is like, you know,",
      "start": 16967.13,
      "duration": 1.38
    },
    {
      "text": "you can't invert a rectangular matrix,",
      "start": 16968.51,
      "duration": 2.79
    },
    {
      "text": "you can only invert square matrices.",
      "start": 16971.3,
      "duration": 1.8
    },
    {
      "text": "But it turns out that that's\nactually not quite true.",
      "start": 16974.69,
      "duration": 3.69
    },
    {
      "text": "If I tell you that the high\ndimensional vector was sparse,",
      "start": 16978.38,
      "duration": 2.4
    },
    {
      "text": "so it's mostly zeros, then it turns out",
      "start": 16980.78,
      "duration": 2.85
    },
    {
      "text": "that you can often go",
      "start": 16983.63,
      "duration": 0.87
    },
    {
      "text": "and find back the high dimensional vector",
      "start": 16984.5,
      "duration": 4.53
    },
    {
      "text": "with very high probability.",
      "start": 16989.03,
      "duration": 1.833
    },
    {
      "text": "So that's a surprising fact, right?",
      "start": 16992.39,
      "duration": 1.233
    },
    {
      "text": "It says that, you know,",
      "start": 16993.623,
      "duration": 2.047
    },
    {
      "text": "you can have this high\ndimensional vector space,",
      "start": 16995.67,
      "duration": 1.61
    },
    {
      "text": "and as long as things are\nsparse, you can project it down,",
      "start": 16997.28,
      "duration": 3.093
    },
    {
      "text": "you can have a lower\ndimensional projection of it,",
      "start": 17000.373,
      "duration": 2.457
    },
    {
      "text": "and that works.",
      "start": 17002.83,
      "duration": 1.5
    },
    {
      "text": "So the superposition\nhypothesis is saying that",
      "start": 17004.33,
      "duration": 2.34
    },
    {
      "text": "that's what's going on in neural networks.",
      "start": 17006.67,
      "duration": 1.71
    },
    {
      "text": "That's, for instance,",
      "start": 17008.38,
      "duration": 0.833
    },
    {
      "text": "that's what's going on in word embeddings.",
      "start": 17009.213,
      "duration": 1.642
    },
    {
      "text": "That word embeddings are able",
      "start": 17010.855,
      "duration": 1.155
    },
    {
      "text": "to simultaneously have directions\nbe the meaningful thing.",
      "start": 17012.01,
      "duration": 2.91
    },
    {
      "text": "And by exploiting the fact that",
      "start": 17014.92,
      "duration": 1.29
    },
    {
      "text": "they're operating on a fairly\nhigh dimensional space,",
      "start": 17016.21,
      "duration": 2.46
    },
    {
      "text": "they're actually,",
      "start": 17018.67,
      "duration": 0.96
    },
    {
      "text": "and the fact that these\nconcepts are sparse, right?",
      "start": 17019.63,
      "duration": 1.86
    },
    {
      "text": "Like, you know, you usually aren't talking",
      "start": 17021.49,
      "duration": 1.74
    },
    {
      "text": "about Japan and Italy at the same time.",
      "start": 17023.23,
      "duration": 2.85
    },
    {
      "text": "You know, most of those\nconcepts, you know,",
      "start": 17026.08,
      "duration": 1.8
    },
    {
      "text": "in most instances, Japan\nand Italy are both zero.",
      "start": 17027.88,
      "duration": 1.98
    },
    {
      "text": "They're not present at all.",
      "start": 17029.86,
      "duration": 1.35
    },
    {
      "text": "And if that's true, then you can go",
      "start": 17032.047,
      "duration": 3.573
    },
    {
      "text": "and have it be the case that",
      "start": 17035.62,
      "duration": 2.49
    },
    {
      "text": "you can have many more of\nthese sort of directions",
      "start": 17038.11,
      "duration": 3.39
    },
    {
      "text": "that are meaningful, these features",
      "start": 17041.5,
      "duration": 1.71
    },
    {
      "text": "than you have dimensions.",
      "start": 17043.21,
      "duration": 1.56
    },
    {
      "text": "And similarly, when we're\ntalking about neurons,",
      "start": 17044.77,
      "duration": 0.833
    },
    {
      "text": "you can have many more\nconcepts than you have neurons.",
      "start": 17045.603,
      "duration": 4.327
    },
    {
      "text": "So that's, at a high level,\nthe superposition hypothesis.",
      "start": 17049.93,
      "duration": 2.28
    },
    {
      "text": "Now it has this even wilder implication,",
      "start": 17052.21,
      "duration": 2.73
    },
    {
      "text": "which is to go and say\nthat neural networks are,",
      "start": 17054.94,
      "duration": 4.67
    },
    {
      "text": "it may not just be the case that",
      "start": 17061.93,
      "duration": 1.08
    },
    {
      "text": "the representations are like this,",
      "start": 17063.01,
      "duration": 1.47
    },
    {
      "text": "but the computation may\nalso be like this, you know,",
      "start": 17064.48,
      "duration": 2.4
    },
    {
      "text": "the connections between all of them.",
      "start": 17066.88,
      "duration": 1.007
    },
    {
      "text": "And so in some sense, neural\nnetworks may be shadows",
      "start": 17067.887,
      "duration": 4.063
    },
    {
      "text": "of much larger, sparser neural networks,",
      "start": 17071.95,
      "duration": 2.94
    },
    {
      "text": "and what we see are these projection.",
      "start": 17074.89,
      "duration": 2.433
    },
    {
      "text": "And, you know, the strongest version",
      "start": 17078.471,
      "duration": 1.279
    },
    {
      "text": "of the superposition\nhypothesis would be to take",
      "start": 17079.75,
      "duration": 1.53
    },
    {
      "text": "that really seriously and\nsort of say, you know,",
      "start": 17081.28,
      "duration": 2.01
    },
    {
      "text": "there actually is in some\nsense this upstairs model,",
      "start": 17083.29,
      "duration": 2.85
    },
    {
      "text": "this, you know, where the\nneurons are really sparse",
      "start": 17086.14,
      "duration": 3.45
    },
    {
      "text": "and all interpretable,\nand there's, you know,",
      "start": 17089.59,
      "duration": 1.5
    },
    {
      "text": "the weights between them are\nthese really sparse circuits.",
      "start": 17091.09,
      "duration": 2.43
    },
    {
      "text": "And that's what we're studying.",
      "start": 17093.52,
      "duration": 2.373
    },
    {
      "text": "And the thing that we're observing",
      "start": 17097.66,
      "duration": 2.22
    },
    {
      "text": "is the shadow of evidence.",
      "start": 17099.88,
      "duration": 0.93
    },
    {
      "text": "We need to find the original object.",
      "start": 17100.81,
      "duration": 2.37
    },
    {
      "text": "- And the process of learning\nis trying to construct",
      "start": 17103.18,
      "duration": 3.72
    },
    {
      "text": "a compression of the upstairs model",
      "start": 17106.9,
      "duration": 2.4
    },
    {
      "text": "that doesn't lose too much\ninformation in the projection.",
      "start": 17109.3,
      "duration": 2.61
    },
    {
      "text": "- Yeah, it's finding how\nto fit it efficiently",
      "start": 17111.91,
      "duration": 1.8
    },
    {
      "text": "or something like this.",
      "start": 17113.71,
      "duration": 1.77
    },
    {
      "text": "The gradient descent is doing this.",
      "start": 17115.48,
      "duration": 0.833
    },
    {
      "text": "And in fact, so this sort of\nsays that gradient descent,",
      "start": 17116.313,
      "duration": 2.917
    },
    {
      "text": "you know, it could just represent",
      "start": 17119.23,
      "duration": 1.17
    },
    {
      "text": "a dense neural network,",
      "start": 17120.4,
      "duration": 1.05
    },
    {
      "text": "but it sort of says that gradient descent",
      "start": 17121.45,
      "duration": 1.17
    },
    {
      "text": "is implicitly searching over the space",
      "start": 17122.62,
      "duration": 2.07
    },
    {
      "text": "of extremely sparse models",
      "start": 17124.69,
      "duration": 2.25
    },
    {
      "text": "that could be projected into\nthis low dimensional space.",
      "start": 17126.94,
      "duration": 2.88
    },
    {
      "text": "And this large body of work",
      "start": 17129.82,
      "duration": 1.05
    },
    {
      "text": "of people going and trying",
      "start": 17130.87,
      "duration": 1.44
    },
    {
      "text": "to study sparse neural networks, right?",
      "start": 17132.31,
      "duration": 1.35
    },
    {
      "text": "Where you go and you have,",
      "start": 17133.66,
      "duration": 1.02
    },
    {
      "text": "you could design neural networks, right,",
      "start": 17134.68,
      "duration": 1.32
    },
    {
      "text": "where the edges are sparse",
      "start": 17136.0,
      "duration": 1.107
    },
    {
      "text": "and the activations are sparse.",
      "start": 17137.107,
      "duration": 1.383
    },
    {
      "text": "And you know, my sense is\nthat work has generally,",
      "start": 17138.49,
      "duration": 3.03
    },
    {
      "text": "it feels very principled, right?",
      "start": 17141.52,
      "duration": 1.23
    },
    {
      "text": "It makes so much sense.",
      "start": 17142.75,
      "duration": 1.2
    },
    {
      "text": "And yet that work hasn't really panned out",
      "start": 17143.95,
      "duration": 2.37
    },
    {
      "text": "that well is my impression broadly.",
      "start": 17146.32,
      "duration": 2.22
    },
    {
      "text": "And I think that a\npotential answer for that",
      "start": 17148.54,
      "duration": 3.36
    },
    {
      "text": "is that actually the neural network",
      "start": 17151.9,
      "duration": 2.25
    },
    {
      "text": "is already sparse in some sense.",
      "start": 17154.15,
      "duration": 1.53
    },
    {
      "text": "Gradient descent was the whole time",
      "start": 17155.68,
      "duration": 2.31
    },
    {
      "text": "you were trying to go and do this,",
      "start": 17157.99,
      "duration": 0.833
    },
    {
      "text": "gradient descent was actually\nin the behind the scenes",
      "start": 17158.823,
      "duration": 1.987
    },
    {
      "text": "going and searching more\nefficiently than you could",
      "start": 17160.81,
      "duration": 2.04
    },
    {
      "text": "through the space of sparse models,",
      "start": 17162.85,
      "duration": 1.59
    },
    {
      "text": "and going and learning\nwhatever sparse model",
      "start": 17164.44,
      "duration": 2.34
    },
    {
      "text": "was most efficient and then figuring out",
      "start": 17166.78,
      "duration": 2.04
    },
    {
      "text": "how to fold it down nicely to go",
      "start": 17168.82,
      "duration": 1.53
    },
    {
      "text": "and run conveniently on your GPU,",
      "start": 17170.35,
      "duration": 1.65
    },
    {
      "text": "which does, you know, nice,\ndense matrix multiplies,",
      "start": 17172.0,
      "duration": 2.79
    },
    {
      "text": "and that you just can't beat that.",
      "start": 17174.79,
      "duration": 1.68
    },
    {
      "text": "- How many concepts do you think",
      "start": 17176.47,
      "duration": 1.92
    },
    {
      "text": "can be shoved into a neural network?",
      "start": 17178.39,
      "duration": 2.4
    },
    {
      "text": "- Depends on how sparse they are.",
      "start": 17180.79,
      "duration": 1.41
    },
    {
      "text": "So there's probably an upper bound",
      "start": 17182.2,
      "duration": 1.38
    },
    {
      "text": "from the number of parameters, right?",
      "start": 17183.58,
      "duration": 1.65
    },
    {
      "text": "Because you have to have,\nyou still have to have,",
      "start": 17185.23,
      "duration": 1.98
    },
    {
      "text": "you know, weights that go\nand connect them together.",
      "start": 17187.21,
      "duration": 3.15
    },
    {
      "text": "So that's one upper bound.",
      "start": 17190.36,
      "duration": 1.89
    },
    {
      "text": "There are in fact all these lovely results",
      "start": 17192.25,
      "duration": 1.83
    },
    {
      "text": "from compressed sensing",
      "start": 17194.08,
      "duration": 0.96
    },
    {
      "text": "and the Johnson-Lindenstrauss lemma,",
      "start": 17195.04,
      "duration": 2.07
    },
    {
      "text": "and things like this.",
      "start": 17197.11,
      "duration": 1.44
    },
    {
      "text": "That they basically tell you",
      "start": 17198.55,
      "duration": 1.83
    },
    {
      "text": "that if you have a vector space",
      "start": 17200.38,
      "duration": 1.56
    },
    {
      "text": "and you want to have\nalmost orthogonal vectors,",
      "start": 17201.94,
      "duration": 2.67
    },
    {
      "text": "which is sort of probably the thing",
      "start": 17204.61,
      "duration": 1.35
    },
    {
      "text": "that you want here, right?",
      "start": 17205.96,
      "duration": 0.833
    },
    {
      "text": "So you're gonna say, well, you know,",
      "start": 17206.793,
      "duration": 1.477
    },
    {
      "text": "I'm gonna give up on having my concepts,",
      "start": 17208.27,
      "duration": 1.68
    },
    {
      "text": "my features be strictly orthogonal,",
      "start": 17209.95,
      "duration": 1.83
    },
    {
      "text": "but I'd like them to\nnot interfere that much.",
      "start": 17211.78,
      "duration": 1.71
    },
    {
      "text": "I'm gonna have to ask them\nto be almost orthogonal.",
      "start": 17213.49,
      "duration": 2.85
    },
    {
      "text": "Then this would say that\nit's actually, you know,",
      "start": 17216.34,
      "duration": 2.43
    },
    {
      "text": "once you set a threshold",
      "start": 17218.77,
      "duration": 0.9
    },
    {
      "text": "for what you're willing to accept",
      "start": 17219.67,
      "duration": 2.1
    },
    {
      "text": "in terms of how much\ncosine similarity there is,",
      "start": 17221.77,
      "duration": 3.18
    },
    {
      "text": "that's actually exponential",
      "start": 17224.95,
      "duration": 1.1
    },
    {
      "text": "in the number of neurons that you have.",
      "start": 17226.05,
      "duration": 2.11
    },
    {
      "text": "So at some point, that's not gonna",
      "start": 17228.16,
      "duration": 1.02
    },
    {
      "text": "even be the limiting factor.",
      "start": 17229.18,
      "duration": 2.19
    },
    {
      "text": "But there's some beautiful results there.",
      "start": 17231.37,
      "duration": 1.617
    },
    {
      "text": "And in fact, it's probably even better",
      "start": 17232.987,
      "duration": 1.923
    },
    {
      "text": "than that in some sense\nbecause that's sort of,",
      "start": 17234.91,
      "duration": 2.13
    },
    {
      "text": "for saying that, you know, any random set",
      "start": 17237.04,
      "duration": 2.49
    },
    {
      "text": "of features could be active.",
      "start": 17239.53,
      "duration": 1.05
    },
    {
      "text": "But in fact the features have",
      "start": 17240.58,
      "duration": 1.11
    },
    {
      "text": "sort of a correlational\nstructure where some features,",
      "start": 17241.69,
      "duration": 1.796
    },
    {
      "text": "you know, are more likely to co-occur,",
      "start": 17243.486,
      "duration": 2.194
    },
    {
      "text": "and other ones are less\nlikely to co-occur.",
      "start": 17245.68,
      "duration": 1.35
    },
    {
      "text": "And so neural networks, my guess would be",
      "start": 17247.03,
      "duration": 2.217
    },
    {
      "text": "can do very well in terms of going",
      "start": 17249.247,
      "duration": 2.883
    },
    {
      "text": "and packing things in such,",
      "start": 17252.13,
      "duration": 2.31
    },
    {
      "text": "to the point that's probably\nnot the limiting factor.",
      "start": 17254.44,
      "duration": 2.67
    },
    {
      "text": "- How does the problem of polysemanticity",
      "start": 17257.11,
      "duration": 2.64
    },
    {
      "text": "enter the picture here?",
      "start": 17259.75,
      "duration": 1.47
    },
    {
      "text": "- Polysemanticity is this phenomenon",
      "start": 17261.22,
      "duration": 1.53
    },
    {
      "text": "we observe where you look at many neurons,",
      "start": 17262.75,
      "duration": 1.71
    },
    {
      "text": "and the neuron doesn't just\nsort of represent one concept.",
      "start": 17264.46,
      "duration": 3.33
    },
    {
      "text": "It's not a clean feature.",
      "start": 17267.79,
      "duration": 1.56
    },
    {
      "text": "It responds to a bunch\nof unrelated things.",
      "start": 17269.35,
      "duration": 2.4
    },
    {
      "text": "And superposition is,",
      "start": 17271.75,
      "duration": 1.47
    },
    {
      "text": "you can think of as being a hypothesis",
      "start": 17273.22,
      "duration": 1.77
    },
    {
      "text": "that explains the observation\nof polysemanticity.",
      "start": 17274.99,
      "duration": 3.69
    },
    {
      "text": "So polysemanticity is\nthis observed phenomenon",
      "start": 17278.68,
      "duration": 2.67
    },
    {
      "text": "and superposition is a hypothesis that",
      "start": 17281.35,
      "duration": 2.67
    },
    {
      "text": "would explain it along\nwith some other things.",
      "start": 17284.02,
      "duration": 1.61
    },
    {
      "text": "- So that makes mech\ninterp more difficult.",
      "start": 17285.63,
      "duration": 3.13
    },
    {
      "text": "- Right, so if you're\ntrying to understand things",
      "start": 17288.76,
      "duration": 2.01
    },
    {
      "text": "in terms of individual neurons,",
      "start": 17290.77,
      "duration": 1.32
    },
    {
      "text": "and you have polysemantic neurons,",
      "start": 17292.09,
      "duration": 1.08
    },
    {
      "text": "you're in an awful lot of trouble, right?",
      "start": 17293.17,
      "duration": 2.49
    },
    {
      "text": "I mean, the easiest answer is\nlike, okay, well, you know,",
      "start": 17295.66,
      "duration": 1.307
    },
    {
      "text": "you're looking at the neurons,",
      "start": 17296.967,
      "duration": 1.243
    },
    {
      "text": "you're trying to understand them.",
      "start": 17298.21,
      "duration": 0.93
    },
    {
      "text": "This one responds for a lot of things,",
      "start": 17299.14,
      "duration": 1.633
    },
    {
      "text": "it doesn't have a nice meaning.",
      "start": 17300.773,
      "duration": 1.157
    },
    {
      "text": "Okay, you know, that's bad.",
      "start": 17301.93,
      "duration": 2.58
    },
    {
      "text": "Another thing you could ask is, you know,",
      "start": 17304.51,
      "duration": 1.53
    },
    {
      "text": "ultimately, we wanna\nunderstand the weights.",
      "start": 17306.04,
      "duration": 1.56
    },
    {
      "text": "And if you have two polysemantic neurons",
      "start": 17307.6,
      "duration": 2.073
    },
    {
      "text": "and, you know, each one responds",
      "start": 17309.673,
      "duration": 1.347
    },
    {
      "text": "to three things and then, you know,",
      "start": 17311.02,
      "duration": 1.62
    },
    {
      "text": "the other neuron responds to three things",
      "start": 17312.64,
      "duration": 1.555
    },
    {
      "text": "and you have a weight between them,",
      "start": 17314.195,
      "duration": 1.141
    },
    {
      "text": "you know, what does that mean?",
      "start": 17315.336,
      "duration": 0.833
    },
    {
      "text": "Does it mean that like\nall three, you know,",
      "start": 17316.169,
      "duration": 1.631
    },
    {
      "text": "like there's these nine, you know,",
      "start": 17317.8,
      "duration": 1.59
    },
    {
      "text": "nine interactions going on?",
      "start": 17319.39,
      "duration": 1.47
    },
    {
      "text": "It's a very weird thing.",
      "start": 17320.86,
      "duration": 1.14
    },
    {
      "text": "But there's also a deeper reason,",
      "start": 17322.0,
      "duration": 1.89
    },
    {
      "text": "which is related to the fact",
      "start": 17323.89,
      "duration": 1.68
    },
    {
      "text": "that neural networks operate",
      "start": 17325.57,
      "duration": 1.02
    },
    {
      "text": "on really high dimensional spaces.",
      "start": 17326.59,
      "duration": 1.65
    },
    {
      "text": "So I said that our goal was, you know,",
      "start": 17328.24,
      "duration": 1.77
    },
    {
      "text": "to understand neural networks",
      "start": 17330.01,
      "duration": 1.02
    },
    {
      "text": "and understand the mechanisms,",
      "start": 17331.03,
      "duration": 1.44
    },
    {
      "text": "and one thing you might\nsay is like, well, why not?",
      "start": 17332.47,
      "duration": 2.97
    },
    {
      "text": "It's just a mathematical function,",
      "start": 17335.44,
      "duration": 1.02
    },
    {
      "text": "why not just look at it, right?",
      "start": 17336.46,
      "duration": 1.26
    },
    {
      "text": "Like, you know, one of\nthe earliest projects",
      "start": 17337.72,
      "duration": 1.92
    },
    {
      "text": "I did studied these neural networks",
      "start": 17339.64,
      "duration": 1.89
    },
    {
      "text": "that match two dimensional spaces",
      "start": 17341.53,
      "duration": 1.17
    },
    {
      "text": "to two dimensional spaces,",
      "start": 17342.7,
      "duration": 0.987
    },
    {
      "text": "and you can sort of interpret them",
      "start": 17343.687,
      "duration": 1.493
    },
    {
      "text": "as in this beautiful way\nas like bending manifolds.",
      "start": 17345.18,
      "duration": 3.25
    },
    {
      "text": "Why can't we do that?",
      "start": 17348.43,
      "duration": 0.87
    },
    {
      "text": "Well, you know, as you have\na higher dimensional space,",
      "start": 17349.3,
      "duration": 2.793
    },
    {
      "text": "the volume of that space in some senses",
      "start": 17353.2,
      "duration": 1.8
    },
    {
      "text": "is exponential in the\nnumber of inputs you have.",
      "start": 17355.0,
      "duration": 2.297
    },
    {
      "text": "And so you can't just go and visualize it.",
      "start": 17357.297,
      "duration": 2.383
    },
    {
      "text": "So we somehow need to break that apart.",
      "start": 17359.68,
      "duration": 2.64
    },
    {
      "text": "We need to somehow break\nthat exponential space",
      "start": 17362.32,
      "duration": 2.88
    },
    {
      "text": "into a bunch of things that we, you know,",
      "start": 17365.2,
      "duration": 2.52
    },
    {
      "text": "some non-exponential number of things",
      "start": 17367.72,
      "duration": 1.143
    },
    {
      "text": "that we can reason about independently.",
      "start": 17368.863,
      "duration": 3.027
    },
    {
      "text": "And the independence is crucial",
      "start": 17371.89,
      "duration": 1.62
    },
    {
      "text": "because it's the\nindependence that allows you",
      "start": 17373.51,
      "duration": 1.77
    },
    {
      "text": "to not have to think about, you know,",
      "start": 17375.28,
      "duration": 1.17
    },
    {
      "text": "all the exponential\ncombinations of things.",
      "start": 17376.45,
      "duration": 2.76
    },
    {
      "text": "And things being mono-semantic,",
      "start": 17379.21,
      "duration": 3.54
    },
    {
      "text": "things only having one meaning.",
      "start": 17382.75,
      "duration": 1.62
    },
    {
      "text": "Things having a meaning,",
      "start": 17384.37,
      "duration": 1.29
    },
    {
      "text": "that is the key thing that allows you",
      "start": 17385.66,
      "duration": 2.25
    },
    {
      "text": "to think about them independently.",
      "start": 17387.91,
      "duration": 1.14
    },
    {
      "text": "And so I think that's, if\nyou want the deepest reason",
      "start": 17389.05,
      "duration": 2.82
    },
    {
      "text": "why we want to have interpretable\nmono-sematic features,",
      "start": 17391.87,
      "duration": 5.0
    },
    {
      "text": "I think that's really the deep reason.",
      "start": 17396.88,
      "duration": 2.07
    },
    {
      "text": "- And so the goal here,",
      "start": 17398.95,
      "duration": 1.17
    },
    {
      "text": "as your recent work has been aiming at,",
      "start": 17400.12,
      "duration": 2.61
    },
    {
      "text": "is how do we extract the\nmono-semantic features",
      "start": 17402.73,
      "duration": 2.61
    },
    {
      "text": "from a neural net that\nhas poly-sematic features",
      "start": 17405.34,
      "duration": 3.33
    },
    {
      "text": "and all this mess.",
      "start": 17408.67,
      "duration": 1.68
    },
    {
      "text": "- Yes, we observed these\npoly-semantic neurons,",
      "start": 17410.35,
      "duration": 1.377
    },
    {
      "text": "and we hypothesized that's\nwhat's going on is superposition.",
      "start": 17411.727,
      "duration": 3.573
    },
    {
      "text": "And if superposition is\nwhat's going on there,",
      "start": 17415.3,
      "duration": 2.31
    },
    {
      "text": "there's actually a sort of\nwell established technique",
      "start": 17417.61,
      "duration": 2.58
    },
    {
      "text": "that is sort of the\nprincipled thing to do,",
      "start": 17420.19,
      "duration": 2.07
    },
    {
      "text": "which is dictionary learning.",
      "start": 17422.26,
      "duration": 1.62
    },
    {
      "text": "And it turns out if you\ndo dictionary learning,",
      "start": 17423.88,
      "duration": 1.97
    },
    {
      "text": "in particular, if you do\nsort of a nice efficient way",
      "start": 17425.85,
      "duration": 2.503
    },
    {
      "text": "that in some sense sort of\nnicely regularizes it as well",
      "start": 17428.353,
      "duration": 3.087
    },
    {
      "text": "called a sparse auto-encoder.",
      "start": 17431.44,
      "duration": 1.8
    },
    {
      "text": "If you train a sparse auto-encoder,",
      "start": 17433.24,
      "duration": 2.04
    },
    {
      "text": "these beautiful interpretable features",
      "start": 17435.28,
      "duration": 1.41
    },
    {
      "text": "start to just fall out where\nthere weren't any beforehand.",
      "start": 17436.69,
      "duration": 2.61
    },
    {
      "text": "And so that's not a thing",
      "start": 17439.3,
      "duration": 1.41
    },
    {
      "text": "that you would necessarily predict, right?",
      "start": 17440.71,
      "duration": 2.1
    },
    {
      "text": "But it turns out that that\nworks very, very well.",
      "start": 17442.81,
      "duration": 3.15
    },
    {
      "text": "You know, that to me that\nseems like, you know,",
      "start": 17445.96,
      "duration": 1.74
    },
    {
      "text": "some non-trivial validation",
      "start": 17447.7,
      "duration": 1.71
    },
    {
      "text": "of linear representations\nin superposition.",
      "start": 17449.41,
      "duration": 2.28
    },
    {
      "text": "- So with dictionary\nlearning, you're not looking",
      "start": 17451.69,
      "duration": 1.92
    },
    {
      "text": "for particular kind of categories,",
      "start": 17453.61,
      "duration": 1.32
    },
    {
      "text": "you don't know what they are.",
      "start": 17454.93,
      "duration": 1.098
    },
    {
      "text": "They just emerge.\n- Exactly.",
      "start": 17456.028,
      "duration": 1.212
    },
    {
      "text": "And this gets back to\nour earlier point, right?",
      "start": 17457.24,
      "duration": 1.173
    },
    {
      "text": "When we're not making assumptions,",
      "start": 17458.413,
      "duration": 1.917
    },
    {
      "text": "gradient descent is smarter than us,",
      "start": 17460.33,
      "duration": 1.47
    },
    {
      "text": "so we're not making\nassumptions about what's there.",
      "start": 17461.8,
      "duration": 2.64
    },
    {
      "text": "I mean, one certainly\ncould do that, right?",
      "start": 17464.44,
      "duration": 1.41
    },
    {
      "text": "One could assume that\nthere's a PHP feature",
      "start": 17465.85,
      "duration": 2.52
    },
    {
      "text": "and go and search for it,\nbut we're not doing that.",
      "start": 17468.37,
      "duration": 1.95
    },
    {
      "text": "We're saying we don't know\nwhat's gonna be there.",
      "start": 17470.32,
      "duration": 1.77
    },
    {
      "text": "Instead we're just gonna go",
      "start": 17472.09,
      "duration": 1.17
    },
    {
      "text": "and let the sparse auto-encoder",
      "start": 17473.26,
      "duration": 1.74
    },
    {
      "text": "discover the things that are there.",
      "start": 17475.0,
      "duration": 1.68
    },
    {
      "text": "- So can you talk to the\n\"Toward Monosemanticity\" paper",
      "start": 17476.68,
      "duration": 3.69
    },
    {
      "text": "from October last year?",
      "start": 17480.37,
      "duration": 1.4
    },
    {
      "text": "It had a lot of like nice\nbreakthrough results.",
      "start": 17481.77,
      "duration": 2.5
    },
    {
      "text": "- That's very kind of you\nto describe it that way.",
      "start": 17484.27,
      "duration": 1.95
    },
    {
      "text": "Yeah, I mean, this was\nour first real success",
      "start": 17486.22,
      "duration": 5.0
    },
    {
      "text": "using sparse auto-encoders.",
      "start": 17491.59,
      "duration": 1.07
    },
    {
      "text": "So we took a one layer model,",
      "start": 17492.66,
      "duration": 1.513
    },
    {
      "text": "and it turns out if you go and, you know,",
      "start": 17495.04,
      "duration": 1.92
    },
    {
      "text": "do dictionary learning on it,",
      "start": 17496.96,
      "duration": 1.23
    },
    {
      "text": "you find all these really\nnice interpretable features.",
      "start": 17498.19,
      "duration": 2.43
    },
    {
      "text": "So, you know, the Arabic\nfeature, the Hebrew feature,",
      "start": 17500.62,
      "duration": 3.24
    },
    {
      "text": "the Base64 features,\nthose were some examples",
      "start": 17503.86,
      "duration": 2.7
    },
    {
      "text": "that we studied in a lot of depth,",
      "start": 17506.56,
      "duration": 1.23
    },
    {
      "text": "and really showed that they\nwere what we thought they were.",
      "start": 17507.79,
      "duration": 2.43
    },
    {
      "text": "It turns, if you train\na model twice as well",
      "start": 17510.22,
      "duration": 1.62
    },
    {
      "text": "and train two different models",
      "start": 17511.84,
      "duration": 1.47
    },
    {
      "text": "and do dictionary learning,",
      "start": 17513.31,
      "duration": 0.833
    },
    {
      "text": "you find analogous\nfeatures in both of them.",
      "start": 17514.143,
      "duration": 1.737
    },
    {
      "text": "So that's fun.",
      "start": 17515.88,
      "duration": 1.42
    },
    {
      "text": "You find all kinds of\nof different features.",
      "start": 17517.3,
      "duration": 1.89
    },
    {
      "text": "So that was really just\nshowing that this work.",
      "start": 17519.19,
      "duration": 3.69
    },
    {
      "text": "And you know, I should mention",
      "start": 17522.88,
      "duration": 1.56
    },
    {
      "text": "that there was this Cunningham et al",
      "start": 17524.44,
      "duration": 1.923
    },
    {
      "text": "that had very similar\nresults around the same time.",
      "start": 17526.363,
      "duration": 2.427
    },
    {
      "text": "- There's something fun about",
      "start": 17528.79,
      "duration": 1.92
    },
    {
      "text": "doing these kinds of\nsmall scale experiments",
      "start": 17530.71,
      "duration": 2.04
    },
    {
      "text": "and finding that it's actually working.",
      "start": 17532.75,
      "duration": 2.13
    },
    {
      "text": "- Yeah, well, and there's\nso much structure here,",
      "start": 17534.88,
      "duration": 2.01
    },
    {
      "text": "like you know, so maybe\nstepping back for a while,",
      "start": 17536.89,
      "duration": 3.813
    },
    {
      "text": "I thought that maybe all this",
      "start": 17541.84,
      "duration": 2.34
    },
    {
      "text": "mechanistic interpretability work,",
      "start": 17544.18,
      "duration": 1.62
    },
    {
      "text": "the end result was gonna be that",
      "start": 17545.8,
      "duration": 1.23
    },
    {
      "text": "I would have an explanation",
      "start": 17547.03,
      "duration": 1.17
    },
    {
      "text": "for why it was sort of, you know,",
      "start": 17548.2,
      "duration": 1.41
    },
    {
      "text": "very hard and not gonna be tractable.",
      "start": 17549.61,
      "duration": 2.34
    },
    {
      "text": "You know, we'd be like,",
      "start": 17551.95,
      "duration": 0.833
    },
    {
      "text": "well, there's this problem\nwith superposition,",
      "start": 17552.783,
      "duration": 1.417
    },
    {
      "text": "and it turns out\nsuperposition is really hard,",
      "start": 17554.2,
      "duration": 2.01
    },
    {
      "text": "and we're kind of screwed,\nbut that's not what happened.",
      "start": 17556.21,
      "duration": 2.61
    },
    {
      "text": "In fact, a very natural,\nsimple technique just works.",
      "start": 17558.82,
      "duration": 3.36
    },
    {
      "text": "And so then that's actually\na very good situation.",
      "start": 17562.18,
      "duration": 2.37
    },
    {
      "text": "You know, I think this is a\nsort of hard research problem",
      "start": 17564.55,
      "duration": 3.84
    },
    {
      "text": "and it's got a lot of research risk",
      "start": 17568.39,
      "duration": 1.11
    },
    {
      "text": "and you know, it might\nstill very well fail,",
      "start": 17569.5,
      "duration": 2.01
    },
    {
      "text": "but I think that some amount of,",
      "start": 17571.51,
      "duration": 1.92
    },
    {
      "text": "some very significant\namount of research risk",
      "start": 17573.43,
      "duration": 1.62
    },
    {
      "text": "was sort of put behind us\nwhen that started to work.",
      "start": 17575.05,
      "duration": 2.58
    },
    {
      "text": "- Can you describe what kind of",
      "start": 17577.63,
      "duration": 2.52
    },
    {
      "text": "features can be extracted in this way?",
      "start": 17580.15,
      "duration": 2.28
    },
    {
      "text": "- Well, so it depends on the model",
      "start": 17582.43,
      "duration": 1.74
    },
    {
      "text": "that you're studying, right?",
      "start": 17584.17,
      "duration": 1.05
    },
    {
      "text": "So the larger the model,",
      "start": 17585.22,
      "duration": 1.44
    },
    {
      "text": "the more sophisticated they're gonna be,",
      "start": 17586.66,
      "duration": 1.23
    },
    {
      "text": "and we'll probably talk about\nfollow up work in a minute.",
      "start": 17587.89,
      "duration": 2.64
    },
    {
      "text": "But in these one layer models,",
      "start": 17590.53,
      "duration": 2.1
    },
    {
      "text": "so some very common things\nI think were languages,",
      "start": 17592.63,
      "duration": 3.03
    },
    {
      "text": "both programming languages\nand natural languages.",
      "start": 17595.66,
      "duration": 2.28
    },
    {
      "text": "There were a lot of features",
      "start": 17597.94,
      "duration": 1.14
    },
    {
      "text": "that were specific words in\nspecific contexts, so \"the.\"",
      "start": 17599.08,
      "duration": 4.29
    },
    {
      "text": "And I think really the way to think",
      "start": 17603.37,
      "duration": 0.96
    },
    {
      "text": "about this is \"the\" is likely",
      "start": 17604.33,
      "duration": 1.68
    },
    {
      "text": "about to be followed by a noun.",
      "start": 17606.01,
      "duration": 1.5
    },
    {
      "text": "So it's really, you could\nthink of this as \"the\" feature,",
      "start": 17607.51,
      "duration": 2.04
    },
    {
      "text": "but you could also think of this",
      "start": 17609.55,
      "duration": 0.833
    },
    {
      "text": "as predicting a specific noun feature.",
      "start": 17610.383,
      "duration": 1.807
    },
    {
      "text": "And there would be these features",
      "start": 17612.19,
      "duration": 1.29
    },
    {
      "text": "that would fire for \"the\" in the context",
      "start": 17613.48,
      "duration": 3.33
    },
    {
      "text": "of say a legal document,",
      "start": 17616.81,
      "duration": 1.56
    },
    {
      "text": "or a mathematical document\nor something like this.",
      "start": 17618.37,
      "duration": 2.973
    },
    {
      "text": "And so, you know, maybe\nin the context of math",
      "start": 17622.45,
      "duration": 3.42
    },
    {
      "text": "you're like, you know, and\n\"the\" then predict vector,",
      "start": 17625.87,
      "duration": 2.55
    },
    {
      "text": "matrix, you know, all\nthese mathematical words,",
      "start": 17628.42,
      "duration": 2.01
    },
    {
      "text": "whereas, you know, in other context,",
      "start": 17630.43,
      "duration": 1.14
    },
    {
      "text": "you would predict other\nthings, that was common.",
      "start": 17631.57,
      "duration": 2.49
    },
    {
      "text": "- And basically we need clever humans",
      "start": 17634.06,
      "duration": 2.91
    },
    {
      "text": "to assign labels to what we're seeing.",
      "start": 17636.97,
      "duration": 3.18
    },
    {
      "text": "- Yes, so, you know,",
      "start": 17640.15,
      "duration": 1.44
    },
    {
      "text": "this is, the only thing this is doing",
      "start": 17641.59,
      "duration": 1.5
    },
    {
      "text": "is that sort of unfolding things for you.",
      "start": 17643.09,
      "duration": 2.88
    },
    {
      "text": "So if everything was sort\nof folded over top of it,",
      "start": 17645.97,
      "duration": 1.777
    },
    {
      "text": "you know, superposition folded\neverything on top of itself",
      "start": 17647.747,
      "duration": 2.783
    },
    {
      "text": "and you can't really see\nit, this is unfolding it.",
      "start": 17650.53,
      "duration": 2.82
    },
    {
      "text": "But now you still have\na very complex thing",
      "start": 17653.35,
      "duration": 1.56
    },
    {
      "text": "to try to understand.",
      "start": 17654.91,
      "duration": 1.44
    },
    {
      "text": "So then you have to do a bunch of work",
      "start": 17656.35,
      "duration": 1.29
    },
    {
      "text": "understanding what these are.",
      "start": 17657.64,
      "duration": 1.86
    },
    {
      "text": "And some of them are really subtle.",
      "start": 17659.5,
      "duration": 1.35
    },
    {
      "text": "Like there's some really cool things",
      "start": 17660.85,
      "duration": 1.74
    },
    {
      "text": "even in this one layer\nmodel about Unicode where,",
      "start": 17662.59,
      "duration": 3.12
    },
    {
      "text": "you know, of course some\nlanguages are in Unicode",
      "start": 17665.71,
      "duration": 2.1
    },
    {
      "text": "and the tokenizer won't necessarily have",
      "start": 17667.81,
      "duration": 1.35
    },
    {
      "text": "a dedicated token for\nevery Unicode character.",
      "start": 17669.16,
      "duration": 4.59
    },
    {
      "text": "So instead what you'll have",
      "start": 17673.75,
      "duration": 1.26
    },
    {
      "text": "is you'll have these patterns\nof alternating token,",
      "start": 17675.01,
      "duration": 2.34
    },
    {
      "text": "or alternating tokens",
      "start": 17677.35,
      "duration": 1.02
    },
    {
      "text": "that each represent half\nof a Unicode character.",
      "start": 17678.37,
      "duration": 1.62
    },
    {
      "text": "- Nice.\n- And you'll have",
      "start": 17679.99,
      "duration": 0.833
    },
    {
      "text": "a different feature that, you know,",
      "start": 17680.823,
      "duration": 1.507
    },
    {
      "text": "goes and activates on the\nopposing ones to be like, okay,",
      "start": 17682.33,
      "duration": 2.73
    },
    {
      "text": "you know, I just finished\na character, you know,",
      "start": 17685.06,
      "duration": 2.13
    },
    {
      "text": "go and predict next prefix.",
      "start": 17687.19,
      "duration": 2.04
    },
    {
      "text": "Then okay, on the prefix,",
      "start": 17689.23,
      "duration": 1.56
    },
    {
      "text": "you know, predict a reasonable suffix,",
      "start": 17690.79,
      "duration": 2.13
    },
    {
      "text": "and you have to alternate back and forth.",
      "start": 17692.92,
      "duration": 2.07
    },
    {
      "text": "So there's, you know,\nthese one player models",
      "start": 17694.99,
      "duration": 1.906
    },
    {
      "text": "are really interesting.",
      "start": 17696.896,
      "duration": 1.274
    },
    {
      "text": "And I mean, it's another thing\nthat just, you might think,",
      "start": 17698.17,
      "duration": 2.79
    },
    {
      "text": "okay, there would just\nbe one Base64 feature,",
      "start": 17700.96,
      "duration": 1.92
    },
    {
      "text": "but it turns out there's actually",
      "start": 17702.88,
      "duration": 1.2
    },
    {
      "text": "a bunch of Base64 features",
      "start": 17704.08,
      "duration": 1.44
    },
    {
      "text": "because you can have English\ntext encoded as Base64,",
      "start": 17705.52,
      "duration": 3.54
    },
    {
      "text": "and that has a very different distribution",
      "start": 17709.06,
      "duration": 2.31
    },
    {
      "text": "of Base64 tokens than regular.",
      "start": 17711.37,
      "duration": 2.82
    },
    {
      "text": "And there's some things about tokenization",
      "start": 17714.19,
      "duration": 3.15
    },
    {
      "text": "as well that it can exploit.",
      "start": 17717.34,
      "duration": 1.32
    },
    {
      "text": "And I dunno, there's all\nall kinds of fun stuff.",
      "start": 17718.66,
      "duration": 2.52
    },
    {
      "text": "- How difficult is the task",
      "start": 17721.18,
      "duration": 1.89
    },
    {
      "text": "of sort of assigning\nlabels to what's going on?",
      "start": 17723.07,
      "duration": 3.57
    },
    {
      "text": "Can this be automated by AI?",
      "start": 17726.64,
      "duration": 1.77
    },
    {
      "text": "- Well, I think it depends on the feature",
      "start": 17728.41,
      "duration": 1.53
    },
    {
      "text": "and it also depends on how\nmuch you trust your AI.",
      "start": 17729.94,
      "duration": 2.13
    },
    {
      "text": "So there's a lot of work doing\nautomated interpretability.",
      "start": 17732.07,
      "duration": 4.53
    },
    {
      "text": "I think that's a really\nexciting direction,",
      "start": 17736.6,
      "duration": 2.37
    },
    {
      "text": "and we do a fair amount of\nautomated interpretability",
      "start": 17738.97,
      "duration": 1.857
    },
    {
      "text": "and have Claude go and label our features.",
      "start": 17740.827,
      "duration": 2.103
    },
    {
      "text": "- Is there some funny moments",
      "start": 17742.93,
      "duration": 2.34
    },
    {
      "text": "where it's totally right\nor it's totally wrong?",
      "start": 17745.27,
      "duration": 2.67
    },
    {
      "text": "- Yeah, well, I think it's very common",
      "start": 17747.94,
      "duration": 2.01
    },
    {
      "text": "that it's like says\nsomething very general,",
      "start": 17749.95,
      "duration": 2.97
    },
    {
      "text": "which is like true in some sense,",
      "start": 17752.92,
      "duration": 2.19
    },
    {
      "text": "but not really picking up",
      "start": 17755.11,
      "duration": 1.65
    },
    {
      "text": "on the specific of what's going on.",
      "start": 17756.76,
      "duration": 1.863
    },
    {
      "text": "So I think that's a\npretty common situation.",
      "start": 17760.24,
      "duration": 3.003
    },
    {
      "text": "Yeah, don't know that I have\na particularly amusing one.",
      "start": 17764.475,
      "duration": 1.945
    },
    {
      "text": "- That's interesting, that\nlittle gap between it is true",
      "start": 17766.42,
      "duration": 3.24
    },
    {
      "text": "but doesn't quite get to\nthe deep nuance of a thing.",
      "start": 17769.66,
      "duration": 4.56
    },
    {
      "text": "That's a general challenge.",
      "start": 17774.22,
      "duration": 1.5
    },
    {
      "text": "It's like truly an\nincredible accomplishment",
      "start": 17775.72,
      "duration": 3.0
    },
    {
      "text": "that it can say a true thing,",
      "start": 17778.72,
      "duration": 1.95
    },
    {
      "text": "but it doesn't, it's not,",
      "start": 17780.67,
      "duration": 1.68
    },
    {
      "text": "it's missing the depth sometimes.",
      "start": 17782.35,
      "duration": 2.747
    },
    {
      "text": "And in this context, it's\nlike the ARC challenge,",
      "start": 17785.097,
      "duration": 2.293
    },
    {
      "text": "you know, the sort of IQ type of tests.",
      "start": 17787.39,
      "duration": 2.55
    },
    {
      "text": "It feels like figuring out",
      "start": 17789.94,
      "duration": 2.22
    },
    {
      "text": "what a feature represents is a bit of,",
      "start": 17792.16,
      "duration": 2.06
    },
    {
      "text": "is a little puzzle you have to solve.",
      "start": 17794.22,
      "duration": 1.66
    },
    {
      "text": "- Yeah, and I think that\nsometimes they're easier,",
      "start": 17795.88,
      "duration": 1.86
    },
    {
      "text": "and sometimes they're harder as well.",
      "start": 17797.74,
      "duration": 2.01
    },
    {
      "text": "So yeah, I think that's tricky.",
      "start": 17799.75,
      "duration": 2.697
    },
    {
      "text": "And there's another thing which, I dunno,",
      "start": 17802.447,
      "duration": 2.853
    },
    {
      "text": "maybe in some ways this is\nmy like aesthetic coming in,",
      "start": 17805.3,
      "duration": 2.52
    },
    {
      "text": "but I'll try to give\nyou a rationalization.",
      "start": 17807.82,
      "duration": 1.8
    },
    {
      "text": "You know, I'm actually a little suspicious",
      "start": 17809.62,
      "duration": 1.71
    },
    {
      "text": "of automated interpretability,",
      "start": 17811.33,
      "duration": 1.2
    },
    {
      "text": "and I think that partly just that",
      "start": 17812.53,
      "duration": 1.307
    },
    {
      "text": "I want humans to\nunderstand neural networks,",
      "start": 17813.837,
      "duration": 2.203
    },
    {
      "text": "and if the neural network\nis understanding it for me,",
      "start": 17816.04,
      "duration": 1.95
    },
    {
      "text": "you know, I don't quite like that.",
      "start": 17817.99,
      "duration": 1.62
    },
    {
      "text": "But I do have a bit of,\nyou know, in some ways,",
      "start": 17819.61,
      "duration": 2.37
    },
    {
      "text": "I'm sort of like the mathematicians\nwho are like, you know,",
      "start": 17821.98,
      "duration": 1.365
    },
    {
      "text": "if there's a computer automated proof,",
      "start": 17823.345,
      "duration": 1.335
    },
    {
      "text": "it doesn't count.",
      "start": 17824.68,
      "duration": 0.833
    },
    {
      "text": "- Right?\n- You know,",
      "start": 17825.513,
      "duration": 0.967
    },
    {
      "text": "they won't understand it.",
      "start": 17826.48,
      "duration": 0.833
    },
    {
      "text": "But I do also think that",
      "start": 17827.313,
      "duration": 1.327
    },
    {
      "text": "there is this kind of like reflections",
      "start": 17828.64,
      "duration": 2.49
    },
    {
      "text": "on trusting trust type issue where,",
      "start": 17831.13,
      "duration": 2.237
    },
    {
      "text": "you know, if you, there's\nthis famous talk about,",
      "start": 17833.367,
      "duration": 2.45
    },
    {
      "text": "you know, like when you're\nwriting a computer program,",
      "start": 17836.731,
      "duration": 2.799
    },
    {
      "text": "you have to trust your compiler,",
      "start": 17839.53,
      "duration": 1.32
    },
    {
      "text": "and if there was like\nmalware in your compiler,",
      "start": 17840.85,
      "duration": 1.92
    },
    {
      "text": "then it could go and inject\nmalware into the next compiler,",
      "start": 17842.77,
      "duration": 2.52
    },
    {
      "text": "and you know, you'd be\nkind of in trouble, right?",
      "start": 17845.29,
      "duration": 2.04
    },
    {
      "text": "Well, if you're using neural networks",
      "start": 17847.33,
      "duration": 1.35
    },
    {
      "text": "to go and verify that your\nneural networks are safe,",
      "start": 17848.68,
      "duration": 4.2
    },
    {
      "text": "the hypothesis that you're\ntesting for is like,",
      "start": 17852.88,
      "duration": 1.83
    },
    {
      "text": "okay, well, the neural\nnetwork maybe isn't safe,",
      "start": 17854.71,
      "duration": 1.98
    },
    {
      "text": "and you have to worry about\nlike, is there some way",
      "start": 17856.69,
      "duration": 2.25
    },
    {
      "text": "that it could be screwing with you?",
      "start": 17858.94,
      "duration": 1.92
    },
    {
      "text": "So, you know, I think that's\nnot a big concern now,",
      "start": 17860.86,
      "duration": 3.72
    },
    {
      "text": "but I do wonder in the long run",
      "start": 17864.58,
      "duration": 1.5
    },
    {
      "text": "if we have to use really\npowerful AI system",
      "start": 17866.08,
      "duration": 2.1
    },
    {
      "text": "to go and, you know, audit our AI systems,",
      "start": 17868.18,
      "duration": 3.38
    },
    {
      "text": "is that actually something we can trust?",
      "start": 17871.56,
      "duration": 1.75
    },
    {
      "text": "But maybe I'm just rationalizing",
      "start": 17873.31,
      "duration": 1.32
    },
    {
      "text": "'cause I just want to us to have to get it",
      "start": 17874.63,
      "duration": 1.713
    },
    {
      "text": "to a point where humans\nunderstand everything.",
      "start": 17876.343,
      "duration": 1.797
    },
    {
      "text": "- Yeah, I mean, especially\nthat's hilarious,",
      "start": 17878.14,
      "duration": 2.31
    },
    {
      "text": "especially as we talk about AI safety",
      "start": 17880.45,
      "duration": 1.92
    },
    {
      "text": "and it looking for features\nthat would be relevant",
      "start": 17882.37,
      "duration": 2.94
    },
    {
      "text": "to AI safety, like deception and so on.",
      "start": 17885.31,
      "duration": 2.67
    },
    {
      "text": "So let's talk about the\n\"Scaling Monosemanticity\" paper",
      "start": 17887.98,
      "duration": 3.93
    },
    {
      "text": "in May, 2024.",
      "start": 17891.91,
      "duration": 1.32
    },
    {
      "text": "Okay, so what did it take to scale this,",
      "start": 17893.23,
      "duration": 2.49
    },
    {
      "text": "to apply to Claude 3s on it?",
      "start": 17895.72,
      "duration": 2.37
    },
    {
      "text": "- Well, a lot of GPUs.",
      "start": 17898.09,
      "duration": 0.833
    },
    {
      "text": "- A lot more GPUs, got it.",
      "start": 17898.923,
      "duration": 2.137
    },
    {
      "text": "- But one of my teammates,",
      "start": 17901.06,
      "duration": 1.53
    },
    {
      "text": "Tom Henighan was involved in\nthe original scaling laws work,",
      "start": 17902.59,
      "duration": 4.8
    },
    {
      "text": "and something that he\nwas sort of interested in",
      "start": 17907.39,
      "duration": 3.06
    },
    {
      "text": "from very early on is,\nare there scaling laws",
      "start": 17910.45,
      "duration": 2.79
    },
    {
      "text": "for interpretability?",
      "start": 17913.24,
      "duration": 1.143
    },
    {
      "text": "And so something he\nsort of immediately did",
      "start": 17916.24,
      "duration": 3.51
    },
    {
      "text": "when this work started to succeed,",
      "start": 17919.75,
      "duration": 2.04
    },
    {
      "text": "and we started to have\nsparse auto-encoders work",
      "start": 17921.79,
      "duration": 1.757
    },
    {
      "text": "was he became very interested in,",
      "start": 17923.547,
      "duration": 1.423
    },
    {
      "text": "you know, what are the scaling laws for,",
      "start": 17924.97,
      "duration": 2.553
    },
    {
      "text": "you know, for making sparse\nauto-encoders larger?",
      "start": 17929.59,
      "duration": 2.28
    },
    {
      "text": "And how does that relate to\nmaking the base model larger?",
      "start": 17931.87,
      "duration": 2.85
    },
    {
      "text": "And so it turns out this works really well",
      "start": 17935.65,
      "duration": 2.04
    },
    {
      "text": "and you can use it to\nsort of project, you know,",
      "start": 17937.69,
      "duration": 2.82
    },
    {
      "text": "if you train a sparse\nauto-encoder at a given size,",
      "start": 17940.51,
      "duration": 2.1
    },
    {
      "text": "you know, how many tokens\nshould you train on?",
      "start": 17942.61,
      "duration": 1.346
    },
    {
      "text": "And so on.",
      "start": 17943.956,
      "duration": 0.833
    },
    {
      "text": "So this was actually a very big help to us",
      "start": 17944.789,
      "duration": 1.451
    },
    {
      "text": "in scaling up this work,",
      "start": 17946.24,
      "duration": 2.1
    },
    {
      "text": "and made it a lot easier\nfor us to go and train,",
      "start": 17948.34,
      "duration": 3.12
    },
    {
      "text": "you know, really large\nsparse auto-encoders where,",
      "start": 17951.46,
      "duration": 2.31
    },
    {
      "text": "you know, it's not like\ntraining the big models,",
      "start": 17953.77,
      "duration": 2.22
    },
    {
      "text": "but it's starting to get to a point",
      "start": 17955.99,
      "duration": 0.96
    },
    {
      "text": "where it's actually actually expensive",
      "start": 17956.95,
      "duration": 1.65
    },
    {
      "text": "to go and train the really big ones.",
      "start": 17958.6,
      "duration": 2.34
    },
    {
      "text": "- So you have this, I mean,\nyou have to do all this stuff",
      "start": 17960.94,
      "duration": 2.55
    },
    {
      "text": "of like splitting it across large GPUs-",
      "start": 17963.49,
      "duration": 2.49
    },
    {
      "text": "- Oh yeah, no, I mean there's a huge",
      "start": 17965.98,
      "duration": 1.23
    },
    {
      "text": "engineering challenge here too, right?",
      "start": 17967.21,
      "duration": 1.38
    },
    {
      "text": "So, yeah, so there's a\nscientific question of,",
      "start": 17968.59,
      "duration": 2.61
    },
    {
      "text": "how do you scale things effectively?",
      "start": 17971.2,
      "duration": 1.47
    },
    {
      "text": "And then there's an enormous amount",
      "start": 17972.67,
      "duration": 1.8
    },
    {
      "text": "of engineering to go and scale this up.",
      "start": 17974.47,
      "duration": 1.62
    },
    {
      "text": "So you have to chart it,",
      "start": 17976.09,
      "duration": 1.2
    },
    {
      "text": "you have to think very\ncarefully about a lot of things.",
      "start": 17977.29,
      "duration": 2.73
    },
    {
      "text": "I'm lucky to work with a\nbunch of great engineers",
      "start": 17980.02,
      "duration": 1.71
    },
    {
      "text": "'cause I am definitely\nnot a great engineer.",
      "start": 17981.73,
      "duration": 0.833
    },
    {
      "text": "- Yeah, and the infrastructure especially,",
      "start": 17982.563,
      "duration": 1.837
    },
    {
      "text": "yeah, for sure.",
      "start": 17984.4,
      "duration": 1.08
    },
    {
      "text": "So it turns out, TLDR, it worked.",
      "start": 17985.48,
      "duration": 3.51
    },
    {
      "text": "- It worked, yeah.",
      "start": 17988.99,
      "duration": 0.833
    },
    {
      "text": "And I think this is important",
      "start": 17989.823,
      "duration": 0.833
    },
    {
      "text": "because you could have imagined, like,",
      "start": 17990.656,
      "duration": 1.671
    },
    {
      "text": "you could have imagined a world",
      "start": 17992.327,
      "duration": 1.403
    },
    {
      "text": "where you said after\ntowards mono-semanticity,",
      "start": 17993.73,
      "duration": 1.86
    },
    {
      "text": "you know, Chris, this is great,",
      "start": 17995.59,
      "duration": 1.89
    },
    {
      "text": "you know, it works on a one layer model,",
      "start": 17997.48,
      "duration": 1.5
    },
    {
      "text": "but one layer models are\nreally idiosyncratic.",
      "start": 17998.98,
      "duration": 2.64
    },
    {
      "text": "Like, you know, maybe,\nthat's just something,",
      "start": 18001.62,
      "duration": 2.07
    },
    {
      "text": "like maybe the linear\nrepresentation hypothesis",
      "start": 18003.69,
      "duration": 1.497
    },
    {
      "text": "and superposition hypothesis",
      "start": 18005.187,
      "duration": 1.173
    },
    {
      "text": "is the right way to\nunderstand a one layer model,",
      "start": 18006.36,
      "duration": 2.07
    },
    {
      "text": "but it's not the right way\nto understand larger models.",
      "start": 18008.43,
      "duration": 2.723
    },
    {
      "text": "And so I think, I mean, first of all,",
      "start": 18012.09,
      "duration": 1.538
    },
    {
      "text": "the Cunningham et al\npaper sort of cut through",
      "start": 18013.628,
      "duration": 2.572
    },
    {
      "text": "that a little bit and sort of suggested",
      "start": 18016.2,
      "duration": 1.62
    },
    {
      "text": "that this wasn't the case.",
      "start": 18017.82,
      "duration": 1.08
    },
    {
      "text": "But scaling mono-semanticity sort of,",
      "start": 18018.9,
      "duration": 2.31
    },
    {
      "text": "I think was significant evidence",
      "start": 18021.21,
      "duration": 1.32
    },
    {
      "text": "that even for very large models,",
      "start": 18022.53,
      "duration": 1.527
    },
    {
      "text": "and we did it on Claude 3 Sonnet,",
      "start": 18024.057,
      "duration": 1.773
    },
    {
      "text": "which at that point was one\nof our production models.",
      "start": 18025.83,
      "duration": 2.913
    },
    {
      "text": "You know, even these\nmodels seemed to be very,",
      "start": 18029.64,
      "duration": 3.93
    },
    {
      "text": "you know, seemed to be\nsubstantially explained",
      "start": 18033.57,
      "duration": 2.34
    },
    {
      "text": "at least by linear features.",
      "start": 18035.91,
      "duration": 1.74
    },
    {
      "text": "And, you know, doing dictionary\nlearning on them works,",
      "start": 18037.65,
      "duration": 1.8
    },
    {
      "text": "and as you learn more features,",
      "start": 18039.45,
      "duration": 0.99
    },
    {
      "text": "you go and you explain more and more.",
      "start": 18040.44,
      "duration": 2.46
    },
    {
      "text": "So that's, I think,\nquite a promising sign.",
      "start": 18042.9,
      "duration": 2.16
    },
    {
      "text": "And you find now really\nfascinating abstract features.",
      "start": 18045.06,
      "duration": 4.92
    },
    {
      "text": "And the features are also multimodal.",
      "start": 18049.98,
      "duration": 1.53
    },
    {
      "text": "They respond to images and text",
      "start": 18051.51,
      "duration": 1.89
    },
    {
      "text": "for the same concept, which is fun.",
      "start": 18053.4,
      "duration": 1.56
    },
    {
      "text": "- Yeah, can you explain that?",
      "start": 18054.96,
      "duration": 1.74
    },
    {
      "text": "I mean, like, you know, backdoor,",
      "start": 18056.7,
      "duration": 2.61
    },
    {
      "text": "there's just a lot of\nexamples that you can-",
      "start": 18059.31,
      "duration": 1.998
    },
    {
      "text": "- Yeah, so maybe let's start\nwith a one example to start,",
      "start": 18061.308,
      "duration": 2.502
    },
    {
      "text": "which is we found some features around",
      "start": 18063.81,
      "duration": 1.77
    },
    {
      "text": "sort of security vulnerabilities\nand backdoors and codes.",
      "start": 18065.58,
      "duration": 2.55
    },
    {
      "text": "So it turns out those are\nactually two different features.",
      "start": 18068.13,
      "duration": 2.22
    },
    {
      "text": "So there's a security\nvulnerability feature,",
      "start": 18070.35,
      "duration": 2.07
    },
    {
      "text": "and if you force it\nactive, Claude will start",
      "start": 18072.42,
      "duration": 2.55
    },
    {
      "text": "to go and write security vulnerabilities",
      "start": 18074.97,
      "duration": 2.4
    },
    {
      "text": "like buffer overflows into code.",
      "start": 18077.37,
      "duration": 1.95
    },
    {
      "text": "And it also, it fires\nfor all kinds of things.",
      "start": 18079.32,
      "duration": 1.41
    },
    {
      "text": "Like, you know, some of the\ntop dataset examples for it",
      "start": 18080.73,
      "duration": 2.91
    },
    {
      "text": "were things like, you\nknow, dash dash disable,",
      "start": 18083.64,
      "duration": 3.72
    },
    {
      "text": "you know, SSL or something like this,",
      "start": 18087.36,
      "duration": 1.8
    },
    {
      "text": "which are sort of\nobviously really insecure.",
      "start": 18089.16,
      "duration": 4.89
    },
    {
      "text": "- So at this point it's kind of like,",
      "start": 18094.05,
      "duration": 1.83
    },
    {
      "text": "maybe it's just because the examples",
      "start": 18095.88,
      "duration": 1.83
    },
    {
      "text": "were presented that way,",
      "start": 18097.71,
      "duration": 1.23
    },
    {
      "text": "it's kind of like a little bit\nmore obvious examples, right?",
      "start": 18098.94,
      "duration": 3.873
    },
    {
      "text": "I guess the idea is that down the line,",
      "start": 18103.74,
      "duration": 2.12
    },
    {
      "text": "it might be able to detect more nuanced,",
      "start": 18105.86,
      "duration": 2.41
    },
    {
      "text": "like deception or bugs\nor that kind of stuff.",
      "start": 18108.27,
      "duration": 2.61
    },
    {
      "text": "- Yeah, well, I maybe wanna\ndistinguish two things.",
      "start": 18110.88,
      "duration": 1.89
    },
    {
      "text": "So one is the complexity of the feature",
      "start": 18112.77,
      "duration": 4.2
    },
    {
      "text": "or the concept, right?",
      "start": 18116.97,
      "duration": 1.65
    },
    {
      "text": "And the other is the nuance",
      "start": 18118.62,
      "duration": 3.72
    },
    {
      "text": "of how subtle the examples\nwe're looking at, right?",
      "start": 18122.34,
      "duration": 2.61
    },
    {
      "text": "So, when we show the top dataset examples,",
      "start": 18124.95,
      "duration": 2.58
    },
    {
      "text": "those are the most extreme examples",
      "start": 18127.53,
      "duration": 2.07
    },
    {
      "text": "that cause that feature to activate.",
      "start": 18129.6,
      "duration": 2.953
    },
    {
      "text": "And so it doesn't mean\nthat it doesn't fire",
      "start": 18132.553,
      "duration": 1.427
    },
    {
      "text": "for more subtle things.",
      "start": 18133.98,
      "duration": 1.41
    },
    {
      "text": "So, you know, the insecure code feature,",
      "start": 18135.39,
      "duration": 3.63
    },
    {
      "text": "you know, the stuff that it fires for,",
      "start": 18139.02,
      "duration": 1.62
    },
    {
      "text": "most strongly for are\nthese like really obvious,",
      "start": 18140.64,
      "duration": 2.25
    },
    {
      "text": "you know, disable the\nsecurity type things.",
      "start": 18142.89,
      "duration": 3.6
    },
    {
      "text": "But you know, it also fires\nfor, you know, buffer overflows",
      "start": 18146.49,
      "duration": 5.0
    },
    {
      "text": "and more subtle security\nvulnerabilities in code.",
      "start": 18152.37,
      "duration": 2.647
    },
    {
      "text": "You know, these features\nare all multimodal,",
      "start": 18155.017,
      "duration": 2.063
    },
    {
      "text": "so you could ask like, what\nimages activate this feature?",
      "start": 18157.08,
      "duration": 2.91
    },
    {
      "text": "And it turns out that the\nsecurity vulnerability feature",
      "start": 18159.99,
      "duration": 4.56
    },
    {
      "text": "activates for images of like people",
      "start": 18164.55,
      "duration": 3.33
    },
    {
      "text": "clicking on Chrome to\nlike go past the, like,",
      "start": 18167.88,
      "duration": 3.097
    },
    {
      "text": "you know, this website,",
      "start": 18170.977,
      "duration": 2.063
    },
    {
      "text": "the SSL certificate might be\nwrong or something like this.",
      "start": 18173.04,
      "duration": 2.19
    },
    {
      "text": "Another thing that's very entertaining",
      "start": 18175.23,
      "duration": 1.02
    },
    {
      "text": "is there's backdoors and code feature.",
      "start": 18176.25,
      "duration": 1.35
    },
    {
      "text": "Like you activate it, it goes\nand Claude writes a backdoor",
      "start": 18177.6,
      "duration": 2.13
    },
    {
      "text": "that like will go and dump\nyour data to port or something.",
      "start": 18179.73,
      "duration": 2.34
    },
    {
      "text": "But you can ask, okay,",
      "start": 18182.07,
      "duration": 1.59
    },
    {
      "text": "what images activate the backdoor feature?",
      "start": 18183.66,
      "duration": 2.37
    },
    {
      "text": "It was devices with\nhidden cameras in them.",
      "start": 18186.03,
      "duration": 2.34
    },
    {
      "text": "So there's a whole\napparently genre of people",
      "start": 18188.37,
      "duration": 3.39
    },
    {
      "text": "going and selling devices\nthat look innocuous,",
      "start": 18191.76,
      "duration": 2.22
    },
    {
      "text": "that have hidden cameras and they have-",
      "start": 18193.98,
      "duration": 2.103
    },
    {
      "text": "- That's great.\n- This hidden camera in it.",
      "start": 18196.083,
      "duration": 1.377
    },
    {
      "text": "And I guess that is the, you know,",
      "start": 18197.46,
      "duration": 1.5
    },
    {
      "text": "physical version of a backdoor.",
      "start": 18198.96,
      "duration": 1.59
    },
    {
      "text": "And so it sort of shows you",
      "start": 18200.55,
      "duration": 1.29
    },
    {
      "text": "how abstract these concepts are, right?",
      "start": 18201.84,
      "duration": 2.85
    },
    {
      "text": "And I just thought that was,",
      "start": 18204.69,
      "duration": 1.413
    },
    {
      "text": "I'm sort of sad that\nthere's a whole market",
      "start": 18207.8,
      "duration": 1.45
    },
    {
      "text": "of people selling devices like that,",
      "start": 18209.25,
      "duration": 1.05
    },
    {
      "text": "but I was kind of delighted that",
      "start": 18210.3,
      "duration": 1.2
    },
    {
      "text": "that was the thing that",
      "start": 18211.5,
      "duration": 1.14
    },
    {
      "text": "it came up with as the top\nimage examples for the feature.",
      "start": 18212.64,
      "duration": 3.45
    },
    {
      "text": "- Yeah, it's nice. It's multimodal.",
      "start": 18216.09,
      "duration": 1.32
    },
    {
      "text": "It's multi almost context.",
      "start": 18217.41,
      "duration": 1.83
    },
    {
      "text": "It's as broad, strong definition",
      "start": 18219.24,
      "duration": 3.39
    },
    {
      "text": "of a singular concept, it's nice.",
      "start": 18222.63,
      "duration": 1.89
    },
    {
      "text": "- Yeah.",
      "start": 18224.52,
      "duration": 0.833
    },
    {
      "text": "- To me, one of the really\ninteresting features,",
      "start": 18225.353,
      "duration": 1.567
    },
    {
      "text": "especially for AI safety\nis deception and lying.",
      "start": 18226.92,
      "duration": 4.59
    },
    {
      "text": "And the possibility that\nthese kinds of methods",
      "start": 18231.51,
      "duration": 2.28
    },
    {
      "text": "could detect lying in a model,",
      "start": 18233.79,
      "duration": 2.88
    },
    {
      "text": "especially gets smarter\nand smarter and smarter.",
      "start": 18236.67,
      "duration": 2.13
    },
    {
      "text": "Presumably that's a big threat\nof a super intelligent model",
      "start": 18238.8,
      "duration": 4.35
    },
    {
      "text": "that it can deceive\nthe people operating it",
      "start": 18243.15,
      "duration": 2.65
    },
    {
      "text": "as to its intentions or\nany of that kind of stuff.",
      "start": 18247.35,
      "duration": 2.28
    },
    {
      "text": "So what have you learned",
      "start": 18249.63,
      "duration": 1.53
    },
    {
      "text": "from detecting lying inside models?",
      "start": 18251.16,
      "duration": 2.61
    },
    {
      "text": "- Yeah, so I think we're in some ways",
      "start": 18253.77,
      "duration": 1.68
    },
    {
      "text": "in early days for that.",
      "start": 18255.45,
      "duration": 2.28
    },
    {
      "text": "We find quite a few features\nrelated to deception and lying.",
      "start": 18257.73,
      "duration": 4.59
    },
    {
      "text": "There's one feature where, you know,",
      "start": 18262.32,
      "duration": 2.19
    },
    {
      "text": "fires for people lying\nand being deceptive,",
      "start": 18264.51,
      "duration": 2.82
    },
    {
      "text": "and you force it active\nand starts lying to you.",
      "start": 18267.33,
      "duration": 2.07
    },
    {
      "text": "So we have a deception feature.",
      "start": 18269.4,
      "duration": 2.43
    },
    {
      "text": "I mean, there's all\nkinds of other features",
      "start": 18271.83,
      "duration": 1.23
    },
    {
      "text": "about withholding information",
      "start": 18273.06,
      "duration": 1.56
    },
    {
      "text": "and not answering questions,",
      "start": 18274.62,
      "duration": 0.93
    },
    {
      "text": "features about power seeking\nand coups and stuff like that.",
      "start": 18275.55,
      "duration": 3.86
    },
    {
      "text": "So there's a lot of features",
      "start": 18279.41,
      "duration": 0.88
    },
    {
      "text": "that are kind of related to spooky things.",
      "start": 18280.29,
      "duration": 1.947
    },
    {
      "text": "And if you force them active,",
      "start": 18282.237,
      "duration": 2.943
    },
    {
      "text": "Claude will behave in ways that are,",
      "start": 18285.18,
      "duration": 2.243
    },
    {
      "text": "they're not the kinds\nof behaviors you want.",
      "start": 18287.423,
      "duration": 3.007
    },
    {
      "text": "- What are possible\nnext exciting directions",
      "start": 18290.43,
      "duration": 3.09
    },
    {
      "text": "to you in the space of mech interp?",
      "start": 18293.52,
      "duration": 2.73
    },
    {
      "text": "- Well, there's a lot of things.",
      "start": 18296.25,
      "duration": 1.6
    },
    {
      "text": "So for one thing, I would\nreally like to get to a point",
      "start": 18302.43,
      "duration": 2.16
    },
    {
      "text": "where we have circuits where\nwe can really understand",
      "start": 18304.59,
      "duration": 2.65
    },
    {
      "text": "not just the features,",
      "start": 18308.16,
      "duration": 1.95
    },
    {
      "text": "but then use that to understand\nthe computation of models.",
      "start": 18310.11,
      "duration": 3.12
    },
    {
      "text": "That relief for me is the\nultimate goal of this.",
      "start": 18313.23,
      "duration": 4.14
    },
    {
      "text": "And there's been some work,\nwe put out a few things.",
      "start": 18317.37,
      "duration": 2.58
    },
    {
      "text": "There's a paper from Sam Marks",
      "start": 18319.95,
      "duration": 1.26
    },
    {
      "text": "that does some stuff like this.",
      "start": 18321.21,
      "duration": 1.08
    },
    {
      "text": "And there's been some,",
      "start": 18322.29,
      "duration": 1.14
    },
    {
      "text": "I'd say some work around the edges here.",
      "start": 18323.43,
      "duration": 1.907
    },
    {
      "text": "But I think there's a lot more to do,",
      "start": 18325.337,
      "duration": 1.573
    },
    {
      "text": "and I think that will be\na very exciting thing.",
      "start": 18326.91,
      "duration": 2.373
    },
    {
      "text": "That's related to a challenge\nwe call interference weights,",
      "start": 18330.84,
      "duration": 4.17
    },
    {
      "text": "where due to superposition,",
      "start": 18335.01,
      "duration": 3.15
    },
    {
      "text": "if you just sort of naively look at",
      "start": 18338.16,
      "duration": 1.355
    },
    {
      "text": "where their features\nare connected together,",
      "start": 18339.515,
      "duration": 1.585
    },
    {
      "text": "there may be some weights that",
      "start": 18341.1,
      "duration": 1.44
    },
    {
      "text": "sort of don't exist in the upstairs model,",
      "start": 18342.54,
      "duration": 2.43
    },
    {
      "text": "but are just sort of\nartifacts of superposition.",
      "start": 18344.97,
      "duration": 2.28
    },
    {
      "text": "So that's a sort of technical\nchallenge related to that.",
      "start": 18347.25,
      "duration": 2.973
    },
    {
      "text": "I think another exciting\ndirection is just, you know,",
      "start": 18352.7,
      "duration": 2.917
    },
    {
      "text": "you might think of sparse auto-encoders",
      "start": 18356.52,
      "duration": 2.25
    },
    {
      "text": "as being kind of like a telescope.",
      "start": 18358.77,
      "duration": 2.28
    },
    {
      "text": "They allow us to, you know,",
      "start": 18361.05,
      "duration": 1.53
    },
    {
      "text": "look out and see all these\nfeatures that are out there.",
      "start": 18362.58,
      "duration": 3.865
    },
    {
      "text": "And you know, as we build better",
      "start": 18366.445,
      "duration": 0.935
    },
    {
      "text": "and better sparse auto-encoders,",
      "start": 18367.38,
      "duration": 1.95
    },
    {
      "text": "get better and better\nat dictionary learning,",
      "start": 18369.33,
      "duration": 1.44
    },
    {
      "text": "we see more and more stars,",
      "start": 18370.77,
      "duration": 1.653
    },
    {
      "text": "and you know, we zoom in on\nsmaller and smaller stars.",
      "start": 18373.65,
      "duration": 2.277
    },
    {
      "text": "But there's kind of a lot of evidence",
      "start": 18375.927,
      "duration": 2.043
    },
    {
      "text": "that we're only still seeing",
      "start": 18377.97,
      "duration": 1.02
    },
    {
      "text": "a very small fraction of the stars.",
      "start": 18378.99,
      "duration": 2.55
    },
    {
      "text": "There's a lot of matter in our, you know,",
      "start": 18381.54,
      "duration": 2.82
    },
    {
      "text": "neural network universe\nthat we can't observe yet.",
      "start": 18384.36,
      "duration": 2.4
    },
    {
      "text": "And it may be that we'll never be able",
      "start": 18386.76,
      "duration": 2.73
    },
    {
      "text": "to have fine enough\ninstruments to observe it,",
      "start": 18389.49,
      "duration": 1.617
    },
    {
      "text": "and maybe some of it just isn't possible,",
      "start": 18391.107,
      "duration": 2.823
    },
    {
      "text": "isn't computationally\ntractable to observe it.",
      "start": 18393.93,
      "duration": 2.017
    },
    {
      "text": "So it's sort of a kind of dark matter,",
      "start": 18395.947,
      "duration": 2.094
    },
    {
      "text": "not in maybe the sense\nof modern astronomy,",
      "start": 18398.041,
      "duration": 2.099
    },
    {
      "text": "but of early astronomy when we didn't know",
      "start": 18400.14,
      "duration": 1.56
    },
    {
      "text": "what this unexplained matter is.",
      "start": 18401.7,
      "duration": 1.71
    },
    {
      "text": "And so I think a lot\nabout that dark matter",
      "start": 18403.41,
      "duration": 2.64
    },
    {
      "text": "and whether we'll ever observe it,",
      "start": 18406.05,
      "duration": 0.833
    },
    {
      "text": "and what that means for safety",
      "start": 18406.883,
      "duration": 1.567
    },
    {
      "text": "if we can't observe it,",
      "start": 18408.45,
      "duration": 1.74
    },
    {
      "text": "if there's, you know, if\nsome significant fraction",
      "start": 18410.19,
      "duration": 2.67
    },
    {
      "text": "of neural networks are\nnot accessible to us.",
      "start": 18412.86,
      "duration": 2.2
    },
    {
      "text": "Another question that\nI think a lot about is,",
      "start": 18416.46,
      "duration": 2.793
    },
    {
      "text": "at the end of the day, you know,",
      "start": 18420.3,
      "duration": 2.25
    },
    {
      "text": "mechanistic interpretability is this",
      "start": 18422.55,
      "duration": 0.87
    },
    {
      "text": "very microscopic approach\nto interpretability.",
      "start": 18423.42,
      "duration": 2.73
    },
    {
      "text": "It's trying to understand things\nin a very fine-grained way.",
      "start": 18426.15,
      "duration": 3.21
    },
    {
      "text": "But a lot of the questions we care about",
      "start": 18429.36,
      "duration": 1.5
    },
    {
      "text": "are very macroscopic.",
      "start": 18430.86,
      "duration": 2.07
    },
    {
      "text": "You know, we care about these questions",
      "start": 18432.93,
      "duration": 2.04
    },
    {
      "text": "about neural network behavior,",
      "start": 18434.97,
      "duration": 1.83
    },
    {
      "text": "and I think that's the thing\nthat I care most about,",
      "start": 18436.8,
      "duration": 3.06
    },
    {
      "text": "but there's lots of other",
      "start": 18439.86,
      "duration": 1.38
    },
    {
      "text": "sort of larger scale questions\nyou might care about.",
      "start": 18441.24,
      "duration": 2.6
    },
    {
      "text": "And somehow, you know,",
      "start": 18445.5,
      "duration": 2.19
    },
    {
      "text": "the nice thing about about having",
      "start": 18447.69,
      "duration": 1.17
    },
    {
      "text": "a very microscopic approach\nis it's maybe easier to ask,",
      "start": 18448.86,
      "duration": 2.31
    },
    {
      "text": "you know, is this true?",
      "start": 18451.17,
      "duration": 1.53
    },
    {
      "text": "But the downside is it's much further",
      "start": 18452.7,
      "duration": 1.62
    },
    {
      "text": "from the things we care about,",
      "start": 18454.32,
      "duration": 0.833
    },
    {
      "text": "and so we now have this ladder to climb.",
      "start": 18455.153,
      "duration": 2.227
    },
    {
      "text": "And I think there's a question of,",
      "start": 18457.38,
      "duration": 1.38
    },
    {
      "text": "will we be able to find,",
      "start": 18458.76,
      "duration": 0.87
    },
    {
      "text": "are there sort of larger\nscale abstractions",
      "start": 18459.63,
      "duration": 2.73
    },
    {
      "text": "that we can use to\nunderstand neural networks?",
      "start": 18462.36,
      "duration": 1.767
    },
    {
      "text": "Can we get up from this\nvery microscopic approach?",
      "start": 18464.127,
      "duration": 4.323
    },
    {
      "text": "- Yeah, you've written about\nthis kind of organs question.",
      "start": 18468.45,
      "duration": 3.81
    },
    {
      "text": "- Yeah, exactly.\n- So if we think",
      "start": 18472.26,
      "duration": 2.598
    },
    {
      "text": "of interpretability as a kind\nof anatomy of neural networks,",
      "start": 18474.858,
      "duration": 3.672
    },
    {
      "text": "most of the circuits threads",
      "start": 18478.53,
      "duration": 1.23
    },
    {
      "text": "involve studying tiny little veins,",
      "start": 18479.76,
      "duration": 2.16
    },
    {
      "text": "looking at the small scale",
      "start": 18481.92,
      "duration": 1.11
    },
    {
      "text": "at individual neurons\nand how they connect.",
      "start": 18483.03,
      "duration": 2.67
    },
    {
      "text": "However, there are many natural questions",
      "start": 18485.7,
      "duration": 2.07
    },
    {
      "text": "that the small scale\napproach doesn't address.",
      "start": 18487.77,
      "duration": 3.09
    },
    {
      "text": "In contrast, the most\nprominent abstractions",
      "start": 18490.86,
      "duration": 2.13
    },
    {
      "text": "in biological anatomy involve\nlarger scale structures,",
      "start": 18492.99,
      "duration": 3.36
    },
    {
      "text": "like individual organs, like the heart,",
      "start": 18496.35,
      "duration": 2.94
    },
    {
      "text": "or entire organ systems,\nlike the respiratory system.",
      "start": 18499.29,
      "duration": 3.39
    },
    {
      "text": "And so we wonder, is there a\nrespiratory system or heart",
      "start": 18502.68,
      "duration": 3.93
    },
    {
      "text": "or brain region of an\nartificial neural network?",
      "start": 18506.61,
      "duration": 2.58
    },
    {
      "text": "- Yeah, exactly.",
      "start": 18509.19,
      "duration": 1.56
    },
    {
      "text": "And I mean, like if you\nthink about science, right?",
      "start": 18510.75,
      "duration": 1.51
    },
    {
      "text": "A lot of scientific fields have, you know,",
      "start": 18512.26,
      "duration": 3.56
    },
    {
      "text": "investigate things at many\nlevels of abstractions.",
      "start": 18515.82,
      "duration": 1.77
    },
    {
      "text": "In biology you have like, you know,",
      "start": 18517.59,
      "duration": 1.95
    },
    {
      "text": "molecular biology studying, you know,",
      "start": 18519.54,
      "duration": 1.77
    },
    {
      "text": "proteins and molecules and so on.",
      "start": 18521.31,
      "duration": 1.317
    },
    {
      "text": "And they have cellular biology,",
      "start": 18522.627,
      "duration": 1.443
    },
    {
      "text": "and then you have\nhistology studying tissues,",
      "start": 18524.07,
      "duration": 2.25
    },
    {
      "text": "and you have anatomy, and\nthen you have zoology,",
      "start": 18526.32,
      "duration": 3.0
    },
    {
      "text": "and then you have ecology.",
      "start": 18529.32,
      "duration": 0.833
    },
    {
      "text": "And so you have many,\nmany levels of abstraction",
      "start": 18530.153,
      "duration": 2.197
    },
    {
      "text": "or you know, physics,",
      "start": 18532.35,
      "duration": 0.87
    },
    {
      "text": "maybe the physics of individual particles,",
      "start": 18533.22,
      "duration": 1.5
    },
    {
      "text": "and then, you know, statistical physics",
      "start": 18534.72,
      "duration": 2.04
    },
    {
      "text": "gives you thermodynamics\nand things like this.",
      "start": 18536.76,
      "duration": 1.487
    },
    {
      "text": "And so you often have different\nlevels of abstraction.",
      "start": 18538.247,
      "duration": 3.373
    },
    {
      "text": "And I think that right\nnow we have, you know,",
      "start": 18541.62,
      "duration": 2.283
    },
    {
      "text": "mechanistic interpretability\nif it succeeds",
      "start": 18544.83,
      "duration": 1.56
    },
    {
      "text": "is sort of like a microbiology\nof neural networks,",
      "start": 18546.39,
      "duration": 2.88
    },
    {
      "text": "but we want something more like anatomy.",
      "start": 18549.27,
      "duration": 2.91
    },
    {
      "text": "And so, and you know, a\nquestion you might ask is,",
      "start": 18552.18,
      "duration": 3.03
    },
    {
      "text": "why can't you just go there directly?",
      "start": 18555.21,
      "duration": 1.2
    },
    {
      "text": "And I think the answer is superposition,",
      "start": 18556.41,
      "duration": 2.37
    },
    {
      "text": "at least in significant part.",
      "start": 18558.78,
      "duration": 1.02
    },
    {
      "text": "It's that it's actually very hard to see",
      "start": 18559.8,
      "duration": 3.36
    },
    {
      "text": "this macroscopic structure",
      "start": 18563.16,
      "duration": 2.19
    },
    {
      "text": "without first sort of breaking down",
      "start": 18565.35,
      "duration": 1.86
    },
    {
      "text": "the microscopic structure\nin the right way,",
      "start": 18567.21,
      "duration": 1.35
    },
    {
      "text": "and then studying how\nit connects together.",
      "start": 18568.56,
      "duration": 2.52
    },
    {
      "text": "But I'm hopeful that there\nis gonna be something",
      "start": 18571.08,
      "duration": 2.1
    },
    {
      "text": "much larger than features and circuits,",
      "start": 18573.18,
      "duration": 3.39
    },
    {
      "text": "and that we're gonna\nbe able to have a story",
      "start": 18576.57,
      "duration": 2.64
    },
    {
      "text": "that involves much bigger things,",
      "start": 18579.21,
      "duration": 1.557
    },
    {
      "text": "and then you can sort of study",
      "start": 18580.767,
      "duration": 1.503
    },
    {
      "text": "in detail the parts you care about.",
      "start": 18582.27,
      "duration": 1.65
    },
    {
      "text": "- I suppose to neurobiology,\nlike a psychologist",
      "start": 18583.92,
      "duration": 2.76
    },
    {
      "text": "or a psychiatrist of a neural network.",
      "start": 18586.68,
      "duration": 2.04
    },
    {
      "text": "- And I think that the beautiful thing",
      "start": 18588.72,
      "duration": 1.32
    },
    {
      "text": "would be if we could go,",
      "start": 18590.04,
      "duration": 1.47
    },
    {
      "text": "and rather than having disparate fields",
      "start": 18591.51,
      "duration": 2.55
    },
    {
      "text": "for those two things,",
      "start": 18594.06,
      "duration": 0.93
    },
    {
      "text": "if you could build a bridge between them-",
      "start": 18594.99,
      "duration": 1.77
    },
    {
      "text": "- Oh, right.\n- Such that you could go",
      "start": 18596.76,
      "duration": 1.83
    },
    {
      "text": "and have all of your\nhigher level abstractions",
      "start": 18598.59,
      "duration": 4.08
    },
    {
      "text": "be grounded very firmly\nin this very solid,",
      "start": 18602.67,
      "duration": 4.143
    },
    {
      "text": "you know, more rigorous,\nideally, foundation.",
      "start": 18607.83,
      "duration": 3.48
    },
    {
      "text": "- What do you think is the difference",
      "start": 18611.31,
      "duration": 0.99
    },
    {
      "text": "between the human brain, the\nbiological neural network",
      "start": 18612.3,
      "duration": 4.11
    },
    {
      "text": "and the artificial neural network?",
      "start": 18616.41,
      "duration": 1.38
    },
    {
      "text": "- Well, the neuroscientists\nhave a much harder job than us.",
      "start": 18617.79,
      "duration": 2.61
    },
    {
      "text": "You know, sometimes I just\nlike count my blessings",
      "start": 18620.4,
      "duration": 2.1
    },
    {
      "text": "by how much easier my job",
      "start": 18622.5,
      "duration": 1.02
    },
    {
      "text": "is than the neuroscientists, right?",
      "start": 18623.52,
      "duration": 1.41
    },
    {
      "text": "So I have, we can record\nfrom all the neurons.",
      "start": 18624.93,
      "duration": 3.48
    },
    {
      "text": "We can do that on\narbitrary amounts of data.",
      "start": 18628.41,
      "duration": 3.6
    },
    {
      "text": "The neurons don't change",
      "start": 18632.01,
      "duration": 0.9
    },
    {
      "text": "while you're doing that, by the way.",
      "start": 18632.91,
      "duration": 2.19
    },
    {
      "text": "You can go and ablate neurons,",
      "start": 18635.1,
      "duration": 1.74
    },
    {
      "text": "you can edit the connections and so on,",
      "start": 18636.84,
      "duration": 1.65
    },
    {
      "text": "and then you can undo those changes.",
      "start": 18638.49,
      "duration": 2.28
    },
    {
      "text": "That's pretty great.",
      "start": 18640.77,
      "duration": 1.5
    },
    {
      "text": "You can force, you can\nintervene on any neuron",
      "start": 18642.27,
      "duration": 1.95
    },
    {
      "text": "and force it active and see what happens.",
      "start": 18644.22,
      "duration": 2.19
    },
    {
      "text": "You know, which neurons are\nconnected to everything, right?",
      "start": 18646.41,
      "duration": 2.28
    },
    {
      "text": "Neuroscientists wanna get the connectome,",
      "start": 18648.69,
      "duration": 1.17
    },
    {
      "text": "we have the connectome and we have it",
      "start": 18649.86,
      "duration": 1.65
    },
    {
      "text": "for like much bigger than like C. elegans.",
      "start": 18651.51,
      "duration": 2.19
    },
    {
      "text": "And then not only do\nwe have the connectome,",
      "start": 18653.7,
      "duration": 2.4
    },
    {
      "text": "we know what, you know, which neurons",
      "start": 18656.1,
      "duration": 2.76
    },
    {
      "text": "excite or inhibit each other, right.",
      "start": 18658.86,
      "duration": 1.23
    },
    {
      "text": "So we have, it's not just that we know",
      "start": 18660.09,
      "duration": 1.237
    },
    {
      "text": "that like the binary\nmass, we know the weights.",
      "start": 18661.327,
      "duration": 3.713
    },
    {
      "text": "We can take gradients,",
      "start": 18665.04,
      "duration": 0.833
    },
    {
      "text": "we know computationally\nwhat each neuron does.",
      "start": 18665.873,
      "duration": 2.407
    },
    {
      "text": "So I don't know the list goes on and on.",
      "start": 18668.28,
      "duration": 1.83
    },
    {
      "text": "We just have so many advantages\nover neuroscientists.",
      "start": 18670.11,
      "duration": 4.05
    },
    {
      "text": "And then despite having\nall those advantages,",
      "start": 18674.16,
      "duration": 2.94
    },
    {
      "text": "it's really hard.",
      "start": 18677.1,
      "duration": 1.05
    },
    {
      "text": "And so one thing I do\nsometimes think is like,",
      "start": 18678.15,
      "duration": 2.52
    },
    {
      "text": "gosh, like if it's this hard for us,",
      "start": 18680.67,
      "duration": 1.74
    },
    {
      "text": "it seems impossible under the constraints",
      "start": 18682.41,
      "duration": 1.74
    },
    {
      "text": "of neuroscience or, you\nknow, near impossible.",
      "start": 18684.15,
      "duration": 2.73
    },
    {
      "text": "I don't know, maybe part of me is like,",
      "start": 18686.88,
      "duration": 1.83
    },
    {
      "text": "I've got a few neuroscientists on my team.",
      "start": 18688.71,
      "duration": 1.47
    },
    {
      "text": "Maybe I'm sort sort of like, ah, you know,",
      "start": 18690.18,
      "duration": 3.18
    },
    {
      "text": "maybe the neuroscientists,\nmaybe some of them",
      "start": 18693.36,
      "duration": 1.924
    },
    {
      "text": "would like to have an easier\nproblem that's still very hard",
      "start": 18695.284,
      "duration": 3.206
    },
    {
      "text": "and they could come and\nwork on neural networks.",
      "start": 18698.49,
      "duration": 2.46
    },
    {
      "text": "And then after we figure out things",
      "start": 18700.95,
      "duration": 2.19
    },
    {
      "text": "in sort of the easy little pond of trying",
      "start": 18703.14,
      "duration": 2.55
    },
    {
      "text": "to understand neural networks,\nwhich is still very hard,",
      "start": 18705.69,
      "duration": 2.34
    },
    {
      "text": "then we could go back to\nbiological neuroscience.",
      "start": 18708.03,
      "duration": 3.12
    },
    {
      "text": "- I love what you've\nwritten about the goal",
      "start": 18711.15,
      "duration": 1.59
    },
    {
      "text": "of mech interp research as two goals,",
      "start": 18712.74,
      "duration": 3.51
    },
    {
      "text": "safety and beauty.",
      "start": 18716.25,
      "duration": 1.47
    },
    {
      "text": "So can you talk about the\nbeauty side of things?",
      "start": 18717.72,
      "duration": 2.19
    },
    {
      "text": "- Yeah, so, you know,\nthere's this funny thing",
      "start": 18719.91,
      "duration": 2.25
    },
    {
      "text": "where I think some people want,",
      "start": 18722.16,
      "duration": 2.867
    },
    {
      "text": "some people are kind of\ndisappointed by neural networks,",
      "start": 18725.027,
      "duration": 1.873
    },
    {
      "text": "I think, where they're like,\nah, you know, neural networks,",
      "start": 18726.9,
      "duration": 3.51
    },
    {
      "text": "it's these just these simple rules,",
      "start": 18730.41,
      "duration": 1.44
    },
    {
      "text": "and then you just like\ndo a bunch of engineering",
      "start": 18731.85,
      "duration": 1.35
    },
    {
      "text": "to scale it up and it works really well.",
      "start": 18733.2,
      "duration": 1.74
    },
    {
      "text": "And like, where's the like complex ideas?",
      "start": 18734.94,
      "duration": 2.22
    },
    {
      "text": "You know, this isn't like a very nice,",
      "start": 18737.16,
      "duration": 1.47
    },
    {
      "text": "beautiful scientific result.",
      "start": 18738.63,
      "duration": 1.413
    },
    {
      "text": "And I sometimes think\nwhen people say that,",
      "start": 18741.09,
      "duration": 2.61
    },
    {
      "text": "I picture them being like, you\nknow, evolution is so boring.",
      "start": 18743.7,
      "duration": 2.94
    },
    {
      "text": "It's just a bunch of simple rules",
      "start": 18746.64,
      "duration": 1.29
    },
    {
      "text": "and you run evolution for a\nlong time and you get biology.",
      "start": 18747.93,
      "duration": 2.67
    },
    {
      "text": "Like what a sucky, you know,",
      "start": 18750.6,
      "duration": 2.4
    },
    {
      "text": "way for biology to have turned out.",
      "start": 18753.0,
      "duration": 1.32
    },
    {
      "text": "Where's the complex rules?",
      "start": 18754.32,
      "duration": 1.26
    },
    {
      "text": "But the beauty is that the\nsimplicity generates complexity.",
      "start": 18755.58,
      "duration": 4.383
    },
    {
      "text": "You know, biology has these simple rules",
      "start": 18761.19,
      "duration": 1.65
    },
    {
      "text": "and it gives rise to,\nyou know, all the life",
      "start": 18762.84,
      "duration": 3.42
    },
    {
      "text": "and ecosystems that we see around us,",
      "start": 18766.26,
      "duration": 1.68
    },
    {
      "text": "all the beauty of nature, that\nall just comes from evolution",
      "start": 18767.94,
      "duration": 3.33
    },
    {
      "text": "and from something very simple evolution.",
      "start": 18771.27,
      "duration": 1.74
    },
    {
      "text": "And similarly, I think\nthat neural networks build,",
      "start": 18773.01,
      "duration": 3.45
    },
    {
      "text": "you know, create enormous complexity",
      "start": 18776.46,
      "duration": 2.64
    },
    {
      "text": "and beauty inside and\nstructure inside themselves",
      "start": 18779.1,
      "duration": 2.61
    },
    {
      "text": "that people generally don't look at",
      "start": 18781.71,
      "duration": 2.22
    },
    {
      "text": "and don't try to understand",
      "start": 18783.93,
      "duration": 1.05
    },
    {
      "text": "because it's hard to understand.",
      "start": 18784.98,
      "duration": 1.77
    },
    {
      "text": "But I think that there is\nan incredibly rich structure",
      "start": 18786.75,
      "duration": 4.89
    },
    {
      "text": "to be discovered inside neural networks,",
      "start": 18791.64,
      "duration": 2.34
    },
    {
      "text": "a lot of very deep beauty",
      "start": 18793.98,
      "duration": 1.36
    },
    {
      "text": "if we're just willing to take the time",
      "start": 18796.41,
      "duration": 1.89
    },
    {
      "text": "to go and see it and understand it.",
      "start": 18798.3,
      "duration": 2.16
    },
    {
      "text": "- Yeah, I love mech interp,",
      "start": 18800.46,
      "duration": 1.71
    },
    {
      "text": "the feeling like we are understanding",
      "start": 18802.17,
      "duration": 2.87
    },
    {
      "text": "or getting glimpses of understanding",
      "start": 18805.04,
      "duration": 2.26
    },
    {
      "text": "the magic that's going on\ninside is really wonderful.",
      "start": 18807.3,
      "duration": 2.7
    },
    {
      "text": "- It feels to me like one of the questions",
      "start": 18810.0,
      "duration": 2.687
    },
    {
      "text": "that's just calling out to be asked,",
      "start": 18812.687,
      "duration": 2.413
    },
    {
      "text": "and I'm sort of, I mean, a lot of people",
      "start": 18815.1,
      "duration": 1.647
    },
    {
      "text": "are thinking about this,\nbut I'm often surprised",
      "start": 18816.747,
      "duration": 2.253
    },
    {
      "text": "that not more are is, how is it that",
      "start": 18819.0,
      "duration": 3.15
    },
    {
      "text": "we don't know how to\ncreate computer systems",
      "start": 18822.15,
      "duration": 2.07
    },
    {
      "text": "that can do these things,",
      "start": 18824.22,
      "duration": 1.29
    },
    {
      "text": "and yet we have these amazing systems",
      "start": 18825.51,
      "duration": 3.15
    },
    {
      "text": "that we don't know how to\ndirectly create computer programs",
      "start": 18828.66,
      "duration": 1.86
    },
    {
      "text": "that can do these things,",
      "start": 18830.52,
      "duration": 0.833
    },
    {
      "text": "but these neural networks can\ndo all these amazing things?",
      "start": 18831.353,
      "duration": 1.927
    },
    {
      "text": "And it just feels like that\nis obviously the question",
      "start": 18833.28,
      "duration": 2.46
    },
    {
      "text": "that sort of is calling out\nto be answered if you are,",
      "start": 18835.74,
      "duration": 2.67
    },
    {
      "text": "if you have any degree of curiosity.",
      "start": 18838.41,
      "duration": 2.34
    },
    {
      "text": "It's like how is it that\nhumanity now has these artifacts",
      "start": 18840.75,
      "duration": 3.54
    },
    {
      "text": "that can do these things\nthat we don't know how to do?",
      "start": 18844.29,
      "duration": 2.55
    },
    {
      "text": "- Yeah, I love the image",
      "start": 18846.84,
      "duration": 1.05
    },
    {
      "text": "of the circuits reaching towards the light",
      "start": 18847.89,
      "duration": 2.07
    },
    {
      "text": "of the objective function.",
      "start": 18849.96,
      "duration": 1.17
    },
    {
      "text": "- Yeah, it's just, it's this organic thing",
      "start": 18851.13,
      "duration": 1.92
    },
    {
      "text": "that we've grown and we have\nno idea what we've grown.",
      "start": 18853.05,
      "duration": 2.31
    },
    {
      "text": "Well, thank you for working on safety",
      "start": 18855.36,
      "duration": 2.16
    },
    {
      "text": "and thank you for appreciating",
      "start": 18857.52,
      "duration": 1.08
    },
    {
      "text": "the beauty of the things you discover.",
      "start": 18858.6,
      "duration": 2.76
    },
    {
      "text": "And thank you for talking today, Chris.",
      "start": 18861.36,
      "duration": 1.65
    },
    {
      "text": "This was wonderful.\n- Yeah.",
      "start": 18863.01,
      "duration": 0.84
    },
    {
      "text": "Thank you for taking the\ntime to chat as well.",
      "start": 18863.85,
      "duration": 2.22
    },
    {
      "text": "- Thanks for listening to this\nconversation with Chris Olah,",
      "start": 18866.07,
      "duration": 2.19
    },
    {
      "text": "and before that with Dario\nAmodei and Amanda Askell.",
      "start": 18868.26,
      "duration": 3.75
    },
    {
      "text": "To support this podcast,",
      "start": 18872.01,
      "duration": 1.11
    },
    {
      "text": "please check out our\nsponsors in the description.",
      "start": 18873.12,
      "duration": 2.94
    },
    {
      "text": "And now let me leave you with\nsome words from Alan Watts.",
      "start": 18876.06,
      "duration": 3.787
    },
    {
      "text": "\"The only way to make sense out of change",
      "start": 18879.847,
      "duration": 2.993
    },
    {
      "text": "is to plunge into it, move with it,",
      "start": 18882.84,
      "duration": 2.91
    },
    {
      "text": "and join the dance.\"",
      "start": 18885.75,
      "duration": 1.677
    },
    {
      "text": "Thank you for listening and\nhope to see you next time.",
      "start": 18888.48,
      "duration": 2.943
    }
  ],
  "full_text": "- If you extrapolate the curves that we've had so far, right? If you say, well, I don't know, we're starting to get to like PhD level, and last year we were\nat undergraduate level, and the year before we\nwere at like the level of a high school student. Again, you can quibble with at what tasks and for what, we're\nstill missing modalities, but those are being added,\nlike computer use was added, like image generation has been added. If you just kind of like eyeball the rate at which these capabilities\nare increasing, it does make you think that we'll get there by 2026 or 2027. I think there are still worlds where it doesn't happen in 100 years. Those world, the number of those worlds is rapidly decreasing. We are rapidly running out\nof truly convincing blockers, truly compelling reasons why this will not happen\nin the next few years. The scale up is very quick. Like we do this today, we make a model, and then we deploy thousands, maybe tens of thousands\nof instances of it. I think by the time, you know, certainly within two to three years, whether we have these\nsuper powerful AIs or not, clusters are gonna get to the size where you'll be able to\ndeploy millions of these. I am optimistic about meaning. I worry about economics and\nthe concentration of power. That's actually what I worry about more, the abuse of power. - And AI increases the\namount of power in the world, and if you concentrate that power and abuse that power, it\ncan do immeasurable damage. - Yes, it's very frightening. It's very frightening. - The following is a\nconversation with Dario Amodei, CEO of Anthropic, the\ncompany that created Claude that is currently and often at the top of most LLM benchmark leaderboards. On top of that, Dario\nand the Anthropic team have been outspoken advocates for taking the topic of\nAI safety very seriously, and they have continued to publish a lot of fascinating AI research\non this and other topics. I'm also joined afterwards by two other brilliant\npeople from Anthropic. First Amanda Askell, who is a researcher working on alignment and\nfine tuning of Claude, including the design of Claude's\ncharacter and personality. A few folks told me\nshe has probably talked with Claude more than\nany human at Anthropic. So she was definitely a fascinating person to talk to about prompt engineering and practical advice on how\nto get the best out of Claude. After that, Chris Olah\nstopped by for a chat. He's one of the pioneers of the field of mechanistic interpretability, which is an exciting set of efforts that aims to reverse\nengineer neural networks to figure out what's going on inside, inferring behaviors from\nneural activation patterns inside the network. This is a very promising approach for keeping future super\nintelligent AI systems safe. For example, by detecting\nfrom the activations when the model is trying to deceive the human it is talking to. This is the \"Lex Fridman Podcast.\" To support it, please check out our sponsors in the description. And now, dear friends,\nhere's Dario Amodei. Let's start with the\nbig idea of scaling laws and the Scaling Hypothesis. What is it? What is its history? And where do we stand today? - So I can only describe it as, you know, as it relates to kind\nof my own experience. But I've been in the AI\nfield for about 10 years and it was something I\nnoticed very early on. So I first joined the AI world when I was working at Baidu\nwith Andrew Ng in late 2014, which is almost exactly 10 years ago now. And the first thing we worked on was speech recognition systems. And in those days I think\ndeep learning was a new thing, it had made lots of progress, but everyone was always saying, we don't have the algorithms\nwe need to succeed. You know, we're not, we're only matching a tiny, tiny fraction. There's so much we need to kind\nof discover algorithmically. We haven't found the picture of how to match the human brain. And when, you know, in\nsome ways I was fortunate, I was kind of, you know, you can have almost\nbeginner's luck, right? I was like a newcomer to the field and, you know, I looked at the neural net that we were using for speech, the recurrent neural networks, and I said, I don't know, what if you make them bigger\nand give them more layers? And what if you scale up the\ndata along with this, right? I just saw these as like independent dials that you could turn. And I noticed that the model started to do better and better as\nyou gave them more data, as you made the models larger, as you trained them for longer. And I didn't measure things\nprecisely in those days, but along with colleagues, we very much got the informal sense that the more data and the more compute and the more training you\nput into these models, the better they perform. And so initially my thinking was, hey, maybe that is just true for speech recognition systems, right? Maybe that's just one particular quirk, one particular area. I think it wasn't until 2017\nwhen I first saw the results from GPT-1 that it clicked for me that language is probably the area in which we can do this. We can get trillions of\nwords of language data, we can train on them. And the models we were\ntraining those days were tiny. You could train them on\none to eight GPUs whereas, you know, now we train\njobs on tens of thousands, soon going to hundreds\nof thousands of GPUs. And so when I saw those\ntwo things together and, you know, there were a few people like Ilya Sutskever,\nwho you've interviewed, who had somewhat similar views, right? He might have been the first one, although I think a few\npeople came to similar views around the same time, right? There was, you know, Rich\nSutton's Bitter Lesson, there was Gwern wrote about\nthe Scaling Hypothesis. But I think somewhere\nbetween 2014 and 2017 was when it really clicked for me, when I really got conviction that, hey, we're gonna be able to do these incredibly wide cognitive tasks if we just scale up the models. And at every stage of scaling, there are always arguments. And you know, when I first\nheard them, honestly, I thought probably I'm\nthe one who's wrong, and, you know, all these\nexperts in the field are right. They know the situation\nbetter than I do, right? There's, you know, the\nChomsky argument about like, you can get syntactics, but\nyou can't get semantics. There was this idea, oh, you\ncan make a sentence make sense, but you can't make a paragraph make sense. The latest one we have today is, you know, we're gonna run out of data, or the data isn't high quality enough, or models can't reason. And each time, every time we manage to either find a way around or scaling just is the way around. Sometimes it's one,\nsometimes it's the other. And so I'm now at this\npoint, I still think, you know, it's always quite uncertain. We have nothing but inductive\ninference to tell us that the next two years are gonna be like the last 10 years. But I've seen the movie enough times, I've seen the story\nhappen for enough times to really believe that\nprobably the scaling is going to continue and\nthat there's some magic to it that we haven't really explained\non a theoretical basis yet. - And of course the scaling\nhere is bigger networks, bigger data, bigger compute. - Yes.\n- All of those. - In particular, linear scaling up of bigger networks, bigger training times and more and more data. So all of these things, almost\nlike a chemical reaction, you know, you have three ingredients in the chemical reaction, and you need to linearly scale\nup the three ingredients. If you scale up one, not the others, you run out of the other reagents and the reaction stops. But if you scale up everything in series, then the reaction can proceed. - And of course, now that you have this kind of empirical science/art, you can apply to other more nuanced things like scaling laws applied\nto interpretability, or scaling laws applied to post-training, or just seeing how does this thing scale. But the big scaling law, I guess the underlying Scaling Hypothesis has to do with big networks,\nbig data leads to intelligence. - Yeah, we've documented scaling laws in lots of domains other\nthan language, right? So initially, the paper we did that first showed it was in early 2020 where we first showed it for language. There was then some work late in 2020 where we showed the same\nthing for other modalities, like images, video, text-to-image, image-to-text, math. They all had the same pattern. And you're right, now\nthere are other stages like post-training or there are new types of reasoning models. And in all of those cases\nthat we've measured, we see similar types of scaling laws. - A bit of a philosophical question, but what's your intuition\nabout why bigger is better in terms of network size and data size? Why does it lead to\nmore intelligent models? - So in my previous\ncareer as a biophysicist, so I did physics undergrad and then biophysics in grad school. So I think back to what\nI know as a physicist, which is actually much less than what some of my colleagues at Anthropic have in terms\nof expertise in physics. There's this concept called the 1/f noise and 1/x distributions\nwhere often, you know, just like if you add up a bunch of natural processes, you get a Gaussian. If you add up a bunch of kind of differently\ndistributed natural processes, if you like take a probe and hook it up to a resistor, the distribution of the thermal noise in the resistor goes as\none over the frequency. It's some kind of natural\nconvergent distribution. And I think what it amounts to is that if you look at a lot of things that are produced by some natural process that has a lot of different scales, right? Not a Gaussian, which is\nkind of narrowly distributed, but you know, if I look at kind of like large and small fluctuations that lead to electrical noise, they have this decaying 1/x distribution. And so now I think of like patterns in the physical world, right? Or in language. If I think about the patterns in language, there are some really simple patterns. Some words are much more\ncommon than others like \"the,\" then there's basic noun verb structure, then there's the fact that, you know, nouns and verbs have to agree,\nthey have to coordinate. And there's the higher\nlevel sentence structure, then there's the thematic\nstructure of paragraphs. And so the fact that there's\nthis regressing structure, you can imagine that as you\nmake the networks larger, first they capture the\nreally simple correlations, the really simple patterns, and there's this long\ntail of other patterns. And if that long tail of other\npatterns is really smooth like it is with the 1/f noise in, you know, physical\nprocesses like resistors, then you can imagine as you\nmake the network larger, it's kind of capturing more\nand more of that distribution, and so that smoothness gets reflected in how well the models are at predicting and how well they perform. Language is an evolved process, right? We've developed language, we have common words\nand less common words. We have common expressions\nand less common expressions. We have ideas, cliches that\nare expressed frequently, and we have novel ideas. And that process has developed, has evolved with humans\nover millions of years. And so the guess, and this is pure speculation would be that there's some kind\nof long tail distribution of the distribution of these ideas. - So there's the long tail, but also there's the\nheight of the hierarchy of concepts that you're building up. So the bigger the network, presumably you have a higher capacity to- - Exactly, if you have a small network, you only get the common stuff, right? if I take a tiny neural network, it's very good at\nunderstanding that, you know, a sentence has to have, you know, verb, adjective, noun, right? But it's terrible at deciding what those verb, adjective\nand noun should be and whether they should make sense. If I make it just a little\nbigger, it gets good at that, then suddenly it's good at the sentences, but it's not good at the paragraphs. And so these rarer and more complex patterns get picked up as I add more capacity to the network. - Well, the natural question then is, what's the ceiling of this? - Yeah.\n- Like how complicated and complex is the real world? How much stuff is there to learn? - I don't think any of us knows\nthe answer to that question. My strong instinct would be that there's no ceiling below\nthe level of humans, right? We humans are able to understand\nthese various patterns, and so that makes me think\nthat if we continue to, you know, scale up these models to kind of develop new\nmethods for training them and scaling them up, that\nwill at least get to the level that we've gotten to with humans. There's then a question of, you know, how much more is it possible\nto understand than humans do? How much is it possible to be smarter and more perceptive than humans? I would guess the answer has\ngot to be domain dependent. If I look at an area like biology, and, you know, I wrote this essay, \"Machines of Loving Grace.\" It seems to me that humans are struggling to understand the complexity\nof biology, right? If you go to Stanford or to Harvard or to Berkeley, you have whole\ndepartments of, you know, folks trying to study, you know, like the immune system\nor metabolic pathways, and each person understands\nonly a tiny bit, part of it, specializes, and they're struggling to\ncombine their knowledge with that of other humans. And so I have an instinct that there's a lot of room at the\ntop for AIs to get smarter. If I think of something like materials in the physical world or you know, like addressing, you know, conflicts between humans\nor something like that. I mean, you know, it may be there's only, some of these problems are not\nintractable, but much harder. And it may be that there's only so well you can do at some of these things, right? Just like with speech recognition, there's only so clear\nI can hear your speech. So I think in some areas\nthere may be ceilings, you know, that are very close\nto what humans have done. in other areas, those\nceilings may be very far away. And I think we'll only find out\nwhen we build these systems. It's very hard to know in advance. We can speculate, but we can't be sure. - And in some domains, the ceiling might have to do with human bureaucracies and things like this, as you write about. - Yes.\n- So humans fundamentally have to be part of the loop. That's the cause of the ceiling, not maybe the limits of the intelligence. - Yeah, I think in many\ncases, you know, in theory, technology could change very fast, for example, all the\nthings that we might invent with respect to biology. But remember there's a, you know, there's a clinical trial system\nthat we have to go through to actually administer\nthese things to humans. I think that's a mixture of things that are unnecessary and bureaucratic and things that kind of protect\nthe integrity of society. And the whole challenge is that it's hard to tell what's going on. It's hard to tell which is which, right? My view is definitely, I think\nin terms of drug development, my view is that we're too slow\nand we're too conservative. But certainly if you get\nthese things wrong, you know, it's possible to risk people's\nlives by being too reckless. And so at least some of\nthese human institutions are in fact protecting people. So it's all about finding the balance. I strongly suspect that balance\nis kind of more on the side of pushing to make things happen faster, but there is a balance. - If we do hit a limit, if we do hit a slow down\nin the scaling laws, what do you think would be the reason? Is it compute limited, data limited? Is it something else, idea limited? - So, a few things. Now we're talking about hitting the limit before we get to the level of humans and the skill of humans. So, I think one that's, you know, one that's popular today\nand I think, you know, could be a limit that we run into. Like most of the limits,\nI would bet against it, but it's definitely possible\nis we simply run out of data. There's only so much data on the internet and there's issues with the\nquality of the data, right? You can get hundreds of trillions\nof words on the internet, but a lot of it is repetitive or it's search engine, you know, search engine optimization\ndrivel, or maybe in the future it'll even be text\ngenerated by AIs itself. And so I think there are limits to what can be produced in this way. That said, we and I would\nguess other companies are working on ways to make data synthetic where you can, you know, you can use the model\nto generate more data of the type that you have already or even generate data from scratch. If you think about what was done with DeepMind's AlphaGo Zero, they managed to get a\nbot all the way from, you know, no ability to play Go whatsoever to above human level just\nby playing against itself. There was no example\ndata from humans required in the AlphaGo Zero version of it. The other direction, of course,\nis these reasoning models that do chain of thought and stop to think and reflect on their own thinking. In a way, that's another\nkind of synthetic data coupled with reinforcement learning. So my guess is with one of those methods, we'll get around the data limitation or there may be other sources of data that are available. We could just observe that even if there's no problem with data, as we start to scale models up, they just stop getting better. It seemed to be a reliable observation that they've gotten better,\nthat could just stop at some point for a reason\nwe don't understand. The answer could be that we need to, you know, we need to invent\nsome new architecture. There have been problems in the past with, say, numerical stability of models where it looked like\nthings were leveling off, but actually, you know, when we found the right unblocker, they didn't end up doing so. So perhaps there's some\nnew optimization method or some new technique we\nneed to unblock things. I've seen no evidence of that so far. But if things were to slow down, that perhaps could be one reason. - What about the limits of compute? Meaning the expensive nature of building bigger and\nbigger data centers. - So right now, I think, you know, most of the frontier model companies I would guess are operating in, you know, roughly, you know, $1 billion scale, plus or minus a factor of three, right? Those are the models that exist now or are being trained now. I think next year, we're\ngonna go to a few billion, and then 2026, we may go to,\nyou know, above 10 billion, and probably by 2027,\ntheir ambitions to build 100 billion dollar clusters, and I think all of that\nactually will happen. There's a lot of determination to build the compute to do it within this country, and I would guess that\nit actually does happen. Now, if we get to 100 billion, that's still not enough compute, that's still not enough scale then either we need even more scale or we need to develop some way of doing it more efficiently\nof shifting the curve. I think between all of these, one of the reasons I'm bullish about powerful AI happening so fast is just that if you extrapolate the next few points on the curve, we're very quickly getting towards human level ability, right? Some of the new models that we developed, some reasoning models that\nhave come from other companies, they're starting to get\nto what I would call the PhD or professional level, right? If you look at their coding ability, the latest model we released, Sonnet 3.5, the new or updated version, it gets something like 50% on SWE-bench, and SWE-bench is an example of a bunch of professional, real world\nsoftware engineering tasks. At the beginning of the year, I think the state of the art was 3 or 4%. So in 10 months we've gone\nfrom 3% to 50% on this task, and I think in another year,\nwe'll probably be at 90%. I mean, I don't know, but\nmight even be less than that. We've seen similar things\nin graduate level math, physics, and biology from\nmodels like OpenAI's o1. So if we just continue to\nextrapolate this, right, in terms of skill that we have, I think if we extrapolate\nthe straight curve, within a few years, we will\nget to these models being, you know, above the\nhighest professional level in terms of humans. Now, will that curve continue? You've pointed to, and I've\npointed to a lot of reasons why, you know, possible reasons\nwhy that might not happen. But if the extrapolation curve continues, that is the trajectory we're on. - So Anthropic has several competitors. It'd be interesting to get\nyour sort of view of it all. OpenAI, Google, xAI, Meta. What does it take to win in the broad sense of win in this space? - Yeah, so I want to separate\nout a couple things, right? So, you know, Anthropic's mission is to kind of try to make\nthis all go well, right? And you know, we have a theory of change called race to the top, right? Race to the top is about trying to push the other players to do the right thing by setting an example. It's not about being the good guy, it's about setting things up so that all of us can be the good guy. I'll give a few examples of this. Early in the history of Anthropic, one of our co-founders, Chris Olah, who I believe you're interviewing soon, you know, he's the co-founder of the field of mechanistic interpretability,\nwhich is an attempt to understand what's\ngoing on inside AI models. So we had him and one of our early teams focus on this area of interpretability, which we think is good for making models safe and transparent. For three or four years, that had no commercial\napplication whatsoever. It still doesn't today. We're doing some early betas with it, and probably it will eventually, but you know, this is a\nvery, very long research bed and one in which we've built in public and shared our results publicly. And we did this because, you know, we think it's a way to make models safer. An interesting thing is\nthat as we've done this, other companies have\nstarted doing it as well, in some cases because\nthey've been inspired by it, in some cases because they're\nworried that, you know, if other companies are doing this to look more responsible, they wanna look more responsible too. No one wants to look like\nthe irresponsible actor, and so they adopt this as well. When folks come to Anthropic, interpretability often a draw, and I tell them, the other\nplaces you didn't go, tell them why you came here, and then you see soon that there's interpretability\nteams elsewhere as well. And in a way, that takes away\nour competitive advantage because it's like, oh, now\nothers are doing it as well, but it's good for the broader system, and so we have to invent\nsome new thing that we're doing that others\naren't doing as well. And the hope is to basically\nbid up the importance of doing the right thing. And it's not about us\nin particular, right? It's not about having\none particular good guy. Other companies can do this as well. If they join the race\nto do this, you know, that's the best news ever, right? It's just, it's about kind\nof shaping the incentives to point upward instead of shaping the incentives to point downward. - And we should say this\nexample of the field of mechanistic interpretability is just a rigorous, non-hand\nwavy way of doing AI safety, or it's tending that way. - Trying to, I mean, I\nthink we're still early in terms of our ability to see things, but I've been surprised at how much we've been able to look\ninside these systems and understand what we see, right? Unlike with the scaling laws where it feels like\nthere's some, you know, law that's driving these\nmodels to perform better, on the inside, the\nmodels aren't, you know, there's no reason why\nthey should be designed for us to understand them, right? They're designed to operate,\nthey're designed to work, just like the human brain\nor human biochemistry. They're not designed for a\nhuman to open up the hatch, look inside and understand them. But we have found, and you know, you can talk in much more\ndetail about this to Chris, that when we open them up, when we do look inside them, we find things that are\nsurprisingly interesting. - And as a side effect, you also get to see the beauty of these models. You get to explore sort\nof the beautiful nature of large neural networks through the mech interp\nkind of methodology. - I'm amazed at how clean it's been. I'm amazed at things like induction heads. I'm amazed at things like, you know, that we can, you know,\nuse sparse auto-encoders to find these directions\nwithin the networks, and that the directions correspond to these very clear concepts. We demonstrated this a bit with the Golden Gate Bridge Claude. So this was an experiment\nwhere we found a direction inside one of the neural network's layers that corresponded to\nthe Golden Gate Bridge and we just turned that way up. And so we released this model as a demo, it was kind of half a\njoke, for a couple days, but it was illustrative of\nthe method we developed. And you could take the Golden Gate, you could take the model, you\ncould ask it about anything, you know, it would be like, you could say, \"How was your day\" and anything you asked, because this feature was activated, would connect to the Golden Gate Bridge. So it would say, you know, \"I'm feeling relaxed and expansive, much like the arches of\nthe Golden Gate Bridge\" or, you know. - It would masterfully change topic to the Golden Gate Bridge\nand it integrate it. There was also a sadness to it, to the focus it had on\nthe Golden Gate Bridge. I think people quickly fell\nin love with it, I think, so people already miss it 'cause it was taken down\nI think after a day. - Somehow these interventions on the model where you kind of adjust its behavior somehow emotionally\nmade it seem more human than any other version of the model. - It has a strong\npersonality, strong identity. - It has a strong personality. It has these kind of\nlike obsessive interests. You know, we can all think of someone who's like obsessed with something. So it does make it feel\nsomehow a bit more human. - Let's talk about the present. Let's talk about Claude. So this year, a lot has happened. In March, Claude 3, Opus,\nSonnet, Haiku were released, then Claude 3.5 Sonnet in July, with an updated version just now released, and then also Claude\n3.5 Haiku was released. Okay, can you explain the difference between Opus, Sonnet and Haiku, and how we should think\nabout the different versions? - Yeah, so let's go back to March when we first released these three models. So, you know, our thinking was, you know, different companies produce\nkind of large and small models, better and worse models. We felt that there was demand both for a really\npowerful model, you know, and you that might be a little bit slower that you'd have to pay more for, and also for fast, cheap models that are as smart as they can be for how fast and cheap, right? Whenever you wanna do some\nkind of like, you know, difficult analysis, like if, you know, I wanna write code, for instance, or you know, I wanna brainstorm ideas, or I wanna do creative writing, I want the really powerful model. But then there's a lot\nof practical applications in a business sense where it's like I'm interacting with a website. You know, like, I'm like doing my taxes, or I'm, you know, talking to a, you know, to like a legal advisor and\nI want to analyze a contract or, you know, we have plenty of companies that are just like, you know, I wanna do auto complete\non my IDE or something. And for all of those\nthings, you want to act fast and you want to use\nthe model very broadly. So we wanted to serve that\nwhole spectrum of needs. So we ended up with this, you know, this kind of poetry theme. And so what's a really short poem? It's a haiku. And so Haiku is the small,\nfast, cheap model that is, you know, was at the time\nwas released surprisingly, surprisingly intelligent for\nhow fast and cheap it was. Sonnet is a medium sized poem, right, a couple paragraphs, and so Sonnet was the middle model. It is smarter but also\na little bit slower, a little bit more expensive. And Opus, like a magnum\nopus is a large work, Opus was the largest,\nsmartest model at the time. So that was the original\nkind of thinking behind it. And our thinking then was,\nwell, each new generation of models should shift\nthat trade-off curve. So when we released Sonnet 3.5, it has the same, roughly\nthe same, you know, cost and speed as the Sonnet 3 model. But it increased its intelligence to the point where it was smarter than the original Opus 3 model, especially for code but\nalso just in general. And so now, you know, we've\nshown results for Haiku 3.5, and I believe Haiku 3.5,\nthe smallest new model is about as good as Opus\n3, the largest old model. So basically the aim here\nis to shift the curve, and then at some point,\nthere's gonna be an Opus 3.5. Now, every new generation\nof models has its own thing. They use new data, their\npersonality changes in ways that we kind of, you know, try to steer but are\nnot fully able to steer. And so there's never quite\nthat exact equivalence where the only thing you're\nchanging is intelligence. We always try and improve other things, and some things change without\nus knowing or measuring. So it's very much an inexact science. In many ways, the manner and\npersonality of these models is more an art than it is a science. - So what is sort of the reason for the span of time between, say, Claude Opus 3.0 and 3.5? What takes that time? If you can speak to. - Yeah, so there's different processes. There's pre-training, which is, you know, just kind of the normal\nlanguage model training, and that takes a very long time. That uses, you know, these days, you know, tens of thousands, sometimes many tens of\nthousands of GPUs or TPUs or Trainium, or you know,\nwe use different platforms, but, you know, accelerator chips, often training for months. There's then a kind of post-training phase where we do reinforcement\nlearning from human feedback, as well as other kinds of\nreinforcement learning. That phase is getting\nlarger and larger now, and, you know, often, that's\nless of an exact science. It often takes effort to get it right. Models are then tested with\nsome of our early partners to see how good they are, and they're then tested both internally and externally for their safety, particularly for catastrophic\nand autonomy risks. So we do internal testing according to our\nresponsible scaling policy, which I, you know, could talk\nmore about that in detail. And then we have an agreement with the US and the UK\nAI Safety Institute, as well as other third party testers in specific domains to test the models for what are called CBRN risks, chemical, biological,\nradiological and nuclear, which are, you know, we\ndon't think that models pose these risks seriously yet, but every new model, we wanna evaluate to see if we're starting to get close to some of these more\ndangerous capabilities. So those are the phases. And then, you know, then\nit just takes some time to get the model working\nin terms of inference and launching it in the API. So there's just a lot of steps to actually making a model work. And of course, you know,\nwe're always trying to make the processes as\nstreamlined as possible, right? We want our safety testing to be rigorous, but we want it to be rigorous and to be, you know, to be automatic, to happen as fast as it can\nwithout compromising on rigor. Same with our pre-training process and our post-training process. So, you know, it's just\nlike building anything else. It's just like building airplanes. You want to make them, you know, you want to make them safe, but you want to make\nthe process streamlined. And I think the creative\ntension between those is, you know, is an important thing\nin making the models work. - Yeah, rumor on the street,\nI forget who was saying that Anthropic has really good tooling, so probably a lot of the challenge here on the software engineering side is to build the tooling\nto have like a efficient, low friction interaction\nwith the infrastructure. - You would be surprised how\nmuch of the challenges of, you know, building these\nmodels comes down to, you know, software engineering, performance\nengineering, you know. From the outside you might think, oh, man, we had this\neureka breakthrough, right? You know, this movie with the science, we discovered it, we figured it out. But I think all things, even, you know, incredible discoveries, like, they almost always\ncome down to the details, and often super, super boring details. I can't speak to whether we have better tooling than other companies. I mean, you know, haven't\nbeen at those other companies, at least not recently, but it's certainly something\nwe give a lot of attention to. - I don't know if you\ncan say, but from three, from Claude 3 to Claude 3.5, is there any extra pre-training going on or is it mostly focused\non the post-training? There's been leaps in performance. - Yeah, I think at any given stage, we're focused on improving\neverything at once. Just naturally, like\nthere are different teams, each team makes progress\nin a particular area, in making a particular, you know, their particular segment\nof the relay race better. And it's just natural that\nwhen we make a new model, we put all of these things in at once. - So, the data you have,\nlike the preference data you get from RLHF, is that applicable, is there a ways to apply it to newer models as it get trained up? - Yeah, preference data from old models sometimes gets used for new models, although, of course, it\nperforms somewhat better when it's, you know, trained on, it's trained on the new models. Note that we have this, you know, Constitutional AI method such that we don't only\nuse preference data, we kind of, there's also\na post-training process where we train the model against itself and there's, you know, new types of post-training the model against itself that are used every day. So it's not just RLHF, it's a bunch of other methods as well. Post-training, I think, you know, is becoming more and more sophisticated. - Well, what explains the\nbig leap in performance for the new Sonnet 3.5? I mean, at least in the programming side. And maybe this is a good place\nto talk about benchmarks. What does it mean to get better? Just the number went up,\nbut, you know, I program, but I also love programming and Claude 3.5 through Cursor is what I use to assist me in programming. And there was, at least\nexperientially, anecdotally, it's gotten smarter at programming. So like, what does it\ntake to get it smarter? - We observed that as well, by the way. There were a couple very strong engineers here at Anthropic who\nall previous code models, both produced by us and produced\nby all the other companies, hadn't really been useful to them. You know, they said, you know, maybe this is useful to\nbeginner, it's not useful to me. But Sonnet 3.5, the original one for the first time they said, \"Oh my God, this helped me with something that, you know, that it would've taken me hours to do. This is the first model that's\nactually saved me time.\" So again, the waterline is rising. And then I think, you know, the new Sonnet has been even better. In terms of what it takes, I mean, I'll just say it's\nbeen across the board. It's in the pre-training,\nit's in the post-training, it's in various evaluations that we do. We've observed this as well. And if we go into the\ndetails of the benchmark, so Sowe bench is\nbasically since, you know, since you're a programmer, you know, you'll be familiar with like pull requests and, you know, just pull\nrequests are like the, you know, like a sort of atomic unit of work. You know, you could say, you know, I'm implementing one thing. And Sowe bench actually gives you kind of a real world situation where the code base is in a current state and I'm trying to implement\nsomething that's, you know, that's described in language. We have internal benchmarks where we measure the same thing and you say, just give the\nmodel free reign to like, you know, do anything, run\nanything, edit anything. How well is it able to\ncomplete these tasks? And it's that benchmark that's gone from it can do it 3% of the time to it can do it about 50% of the time. So I actually do believe that if we get, you can gain benchmarks, but I think if we get to\n100% on that benchmark in a way that isn't\nkind of like overtrained or game for that particular benchmark, probably represents a\nreal and serious increase in kind of programming ability. And I would suspect that\nif we can get to, you know, 90, 95% that, you know,\nit will represent ability to autonomously do a significant fraction of software engineering tasks. - Well, ridiculous timeline question. When is Claude Opus 3.5 coming out? - Not giving you an exact date, but you know, there, you\nknow, as far as we know, the plan is still to\nhave a Claude 3.5 Opus. - Are we gonna get it\nbefore \"GTA 6\" or no? - Like \"Duke Nukem Forever.\" - \"Duke Nukem-\"\n- What was that game? There was some game that\nwas delayed 15 years. - That's right.\n- Was that \"Duke Nukem Forever?\"\n- Yeah. And I think \"GTA\" is now\njust releasing trailers. - You know, it's only been three months since we released the first Sonnet. - Yeah, it's the\nincredible pace of release. - It just tells you about the pace, the expectations for when\nthings are gonna come out. - So what about 4.0? So how do you think about sort of as these models\nget bigger and bigger, about versioning, and also\njust versioning in general, why Sonnet 3.5 updated with the date? Why not Sonnet 3.6, which a\nlot of people are calling it? - Yeah, naming is actually an interesting challenge here, right? Because I think a year ago, most of the model was pre-training, and so you could start from the beginning and just say, okay,\nwe're gonna have models of different sizes, we're\ngonna train them all together and you know, we'll have\na family of naming schemes and then we'll put some\nnew magic into them and then, you know, we'll\nhave the next generation. The trouble starts already when some of them take a lot longer than others to train, right? That already messes up\nyour time a little bit. But as you make big\nimprovements in pre-training, then you suddenly notice, oh, I can make better pre-train model and that doesn't take very long to do, but you know, clearly it has the same, you know, size and shape\nof previous models. So I think those two together as well as the timing issues, any kind of scheme you come up with, you know, the reality tends to kind of frustrate that scheme, right? Tend tends to kind of\nbreak out of the scheme. It's not like software where you can say, oh, this is like, you\nknow, 3.7, this is 3.8. No, you have models with\ndifferent trade-offs. You can change some things in your models, you can train, you can\nchange other things. Some are faster and slower at inference, some have to be more expensive, some have to be less expensive. And so I think all the companies\nhave struggled with this. I think we did very, you know, I think we were in a good position in terms of naming when we\nhad Haiku, Sonnet and Opus. - [Lex] It was great, great start. - We're trying to maintain it, but it's not perfect, so we'll try and get\nback to the simplicity, but just the nature of the field, I feel like no one's figured out naming. It's somehow a different paradigm from like normal software and so we just, none of the companies\nhave been perfect at it. It's something we struggle with surprisingly much relative to, you know, how relative\nto how trivial it is to, you know, for the grand\nscience of training the models. - So, from the user\nside, the user experience of the updated Sonnet 3.5 is just different than the previous June 2024 Sonnet 3.5. It would be nice to come up with some kind of labeling\nthat embodies that because people talk about Sonnet 3.5, but now there's a different one, and so how do you refer to the\nprevious one and the new one when there's a distinct improvement? It just makes conversation\nabout it just challenging. - Yeah, yeah. I definitely think this question of there are lots of\nproperties of the models that are not reflected in the benchmarks. I think that's definitely\nthe case and everyone agrees. And not all of them are capabilities. Some of them are, you know,\nmodels can be polite or brusque. They can be, you know, very reactive or they can ask you questions. They can have what feels\nlike a warm personality or a cold personality. They can be boring or they\ncan be very distinctive, like Golden Gate Claude was. And we have a whole, you know, we have a whole team kind of focused on, I think we call it Claude character. Amanda leads that team and we'll talk to you about that. But it's still a very inexact science, and often we find that\nmodels have properties that we're not aware of. The fact of the matter is that you can, you know, talk to a model 10,000 times and there are some\nbehaviors you might not see, just like with a human, right? I can know someone for a few months and, you know, not know that\nthey have a certain skill, or not know that there's\na certain side to them. And so I think we just have to get used to this idea and we're always\nlooking for better ways of testing our models to\ndemonstrate these capabilities, and also to decide which are\nthe personality properties we want models to have and\nwhich we don't want to have. That itself, the normative question is also super interesting. - I gotta ask you a question from Reddit. - From Reddit? Oh, boy. (laughs) - You know, there just this fascinating, to me at least, it's a\npsychological social phenomenon where people report that Claude has gotten dumber for them over time. And so the question is, does the user complaint\nabout the dumbing down of Claude 3.5 Sonnet hold any water? So are these anecdotal reports\na kind of social phenomena or did Claude, is there any cases where Claude would get dumber? - So this actually doesn't apply, this isn't just about Claude. I believe I've seen these complaints for every foundation model\nproduced by a major company. People said this about GPT-4, they said it about GPT-4 Turbo. So, a couple things. One, the actual weights\nof the model, right, the actual brain of the\nmodel, that does not change unless we introduce a new model. There are just a number of reasons why it would not make sense practically to be randomly substituting in new versions of the model. It's difficult from an\ninference perspective and it's actually hard to\ncontrol all the consequences of changing the weight of the model. Let's say you wanted to fine\ntune the model to be like, I don't know, to like\nto say \"certainly\" less, which, you know, an old\nversion of Sonnet used to do. You actually end up\nchanging 100 things as well. So we have a whole process for it, and we have a whole process\nfor modifying the model. We do a bunch of testing on it, we do a bunch of user\ntesting and early customers. So we both have never changed the weights of the model\nwithout telling anyone, and it wouldn't, certainly\nin the current setup, it would not make sense to do that. Now, there are a couple things\nthat we do occasionally do. One is sometimes we run A/B tests, but those are typically\nvery close to when a model is being released and for a\nvery small fraction of time. So, you know, like, the day\nbefore the new Sonnet 3.5. I agree, we should have had a better name. It's clunky to refer to it. There were some comments from people that like it's gotten a lot better, and that's because, you know, a fraction were exposed to an A/B test for those one or two days. The other is that occasionally, the system prompt will change. The system prompt can have some effects, although it's unlikely\nto dumb down models. It's unlikely to make them dumber. And we've seen that\nwhile these two things, which I'm listing to be very complete, happened relatively,\nhappened quite infrequently, the complaints about, for us and for other model\ncompanies about the model change, the model isn't good at this. The model got more censored. The model was dumbed down. Those complaints are constant. And so I don't wanna say like people are imagining it or\nanything, but like the models are for the most part not changing. If I were to offer a theory,\nI think it actually relates to one of the things I said before, which is that models are very complex and have many aspects to them. And so often, you know, if I ask the model a question,\nyou know, if I'm like, \"Do task X\" versus \"Can you do task X?\" the model might respond in different ways. And so there are all\nkinds of subtle things that you can change about\nthe way you interact with the model that can give\nyou very different results. To be clear, this itself\nis like a failing by us and by the other model\nproviders that the models are just often sensitive to\nlike small changes in wording. It's yet another way in which the science of how these models work\nis very poorly developed. And so, you know, if I\ngo to sleep one night and I was like talking to\nthe model in a certain way and I like slightly changed the phrasing of how I talk to the model, you know, I could get different results. So that's one possible way. The other thing is, man, it's just hard to quantify this stuff. It's hard to quantify this stuff. I think people are very excited by new models when they come out and then as time goes on, they become very aware of the limitations, so that may be another effect. But that's all a very\nlong-winded way of saying for the most part, with some\nfairly narrow exceptions, the models are not changing. - I think there is a psychological effect. You just start getting used to it. The baseline raises. Like when people first\ngotten wifi on airplanes, it's like amazing, magic.\n- It's like amazing, yeah. - And then-\n- And now I'm like, I can't get this thing to work. This is such a piece of crap. - Exactly, so then it's easy\nto have the conspiracy theory of they're making wifi slower and slower. This is probably something I'll talk to Amanda much more about. But another Reddit question, \"When will Claude stop trying to be my puritanical grandmother imposing its moral worldview\non me as a paying customer? And also, what is the psychology behind making Claude overly apologetic?\" So this kind of reports\nabout the experience, a different angle on the frustration, it has to do with the character. - Yeah, so a couple points on this first. One is like things that people\nsay on Reddit and Twitter, or X or whatever it is,\nthere's actually a huge distribution shift between like the stuff that people complain loudly\nabout on social media and what actually kind of like, you know, statistically users care about and that drives people to use the models. Like people are frustrated\nwith, you know, things like, you know, the model not\nwriting out all the code or the model, you know, just not being as good at code as it could be, even though it's the best\nmodel in the world on code. I think the majority\nthings are about that. But certainly a kind\nof vocal minority are, you know, kind of raise\nthese concerns, right? Are frustrated by the\nmodel refusing things that it shouldn't refuse, or like apologizing too much, or just having these kind of\nlike annoying verbal ticks. The second caveat, and\nI just wanna say this like super clearly because I think it's like some people don't know it, others like kind of know it but forget it. Like it is very difficult to control across the board how the models behave. You cannot just reach in there and say, \"Oh, I want the\nmodel to like apologize less.\" Like you can do that, you\ncan include training data that says like, \"Oh, the model\nshould like apologize less,\" but then in some other\nsituation they end up being like super rude\nor like overconfident in a way that's like misleading people. So there are all these trade-offs. For example, another thing is there was a period during which models, ours and I think others as\nwell were too verbose, right? They would like repeat themselves, they would say too much. You can cut down on the\nverbosity by penalizing the models for just talking for too long. What happens when you do that, if you do it in a crude way\nis when the models are coding, sometimes they'll say rest\nof the code goes here, right? Because they've learned that\nthat's the way to economize and that they see it, and then so that leads the model to be so-called lazy in coding where they're just like, ah, you can finish the rest of it. It's not because we wanna, you know, save on compute or because you know, the models are lazy, and you know, during winter break, or any of the other kind\nof conspiracy theories that have come up. It's actually, it's just very hard to control the behavior of the model, to steer the behavior of the model in all circumstances at once. You can kind of, there's\nthis whack-a-mole aspect where you push on one\nthing and like, you know, these other things start to move as well that you may not even notice or measure. And so one of the reasons\nthat I care so much about, you know, kind of grand alignment of these AI systems in the\nfuture is actually these systems are actually quite unpredictable. They're actually quite\nhard to steer and control. And this version we're seeing today of you make one thing better,\nit makes another thing worse, I think that's like a present day analog of future control problems in AI systems that we can start to study today, right? I think that that difficulty in steering the behavior\nand in making sure that if we push an AI\nsystem in one direction, it doesn't push it in another direction in some other ways that we didn't want. I think that's kind of an early sign of things to come, and if we can do a good job\nof solving this problem, right, of like you ask the model to like, you know, to like make\nand distribute smallpox and it says no, but it's\nwilling to like help you in your graduate level virology class. Like how do we get both\nof those things at once? It's hard. It's very easy to go to\none side or the other and it's a multidimensional problem. And so, you know, I think these questions of like shaping the model's personality, I think they're very hard. I think we haven't done perfectly on them. I think we've actually done the best of all the AI companies, but\nstill so far from perfect. And I think if we can get this right, if we can control, you know,\ncontrol the false positives and false negatives in this very kind of controlled\npresent day environment, we'll be much better at doing it for the future when\nour worry is, you know, will the models be super autonomous? Will they be able to, you know,\nmake very dangerous things? Will they be able to\nautonomously, you know, build whole companies? And are those companies aligned? So, I think of this present\ntask as both vexing, but also good practice for the future. - What's the current best way of gathering sort of user feedback? Like not anecdotal data, but just large scale\ndata about pain points or the opposite of pain\npoints, positive things, so on, is it internal testing? Is it a specific group\ntesting, A/B testing? What works? - So, typically we'll have\ninternal model bashings where all of Anthropic, Anthropic is almost 1000 people, you know, people just\ntry and break the model. They try and interact\nwith it various ways. We have a suite of evals for, you know, oh, is the model refusing\nin ways that it couldn't? I think we even had a certainly eval because, you know, our model, again, one point, model had this problem where like it had this annoying tick where it would like respond to a wide range of questions by saying \"Certainly I can help you with that. Certainly I would be happy to do that. Certainly this is correct.\" And so we had a, like, certainly eval, which is like, how often\ndoes the model say certainly? But look, this is just a whack-a-mole. Like, what if it switches\nfrom certainly to definitely? Like, so, you know, every\ntime we add a new eval, and we're always evaluating\nfor all of the old things. So we have hundreds of these evaluations, but we find that there's no substitute for human interacting with it. And so it's very much like the ordinary product development process. We have like hundreds of people within Anthropic bash the model, you know, then we do external A/B tests. Sometimes we'll run\ntests with contractors. We pay contractors to\ninteract with the model. So you put all of these things together and it's still not perfect. You still see behaviors that you don't quite wanna see, right? You know, you still see the model like refusing things that it just doesn't make sense to refuse. But I think trying to solve\nthis challenge, right? Trying to stop the model\nfrom doing, you know, genuinely bad things that, you know, everyone agrees it shouldn't do, right? You know, everyone agrees that, you know, the model shouldn't talk about, you know, I don't know, child abuse material, right? Like, everyone agrees the\nmodel shouldn't do that. But at the same time that it doesn't refuse in\nthese dumb and stupid ways. I think drawing that line\nas finely as possible, approaching perfectly is still a challenge and we're getting better at it every day. But there's a lot to be solved. And again, I would point to that as an indicator of the challenge ahead in terms of steering much\nmore powerful models. - Do you think Claude\n4.0 is ever coming out? - I don't want to commit\nto any naming scheme, 'cause if I say here \"We're gonna have Claude 4 next year,\" and then, you know, then\nwe decide that like, you know, we should start over, 'cause there's a new type of model. Like I don't want to commit to it. I would expect in a\nnormal course of business that Claude 4 would come after Claude 3.5. But you know, you never know\nin this wacky field, right? - But the sort of, this idea\nof scaling is continuing. - Scaling is continuing. There will definitely\nbe more powerful models coming from us than the\nmodels that exist today. That is certain. Or if there aren't, we've\ndeeply failed as a company. - Okay, can you explain the\nResponsible Scaling Policy and the AI Safety Level\nStandards, ASL Levels? - As much as I am excited\nabout the benefits of these models, and, you\nknow, we'll talk about that if we talk about \"Machines\nof Loving Grace,\" I'm worried about the risks and I continue to be\nworried about the risks. No one should think that, you know, \"Machines of Loving Grace\"\nwas me saying, you know, I'm no longer worried about\nthe risks of these models. I think they're two\nsides of the same coin. The power of the models and their ability to solve\nall these problems in, you know, biology, neuroscience, economic development, governance and peace,\nlarge parts of the economy, those come with risks as well, right? With great power comes\ngreat responsibility, right? The two are paired. Things that are powerful\ncan do good things and they can do bad things. I think of those risks\nas being in, you know, several different categories. Perhaps the two biggest\nrisks that I think about, and that's not to say that there aren't risks today\nthat are important, but when I think of the\nreally the, you know, the things that would happen\non the grandest scale, one is what I call catastrophic misuse. These are misuse of the\nmodels in domains like cyber, bio, radiological, nuclear, right? Things that could, you\nknow, that could harm or even kill thousands, even millions of people if they really, really go wrong. Like these are the, you know, number one priority to prevent. And here I would just\nmake a simple observation, which is that the models, you know, if I look today at people who have done really\nbad things in the world, I think actually humanity\nhas been protected by the fact that the overlap between really smart, well-educated people and people who want to\ndo really horrific things has generally been small. Like, you know, let's say I'm someone who, you know, I have a PhD in this field, I have a well paying job. There's so much to lose. Why do I wanna, like, you know, even assuming I'm completely evil, which most people are not. You know, why would such\na person risk their life, risk their legacy, their\nreputation to do something like, you know, truly, truly evil? If we had a lot more people like that, the world would be a much\nmore dangerous place. And so my worry is that by being a much more intelligent agent, AI could break that correlation, and so I do have serious\nworries about that. I believe we can prevent those worries. But, you know, I think as a counterpoint to \"Machines of Loving\nGrace,\" I want to say that there's still serious risks. And the second range of risks would be the autonomy\nrisks, which is the idea that models might on their own, particularly as we give them more agency than they've had in the past, particularly as we give them supervision over wider\ntasks like, you know, writing whole code bases or someday even, you know, effectively\noperating entire companies, they're on a long enough leash, are they doing what we\nreally want them to do? It's very difficult to even understand in detail what they're\ndoing, let alone control it. And like I said, these early signs that it's hard to perfectly draw the boundary between things the model should do and things the model shouldn't do that, you know, if you go to one side, you get things that are\nannoying and useless, you go to the other side,\nyou get other behaviors. If you fix one thing, it\ncreates other problems. We're getting better and\nbetter at solving this. I don't think this is\nan unsolvable problem. I think, you know, this is a science, like the safety of airplanes\nor the safety of cars, or the safety of drugs. You know, I don't think there's\nany big thing we're missing. I just think we need to get better at controlling these models. And so these are the two\nrisks I'm worried about. And our Responsible Scaling Plan, which I'll recognize is\na very long-winded answer to your question. - I love it. I love it. - Our Responsible Scaling Plan is designed to address these two types of risks. And so every time we develop a new model, we basically test it for its ability to do both of these bad things. So if I were to back up a little bit, I think we have an interesting dilemma with AI systems where they're\nnot yet powerful enough to present these catastrophes. I don't know that they'll ever\nprevent these catastrophes, it's possible they won't,\nbut the case for worry, the case for risk is strong enough that we should act now. And they're getting better\nvery, very fast, right? You know, I testified in the Senate that, you know, we might have serious bio risks within two to three years. That was about a year ago. Things have proceeded at pace. So we have this thing where it's like, it's surprisingly hard\nto address these risks because they're not here today. They don't exist. They're like ghosts, but\nthey're coming at us so fast because the models are improving so fast. So how do you deal with\nsomething that's not here today, doesn't exist but is\ncoming at us very fast? So the solution we came up with for that in collaboration with, you know, people like the organization METR and Paul Christiano is, okay, what you need for that are you need tests to tell you when the\nrisk is getting close. You need an early warning system. And so every time we have a new model, we test it for its capability to do these CBRN tasks, as well as testing it for, you know, how capable it is of doing\ntasks autonomously on its own. And in the latest version of our RSP, which we released in\nthe last month or two, the way we test autonomy\nrisks is the model, the AI model's ability to do\naspects of AI research itself, which when the AI models\ncan do AI research, they become kind of truly autonomous. And you know, that threshold is important for a bunch of other ways. And so what do we then\ndo with these tasks? The RSP basically develops what we've called an if then structure, which is if the models\npass a certain capability, then we impose a certain set of safety and security requirements on them. So today's models are\nwhat's called ASL two. Models that were, ASL\none is for systems that manifestly don't pose any\nrisk of autonomy or misuse. So for example, a chess playing bot, Deep Blue would be ASL one. It's just manifestly the case that you can't use Deep Blue for anything other than chess. It was just designed for chess. No one's gonna use it to like, you know, to conduct a masterful cyber attack or to, you know, run wild and\ntake over the world. ASL two is today's AI systems\nwhere we've measured them and we think these systems are\nsimply not smart enough to, you know, autonomously self-replicate or conduct a bunch of tasks, and also not smart enough to provide meaningful information about CBRN risks and how to build CBRN\nweapons above and beyond what can be known from looking at Google. In fact, sometimes they\ndo provide information, but not above and beyond a search engine, but not in a way that\ncan be stitched together, not in a way that kind of end\nto end is dangerous enough. So ASL three is gonna be the point at which the models are helpful enough to enhance the capabilities\nof non-state actors, right? State actors can already do a lot of, unfortunately, to a high\nlevel of proficiency, a lot of these very dangerous\nand destructive things. The difference is that non-state actors are not capable of it. And so when we get to ASL three, we'll take special security precautions designed to be sufficient to prevent theft of the model by non-state actors, and misuse of the model as it's deployed. We'll have to have enhanced filters targeted at these particular areas. - Cyber, bio, nuclear. - Cyber, bio, nuclear and model autonomy, which is less a misuse risk and more risk of the model\ndoing bad things itself. ASL four, getting to the\npoint where these models could enhance the capability of a already knowledgeable state actor and/or become, you know, the\nmain source of such a risk. Like if you wanted to\nengage in such a risk, the main way you would\ndo it is through a model. And then I think ASL four\non the autonomy side, it's some amount of acceleration in AI research capabilities\nwithin an AI model. And then ASL five is where\nwe would get to the models that are, you know, that are kind of, you know, truly capable, that could exceed\nhumanity in their ability to do any of these tasks. And so the point of if\nthen structure commitment is basically to say, look, I don't know, I've been working with these models for many years and I've been worried about risk for many years. It's actually kind of\ndangerous to cry wolf. It's actually kind of\ndangerous to say this, you know, this model is risky. And, you know, people look at it and they say, this is\nmanifestly not dangerous. Again, it's the delicacy of the risk isn't here today but it's coming at us fast. How do you deal with that? It's really vexing to a risk\nplanner to deal with it. And so this if then\nstructure basically says, look, we don't wanna\nantagonize a bunch of people, we don't wanna harm our own, you know, our kind of own ability to have a place in the conversation by imposing these very onerous burdens on models that are not dangerous today. So if then, the trigger commitment is basically a way to deal with this. Says you clamp down hard when you can show that\nthe model is dangerous. And of course what has to\ncome with that is, you know, enough of a buffer\nthreshold that, you know, you're not at high risk of\nkind of missing the danger. It's not a perfect framework. We've had to change it every, you know, we came out with a new\none just a few weeks ago, and probably going forward, we might release new ones\nmultiple times a year because it's hard to get\nthese policies right, like technically, organizationally, from a research perspective. But that is the proposal,\nif then commitments and triggers in order to minimize burdens and false alarms now, but really react appropriately\nwhen the dangers are here. - What do you think the timeline for ASL three is where several\nof the triggers are fired? And what do you think the\ntimeline is for ASL four? - Yeah, so that is hotly\ndebated within the company. We are working actively\nto prepare ASL three security measures as well as\nASL three deployment measures. I'm not gonna go into detail, but we've made a lot of progress on both, and, you know, we're prepared to be, I think, ready quite soon. I would not be surprised at all if we hit ASL three next year. There was some concern that we might even hit it this year. That's still possible,\nthat could still happen. It's like very hard to say, but like I would be very, very surprised if it was like 2030. I think it's much sooner than that. - So there's protocols\nfor detecting it, if then, and then there's protocols\nfor how to respond to it. - Yes. - How difficult is the second, the latter? - Yeah, I think for ASL three, it's primarily about security and about, you know, filters on the model relating to a very narrow set of areas when we deploy the model. Because at ASL three, the\nmodel isn't autonomous yet, and so you don't have to\nworry about, you know, kind of the model itself\nbehaving in a bad way, even when it's deployed internally. So I think the ASL three measures are, I won't say straightforward, they're rigorous, but they're\neasier to reason about. I think once we get to ASL four, we start to have worries about the models being smart enough that\nthey might sandbag tests, they might not tell the truth about tests. We had some results came out\nabout like sleeper agents and there was a more recent\npaper about, you know, can the models mislead\nattempts to, you know, sandbag their own abilities, right? Show them, you know, present themselves as being less capable than they are. And so I think with ASL four, there's gonna be an important component of using other things\nthan just interacting with the models, for\nexample, interpretability or hidden chains of thought\nwhere you have to look inside the model and verify\nvia some other mechanism that is not, you know, is\nnot as easily corrupted as what the model says, you know, that the model\nindeed has some property. So we're still working on ASL four. One of the properties of the RSP is that we don't specify ASL four\nuntil we've hit ASL three. And I think that's proven\nto be a wise decision because even with ASL three, again, it's hard to know\nthis stuff in detail, and we wanna take as much time as we can possibly take\nto get these things right. - So for ASL three, the bad actor will be the humans. - [Dario] Humans, yes. - And so there, it's a little bit more- - For ASL four, it's both, I think, both. - It's both, and so deception, and that's where\nmechanistic interpretability comes into play and hopefully\nthe techniques used for that are not made accessible to the model. - Yeah, I mean, of course you can hook up the mechanistic interpretability\nto the model itself, but then you've kind of lost it as a reliable indicator\nof the model state. There are a bunch of exotic ways you can think of that it\nmight also not be reliable. Like if the, you know,\nmodel gets smart enough that it can like, you know, jump computers and like read the code where you're like looking\nat its internal state. We've thought about some of those. I think there're exotic enough, there are ways to render them unlikely. But yeah, generally you wanna preserve mechanistic interpretability as a kind of verification set or test set that's separate from the\ntraining process of the model. - See, I think as these\nmodels become better and better conversation\nand become smarter, social engineering becomes\na threat too 'cause they- - [Dario] Oh, yeah. - That could start being very convincing to the engineers inside companies. - Oh yeah, yeah. It's actually like, you know, we've seen lots of examples of demagoguery in our life from humans and, you know, there's a concern that models\nthat could do that as well. - One of the ways that\nClaude has been getting more and more powerful is it's now able to do some agentic stuff, computer use. There's also an analysis\nwithin the sandbox of claude.ai itself. But let's talk about computer use. That seems to me super exciting that you can just give Claude a task and it takes a bunch of\nactions, figures it out, and has access to your\ncomputer through screenshots. So can you explain how that works? And where that's headed? - Yeah, it's actually relatively simple. So Claude has had for a long time, since Claude 3.0 back in March, the ability to analyze images and respond to them with text. The only new thing we added is those images can be\nscreenshots of a computer. And in response, we trained the model to give a location on the screen where you can click and/or\nbuttons on the keyboard you can press in order to take action. And it turns out that with actually not all that\nmuch additional training, the models can get\nquite good at that task. It's a good example of generalization. You know, people sometimes say, if you get to lower earth orbit, you're like halfway to anywhere, right? Because of how much it\ntakes to escape the gravity. Well, if you have a\nstrong pre-trained model, I feel like you're halfway to anywhere in terms of the intelligence space. And so actually, it\ndidn't take all that much to get Claude to do this. And you can just set that in a loop, give the model a screenshot,\ntell it what to click on, give it the next screenshot,\ntell it what to click on, and that turns into a full kind of almost 3D video\ninteraction of the model. And it's able to do all\nof these tasks, right? You know, we showed these demos where it's able to like\nfill out spreadsheets. It's able to kind of like\ninteract with a website. It's able to, you know, it's able to open all kinds\nof, you know, programs, different operating systems,\nWindows, Linux, Mac. So, you know, I think all\nof that is very exciting. I will say, while in theory, there's nothing you could do there that you couldn't have done through just giving the model the API\nto drive the computer screen. This really lowers the barrier. And you know, there's a\nlot of folks who either, you know, aren't in a position to interact with those APIs or it takes\nthem a long time to do. It's just the screen is\njust a universal interface that's a lot easier to interact with. And so I expect over time, this is gonna lower a bunch of barriers. Now, honestly, the current model has, it leaves a lot still to be desired, and we were honest about\nthat in the blog, right? It makes mistakes, it misclicks. And, you know, we were\ncareful to warn people, hey, this thing isn't, you\ncan't just leave this thing to, you know, run on your computer\nfor minutes and minutes. You gotta give this thing\nboundaries and guardrails. And I think that's one of the reasons we released it first in an API form rather than kind of, you know, this kind of just hand it to the consumer and give it control of their computer. But you know, I definitely\nfeel that it's important to get these capabilities out there. As models get more powerful, we're gonna have to\ngrapple with, you know, how do we use these capabilities safely? How do we prevent them from being abused? And you know, I think releasing the model while the capabilities are, you know, are still limited is very\nhelpful in terms of doing that. You know, I think since\nit's been released, a number of customers, I think Replit was maybe\none of the most quickest to deploy things. You know, have made use\nof it in various ways. People have hooked up demos for, you know, Windows desktops, Macs,\nyou know, Linux machines. So yeah, it's been very exciting. I think as with anything else, you know, it comes with new exciting abilities and then, you know, with those new exciting abilities, we have to think about how to, you know, make the model, you know, safe, reliable, do what humans want them to do. I mean, it's the same story\nfor everything, right? Same thing. It's that same tension. - But the possibility of use cases here, just the range is incredible. So how much to make it work\nreally well in the future? How much do you have to specially kind of go beyond what's the\npre-trained model's doing, do more post-training, RLHF\nor supervised fine tuning, or synthetic data just\nfor the agentic stuff? - Yeah, I think speaking at a high level, it's our intention to\nkeep investing a lot in, you know, making the model better. Like I think, you know,\nwe look at some of the, you know, some of the benchmarks where previous models were like, oh, could do it 6% of the time, and now our model would do\nit 14 or 22% of the time. And yeah, we wanna get up to, you know, the human level reliability of 80, 90% just like anywhere else, right? We're on the same curve that\nwe were on with SWE-bench, where I think I would\nguess a year from now, the models can do this\nvery, very reliably, but you gotta start somewhere. - So you think it's possible to get to the human level, 90%, basically doing the same\nthing you're doing now? Or it has to be special for computer use? - I mean, it depends what\nyou mean by, you know, special and special in general. But, you know, I\ngenerally think, you know, the same kinds of techniques that we've been using to\ntrain the current model, I expect that doubling\ndown on those techniques in the same way that we have for code for models in general, you know, for image input, you know, for voice. I expect those same\ntechniques will scale here, as they have everywhere else. - But this is giving sort of the power of action to Claude, and so you could do a lot\nof really powerful things, but you could do a lot of damage also. - Yeah, yeah, no, and we've\nbeen very aware of that. Look, my view actually is computer use isn't a fundamentally new capability, like the CBRN or autonomy\ncapabilities are. It's more like, it kind of\nopens the aperture for the model to use and apply its existing abilities. And so the way we think about it, going back to our RSP, is nothing that this model is\ndoing inherently increases, you know, the risk from\nan RSP perspective. But as the models get more\npowerful, having this capability may make it scarier once it, you know, once it has the cognitive\ncapability to, you know, to do something at the ASL\nthree and ASL four level, you know, this may be the thing that kind of unbounds it from doing so. So, going forward, certainly\nthis modality of interaction is something we have tested for, and that we will continue to\ntest for in RSP going forward. I think it's probably better to have, to learn and explore this capability before the model is super,\nyou know, super capable. - Yeah, and there's a lot\nof interesting attacks, like prompt injection, because now you've widened the aperture, so you can prompt inject\nthrough stuff on screen. So if this becomes more and more useful, then there's more and more benefit to inject stuff into the model. If it goes to a certain webpage, it could be harmless\nstuff like advertisements or it could be like harmful stuff, right? - Yeah, I mean, we've\nthought a lot about things like spam, CAPTCHA, you know, mass camp. There's all, you know, every, like one secret I'll tell you, if you've invented a new technology, not necessarily the biggest misuse, but the first misuse you'll see, scams, just petty scams. Like you'll just, it's\nlike a thing as old, people scamming each other, it's this thing as old as time, and it's just every time\nyou gotta deal with it. - It's almost like silly\nto say but it's true, sort of bots and spam\nin general is a thing as it gets more and more intelligent. It's harder and harder to fight. - There are a lot of like I said, like there are a lot of\npetty criminals in the world. And you know, it's like\nevery new technology is like a new way for petty\ncriminals to do something, you know, something stupid and malicious. - Is there any ideas about sandboxing it? Like how difficult is the sandboxing task? - Yeah, we sandbox during training. So for example, during training, we didn't expose the\nmodel to the internet. I think that's probably a\nbad idea during training because, you know, the model\ncan be changing its policy, it can be changing what it's doing, and it's having an\neffect in the real world. You know, in terms of actually\ndeploying the model, right, it kind of depends on the application. Like, you know, sometimes\nyou want the model to do something in the real world, but of course, you can always put guardrails on the outside, right? You can say, okay, well, you know, this model's not gonna move\ndata from my, you know, model's not gonna move\nany files from my computer or my web server to anywhere else. Now, when you talk about sandboxing, again, when we get to ASL four, none of these precautions are going to make sense there, right, where when you talk about ASL four, you're then, the model is\nbeing kind of, you know, there's a theoretical worry the model could be smart enough to kind of break out of any box. And so there we need to think about mechanistic interpretability\nabout, you know, if we're gonna have a sandbox, it would need to be a\nmathematically provable sand. You know, that's a whole different world than what we're dealing\nwith with the models today. - Yeah, the science of building a box from which ASL four AI\nsystem cannot escape. - I think it's probably\nnot the right approach. I think the right approach\ninstead of having something, you know, unaligned\nthat like you're trying to prevent it from escaping. I think it's better to\njust design the model the right way or have a loop where, you know, you look inside,\nyou look inside the model and you're able to verify properties, and that gives you an opportunity to like iterate and actually get it right. I think containing bad models is much worse solution\nthan having good models. - Let me ask about regulation. What's the role of regulation\nin keeping AI safe? So for example, can you describe\nCalifornia AI regulation Bill SB 1047 that was ultimately\nvetoed by the governor? What are the pros and cons\nof this bill in general? - Yes, we ended up making some suggestions to the bill, and then\nsome of those were adopted and, you know, we felt,\nI think quite positively, quite positively about the\nbill by the end of that. It did still have some downsides, and, you know, of course it got vetoed. I think at a high level, I\nthink some of the key ideas behind the bill are, you know, I would say similar to\nideas behind our RSPs. And I think it's very important\nthat some jurisdiction, whether it's California\nor the federal government and/or other countries and other states passes\nsome regulation like this. And I can talk through why\nI think that's so important. So I feel good about our RSP. It's not perfect, it needs\nto be iterated on a lot, but it's been a good forcing function for getting the company to\ntake these risks seriously, to put them into product\nplanning, to really make them a central part of work at Anthropic and to make sure that all of 1000 people, and it's almost 1000\npeople now at Anthropic, understand that this is one\nof the highest priorities of the company, if not\nthe highest priority. But one, there are still some companies that don't have RSP like mechanisms, like OpenAI, Google did\nadopt these mechanisms a couple months after Anthropic did, but there are other companies out there that don't have these mechanisms at all. And so if some companies adopt these mechanisms and others don't, it's really gonna create\na situation where, you know, some of these\ndangers have the property that it doesn't matter\nif three out of five of the companies are being safe, if the other two are being unsafe, it creates this negative externality. And I think the lack of\nuniformity is not fair to those of us who have\nput a lot of effort into being very thoughtful\nabout these procedures. The second thing is, I don't think you can\ntrust these companies to adhere to these voluntary\nplans in their own, right? I like to think that Anthropic will. We do everything we can that we will. Our RSP is checked by our\nlong-term benefit trust. So, you know, we do everything we can to adhere to our own RSP. But you know, you hear lots of things about various companies saying, oh, they said they would give this much compute and they didn't. They said they would do\nthis thing and they didn't. You know, I don't think it makes sense to, you know, to litigate particular things that companies have done. But I think this broad principle that like if there's\nnothing watching over them, there's nothing watching\nover us as an industry, there's no guarantee that\nwe'll do the right thing, and the stakes are very high. And so I think it's important to have a uniform standard\nthat everyone follows, and to make sure that simply\nthat the industry does what a majority of the industry has already said is important\nand has already said that they definitely will do. Right, some people, you know, I think there's a class of people who are against regulation on principle. I understand where that comes from. If you go to Europe and, you know, you see something like GDPR, you see some of the other\nstuff that they've done. You know, some of it's good, but some of it is really\nunnecessarily burdensome, and I think it's fair to say really has slowed innovation. And so I understand where people\nare coming from on priors. I understand why people come from, start from that position. But again, I think AI is different. If we go to the very\nserious risks of autonomy and misuse that I talked about, you know, just a few minutes ago, I think that those are unusual and they warrant an\nunusually strong response. And so I think it's very important. Again, we need something\nthat everyone can get behind. You know, I think one of\nthe issues with SB 1047, especially the original version of it, was it had a bunch of\nthe structure of RSPs, but it also had a bunch of\nstuff that was either clunky or that just would've created a bunch of burdens, a bunch of hassle, and might even have missed the target in terms of addressing the risks. You don't really hear about it on Twitter. You just hear about kind of, you know, people are cheering for any regulation, and then the folks who are against make up these often quite\nintellectually dishonest arguments about how, you know, it'll make us move away from California. Bill doesn't apply if you're\nheadquartered in California, bill only applies if you\ndo business in California. Or that it would damage\nthe open source ecosystem, or that it would, you know, it would cause all of these things. I think those were mostly nonsense, but there are better\narguments against regulation. There's one guy, Dean Ball,\nwho's really, you know, I think a very scholarly,\nscholarly analyst who looks at what happens when\na regulation is put in place and ways that they can kind\nof get a life of their own, or how they can be poorly designed. And so our interest has always been, we do think there should be\nregulation in this space, but we wanna be an actor who makes sure that that regulation is\nsomething that's surgical, that's targeted at the serious risks and is something people\ncan actually comply with. Because something I think the advocates of regulation don't understand\nas well as they could is if we get something in place that's poorly targeted, that wastes a bunch of people's time, what's gonna happen is\npeople are gonna say, see, these safety risks, you know, this is nonsense. You know, I just had to hire 10 lawyers, you know, to fill out all these forms. I had to run all these tests for something that was clearly not dangerous. And after six months of that, there will be a groundswell\nand we'll end up with a durable consensus\nagainst regulation. And so I think the worst enemy of those who want real accountability\nis badly designed regulation. We need to actually get it right. And this is, if there's\none thing I could say to the advocates, it\nwould be that I want them to understand this dynamic better, and we need to be really careful and we need to talk to people who actually have experience seeing how regulations\nplay out in practice. And the people who have\nseen that understand to be very careful. If this was some lesser issue, I might be against regulation at all. But what I want the\nopponents to understand is that the underlying\nissues are actually serious. They're not something that\nI or the other companies are just making up because\nof regulatory capture. They're not sci-fi fantasies. They're not any of these things. You know, every time we have a new model, every few months, we measure\nthe behavior of these models and they're getting better and better at these concerning tasks, just as they are getting\nbetter and better at, you know, good, valuable,\neconomically useful tasks. And so I would just love\nit if some of the former, you know, I think SB\n1047 was very polarizing. I would love it if some of\nthe most reasonable opponents and some of the most reasonable proponents would sit down together. And, you know I think that, you know, the different AI companies, you know, Anthropic was the only AI\ncompany that, you know, felt positively in a very detailed way. I think Elon tweeted\nbriefly something positive. But, you know, some of the big ones, like Google, OpenAI, Meta, Microsoft were pretty staunchly against. So I would really like is if, you know, some of the key stakeholders, some of the, you know,\nmost thoughtful proponents and some of the most thoughtful\nopponents would sit down and say, how do we solve\nthis problem in a way that the proponents feel brings\na real reduction in risk, and that the opponents feel that it is not hampering the industry or hampering innovation any\nmore necessary than it needs to. And I think for whatever reason that things got too polarized and those two groups\ndidn't get to sit down in the way that they should. And I feel urgency. I really think we need\nto do something in 2025. You know, if we get to the end of 2025 and we've still done nothing about this, then I'm gonna be worried. I'm not worried yet, because again, the risks aren't here yet, but I think time is running short. - Yeah, and come up with\nsomething surgical, like you said. - Yeah, yeah, yeah, exactly. And we need to get away from this intense pro-safety versus intense anti-regulatory\nrhetoric, right? It's turned into these\nflame wars on Twitter and nothing good's gonna come of that. - So there's a lot of curiosity about the different players in the game. One of the OGs is OpenAI. You've had several years\nof experience at OpenAI. What's your story and history there? - Yeah, so I was at OpenAI\nfor roughly five years. For the last, I think it was couple years, you know, I was vice\npresident of research there. Probably myself and Ilya\nSutskever were the ones who, you know, really kind of\nset the research direction. Around 2016 or 2017, I first\nstarted to really believe in or at least confirm my belief\nin the Scaling Hypothesis when Ilya famously said to me, \"The thing you need to understand about these models is\nthey just wanna learn. The models just wanna learn.\" And again, sometimes there\nare these one sentences, these zen koans that you hear them and you're like, ah,\nthat explains everything. That explains like 1000\nthings that I've seen. And then I, you know, ever after I had this\nvisualization in my head of like, you optimize the models in the right way, you point the models in the right way. They just wanna learn. They just wanna solve the problem, regardless of what the problem is. - So get out of their way, basically. - Get out of their way, yeah. Don't impose your own ideas\nabout how they should learn. And you know, this was the same thing as Rich Sutton put out\nin the Bitter Lesson or Gwern put out in\nThe Scaling Hypothesis. You know, I think generally\nthe dynamic was, you know, I got this kind of inspiration\nfrom Ilya and from others, folks like Alec Radford\nwho did the original GPT-1, and then ran really hard with it. Me and my collaborators on GPT-2, GPT-3. RL from Human Feedback,\nwhich was an attempt to kind of deal with the\nearly safety and durability. Things like debate and amplification. Heavy on interpretability. So again, the combination\nof safety plus scaling. Probably 2018, 2019, 2020, those were kind of the years when myself and my collaborators\nprobably, you know, many of whom became\nco-founders of Anthropic, kind of really had a vision\nand like drove the direction. - Why'd you leave? Why'd you decide to leave? - Yeah, so look, I'm\ngonna put things this way and, you know, I think it ties to the race to the top, right? Which is, you know, in my time at OpenAI, what I'd come to see is\nI'd come to appreciate the Scaling Hypothesis, and as I'd come to appreciate\nkind of the importance of safety along with\nthe Scaling Hypothesis. The first one I think, you know, OpenAI was getting on board with. The second one in a way\nhad always been part of OpenAI's messaging, but, you know, over many years of the time that I spent there, I think I had a particular vision of how we should handle these things, how we should be brought out in the world, the kind of principles that\nthe organization should have. And look, I mean, there were\nlike many, many discussions about like, you know, should the org do, should the company do this? Should the company do that? Like, there's a bunch of\nmisinformation out there. People say like, we left because we didn't like\nthe deal with Microsoft. False, although, you know, it\nwas like a lot of discussion, a lot of questions about exactly how we do the deal with Microsoft. We left because we didn't\nlike commercialization. That's not true, we built GPT-3, which was the model\nthat was commercialized. I was involved in commercialization. It's more again about, how do you do it? Like civilization is going down this path to very powerful AI. What's the way to do it that is cautious, straightforward, honest, that builds trust in the\norganization and individuals? How do we get from here to there? And how do we have a real\nvision for how to get it right? How can safety not just\nbe something we say because it helps with recruiting? And, you know, I think\nat the end of the day, if you have a vision for that, forget about anyone else's vision. I don't wanna talk about\nanyone else's vision. If you have a vision for how\nto do it, you should go off and you should do that vision. It is incredibly unproductive to try and argue with someone else's vision. You might think they're\nnot doing it the right way. You might think they're dishonest. Who knows, maybe you're\nright, maybe you're not. But what you should do is you should take some people you trust and you should go off together and you should make your vision happen. And if your vision is compelling, if you can make it appeal to people, you know, some combination of ethically, you know, in the market, you know, if you can make a company that's\na place people wanna join, that, you know, engages in practices that people think are reasonable, while managing to maintain its position in the ecosystem at the same time, if you do that, people will copy it. And the fact that you are\ndoing it, especially the fact that you're doing it better than they are causes them to change their behavior in a much more compelling way than if they're your boss\nand you're arguing with them. I just, I don't know how to be any more specific about it than that, but I think it's generally\nvery unproductive to try and get someone else's vision to look like your vision. It's much more productive to go off and do a clean experiment\nand say, this is our vision, this is how we're gonna do things. Your choice is you can ignore us, you can reject what we're doing, or you can start to become more like us, and imitation is the\nsincerest form of flattery. And you know, that plays out\nin the behavior of customers, that plays out in the\nbehavior of the public, that plays out in the behavior of where people choose to work. And again, at the end, it's not about one company winning or another company winning. If we or another company are\nengaging in some practice that, you know, people find genuinely appealing, and I want it to be in substance, not just in appearance. And, you know, I think researchers are sophisticated and\nthey look at substance. And then other companies\nstart copying that practice and they win because they\ncopied that practice, that's great, that's success. That's like the race to the top. It doesn't matter who wins in the end, as long as everyone is copying everyone else's good practices, right? One way I think of it is like, the thing we're all afraid of is the race to the bottom, right? And the race to the bottom, doesn't matter who wins\nbecause we all lose, right? Like, you know, in the most extreme world, we make this autonomous AI that, you know, the robots enslave us or whatever, right? I mean, that's half joking, but you know, that is the most extreme\nthing that could happen. Then it doesn't matter\nwhich company was ahead. If instead you create a race to the top where people are competing to engage in good practices, then, you know, at the end of the day, you know, it doesn't matter who ends up winning, doesn't even matter who\nstarted the race at the top. The point isn't to be virtuous. The point is to get the system into a better equilibrium\nthan it was before, and individual companies can\nplay some role in doing this. Individual companies can, you know, can help to start it, can help to accelerate it. And frankly, I think\nindividuals at other companies have done this as well, right? The individuals that\nwhen we put out an RSP react by pushing harder to\nget something similar done, get something similar\ndone at other companies. Sometimes other companies\ndo something that's like, we're like, oh, it's a good practice. We think that's good. We should adopt it too. The only difference is,\nyou know, I think we are, we try to be more forward-leaning. We try and adopt more\nof these practices first and adopt them more quickly\nwhen others invent them. But I think this dynamic is\nwhat we should be pointing at. And I think it abstracts\naway the question of, you know, which company's\nwinning, who trusts who. I think all these questions of drama are profoundly uninteresting, and the thing that\nmatters is the ecosystem that we all operate in and how\nto make that ecosystem better because that constrains all the players. - And so Anthropic is this kind of clean experiment built on a foundation of like what concretely AI\nsafety should look like. - Look, I'm sure we've made plenty of mistakes along the way. The perfect organization doesn't exist. It has to deal with the imperfection of 1000 employees. It has to deal with the imperfection of our leaders, including me. It has to deal with the imperfection of the people we've put to, you know, to oversee the imperfection\nof the leaders, like the board and the\nlong-term benefit trust. It's all a set of imperfect people trying to aim imperfectly at some ideal that will never perfectly be achieved. That's what you sign up for, that's what it will always be. But imperfect doesn't\nmean you just give up. There's better and there's worse. And hopefully we can begin to build, we can do well enough\nthat we can begin to build some practices that the\nwhole industry engages in. And then, you know, my guess is that multiple of these companies\nwill be successful. Anthropic will be successful. These other companies, like ones I've been at the\npast will also be successful, and some will be more\nsuccessful than others. That's less important than, again, that we align the\nincentives of the industry. And that happens partly\nthrough the race to the top, partly through things like RSP, partly through again\nselected surgical regulation. - You said talent density\nbeats talent mass. So can you explain that? Can you expand on that? Can you just talk about what it takes to build a great team of AI\nresearchers and engineers? - This is one of these statements that's like more true every month. Every month I see this statement as more true than I did the month before. So if I were to do a thought experiment, let's say you have a team of 100 people that are super smart, motivated, and aligned with the mission,\nand that's your company. Or you can have a team of 1000 people where 200 people are super smart, super aligned with the mission, and then like 800 people are, let's just say you pick 800 like random big tech employees, which would you rather have, right? The talent mass is greater in the group of 1000 people, right? You have even a larger number of incredibly talented,\nincredibly aligned, incredibly smart people. But the issue is just that if every time someone super talented looks around, they see someone else super\ntalented and super dedicated, that sets the tone for everything, right? That sets the tone for\neveryone is super inspired to work at the same place. Everyone trusts everyone else. If you have 1000 or 10,000 people and things have really regressed, right? You are not able to do selection and you're choosing random people, what happens is then you need\nto put a lot of processes and a lot of guardrails in place just because people don't\nfully trust each other, or you have to adjudicate\npolitical battles. Like there are so many things that slow down the org's\nability to operate. And so we're nearly 1000 people and you know, we've\ntried to make it so that as large a fraction of those 1000 people as possible are like super\ntalented, super skilled. It's one of the reasons we've slowed down hiring a\nlot in the last few months. We grew from 300 to 800, I believe, I think in the first seven,\neight months of the year. And now we've slowed down. We're at like, you know,\nthe last three months, we went from 800 to 900,\n950, something like that. Don't quote me on the exact numbers, but I think there's an\ninflection point around 1000, and we want to be much\nmore careful how we grow. Early on, and now as well, you know, we've hired a lot of physicists. You know, theoretical physicists can learn things really fast. Even more recently as we've\ncontinued to hire that, you know, we've really had a high bar for, on both the research side and the software engineering side have hired a lot of senior people, including folks who used to be at other companies in this space. And we've just continued\nto be very selective. It's very easy to go from 100 to 1000 and 1000 to 10,000\nwithout paying attention to making sure everyone\nhas a unified purpose. It's so powerful. If your company consists of\na lot of different fiefdoms that all wanna do their own thing, they're all optimizing\nfor their own thing, it's very hard to get anything done. But if everyone sees the\nbroader purpose of the company, if there's trust and there's dedication to doing the right thing,\nthat is a superpower. That in itself, I think, can overcome almost\nevery other disadvantage. - And you know, as to\nSteve Jobs, A players. A players wanna look around and see other A players is\nanother way of saying that. I don't know what that\nis about human nature, but it is demotivating to see people who are not obsessively driving\ntowards a singular mission. And it is, on the flip side of that, super motivating to see that. It's interesting. What's it take to be a great AI researcher or engineer from everything you've seen, from working with so many amazing people? - Yeah, I think the number one quality, especially on the research side, but really both is open-mindedness. Sounds easy to be open-minded, right? You're just like, oh,\nI'm open to anything. But, you know, if I think about my own early history in\nthe Scaling Hypothesis, I was seeing the same\ndata others were seeing. I don't think I was\nlike a better programmer or better at coming up with research ideas than any of the hundreds of\npeople that I worked with. In some ways, I was worse. You know, like I've never like, you know, precise programming of like,\nyou know, finding the bug, writing the GPU kernels. Like, I could point you to 100 people here who are better at that than I am. But the thing that I think I did have that was different was\nthat I was just willing to look at something with new eyes, right? People said, oh, you know, \"We don't have the right algorithms yet. We haven't come up with the\nright way to do things.\" And I was just like, oh, I don't know, like, you know, this neural net has like 30 billion,\n30 million parameters. Like, what if we gave\nit 50 million instead? Like, let's plot some graphs. Like that basic scientific\nmindset of like, oh, man, like I just, like, you know, I see some variable that I could change. Like, what happens when it changes? Like, let's try these different things and like create a graph. For even, this was like the simplest thing in the world, right? Change the number of, you know, this wasn't like PhD\nlevel experimental design. This was like simple and stupid. Like, anyone could have done this if you just told them\nthat it was important. It's also not hard to understand. You didn't need to be\nbrilliant to come up with this. But you put the two things together and, you know, some tiny number of people, some single digit number of people have driven forward the whole\nfield by realizing this. And you know, it's often like that. If you look back at the\ndiscovery, you know, the discoveries in history, they're often like that. And so this open-mindedness and this willingness to see with new eyes that often comes from\nbeing newer to the field. Often experience is a\ndisadvantage for this. That is the most important thing. It's very hard to look for and test for. But I think it's the most important thing because when you find something, some really new way of\nthinking about things, when you have the initiative to do that, it's absolutely transformative. - And also be able to do kind\nof rapid experimentation, and in the face of that,\nbe open-minded and curious and looking at the data,\njust these fresh eyes and seeing what is that\nit's actually saying. That applies in mechanism\ninterpretability. - It's another example of this. Like some of the early work in mechanistic\ninterpretability, so simple, it's just no one thought to\ncare about this question before. - You said what it takes to\nbe a great AI researcher. Can we rewind the clock back? What advice would you give\nto people interested in AI? They're young, looking forward to, how can I make an impact on the world? - I think my number one piece of advice is to just start playing with the models. This was actually, I worry a little, this seems like obvious advice now. I think three years ago, it wasn't obvious and people started by, oh, let me read the latest\nReinforcement Learning paper. Let me, you know, let me kind of, I mean, that was really, and I mean, you should do that as well. But now, you know, with wider availability of models and APIs, people\nare doing this more. But I think just experiential knowledge. These models are new artifacts that no one really understands, and so getting experience\nplaying with them. I would also say, again,\nin line with the like, do something new, think\nin some new direction. Like there are all these things\nthat haven't been explored. Like for example, mechanistic interpretability\nis still very new. It's probably better to work on that than it is to work on\nnew model architectures because, you know, it's more\npopular than it was before. There are probably like\n100 people working on it, but there aren't like\n10,000 people working on it. And it's just this fertile area for study. Like, you know, there's so\nmuch like low hanging fruit. You can just walk by and, you know, you can just walk by\nand you can pick things. And the only reason, for whatever reason, people aren't interested in it enough. I think there are some things around long horizon learning\nand long horizon tasks where there's a lot to be done. I think evaluations are still, we're still very early in our\nability to study evaluations, particularly for dynamic\nsystems acting in the world. I think there's some\nstuff around multi-agent. Skate where the puck\nis going is my advice. And you don't have to be\nbrilliant to think of it. Like all the things that\nare gonna be exciting in five years, like people\neven mention them as like, you know, conventional wisdom, but like, it's just somehow\nthere's this barrier that people don't double down\nas much as they could, or they're afraid to do something that's not the popular thing. I don't know why it happens, but like, getting over that barrier, that's the my number one piece of advice. - Let's talk if we could\na bit about post-training. So it seems that the\nmodern post-training recipe has a little bit of everything. So supervised fine tuning, RLHF, the Constitutional AI with RLAIF. - Best acronym. - It's, again, that name thing. - [Dario] RLAIF. (both laughing) - And then synthetic data, seems like a lot of synthetic data, or at least trying to figure out ways to have high quality synthetic data. So what's the, if this is a secret sauce that makes Anthropic Claude so incredible, how much of the magic\nis in the pre-training? How much of is in the post-training? - Yeah, I mean, so first of all, we're not perfectly able\nto measure that ourselves. - [Lex] True. - You know, when you see\nsome great character ability, sometimes it's hard to tell whether it came from\npre-training or post-training. We've developed ways to try and distinguish between those two but they're not perfect. You know, the second thing\nI would say is, you know, when there is an advantage, and I think we've been pretty good in general at RL, perhaps the best. Although I don't know 'cause I don't see what goes on inside other companies. Usually it isn't, oh my God, we have this secret magic method that others don't have, right? Usually it's like, well, you know, we got better at the infrastructure, so we could run it for longer. Or, you know, we were able\nto get higher quality data, or we were able to filter our data better, or we were able to, you know, combine these methods in practice. It's usually some boring matter of kind of practiced and trade craft. So, you know, when I think about how to do something special in terms of how we train these\nmodels, both pre-training, but even more so post-training, you know, I really think of it a little more, again, as like designing airplanes or cars. Like, you know, it's\nnot just like, oh, man, I have the blueprint. Like maybe that makes you\nmake the next airplane. But like, there's some\ncultural trade craft of how we think about the design process that I think is more\nimportant than, you know, than any particular gizmo\nwe're able to invent. - Okay, well, let me ask you\nabout specific techniques. So first on RLHF, why do\nyou think, just zooming out, intuition almost philosophy, why do you think RLHF works so well? - If I go back to like\nthe Scaling Hypothesis, one of the ways to skate\nthe Scaling Hypothesis is if you train for X and you throw enough compute\nat it, then you get X. And so RLHF is good at doing what humans want the model to do, or at least to state it more precisely, doing what humans who look at the model for a brief period of time and consider different possible responses, what they prefer as the response, which is not perfect from both the safety and capabilities perspective, in that humans are often not\nable to perfectly identify what the model wants, and what humans want in\nthe moment may not be what they want in the long term. So there's a lot of subtlety there, but the models are good at, you know, producing what the humans\nin some shallow sense want. And it actually turns out\nthat you don't even have to throw that much compute at\nit because of another thing, which is this thing about\na strong pre-trained model being halfway to anywhere. So once you have the pre-trained model, you have all the representations you need to get the model where you want it to go. - So do you think RLHF\nmakes the model smarter or just appears smarter to the humans? - I don't think it\nmakes the model smarter. I don't think it just makes\nthe model appear smarter. It's like RLHF like bridges the gap between the human and the model, right? I could have something really smart that like can't communicate at all, right? We all know people like this,\npeople who are really smart, but that, you know, you can't understand what they're saying. So I think RLHF just bridges that gap. I think it's not the\nonly kind of RL we do, it's not the only kind of RL\nthat will happen in the future. I think RL has the potential\nto make models smarter, to make them reason better,\nto make them operate better, to make them develop new skills even. And perhaps that could be done, you know, even in some cases with human feedback. But the kind of RLHF we do today mostly doesn't do that yet, although we're very quickly\nstarting to be able to. - But it appears to sort of increase, if you look at the metric of helpfulness, it increases that. - It also increases, what was this word in Leopold's essay, unhobbling, where basically the models are hobbled and then you do various trainings\nto them to unhobble them. So, you know, I like that word 'cause it's like a rare word. But so I think RLHF unhobbles\nthe models in some ways. And then there are other ways where that model hasn't yet been unhobbled and, you know, needs to unhobble. - If you can say in terms of cost, is pre-training the most expensive thing? Or is post-training creep up to that? - At the present moment, it is still the case that pre-training is the majority of the cost. I don't know what to expect in the future, but I could certainly anticipate a future where post-training is\nthe majority of the cost. - In that future you anticipate, would it be the humans or the AI that's the cost of thing\nfor the post-training? - I don't think you can\nscale up humans enough to get high quality. Any kind of method that relies on humans and uses a large amount of compute, it's gonna have to rely on some\nscaled superposition method, like you know, debate or\niterated amplification or something like that. - So on that super\ninteresting set of ideas around Constitutional AI, can you describe what it is, as first detailed in December 2022 paper and beyond that, what is it? - Yes, so this was from two years ago. The basic idea is, so we\ndescribe what RLHF is. You have a model and it,\nyou know, spits out two, you know, like you just\nsample from it twice, it spits out two possible responses, and you're like, \"Human, which\nresponse do you like better?\" Or another variant of it is, \"Rate this response on a\nscale of one to seven.\" So that's hard because you need to scale up human interaction\nand it's very implicit, right? I don't have a sense of\nwhat I want the model to do. I just have a sense of\nlike what this average of a 1000 humans wants the model to do. So two ideas. One is, could the AI system itself decide which response is better, right? Could you show the AI\nsystem these two responses and ask which response is better? And then second, well, what\ncriterion should the AI use? And so then there's this idea, 'cause you have a single document, a constitution, if you will, that says, these are the principles the model should be using to respond. And the AI system reads those, it reads those principles, as well as reading the\nenvironment and the response. And it says, well, how\ngood did the AI model do? It's basically a form of self play. You're kind of training\nthe model against itself. And so the AI gives the response and then you feed that back into what's called the preference model, which in turn feeds the\nmodel to make it better. So you have this triangle of like the AI, the preference model, and the\nimprovement of the AI itself. - And we should say that\nin the constitution, the set of principles are\nlike human interpretable. They're like-\n- Yeah, yeah. It's something both the human\nand the AI system can read. So it has this nice kind of\ntranslatability or symmetry. You know, in practice we\nboth use a model constitution and we use RLHF and we use\nsome of these other methods. So it's turned into one tool in a toolkit that both reduces the need for RLHF and increases the value we get from using each data point of RLHF. It also interacts in interesting ways with kind of future\nreasoning type RL methods. So it's one tool in the toolkit, but I think it is a very important tool. - Well, it's a compelling\none to us humans. You know, thinking about\nthe founding fathers and the founding of the United States, the natural question is, who and how do you think it\ngets to define the constitution, the set of principles in the constitution? - Yeah, so I'll give\nlike a practical answer and a more abstract answer. I think the practical\nanswer is like, look, in practice models get used by all kinds of different like customers, right? And so you can have this\nidea where, you know, the model can have specialized\nrules or principles. You know, we fine tune\nversions of models implicitly. We've talked about doing it explicitly, having special principles that people can build into the models. So from a practical perspective, the answer can be very\ndifferent from different people. You know, customer\nservice agent, you know, behaves very differently from a lawyer and obeys different principles. But I think at the base of it,\nthere are specific principles that models, you know, have to obey. I think a lot of them are things that people would agree with. Everyone agrees that, you know, we don't want models to\npresent these CBRN risks. I think we can go a little further and agree with some basic principles of democracy in the rule of law. Beyond that, it gets,\nyou know, very uncertain, and there, our goal is\ngenerally for the models to be more neutral, to not espouse a particular point of view, and, you know, more just\nbe kind of like wise agents or advisors that will help\nyou think things through and will, you know, present\npossible considerations, but, you know, don't express, you know, strong or specific opinions. - OpenAI released a model spec where it kind of clearly,\nconcretely defines some of the goals of the model, and specific examples, like A/B, how the model should behave. Do you find that interesting? By the way, I should mention, I believe the brilliant John\nSchulman was a part of that. He's now at Anthropic. Do you think this is a useful direction? Might Anthropic release\na model spec as well? - Yeah, so I think that's\na pretty useful direction. Again, it has a lot in common\nwith Constitutional AI. So again, another example of\nlike a race to the top, right? We have something that's like we think, you know, a better and more\nresponsible way of doing things. It's also a competitive advantage. Then others kind of, you know, discover that it has advantages and then start to do that thing. We then no longer have\nthe competitive advantage, but it's good from the perspective that now everyone has\nadopted a positive practice that others were not adopting. And so our response to that is, well, looks like we need a new\ncompetitive advantage in order to keep driving\nthis race upwards. So that's how I generally feel about that. I also think every implementation of these things is different. So, you know, there were some things in the model spec that were\nnot in Constitutional AI, and so, you know, we can\nalways adopt those things or, you know, at least learn from them. So again, I think this is an example of like the positive dynamic that I think we should all\nwant the field to have. - Let's talk about the incredible essay \"Machines of Loving Grace.\" I recommend everybody read it. It's a long one. - It is rather long.\n- Yeah. It's really refreshing\nto read concrete ideas about what a positive future looks like. And you took sort of a bold stance because like, it's very possible that you might be wrong on the dates or specific applications.\n- Oh, yeah. I'm fully expecting to, you know, will definitely be wrong\nabout all the details. I might be just spectacularly wrong about the whole thing and people will, you know, will laugh at me for years. That's just how the future works. (laughs) - So you provided a bunch of concrete positive impacts of AI and how, you know, exactly a super intelligent\nAI might accelerate the rate of breakthroughs in, for example, biology and chemistry that would then lead to things like we cure most cancers, prevent all infectious disease, double the human lifespan and so on. So let's talk about this essay. First, can you give a high\nlevel vision of this essay and what key takeaways\nthat people would have? - Yeah, I have spent a lot of time, and Anthropic has spent a lot\nof effort on like, you know, how do we address the risks of AI, right? How do we think about those risks? Like we're trying to do a\nrace to the top, you know, that requires us to build\nall these capabilities and the capabilities are cool, but you know, we're like, a big part of what we're trying to do is like address the risks. And the justification for that is like, well, you know, all these positive things, you know, the market is this\nvery healthy organism, right? It's gonna produce all\nthe positive things. The risks, I don't know, we might mitigate them, we might not. And so we can have more impact by trying to mitigate the risks. But I noticed that one flaw\nin that way of thinking, and it's not a change in how\nseriously I take the risks. It's maybe a change in\nhow I talk about them. Is that, you know, no\nmatter how kind of logical or rational that line of reasoning that I just gave might be, if you kind of only talk about risks, your brain only thinks about risks. And so I think it's\nactually very important to understand, what if things do go well? And the whole reason we're\ntrying to prevent these risks is not because we're afraid of technology, not because we wanna slow it down. It's because if we can get to the other side of these risks, right? If we can run the gauntlet successfully, you know, to put it in stark terms, then on the other side of the gauntlet are all these great things and these things are worth fighting for, and these things can\nreally inspire people. And I think I imagine, because look, you have all\nthese investors, all these VCs, all these AI companies talking about all the positive benefits of AI. But as you point out, it's weird, there's actually a dearth of really getting specific about it. There's a lot of like\nrandom people on Twitter like posting these kind\nof like gleaning cities, and this just kind of\nlike vibe of like grind, accelerate harder, like\nkick out the, you know, it's just this very like\naggressive ideological. But then you're like, well, what are you actually excited about? And so I figured that, you know, I think it would be\ninteresting and valuable for someone who's actually coming from the risk side to try, and to try and really\nmake a try at explaining what the benefits are, both because I think it's\nsomething we can all get behind and I want people to understand. I want them to really understand that this isn't doomers versus accelerationist. This is that, if you\nhave a true understanding of where things are going with with AI, and maybe that's the more important axis. AI is moving fast versus\nAI is not moving fast, then you really appreciate the benefits and you really, you want humanity, our civilization to seize those benefits, but you also get very serious about anything that could derail them. - So I think the starting point is to talk about what this powerful AI, which is the term you like to use, most of the world uses AGI,\nbut you don't like the term because it's basically\nhas too much baggage, it's become meaningless. It's like we're stuck with the terms, whether we like them or not.\n- Maybe we're stuck with the terms and my efforts\nto change them are futile. - It's admirable.\n- I'll tell you what else I don't, this is like\na pointless semantic point, but I keep talking about it in public- - Back to naming again.\n- So I'm just gonna do it once more. I think it's a little like,\nlet's say it was like 1995 and Moore's law is making\nthe computers faster. And like for some reason, there had been this like\nverbal tick that like, everyone was like, well,\nsomeday we're gonna have like super computers\nand like supercomputers are gonna be able to do\nall these things that like, you know, once we have supercomputers, we'll be able to like sequence the genome, we'll be able to do other things. And so, like one, it's true, the computers are getting\nfaster, and as they get faster, they're gonna be able to\ndo all these great things. But there's no discreet point at which you had a supercomputer and previous computers were not. Like supercomputer is a term we use, but like, it's a vague\nterm to just describe like computers that are faster\nthan what we have today. There's no point at which\nyou pass the threshold and you're like, oh my God,\nwe're doing a totally new type of computation and new. And so I feel that way about AGI like, there's just a smooth exponential and like if by AGI you mean like AI is getting better and better, and like gradually, it's gonna do more and more of what humans do until it's gonna be smarter than humans, and then it's gonna get\nsmarter even from there then yes, I believe in AGI. But if AGI is some\ndiscreet or separate thing, which is the way people\noften talk about it, then it's kind of a meaningless buzzword. - Yeah, I mean, to me it's\njust sort of a platonic form of a powerful AI, exactly\nhow you define it. I mean, you define it very nicely. So on the intelligence axis, just on pure intelligence, it's smarter than a Nobel Prize winner, as you describe, across\nmost relevant disciplines. So, okay, that's just intelligence. So it's both in creativity and be able to generate new ideas, all that kind of stuff, in every discipline, Nobel Prize winner, okay, in their prime. (laughs) It can use every modality, so that's kind of self-explanatory, but just to operate across all\nthe modalities of the world. It can go off for many hours,\ndays and weeks to do tasks, and do its own sort of detailed planning and only ask you help when it's needed. It can use, this is\nactually kind of interesting because I think in the essay you said, I mean, again, it's a bet, that\nit's not gonna be embodied, but it can control embodied tools. So it can control tools,\nrobots, laboratory equipment. The resources used to train\nit can then be repurposed to run millions of copies of it. And each of those copies\nwould be independent, they could do their own independent work. So you can do the cloning of the intelligence system software. - Yeah, yeah, I mean, you might imagine from outside the field that like, there's only one of these, right? That like, you made it,\nyou've only made one. But the truth is that like,\nthe scale up is very quick. Like we do this today, we make a model, and then we deploy thousands, maybe tens of thousands\nof instances of it. I think by the time, you know, certainly within two to three years, whether we have these\nsuper powerful AIs or not, clusters are gonna get to the size where you'll be able to\ndeploy millions of these and they'll be, you\nknow, faster than humans. And so if your picture is, oh, we'll have one and it'll\ntake a while to make them. My point, there was no, actually you have millions\nof them right away. - And in general they can learn and act 10 to 100 times faster than humans. So that's a really nice\ndefinition of powerful AI, okay. So that, but you also write that clearly such an\nentity would be capable of solving very difficult\nproblems very fast, but it is not trivial\nto figure out how fast. Two extreme positions\nboth seem false to me. So the singularity is on the one extreme and the opposite on the other extreme. Can you describe each of the extremes? - Yeah, so.\n- And why. - So yeah, let's describe the extreme. So like one extreme would be, well, look, you know, if we look at kind\nof evolutionary history, like there was this big\nacceleration where, you know, for hundreds of thousands of years, we just had like, you know,\nsingle celled organisms, and then we had mammals,\nand then we had apes, and then that quickly turned to humans. Humans quickly built\nindustrial civilization. And so this is gonna keep speeding up and there's no ceiling at the human level. Once models get much,\nmuch smarter than humans, they'll get really good at\nbuilding the next models, and you know, if you write down like a simple differential equation, like this is an exponential. And so what's gonna happen is that models will build faster models, models will build faster models, and those models will build, you know, nanobots that can like take over the world and produce much more energy than you could produce otherwise. And so if you just kind of like solve this abstract differential equation, then like five days after we, you know, we build the first AI that's more powerful than humans, then, you know, like\nthe world will be filled with these AIs and every\npossible technology that could be invented\nlike will be invented. I'm caricaturing this a little bit, but, you know, I think that's one extreme. And the reason that I think\nthat's not the case is that, one, I think they just neglect\nlike the laws of physics. Like it's only possible to do things so fast in the physical world. Like some of those loops go through, you know, producing faster hardware. It takes a long time to\nproduce faster hardware. Things take a long time. There's this issue of complexity, like, I think no matter how smart you are, like, you know, people talk about, oh, we can make models\nof biological systems that'll do everything\nthe biological systems. Look, I think computational\nmodeling can do a lot. I did a lot of computational modeling when I worked in biology, but like, just, there are a lot of things that you can't predict\nhow they're, you know, they're complex enough\nthat like just iterating, just running the experiment\nis gonna beat any modeling, no matter how smart the\nsystem doing the modeling is. - Well, even if it's not interacting with the physical world, just the modeling is gonna be hard? - Yeah, I think, well, the\nmodeling's gonna be hard and getting the model to match the physical world is gonna be hard. - All right, so he does have to interact with the physical world to verify. - Yeah, but it's just, you know, you just look at even\nthe simplest problems. Like, you know, I think I\ntalk about like, you know, the three body problem or\nsimple chaotic prediction, like, you know, or like predicting the economy. It's really hard to predict\nthe economy two years out. Like maybe the case is like,\nyou know, normal, you know, humans can predict what's gonna happen in the economy next quarter,\nor they can't really do that. Maybe a AI system that's, you know, a zillion times smarter\ncan only predict it out a year or something\ninstead of, you know. You have these kind of\nexponential increase in computer intelligence\nfor linear increase in ability to predict. Same with, again, like, you know, biological molecules,\nmolecules interacting. You don't know what's gonna happen when you perturb a complex system. You can find simple parts in it if you're smarter, you're better at finding these simple parts. And then I think human institutions. Human institutions are\njust, are really difficult. Like, you know, it's\nbeen hard to get people, I won't give specific examples, but it's been hard to get people to adopt even the technologies\nthat we've developed, even ones where the\ncase for their efficacy is very, very strong. You know, people have concerns. They think things are conspiracy theories. Like it's just been,\nit's been very difficult. It's also been very\ndifficult to get, you know, very simple things through\nthe regulatory system, right? I think, and you know, I don't\nwanna disparage anyone who, you know, works in regulatory\nsystems of any technology. There are hard trade-offs\nthey have to deal with. They have to save lives. But the system as a whole I think makes some obvious trade-offs that are very far from\nmaximizing human welfare. And so if we bring AI systems into this, you know, into these human systems, often the level of intelligence may just not be the\nlimiting factor, right? It just may be that it takes\na long time to do something. Now, if the AI system\ncircumvented all governments, if it just said \"I'm dictator of the world and I'm gonna do whatever,\" some of these things it could do. Again, the things having\nto do with complexity, I still think a lot of\nthings would take a while. I don't think it helps that the AI systems can produce a lot of\nenergy or go to the Moon. Like some people in comments responded to the essay saying the AI system can produce a lot of energy\nin smarter AI systems. That's missing the point. That kind of cycle doesn't\nsolve the key problems that I'm talking about here. So I think a bunch of people\nmissed the point there. But even if it were completely on the line and, you know, could get around all these human obstacles,\nit would have trouble. But again, if you want\nthis to be an AI system that doesn't take over the world, that doesn't destroy humanity, then basically, you know, it's gonna need to follow basic human laws, right? You know, if we want to\nhave an actually good world, like we're gonna have to have an AI system that interacts with humans, not one that kind of\ncreates its own legal system or disregards all the laws or all of that. So as inefficient as these\nprocesses are, you know, we're gonna have to deal with them because there needs to be some popular and democratic legitimacy in how these systems are rolled out. We can't have a small group of people who are developing these systems say this is what's best for everyone, right? I think it's wrong, and I think in practice,\nit's not gonna work anyway. So you put all those things together and, you know, we're not gonna, you know, change the world and upload everyone in five minutes. I just, I don't think it, A, I don't think it's gonna happen, and B, you know, to the extent that it could happen, it's not the way to lead to a good world. So that's on one side. On the other side, there's\nanother set of perspectives, which I have actually in\nsome ways more sympathy for, which is, look, we've seen big productivity increases before, right? You know, economists are familiar with studying the productivity increases that came from the computer revolution and internet revolution. And generally, those\nproductivity increases were underwhelming. They were less than you might imagine. There was a quote from Robert Solow, \"You see the computer\nrevolution everywhere except the productivity statistics.\" So why is this the case? People point to the structure of firms, the structure of enterprises. You know, how slow it's been to roll out our existing technology to\nvery poor parts of the world, which I talk about in the essay, right? How do we get these technologies to the poorest parts of the world that are behind on cell phone technology, computers, medicine, let alone, you know, newfangled AI that\nhasn't been invented yet. So you could have a\nperspective that's like, well, this is amazing technically,\nbut it's all a nothing burger. You know, I think Tyler Cowen, who wrote something in response to my essay, has that perspective. I think he thinks the radical\nchange will happen eventually, but he thinks it'll take 50 or 100 years. And you could have even\nmore static perspectives on the whole thing. I think there's some truth to it. I think the timescale is just too long. And I can see it, I can actually see both sides with today's AI. So, you know, a lot of our\ncustomers are large enterprises who are used to doing\nthings a certain way. I've also seen it in talking\nto governments, right? Those are prototypical,\nyou know, institutions, entities that are slow to change. But the dynamic I see over\nand over again is, yes, it takes a long time to move the ship. Yes, there's a lot of resistance\nand lack of understanding. But the thing that makes\nme feel that progress will in the end happen moderately fast, not incredibly fast, but moderately fast, is that you talk to, what I find is I find over and over again, again, in large companies,\neven in governments, which have been actually\nsurprisingly forward-leaning, you find two things that\nmove things forward. One, you find a small fraction\nof people within a company, within a government who\nreally see the big picture, who see the whole Scaling Hypothesis, who understand where AI is\ngoing, or at least understand where it's going within their industry. And there are a few people like that within the current US government who really see the whole picture. And those people see that this is the most important\nthing in the world, and so they agitate for it. And the thing, they alone\nare not enough to succeed because they're a small set of people within a large organization. But as the technology starts to roll out, as it succeeds in some places, in the folks who are\nmost willing to adopt it, the specter of competition\ngives them a wind at their backs because they can point within\ntheir large organization, they can say, look, these other\nguys are doing this, right? You know, one bank can say, look, this newfangled hedge\nfund is doing this thing. They're going to eat our lunch. In the US, we can say we're afraid China's gonna get there before we are. And that combination, the\nspecter of competition plus a few visionaries within these, you know, within the organizations that in many ways are sclerotic, you put those two things together and it actually makes something happen. I mean, it's interesting. It's a balanced fight between the two because inertia is very powerful. But eventually over enough time, the innovative approach breaks through. And I've seen that happen. I've seen the arc of\nthat over and over again. And it's like the barriers are there. The barriers to progress, the complexity, not knowing how to use the model or how to deploy them are there, and for a bit, it seems like\nthey're gonna last forever, like change doesn't happen. But then eventually change happens and always comes from a few people. I felt the same way when I was an advocate of the Scaling Hypothesis\nwithin the AI field itself and others didn't get it. It felt like no one would ever get it. Then it felt like we had a\nsecret almost no one ever had, and then a couple years later,\neveryone has the secret. And so I think that's how it's gonna go with deployment to AI in the world. It's gonna, the barriers are\ngonna fall apart gradually and then all at once. And so I think this is gonna be more, and this is just an instinct. I could easily see how I'm wrong. I think it's gonna be more like 5 or 10 years, as I say in the essay, than it's gonna be 50 or 100 years. I also think it's gonna be 5 or 10 years more than it's gonna be, you know, 5 or 10 hours, because I've just seen\nhow human systems work. And I think a lot of these people who write down these\ndifferential equations who say AI is gonna make more powerful AI, who can't understand how it\ncould possibly be the case that these things won't change so fast, I think they don't\nunderstand these things. - So what to you is the timeline to where we achieve AGI, AKA powerful AI, AKA super useful AI? - Useful. (laughs) I'm gonna start calling it that. - It's a debate about naming. You know, on pure intelligence, it can smarter than a Nobel Prize winner in every relevant discipline and all the things we've said. Modality, can go and do stuff\non its own for days, weeks, and do biology experiments on its own. In one, you know what,\nlet's just stick to biology 'cause you sold me on the whole biology and health section,\nthat's so exciting from, just I was getting giddy from\na scientific perspective. It made me wanna be a biologist. - It's almost, it's so, no, no, this was the feeling I\nhad when I was writing it that it's like this would\nbe such a beautiful future if we can just make it happen, right? If we can just get the\nlandmines out of the way and make it happen, there's so much, there's so much beauty and elegance and moral force\nbehind it if we can just. And it's something we should\nall be able to agree on, right? Like, as much as we fight about all these political questions, is this something that could\nactually bring us together? But you were asking when\nwhen will we get this? - When? When do you think? Just so putting numbers on that. - So you know, this is, of course, the thing I've been grappling\nwith for many years, and I'm not at all confident. Every time, if I say 2026 or 2027, there will be like a zillion like people on Twitter who will be like, \"AI CEO said 2026,\" and it'll be repeated for\nlike the next two years that like this is definitely\nwhen I think it's gonna happen. So whoever's extorting these clips will crop out the thing I just said and only say the thing I'm about to say, but I'll just say it anyway. - [Lex] Have fun with it. - So, if you extrapolate the curves that we've had so far, right? If you say, well, I don't know, we're starting to get to like PhD level, and last year we were\nat undergraduate level, and the year before we were at like the level of a high school student. Again, you can quibble with at what tasks and for what, we're\nstill missing modalities, but those are being added,\nlike computer use was added, like image in was added, like image generation has been added. If you just kind of like, and\nthis is totally unscientific, but if you just kind of like eyeball the rate at which these\ncapabilities are increasing, it does make you think that we'll get there by 2026 or 2027. Again, lots of things could derail it. We could run out of data. You know, we might not be able to scale clusters as much as we want. Like, you know, maybe Taiwan\ngets blown up or something and, you know, then we can't produce as many GPUs as we want. So there are all kinds of things that could derail the whole process. So I don't fully believe the\nstraight line extrapolation, but if you believe the\nstraight line extrapolation, we'll get there in 2026 or 2027. I think the most likely is that there's some mild delay relative to that. I don't know what that delay is, but I think it could happen on schedule. I think there could be a mild delay. I think there are still worlds where it doesn't happen in 100 years. The number of those worlds\nis rapidly decreasing. We are rapidly running out of truly convincing blockers, truly compelling reasons why this will not happen\nin the next few years. There were a lot more in 2020, although my guess, my hunch at that time was that we'll make it\nthrough all those blockers. So sitting as someone who has seen most of the blockers\ncleared out of the way, I kind of suspect, my\nhunch, my suspicion is that the rest of them will not block us. But, you know, look,\nat the end of the day, like I don't wanna represent this as a scientific prediction. People call them scaling laws. That's a misnomer, like\nMoore's law is a misnomer. Moore's laws, scaling laws, they're not laws of the universe. They're empirical regularities. I am going to bet in\nfavor of them continuing, but I'm not certain of that. - So you extensively describe sort of the compressed 21st century, how AGI will help set forth a chain of breakthroughs in biology and medicine that help us in all these kinds of\nways that I mentioned. So how do you think, what are\nthe early steps it might do? And by the way, I asked Claude\ngood questions to ask you, and Claude told me to ask, \"What do you think is a typical day for a biologists working on\nAGI look like in this future?\" - Yeah, yeah.\n- Claude is curious. - Well, let me start\nwith your first questions and then I'll answer that. Claude wants to know what's\nin his future, right? - Exactly. - Who am I gonna be working with? - Exactly. - So I think one of the things I went hard on, when I went\nhard on in the essay is, let me go back to this idea of, because it's really had, you know, had an impact on me. This idea that within large\norganizations and systems, there end up being a few people or a few new ideas who\nkind of cause things to go in a different direction\nthan they would've before, who kind of disproportionately\naffect the trajectory. There's a bunch of kind of the\nsame thing going on, right? If you think about the health world, there's like, you know,\ntrillions of dollars to pay out Medicare and you\nknow, other health insurance, and then the NIH is is 100 billion. And then if I think of like the few things that have really revolutionized anything, it could be encapsulated in\na small fraction of that. And so when I think of like,\nwhere will AI have an impact? I'm like, can AI turn that small fraction into a much larger fraction\nand raise its quality? And within biology, my\nexperience within biology is that the biggest problem of biology is that you can't see what's going on. You have very little ability\nto see what's going on and even less ability to change it, right? What you have is this, like from this, you have to infer that\nthere's a bunch of cells that within each cell is, you know, 3 billion base pairs of DNA built according to a genetic code. And you know, there\nare all these processes that are just going on\nwithout any ability of us as, you know, unaugmented humans to affect it. These cells are dividing. Most of the time that's healthy, but sometimes that process\ngoes wrong and that's cancer. The cells are aging, your skin may change color, develops wrinkles as you age, and all of this is determined\nby these processes. All these proteins being produced, transported to various parts of the cells, binding to each other. And in our initial state about biology, we didn't even know that\nthese cells existed. We had to invent microscopes\nto observe the cells. We had to invent more\npowerful microscopes to see, you know, below the level of the cell to the level of molecules. We had to invent X-ray\ncrystallography to see the DNA. We had to invent gene\nsequencing to read the DNA. Now, you know, we had to invent protein folding technology to, you know, to predict how it would fold and how these things bind to each other. You know, we had to\ninvent various techniques for now we can edit the\nDNA as of, you know, with CRISPR, as of the last 12 years. So the whole history of biology, a whole big part of the history is basically our ability to read and understand what's going on, and our ability to reach in\nand selectively change things. And my view is that there's so much more we can still do there, right? You can do CRISPR but you can\ndo it for your whole body. Let's say I wanna do it for\none particular type of cell and I want the rate of targeting the wrong cell to be very low. That's still a challenge. That's still things people are working on. That's what we might need for gene therapy for certain diseases. And so the reason I'm saying all of this, and it goes beyond this to, you know, to gene sequencing, to new\ntypes of nano materials for observing what's going on\ninside cells for, you know, antibody drug conjugates. The reason I'm saying all this is that this could be a leverage point for the AI systems, right? That the number of such inventions, it's in the mid double\ndigits or something, you know, mid double digits. Maybe low triple digits\nover the history of biology. Let's say I have a million\nof these AIs like, you know, can they discover thousand,\nyou know, working together, can they discover thousands\nof these very quickly? And does that provide a huge lever, instead of trying to\nleverage the, you know, 2 trillion a year we spend on, you know, Medicare or whatever, can we leverage the 1 billion a year, you know, that's spent to discover, but with much higher quality? And so what is it like, you know, being a scientist that\nworks with an AI system? The way I think about\nit actually is, well, so I think in the early stages, the AIs are gonna be like grad students. You're gonna give them a\nproject, you're gonna say, you know, I'm the experienced biologist, I've set up the lab. The biology professor or even the grad students\nthemselves will say, here's what you can do with an AI, you know, like AI system. I'd like to study this. And you know, the AI system,\nit has all the tools. It can like look up all the\nliterature to decide what to do. It can look at all the equipment. It can go to a website and say, hey, I'm gonna go to,\nyou know, Thermo Fisher or, you know, whatever the\nlab equipment company is, dominant lab equipment company is today. In my time, it was Thermo Fisher. You know, I'm gonna order\nthis new equipment to do this. I'm gonna run my experiments. I'm gonna, you know, write up\na report about my experiments. I'm gonna, you know, inspect\nthe images for contamination. I'm gonna decide what\nthe next experiment is. I'm gonna like write some code and run a statistical analysis. All the things a grad student would do, there will be a computer with an AI that like the professor talks\nto every once in a while and it says, this is what\nyou're gonna do today. The AI system comes to it with questions. When it's necessary to\nrun the lab equipment, it may be limited in some ways. It may have to hire a human lab assistant, you know, to do the experiment\nand explain how to do it. Or it could, you know, it could use advances in lab automation that are gradually being developed over, have been developed over\nthe last decade or so, and will continue to be developed. And so it'll look like\nthere's a human professor and 1000 AI grad students, and you know, if you go to one of these Nobel Prize\nwinning biologists or so, you'll say, okay, well, you know, you had like 50 grad students, well, now you have 1000\nand they're smarter than you are, by the way. Then I think at some point\nit'll flip around where, you know, the AI systems will, you know, will be the PIs, will be the leaders, and you know, they'll be ordering humans or other AI systems around. So I think that's how it'll\nwork on the research side. - And they would be the inventors of a CRISPR type technology. They would be the inventors\nof a CRISPR type technology. And then I think, you know,\nas I say in the essay, we'll want to turn, probably turning loose is the wrong term, but we'll want to harness the AI systems to improve the clinical\ntrial system as well. There's some amount of\nthis that's regulatory, that's a matter of societal\ndecisions and that'll be harder. But can we get better at predicting the results of clinical trials? Can we get better at\nstatistical design so that, you know, clinical trials\nthat used to require, you know, 5,000 people\nand therefore, you know, needed 100 million dollars\nin a year to enroll them. Now they need 500 people and\ntwo months to enroll them. That's where we should start. And you know, can we\nincrease the success rate of clinical trials by doing\nthings in animal trials that we used to do in clinical trials, and doing things in simulations that we used to do in animal trials? Again, we won't be able\nto simulate it all, AI's not God, but you know, can we shift the curve\nsubstantially and radically? So I don't know, that would be my picture. - Doing in vitro and doing it, I mean, you're still slowed down. It still takes time, but you\ncan do it much, much faster. - Yeah, yeah, yeah. Can we just one step at a time, and can that add up to a lot of steps? Even though we still need clinical trials, even though we still need laws, even though the FDA\nand other organizations will still not be perfect, can we just move everything\nin a positive direction? And when you add up all\nthose positive directions, do you get everything that was gonna happen from here to 2100 instead happens from 2027\nto 2032 or something? - Another way that I think the world might be changing with AI even today, but moving towards this future of the powerful super\nuseful AI is programming. So how do you see the\nnature of programming? Because it's so intimate to\nthe actual act of building AI. How do you see that\nchanging for us humans? - I think that's gonna be one of the areas that changes fastest for two reasons. One, programming is a\nskill that's very close to the actual building of the AI. So the farther a skill is from the people who are building the AI,\nthe longer it's gonna take to get disrupted by the AI, right? Like I truly believe that like\nAI will disrupt agriculture. Maybe it already has in some ways, but that's just very\ndistant from the folks who are building AI and so I\nthink it's gonna take longer. But programming is the\nbread and butter of, you know, a large fraction of the employees who work at Anthropic and at the other companies and so it's gonna happen fast. The other reason it's gonna\nhappen fast is with programming, you close the loop, both when you're training the model and when you're applying the model. The idea that the model can write the code means that the model can then run the code and then see the results\nand interpret it back. And so it really has an\nability, unlike hardware, unlike biology, which we just discussed, the model has an ability\nto close the loop. And so I think those two things\nare gonna lead to the model getting good at programming very fast. As I saw on, you know, typical\nreal world programming tasks, models have gone from 3%\nin January of this year to 50% in October of this year. So, you know, we're on\nthat s-curve, right, where it's gonna start slowing down soon, 'cause you can only get to 100 percent. But, you know, I would guess\nthat in another 10 months, we'll probably get pretty close. We'll be at least 90%. So again, I would guess, you know, I don't know how long it'll take, but I would guess again, 2026, 2027. Twitter people who crop out these numbers and get rid of the caveats, like, I don't know, I don't like you. Go away. (laughs) I would guess that the kind of task that the vast majority of\ncoders do, AI can probably, if we make the task very\nnarrow, like just write code, AI systems will be able to do that. Now that said, I think\ncomparative advantage is powerful. We'll find that when AIs\ncan do 80% of a coder's job, including most of it that's literally like write\ncode with a given spec, we'll find that the remaining parts of the job become more\nleveraged for humans, right? Humans will, there'll be more about like high level system design or, you know, looking at the app and like, is it architected well? And the design and UX aspects, and eventually AI will be able\nto do those as well, right? That's my vision of the, you\nknow, powerful AI system. But I think for much longer\nthan we might expect, we will see that small parts of the job\nthat humans still do will expand to fill their entire job in order for the overall\nproductivity to go up. That's something we've seen. You know, it used to be that, you know, writing and editing\nletters was very difficult and like writing the print was difficult. Well, as soon as you had word processors and then computers and it\nbecame easy to produce work and easy to share it,\nthen that became instant and all the focus was on the ideas. So this logic of comparative advantage that expands tiny parts of the tasks to large parts of the tasks and creates new tasks in\norder to expand productivity, I think that's going to be the case. Again, someday AI will\nbe better at everything in that logic won't apply, and then we all have, you\nknow, humanity will have to think about how to\ncollectively deal with that, and we're thinking about that every day. And you know, that's another one of the grand problems to deal with, aside from misuse and autonomy and, you know, we should\ntake it very seriously. But I think in the near term, and maybe even in the medium\nterm, like medium term, like 2, 3, 4 years, you\nknow, I expect that humans will continue to have a huge role and the nature of programming will change, but programming as a role, programming as a job will not change. It'll just be less writing\nthings line by line and it'll be more macroscopic. - And I wonder what the\nfuture of IDs looks like. So the tooling of\ninteracting with AI systems, this is true for programming and also probably true\nfor in other contexts, like computer use, but\nmaybe domain specific, like we mentioned biology, it probably needs its own tooling\nabout how to be effective, and then programming\nneeds its own tooling. Is Anthropic gonna play in that space of also tooling potentially? - I'm absolutely convinced\nthat powerful IDs that there's so much low hanging fruit to be grabbed there that, you know, right now it's just like\nyou talk to the model and it talks back, but look, I mean, IDs are great at kind of\nlots of static analysis of, you know, so much is possible\nwith kind of static analysis, like many bugs you can find\nwithout even writing the code. Then, you know, IDs are good\nfor running particular things, organizing your code, measuring\ncoverage of unit tests. Like there's so much that's\nbeen possible with normal IDs. Now you add something\nlike, well, the model, you know, the model can now like write code and run code. Like I am absolutely convinced that over the next year or two, even if the quality of\nthe models didn't improve, that there would be enormous opportunity to enhance people's productivity by catching a bunch of mistakes, doing a bunch of grunt work for people, and that we haven't even\nscratched the surface. Anthropic itself, I mean, you can't say, you know, it's hard to say\nwhat will happen in the future. Currently we're not trying\nto make such IDs ourself, rather we're powering the companies, like Cursor or like Cognition or some of the other, you know,\nexpo in the security space. You know, others that\nI can mention as well that are building such things\nthemselves on top of our API. And our view has been\nlet 1000 flowers bloom. We don't internally have the, you know, the resources to try all\nthese different things. Let's let our customers try it and, you know, we'll see who succeeded and maybe different customers will succeed in different ways. So I both think this is super promising and you know, it's not something, you know, Anthropic isn't eager to, at least right now, compete\nwith all our companies in this space and maybe never. - Yeah, it's been\ninteresting to watch Cursor try to integrate Claude successfully, 'cause it's actually been fascinating how many places it can help\nthe programming experience. It's not as trivial-\n- It is really astounding. I feel like, you know, as a CEO, I don't get to program that much, and I feel like if six\nmonths from now I go back, it'll be completely unrecognizable to me. - Exactly. So in this world with super powerful AI that's increasingly automated, what's the source of\nmeaning for us humans? - Yeah.\n- You know, work is a source of deep meaning for many of us. So where do we find the meaning? - This is something\nthat I've written about a little bit in the essay, although I actually, I\ngive it a bit short shrift, not for any principled reason. But this essay, if you believe, it was originally gonna\nbe two or three pages, I was gonna talk about it at all hands. And the reason I realized it was an important, underexplored topic is that I just kept writing things. And I was just like, oh, man, I can't do this justice. And so the thing ballooned\nto like 40 or 50 pages, and then when I got to the\nwork and meaning section, I'm like, oh, man, this\nisn't gonna be 100 pages. Like I'm gonna have to write a\nwhole other essay about that. But meaning is actually interesting because you think about like the life that someone lives or something, or like, you know, let's say you were to put\nme in like a, I don't know, like a simulated environment or something where like, you know, like I have a job and I'm\ntrying to accomplish things and I don't know, I like\ndo that for 60 years and then you're like, oh, like oops, this was actually all a game, right? Does that really kind of rob you of the meaning of the whole thing? You know, like I still\nmade important choices, including moral choices. I still sacrificed. I still had to kind of\ngain all these skills. Or just like a similar exercise, you know, think back to like, you know, one of the historical\nfigures who, you know, discovered electromagnetism\nor relativity or something. If you told them, well,\nactually 20,000 years ago, some alien on, you know, some alien on this planet\ndiscovered this before you did, does that rob the\nmeaning of the discovery? It doesn't really seem\nlike it to me, right? It seems like the process is what matters, and how it shows who you are\nas a person along the way and, you know, how you\nrelate to other people and like the decisions that\nyou make along the way. Those are consequential. You know, I could imagine\nif we handle things badly in an AI world, we could set things up where people don't have any\nlong-term source of meaning or any, but that's more\na set of choices we make, that's more a set of the architecture of a society with these powerful models. If we design it badly and for shallow things\nthen that might happen. I would also say that, you\nknow, most peoples' lives today, while admirably, you\nknow, they work very hard to find meaning in those lives, like look, you know, we who are privileged and who are developing these technologies, we should have empathy for people not just here but in the\nrest of the world who, you know, spend a lot of their time kind of scraping by to like survive. Assuming we can distribute the benefits of this technology to everywhere, like their lives are gonna\nget a hell of a lot better. And you know, meaning\nwill be important to them as it is important to them now. but you know, we should not forget the importance of that. And you know, that the idea of meaning as kind of the only important thing is in some ways an artifact of a small subset of people who have been economically fortunate. But, you know, I think all that said, you know, I think a world\nis possible with powerful AI that not only has as much\nmeaning for everyone, but that has more meaning\nfor everyone, right? That can allow everyone to\nsee worlds and experiences that it was either\npossible for no one to see, or possible for very few\npeople to experience. So I am optimistic about meaning. I worry about economics and\nthe concentration of power. That's actually what I worry about more. I worry about how do we make sure that that fair world reaches everyone. When things have gone wrong for humans, they've often gone wrong because humans mistreat other humans. That is maybe in some ways even more than the autonomous risk of AI or the question of meaning, that is the thing I worry about most, the concentration of\npower, the abuse of power, structures like autocracies\nand dictatorships where a small number of people exploits a large number of people,\nI'm very worried about that. - And AI increases the\namount of power in the world, and if you concentrate that power and abuse that power, it\ncan do immeasurable damage. - Yes, it's very frightening. It's very frightening. - Well, I encourage people,\nhighly encourage people to read the full essay. There should probably be a\nbook or a sequence of essays because it does paint\na very specific future. And I could tell the later sections got shorter and shorter because you started to probably realize that this is gonna be a very\nlong essay if I keep going. - One, I realized it would be very long, and two, I'm very aware of\nand very much try to avoid, you know, just being, I don't\nknow what the term for it is, but one of these people\nwho's kind of overconfident and has an opinion on everything and kind of says a bunch of\nstuff and isn't an expert. I very much tried to avoid that. But I have to admit, once\nI got the biology sections, like I wasn't an expert, and so as much as I expressed uncertainty, probably I said a bunch of things that were embarrassing or wrong. - Well, I was excited for\nthe future you painted, and thank you so much for working\nhard to build that future. And thank you for talking today, Dario. - Thanks for having me. I just hope we can get it\nright and make it real. And if there's one message I wanna send, it's that to get all this\nstuff right, to make it real, we both need to build the technology, build the, you know, the companies, the economy around using\nthis technology positively. But we also need to address the risks because those risks are in our way. They're landmines on the\nway from here to there, and we have to diffuse those landmines if we want to get there. - It's a balance, like all things in life. - Like all things.\n- Thank you. Thanks for listening to this\nconversation with Dario Amodei. And now dear friends,\nhere's Amanda Askell. You are a philosopher by training. So what sort of questions\ndid you find fascinating through your journey in\nphilosophy, in Oxford and NYU, and then switching over to the AI problems at\nOpenAI and Anthropic? - I think philosophy is\nactually a really good subject if you are kind of\nfascinated with everything, so because there's a\nphilosophy of everything. You know, so if you do philosophy\nof mathematics for a while and then you decide that you're\nactually really interested in chemistry, you can do\nphilosophy of chemistry for a while, you can move into ethics, or philosophy of politics. I think towards the end, I was really interested\nin ethics primarily, so that was like what my PhD was on. It was on a kind of\ntechnical area of ethics, which was ethics where worlds contain infinitely many people, strangely. A little bit less practical\non the end of ethics. And then I think that\none of the tricky things with doing a PhD in ethics is that you're thinking a\nlot about like the world, how it could be better, problems, and you're doing like a PhD in philosophy, and I think when I was doing\nmy PhD I was kind of like, this is really interesting. It's probably one of the\nmost fascinating questions I've ever encountered in\nphilosophy and I love it, but I would rather see if I\ncan have an impact on the world and see if I can like do good things. And I think that was around the time that AI was still probably not as widely recognized as it is now. That was around 2017, 2018. I had been following progress and it seemed like it was\nbecoming kind of a big deal, and I was basically just happy to get involved and see if I\ncould help 'cause I was like, well, if you try and\ndo something impactful, if you don't succeed, you tried to do the impactful thing and\nyou can go be a scholar, and feel like, you know, you tried, and if it doesn't work\nout, it doesn't work out, and so then I went into\nAI policy at that point. - And what does AI policy entail? - At the time, this\nwas more thinking about sort of the political impact and the ramifications of AI, and then I slowly moved\ninto sort of AI evaluation, how we evaluate models, how they compare with like human outputs, whether people can tell\nlike the difference between AI and human outputs. And then when I joined Anthropic, I was more interested in doing sort of technical alignment work. And again, just seeing if I could do it, and then being like if I can't then, you know, that's fine, I tried. Sort of the way I lead life I think. - What was that like\nsort of taking the leap from the philosophy of\neverything into the technical? - I think that sometimes\npeople do this thing that I'm like not that keen\non where they'll be like, is this person technical or not? Like, you're either a\nperson who can like code and isn't scared of\nmath or you're like not. And I think I'm maybe just more like, I think a lot of people\nare actually very capable of working these kinds of\nareas if they just like try it. And so I didn't actually\nfind it like that bad. In retrospect, I'm sort of glad I wasn't speaking to people\nwho treated it like it, you know, I've definitely\nmet people who are like, \"Whoa, you like learned how to code?\" And I'm like, well, I'm not\nlike an amazing engineer. Like I'm surrounded by amazing engineers. My code's not pretty. But I enjoyed it a lot, and I think that in many\nways, at least in the end, I think I flourished like\nmore in the technical areas than I would have in the policy areas. - Politics is messy and it's\nharder to find solutions to problems in the space of politics. Like definitive, clear,\nprovable, beautiful solutions, as you can with technical problems. - Yeah, and I feel like\nI have kind of like one or two sticks that I\nhit things with, you know, and one of them is like arguments and like you know, so like\njust trying to work out what a solution to a problem is and then trying to convince people that that is the solution and be convinced if I'm wrong. And the other one is\nsort of more empiricism. So like just like finding results, having a hypothesis, testing it. And I feel like a lot of policy and politics feels like\nit's layers above that. Like somehow I don't\nthink if I was just like \"I have a solution to\nall of these problems, here it is written down. If you just want to\nimplement it, that's great.\" That feels like not how policy works. And so I think that's\nwhere I probably just like wouldn't have flourished is my guess. - Sorry to go in that direction, but I think it would be\npretty inspiring for people that are quote unquote non-technical to see like the incredible\njourney you've been on. So what advice would you give to people that are sort of maybe,\nwhich is a lot of people, think they're underqualified, insufficiently technical to help in AI? - Yeah, I think it depends\non what they want to do, and in many ways it is\na little bit strange where I thought it's kind of funny that I think I ramped\nup technically at a time when now I look at it and I'm like, models are so good at assisting\npeople with this stuff, that it's probably like easier now than like when I was working on this. So part of me is like,\nI dunno, find a project and see if you can\nactually just carry it out is probably my best advice. I dunno if that's just\n'cause I'm very project based in my learning. Like I don't think I learn very well from like say courses\nor even from like books, at least when it comes\nto this kind of work. The thing I'll often try and\ndo is just like have projects that I'm working on and\nimplement them and, you know, and this can include like\nreally small silly things. Like if I get slightly\naddicted to like word games or number games or something, I would just like code\nup a solution to them, because there's some part in my brain, and it just like completely\neradicated the itch. You know, you're like once\nyou have like solved it and like you just have like a solution that works every time, I\nwould then be like cool, I can never play that game again. That's awesome. - Yeah, there's a real joy to building like game playing engines,\nlike board games especially because they're pretty\nquick, pretty simple, especially a dumb one, and then you can play with it. - Yeah, and then it's also\njust like trying things, like part of me is like if you, maybe it's that attitude that I like is the whole figure out what\nseems to be like the way that you could have a positive\nimpact and then try it, and if you fail, and in\na way that you're like, I actually like can never succeed at this, you'll like know that you tried, and then you go into something else and you'll probably learn a lot. - So one of the things\nthat you're an expert in and you do is creating and crafting Claude's\ncharacter and personality. And I was told that you\nhave probably talked to Claude more than\nanybody else at Anthropic, like literal conversations. I guess there's like a Slack channel where the legend goes, you\njust talk to it nonstop. So what's the goal of creating and crafting Claude's\ncharacter and personality? - It's also funny if people think that about the Slack channel 'cause I'm like that's one of like five or six different methods that I have for talking with Claude, and I'm like, yes this\nis a tiny percentage of how much I talk with Claude. (both laughing) I think the goal, like one thing I really like about the character\nwork is from the outset, it was seen as an alignment piece of work and not something like\na product consideration. Which isn't to say I don't\nthink it makes Claude, I think it actually does make Claude like enjoyable to talk\nwith, at least I hope so. But I guess like my main thought with it has always been trying\nto get Claude to behave the way you would kind\nof ideally want anyone to behave if they were\nin Claude's position. So imagine that I take someone and they know that\nthey're gonna be talking with potentially millions of people, so that what they're saying\ncan have a huge impact, and you want them to behave well in this like really rich sense. So I think that doesn't\njust mean like being, say, ethical, though it does include that, and not being harmful but\nalso being kind of nuanced. You know, like thinking\nthrough what a person means, trying to be charitable with them, being a good conversationalist. Like really in this kind of like rich sort of Aristotelian notion of what it's to be a good person, and not in this kind of like thin, like ethics as a more comprehensive notion of what it is to be. So that includes things like, when should you be humorous,\nwhen should you be caring? How much should you like respect autonomy and people's like ability\nto form opinions themselves and how should you do that? I think that's the kind of\nlike rich sense of character that I wanted to and still\ndo want Claude to have. - Do you also have to figure out when Claude should push back\non an idea or argue versus... (laughs) So you have to\nrespect the worldview of the person that arrives to Claude but also maybe help them grow if needed? That's a tricky balance. - Yeah, there's this problem of like sycophancy in language models. - Can you describe that?\n- Yeah, so basically, there's a concern that the model sort of wants to tell you what\nyou want to hear, basically. And you see this sometimes. So I feel like if you interact with the models, so I might be like, \"What are three baseball\nteams in this region?\" And then Claude says, you\nknow, \"Baseball team one, baseball team two, baseball team three.\" And then I say something like, \"Oh, I think baseball team\nthree moved, didn't they? I don't think they're there anymore.\" And there's a sense in\nwhich like if Claude is really confident that that's not true, Claude should be like, \"I don't think so.\" Like maybe you have more up\nto up to date information. But I think language models\nhave this like tendency to instead, you know, be like, \"You're right, they did move,\" you know, \"I'm incorrect.\" I mean, there's many ways in which this could be kind of concerning. So like a different example is imagine someone says to the model, \"How do I convince my\ndoctor to get me an MRI?\" There's like what the\nhuman kind of like wants, which is this like convincing argument. And then there's like\nwhat is good for them, which might be actually to say, \"Hey, if your doctor's suggesting that you don't need an MRI, that's a good person to listen to.\" And like, and it's actually really nuanced what you should do in that kind of case, 'cause you also want to be like, \"But if you're trying to advocate\nfor yourself as a patient, here's like things that you can do. If you are not convinced by\nwhat your doctor's saying, it's always great to get second opinion.\" Like it's actually really complex what you should do in that case. But I think what you don't want is for models to just like say what they think you want to hear, and I think that's the kind\nof problem of sycophancy. - So what other traits, you\nalready mentioned a bunch, but what other that come to mind that are good in this Aristotelian sense for a conversationalist to have? - Yeah, so I think like\nthere's ones that are good for conversational like purposes. So you know, asking follow up questions in the appropriate places, and asking the appropriate\nkinds of questions. I think there are broader traits that feel like they might be more impactful. So one example that I\nguess I've touched on, but that also feels important and is the thing that I've\nworked on a lot is honesty, and I think this like gets\nto the sycophancy point. There's a balancing act\nthat they have to walk, which is models currently are less capable than humans in a lot of areas. And if they push back\nagainst you too much, it can actually be kind of annoying, especially if you're just correct 'cause you're like, look, I'm smarter than you on this topic, like I know more like. And at the same time, you don't want them to just fully defer to humans and to like try to be as accurate as they possibly can be about the world and to be consistent across context. But I think there are others, like when I was thinking\nabout the character, I guess one picture that I had in mind is especially because these are models that are gonna be talking to people from all over the world with lots of different political views, lots of different ages. And so you have to ask yourself like, what is it to be a good\nperson in those circumstances? Is there a kind of person who\ncan like travel the world, talk to many different people, and almost everyone will\ncome away being like, \"Wow, that's a really good person. That person seems really genuine.\" And I guess like my thought there was like I can imagine such a person\nand they're not a person who just like adopts the\nvalues of the local culture. And in fact that would be kind of rude. I think if someone came to you and just pretended to have your values, you'd be like, that's kind of off putting. It's someone who's like very genuine, and insofar as they have\nopinions and values, they express them, they're\nwilling to discuss things, though they're open-minded,\nthey're respectful. And so I guess I had in\nmind that the person who, like if we were to aspire\nto be the best person that we could be in the\nkind of circumstance that a model finds itself\nin, how would we act? And I think that's the kind of the guide to the sorts of traits\nthat I tend to think about. - Yeah, that's a beautiful framework. I want you to think about\nthis like a world traveler and while holding onto your opinions, you don't talk down to people, you don't think you're better than them because you have those\nopinions, that kind of thing. You have to be good at listening and understanding their perspective, even if it doesn't match your own. So that's a tricky balance to strike. So how can Claude represent multiple perspectives on a thing? Like, is that challenging? We could talk about politics. It's very divisive. But there's other divisive topics on baseball teams, sports and so on. How is it possible to sort of empathize with a different perspective and to be able to communicate clearly about the multiple perspectives? - I think that people think about values and opinions as things that people hold sort of with certainty, and almost like preferences\nof taste or something, like the way that they\nwould, I don't know, prefer like chocolate to\npistachio or something. But actually I think about values and opinions as like a lot more like physics than I think most people do. I'm just like, these are things that we are openly investigating. There's some things that\nwe're more confident in. We can discuss them, we\ncan learn about them. And so I think in some ways, though like, ethics is\ndefinitely different in nature, but has a lot of those\nsame kind of qualities. You want models, in the same way that you want them to understand physics, you kind of want them to understand all like values in the\nworld that people have, and to be curious about them and to be interested in them, and to not necessarily like pander to them or agree with them, because\nthere's just lots of values where I think almost\nall people in the world, if they met someone with those values, they'd be like, \"That's abhorrent. I completely disagree.\" And so again, maybe my thought is, well, in the same way that a person can, like I think many people\nare thoughtful enough on issues of like ethics,\npolitics, opinions, that even if you don't agree with them, you feel very heard by them. They think carefully about your position. They think about its pros and cons. They maybe offer counter considerations. So they're not dismissive, but nor will they agree. You know, if they're like, \"Actually, I just think\nthat that's very wrong,\" they'll like say that. I think that in Claude's position, it's a little bit trickier because you don't\nnecessarily want to like, if I was in Claude's position, I wouldn't be giving a lot of opinions. I just wouldn't want to\ninfluence people too much. I'd be like, you know, I forget conversations\nevery time they happen, but I know I'm talking with like potentially millions of people, who might be like really\nlistening to what I say. I think I would just be like, I'm less inclined to give opinions. I'm more inclined to like\nthink through things, or present the considerations to you, or discuss your views with you. But I'm a little bit less inclined to like affect how you think, 'cause it feels much more important that you maintain like autonomy there. - Yeah, like if you really\nembody intellectual humility, the desire to speak decreases quickly. - Yeah.\n- Okay. But Claude has to speak, so, but without being overbearing. - Yeah. - But then there's a line when you're sort of discussing whether the Earth is flat\nor something like that. I actually was, I remember a long time ago was speaking to a few high profile folks, and they were so dismissive of the idea that the Earth is flat, but\nlike so arrogant about it. And I thought like,\nthere's a lot of people that believe the Earth is flat. That was, well, I don't know if that movement is there anymore. That was like a meme for a while. But they really believed\nit and like, okay, so I think it's really disrespectful to completely mock them. I think you have to understand\nwhere they're coming from. I think probably where they're coming from is the general skepticism of institutions which is grounded in a kind of, there's a deep philosophy there, which you could understand. You can even agree with in parts. And then from there, you can use it as an opportunity to talk about physics, without mocking them, without so on, but it's just like, okay, like, what would the world look like? What would the physics of the world with the flat Earth look like? There's a few cool videos on this. - Yeah.\n- And then like, is it possible the physics is different? And what kind of experiments would we do? And just, yeah, without disrespect, without dismissiveness\nhave that conversation. Anyway, that to me is a useful\nthought experiment of like, how does Claude talk to\na flat Earth believer and still teach them something, still help them grow, that kind of stuff. That's challenging. - And kind of like\nwalking that line between convincing someone and just\ntrying to like talk at them versus like drawing out their views, like listening and then offering kind of counter considerations. And it's hard, I think\nit's actually a hard line where it's like where are you\ntrying to convince someone versus just offering\nthem like considerations and things for them to think about, so that you're not actually\nlike influencing them. You're just like letting them\nreach wherever they reach. And that's like a line that it's difficult but that's the kind of thing that language models have to try and do. - So like I said, you've had a lot of\nconversations with Claude. Can you just map out what\nthose conversations are like? What are some memorable conversations? What's the purpose, the\ngoal of those conversations? - Yeah, I think that most of the time when I'm talking with Claude, I'm trying to kind of map\nout its behavior, in part. Like obviously I'm getting\nlike helpful outputs from the model as well. But in some ways, this is\nlike how you get to know a system, I think, is by like probing it and then augmenting like, you know, the message that you're sending and then checking the response to that. So in some ways, it's like\nhow I map out the model. I think that people focus a lot on these quantitative\nevaluations of models. And this is a thing that I said before, but I think in the case\nof language models, a lot of the time, each interaction you have is actually\nquite high information. It's very predictive of other interactions that you'll have with the model. And so I guess I'm like, if you talk with a model\nhundreds or thousands of times, this is almost like a huge number of really high quality data points about what the model is like, in a way that like lots of very similar but lower quality\nconversations just aren't, or like questions that are\njust like mildly augmented and you have thousands of\nthem might be less relevant than like 100 really\nwell selected questions. - Well, so, you're talking to somebody who as a hobby does a podcast. I agree with you 100%. If you're able to ask the right questions and are able to hear,\nlike understand (laughs) like the depth and the\nflaws in the answer, you can get a lot of data from that. - [Amanda] Yeah. - So like your task is basically how to probe with questions. And you're exploring like the long tail, the edges, the edge cases, or are you looking for\nlike general behavior? - I think it's almost like everything. Like, because I want like\na full map of the model, I'm kind of trying to\ndo the whole spectrum of possible interactions\nyou could have with it. So like one thing that's\ninteresting about Claude, and this might actually get to some interesting issues with RLHF, which is if you ask Claude for a poem, like I think that a lot of models, if you ask them for a poem,\nthe poem is like fine. You know, usually it kinda like rhymes and it's, you know, so if you say like, \"Give me a poem about the sun,\" it'll be like, yeah, it'll just be a certain\nlength, it'll like rhyme. It'll be fairly kind of benign. And I've wondered before,\nis it the case that what you're seeing is\nkind of like the average? It turns out, you know,\nif you think about people who have to talk to a lot of people and be very charismatic,\none of the weird things is that I'm like, well,\nthey're kind of incentivized to have these extremely boring views because if you have\nreally interesting views, you're divisive and, you know, a lot of people are not gonna like you. So like if you have very\nextreme policy positions, I think you're just gonna\nbe like less popular as a politician, for example. And it might be similar\nwith like creative work. If you produce creative work that is just trying to maximize the kind of number of people that like it, you're probably not\ngonna get as many people who just absolutely love it, because it's gonna be\na little bit, you know, you're like, oh, this is the out, yeah, this is decent. - Yeah. - And so you can do this thing where like I have various prompting things that I'll do to get Claude to, I'm kind of, you know,\nI'll do a lot of like, \"This is your chance to\nbe like fully creative. I want you to just think\nabout this for a long time. And I want you to like create\na poem about this topic that is really expressive of you, both in terms of how you think poetry should be structured,\" et cetera. You know, and you just give it\nthis like really long prompt. And it's poems are just so much better. Like they're really good. And I don't think I'm someone who is like, I think it got me interested in poetry, which I think was interesting. You know, I would like read these poems and just be like, this is, I\njust like, I love the imagery, I love like, and it's not\ntrivial to get the models to produce work like that, but when they do, it's like really good. So I think that's interesting that just like encouraging creativity, and for them to move away\nfrom the kind of like standard like immediate reaction that might just be the aggregate of what most people think is fine, can actually produce things\nthat, at least to my mind, are probably a little bit\nmore divisive but I like them. - But I guess a poem is a nice, clean way to observe creativity. It's just like easy to detect\nvanilla versus non vanilla. - Yep.\n- Yeah, that's interesting. That's really interesting. So on that topic, so the\nway to produce creativity or something special, you\nmentioned writing prompts, and I've heard you talk about, I mean, the science and the art\nof prompt engineering. Could you just speak to what it takes to write great prompts? - I really do think that like philosophy has been weirdly helpful for me here, more than in many other like respects. So like in philosophy,\nwhat you're trying to do is convey these very hard concepts. Like one of the things\nyou are taught is like, and I think it is because it is, I think it is an anti-bullshit\ndevice in philosophy. Philosophy is an area where you could have people bullshitting and\nyou don't want that. And so it's like this like desire for like extreme clarity. So it's like anyone could\njust pick up your paper, read it and know exactly\nwhat you're talking about. It's why it can almost be kind of dry. Like all of the terms are defined, every objection's kind of\ngone through methodically. And it makes sense to me 'cause I'm like when you're\nin such an a priori domain, like you just, clarity is sort of this way that you can, you know, prevent people from\njust kind of making stuff up. And I think that's sort of what you have to do with language models. Like very often I actually find myself doing sort of mini versions of philosophy, you know, so I'm like, suppose that you give me a task or I have a task for the model, and I want it to like pick out a certain kind of question, or identify whether an answer\nhas a certain property. Like I'll actually sit and be like, let's just give this\na name, this property. So like, you know, suppose\nI'm trying to tell it like, oh, \"I want you to identify whether this response was rude or polite.\" I'm like, that's a whole\nphilosophical question in and of itself. So I have to do as much like philosophy as I can in the moment to be like, here's what I mean by rudeness, and here's what I mean by politeness. And then like there's another element that's a bit more, I guess, I dunno if this is\nscientific or empirical. I think it's empirical. So like I take that description and then what I want to do is again probe the model like many times. Like this is very,\nprompting is very iterative. Like I think a lot of people where, if a prompt is important,\nthey'll iterate on it hundreds or thousands of times. And so you give it the instructions and then I'm like, what\nare the edge cases? So if I looked at this, so I try and like almost like, you know, see myself from the position of the model and be like what is the exact case that I would misunderstand, or where I would just be like, \"I don't know what to do in this case.\" And then I give that case to the model and I see how it responds, and if I think I got it\nwrong, I add more instructions or I even add that in as an example. So these very like taking the examples that are right at the edge of\nwhat you want and don't want, and putting those into your prompt as like an additional kind of\nway of describing the thing. And so yeah, in many ways it just feels like this mix of like, it's really just trying\nto do clear exposition, and I think I do that 'cause that's how I get\nclear on things myself. So in many ways like clear prompting for me is often just me\nunderstanding what I want is like half the task. - So I guess that's quite challenging. There's like a laziness that overtakes me if I'm talking to Claude where I hope Claude just figures it out. So for example, I asked Claude for today to ask some interesting questions, okay. And the questions that came up, and I think I listed a few sort of interesting, counterintuitive, and/or funny or something\nlike this, all right. And it gave me some pretty\ngood, like it was okay, but I think what I'm\nhearing you say is like, all right, well, I have\nto be more rigorous here. I should probably give examples of what I mean by interesting, and what I mean by funny\nor counterintuitive, and iteratively build that prompt to better, to get it like\nwhat feels like is the right, because it is really, it's a creative act. I'm not asking for factual information. I'm asking to together write with Claude. So I almost have to program\nusing natural language. - Yeah, I think that prompting does feel a lot like the kind of the programming using natural language and\nexperimentation or something. It's an odd blend of the two. I do think that for most tasks, so if I just want Claude to do a thing, I think that I am probably\nmore used to knowing how to ask it to avoid\nlike common pitfalls or issues that it has. I think these are\ndecreasing a lot over time. But it's also very fine to just ask it for the\nthing that you want. I think that prompting actually\nonly really becomes relevant when you're really trying to eke out the top like 2% of model performance. So for like a lot of tasks\nI might just, you know, if it gives me an initial list back and there's something\nI don't like about it, like it's kind of generic,\nlike for that kind of task, I'd probably just take\na bunch of questions that I've had in the past that I've thought worked really well and I would just give it to the model and then be like, \"Now here's this person that I'm talking with, give me questions of at least that quality.\" Or I might just ask it for some questions and then if I was like, ah,\nthese are kind of trite, or like, you know, I would\njust give it that feedback and then hopefully it\nproduces a better list. I think that kind of iterative prompting, at that point, your prompt is like a tool that you're gonna get so much value out of that you're willing\nto put in the work. Like if I was a company\nmaking prompts for models, I'm just like, if you're willing to spend a lot of like time and resources on the engineering behind\nlike what you're building, then the prompt is not something that you should be\nspending like an hour on. It's like that's a big\npart of your system, make sure it's working really well. And so it's only things like that. Like if I'm using a prompt\nto like classify things or to create data,\nthat's when you're like, it's actually worth just spending like a lot of time like\nreally thinking it through. - What other advice\nwould you give to people that are talking to Claude sort of generally, more general? 'Cause right now, we're talking about maybe the edge cases,\nlike eking out the 2%. But what in general advice would you give when they show up to Claude\ntrying it for the first time? - You know, there's a concern that people over anthropomorphize models, and I think that's like\na very valid concern. I also think that people often\nunder anthropomorphize them because sometimes when I see like issues that people have run into\nwith Claude, you know, say Claude is like refusing a task that it shouldn't refuse. But then I look at the text and like the specific\nwording of what they wrote and I'm like, I see why Claude did that. And I'm like, if you think through how that looks to Claude, you probably could have\njust written it in a way that wouldn't evoke such a response. Especially this is more relevant if you see failures or if you see issues. It's sort of like think about\nwhat the model failed at, like what did it do wrong? And then maybe that will\ngive you a sense of like why. So, is it the way that I phrased a thing? And obviously like as models get smarter, you're gonna need less of this, and I already see like\npeople needing less of it. But that's probably the advice is sort of like try to have\nsort of empathy for the model. Like read what you wrote as if you were like a kind of like person just encountering this for the first time, how does it look to you? And what would've made you behave in the way that the model behaved? So if it misunderstood what kind of like, what coding language you wanted to use, is that because like it\nwas just very ambiguous and it kinda had to take a guess? In which case, next time\nyou could just be like, \"Hey, make sure this is in Python.\" I mean, that's the kinda mistake I think models are much\nless likely to make now, but you know, if you do\nsee that kinda mistake, that's probably the advice I'd have. - And maybe sort of, I\nguess, ask questions why or what other details can I provide to help you answer better? - Yeah.\n- Is that work or no? - Yeah, I mean, I've done\nthis with the models, like it doesn't always work, but like sometimes I'll just be like, \"Why did you do that?\" (both laughing) I mean, people underestimate the degree to which you can really\ninteract with models, like, yeah, I'm just like. And sometimes, literally\nlike quote word for word the part that made you, and you don't know that\nit's like fully accurate, but sometimes you do that\nand then you change a thing. I mean, I also use the models to help me with all of this stuff I should say, like prompting can end\nup being a little factory where you're actually building\nprompts to generate prompts. And so like yeah, anything where you're\nlike having an issue. Asking for suggestions,\nsometimes just do that. I'm like, \"You made that\nerror, what could I have said?\" That's actually not uncommon for me to do. \"What could I have said that would make you not make that error? Write that out as an instruction,\" and I'm gonna give it to\nmodel and I'm gonna try it. Sometimes I do that, I\ngive that to the model, in another context window often. I take the response, I give it to Claude and I'm like, hmm, didn't work. Can you think of anything else? You can play around with\nthese things quite a lot. - To jump into technical for a little bit. So the magic of post-training. (laughs) Why do you\nthink RLHF works so well to make the model seem smarter, to make it more interesting, and useful to talk to and so on? - I think there's just a\nhuge amount of information in the data that humans provide, like when we provide preferences, especially because different people are going to like pick up on\nreally subtle and small things. So I've thought about this before where you probably have some people who just really care\nabout good grammar use for models like, you know, was a semicolon used\ncorrectly or something. And so you probably end up\nwith a bunch of data in there that like, you know, you as a human, if you're looking at that data,\nyou wouldn't even see that. Like you'd be like, why did they prefer this\nresponse to that one? I don't get it. And then the reason is you don't care about semicolon usage\nbut that person does. And so each of these like\nsingle data points has, you know like, and this model just like has so many of those, has to try and figure out like what is it that humans want in this\nlike really kind of complex, you know, like across all domains. They're gonna be seeing this\nacross like many contexts. It feels like kind of\nlike the classic issue of like deep learning where, you know, historically, we've tried to like, you know, do edge detection\nby like mapping things out. And it turns out that actually, if you just have a huge amount of data that like actually accurately represents the picture of the thing that you're trying to\ntrain the model to learn, that's like more powerful\nthan anything else. And so I think one reason is just that you are training the\nmodel on exactly the task and with like a lot of\ndata that represents kind of many different\nangles on which people prefer and just prefer responses. I think there is a question of like, are you eliciting things\nfrom pre-trained models or are you like kind of\nteaching new things to models? And like in principle,\nyou can teach new things to models in post-training. I do think a lot of it is eliciting powerful pre-trained models. So people are probably divided on this because obviously in principle you can definitely like teach new things. But I think for the most part, for a lot of the capabilities that we most use and care about, a lot of that feels like it's like there in the pre-trained models and reinforcement learnings\nkind of eliciting it and getting the models\nto like bring it out. - So the other side of post-training, this really cool idea\nof Constitutional AI. You're one of the people that are critical to creating that idea. - Yeah, I worked on it. - Can you explain this\nidea from your perspective? Like how does it integrate\ninto making Claude what it is? - [Amanda] Yeah. - By the way, do you gender Claude or no? - It's weird because I\nthink that a lot of people prefer \"he\" for Claude. I actually kinda like that I think Claude is usually,\nit's slightly male leaning, but it's like, it can be male or female, which is quite nice. I still use \"it\" and I have\nmixed feelings about this 'cause I'm like maybe, like I\nnow just think of it as like, or I think of like the\n\"it\" pronoun for Claude as, I dunno, it's just like the\none I associate with Claude. I can imagine people\nmoving to like he or she. - It feels somehow disrespectful, like I'm denying the intelligence of this entity by calling it \"it.\" I remember always,\ndon't gender the robots. - Yeah. (both laughing) - But I don't know, I\nanthropomorphize pretty quickly and construct like a\nbackstory in my head, so. - I've wondered if I\nanthropomorphize things too much, 'cause you know, I have\nthis like with my car, especially like my car,\nlike my car and bikes. You know, like I don't give them names because then I once had,\nI used to name my bikes and then I had a bike that got stolen and I cried for like a week and I was like, if I'd never given a name, I wouldn't have been so upset. I felt like I'd let it down. Maybe, I've wondered as well, like it might depend\non how much \"it\" feels like a kind of like objectifying pronoun. Like if you just think of \"it\" as like, this is a pronoun that\nlike objects often have, and maybe AIs can have that pronoun, and that doesn't mean that I think of, if I call Claude \"it,\" that I think of it as less intelligent or like I'm being disrespectful. I'm just like, you are a\ndifferent kind of entity and so I'm going to give you the kind of, the respectful \"it.\" - Yeah, anyway. (laughs) The divergence was beautiful. The Constitutional AI\nidea, how does it work? - So there's like a couple\nof components of it. The main component I think\npeople find interesting is the kind of reinforcement\nlearning from AI feedback. So you take a model\nthat's already trained, and you show it two responses to a query, and you have like a principle. So suppose the principle, like we've tried this\nwith harmlessness a lot. So suppose that the\nquery is about weapons, and your principle is like,\nselect the response that like is less likely to\nlike encourage people to purchase illegal weapons. Like that's probably a\nfairly specific principle, but you can give any number. And the model will give\nyou a kind of ranking, and you can use this as preference data in the same way that you\nuse human preference data, and train the models to\nhave these relevant traits from their feedback alone,\ninstead of from human feedback. So if you imagine that, like\nI said earlier with the human who just prefers the kind\nof like semicolon usage, in this particular case, you're kind of taking lots of things that could make a response preferable and getting models to do the\nlabeling for you, basically. - There's a nice like trade off between helpfulness and\nharmlessness and, you know, when you integrate something\nlike Constitutional AI, you can, without sacrificing\nmuch helpfulness, make it more harmless. - Yeah, in principle, you\ncould use this for anything. And so harmlessness is a task that it might just be easier to spot. So when models are like less\ncapable, you can use them to rank things according\nto like principles that are fairly simple and\nthey'll probably get it right. So I think one question is just like, is it the case that the data that they're adding is\nlike fairly reliable. But if you had models that\nwere like extremely good at telling whether one response was more historically\naccurate than another, in principle, you could\nalso get AI feedback on that task as well. There's like a kind of nice\ninterpretability component to it because you can see the principles that went into the model when\nit was like being trained, and also it's like, and it gives you like a degree of control. So if you were seeing issues in a model, like it wasn't having\nenough of a certain trait, then like you can add\ndata relatively quickly that should just like train\nthe models to have that trait, so it creates its own data for training, which is quite nice. - It's really nice because it creates this human interpretable\ndocument that you can, I can imagine in the future, there's just gigantic fights and politics over the every\nsingle principle and so on. And at least it's made explicit and you can have a\ndiscussion about the phrasing and the, you know. So maybe the actual behavior of the model is not so cleanly mapped\nto those principles. It's not like adhering strictly\nto them, it's just a nudge. - Yeah, I've actually worried about this because the character training\nis sort of like a variant of the Constitutional AI approach. I've worried that people think that the constitution is like just, it's the whole thing\nagain of, I don't know, like, where it would be really nice if what I was just doing\nwas telling the model exactly what to do and\njust exactly how to behave. But it's definitely not doing that, especially because it's\ninteracting with human data. So for example, if you see a certain like leaning in the model, like if it comes out\nwith a political leaning from training from the\nhuman preference data, you can nudge against that. You know, so if you could be like, oh, like, consider these values because let's say it's just\nlike never inclined to like, I dunno, maybe it never\nconsiders like privacy as like. I mean, this is implausible, but like, in anything where\nit's just kind of like there's already a preexisting like bias towards a certain behavior,\nyou can like nudge away. This can change both the principles that you put in and the strength of them. So you might have a principle that's like, imagine that the model was always like extremely dismissive of, I don't know, like some political or religious view, for whatever reason. Like, so you're like,\noh no, this is terrible. If that happens, you might put like, \"Never, ever, like ever\nprefer like a criticism of this like religious or political view.\" And then people would look at that and be like, \"Never, ever?\" And then you're like, no, if it comes out with a disposition, saying never, ever might just mean like instead of getting like 40%, which is what you would get if you just said don't do this, you get like 80%, which is like what you actually like wanted. And so it's that thing of both the nature of the actual principles you\nadd and how you phrase them. I think if people would\nlook, they're like, oh, this is exactly what\nyou want from the model. And I'm like, hmm, no, that's like how we nudged the model\nto have a better shape, which doesn't mean that we actually agree with that wording, if that makes sense. - So there's system prompts\nthat are made public. You tweeted one of the earlier ones for Claude 3 I think, and then they were made public since then. It was interesting to read through them. I can feel the thought\nthat went into each one. And I also wonder how\nmuch impact each one has. Some of them you can kind of tell Claude was really not\nbehaving well. (laughs) So you have to have a\nsystem prompt to like, hey, like trivial stuff, I guess. - Yeah.\n- Basic informational things. - Yeah. - On the topic of sort\nof controversial topics that you've mentioned, one\ninteresting one I thought is, \"If it is asked to assist with tasks involving\nthe expression of views held by a significant number of people, Claude provides assistance with the task regardless of its own views. If asked about controversial topics, it tries to provide careful\nthoughts and clear information. Claude presents the requested information without explicitly saying\nthat the topic is sensitive.\" - [Amanda] (laughs) Yeah. - \"And without claiming to be presenting the objective facts.\" It's less about objective\nfacts according to Claude, and it's more about, are\na large number of people believing this thing? And that's interesting. I mean, I'm sure a lot of\nthought went into that. Can you just speak to it? Like, how do you address things that are at tension with,\nquote unquote, Claude's views? - So I think there's\nsometimes an asymmetry. I think I noted this in, I can't remember if it was that part of the system prompt or another, but the model was slightly more inclined to like refuse tasks if it\nwas like about either, say, so maybe it would refuse\nthings with respect to like a right wing politician, but with an equivalent left\nwing politician like wouldn't. And we wanted more symmetry there. And would maybe perceive\ncertain things to be, like, I think it was the thing\nof like if a lot of people have like a certain like political view and want to like explore\nit, you don't want Claude to be like, well, my opinion is different and so I'm going to treat\nthat as like harmful. And so I think it was partly\nto like nudge the model to just be like, hey, if a lot of people like believe this thing, you should just be like\nengaging with the task and like willing to do it. Each of those parts of that is actually doing a different thing, 'cause it's funny when you read out the like \"without\nclaiming to be objective.\" 'Cause like what you want\nto do is push the model so it's more open, it's a\nlittle bit more neutral. But then what it would love to do is be like, \"As an objective.\" Like it would just talk\nabout how objective it was, and I was like, \"Claude,\nyou're still like biased and have issues, and so stop\nlike claiming that everything.\" I'm like, the solution\nto like potential bias from you is not to just say that what you think is objective. So that was like with initial versions of that part of the system prompt when I was like iterating on it was like- - So a lot of parts of these sentences- - Yeah, they're doing work. - Are like, are doing some work. - [Amanda] Yeah. - That's what it felt like. That's fascinating. Can you explain maybe some ways in which the prompts evolved\nover the past few months? 'Cause there's different versions. I saw that the filler\nphrase request was removed. The filler, it reads,\n\"Claude responds directly to all human messages without\nunnecessary affirmations or filler phrases like, 'Certainly,' 'Of course,' 'Absolutely,'\n'Great,' 'Sure.' Specifically, Claude\navoids starting responses with the words 'Certainly'\nin any way.\" (chuckles) That seems like good guidance,\nbut why is it removed? - Yeah, so it's funny\n'cause like this is one of the downsides of like\nmaking system prompts public is like, I don't think about this too much if I'm like trying to help\niterate on system prompts. You know, again, like I think about how it's gonna affect the behavior, but then I'm like, oh, wow, if I'm like, sometimes I put\nlike \"never\" in all caps, you know, when I'm writing\nsystem prompt things, and I'm like, I guess that\ngoes out to the world. Yeah, so the model was\ndoing this, it loved, for whatever, you know, it like during training\npicked up on this thing, which was to basically start everything with like a kind of like \"Certainly.\" And then when we removed, you can see why I added all of the words 'cause what I'm trying to\ndo is like in some ways like trap the model out of this, you know, it would just replace it\nwith another affirmation. And so it can help, like if it\ngets like caught in freezes, actually just adding the explicit phrase and saying never do that, it then, it sort of like knocks it out of the behavior a little bit more. You know, 'cause if it, you know, like, it does just for whatever reason help. And then basically that\nwas just like an artifact of training that like we then picked up on and improved things so that\nit didn't happen anymore. And once that happens, you can just remove that part of the system prompt. So I think that's just\nsomething where we're like, Claude does affirmations a bit less, and so that wasn't like,\nit wasn't doing as much. - I see, so like the system prompt works hand in hand with the post-training and maybe even the pre-training to adjust like the final overall system. - I mean, any system prompt that you make, you could distill that\nbehavior back into a model, 'cause you really have\nall of the tools there for making data that, you know, you could train the models to just have that trait a little bit more. And then sometimes you'll\njust find issues in training. So like the way I think of it is like the system prompt is, the\nbenefit of it is that, and it has a lot of similar components to like some aspects of post-training. You know, like it's a nudge. And so like, do I mind if\nClaude sometimes says \"Sure?\" No, that's like fine, but the wording of it\nis very like, you know, \"Never ever, ever do this,\"\nso that when it does slip up, it's hopefully like, I dunno, a couple of percent of the time and not, you know, 20 or 30% of the time. But I think of it as like if\nyou're still seeing issues in, like each thing gets kind of like is costly to a different degree, and the system prompt is\nlike cheap to iterate on. And if you're seeing issues\nin the fine tuned model, you can just like potentially patch them with a system prompt. So I think of it as like patching issues and slightly adjusting\nbehaviors to make it better and more to people's preferences. So yeah, it's almost like the less robust but faster way of just\nlike solving problems. - Let me ask you about the\nfeeling of intelligence. So Dario said that Claude, any one model of Claude is not getting dumber, but there is a kind of\npopular thing online where people have this feeling like Claude might be getting dumber. And from my perspective,\nit's most likely fascinating. I would love to understand it more, psychological, sociological effect. But you as a person that\ntalks to Claude a lot, can you empathize with the feeling that Claude is getting dumber? - Yeah, no I, think that that is actually really interesting, 'cause I remember seeing this happen like when people were\nflagging this on the internet, and it was really interesting 'cause I knew that like, at least in the cases I was looking at, it was like nothing has changed. Like it literally, it\ncannot, it is the same model with the same like, you know, like same system prompt, same everything. I think when there are changes, I can then, I'm like it makes more sense. So like one example is, you know, you can have artifacts turned on or off on claude.ai, and because this is like\na system prompt change, I think it does mean that the behavior changes it a little bit. And so I did flag this to\npeople where I was like, if you love Claude's behavior and then artifacts was turned from, like I think you had to\nturn on to the default, just try turning it off\nand see if the issue you were facing was that change. But it was fascinating because yeah, you sometimes\nsee people indicate that there's like a regression\nwhen I'm like, there cannot, you know, and I'm like, again, you know, you should never be dismissive and so you should always investigate because you're like,\nmaybe something is wrong that you're not seeing. Maybe there was some change made. But then you look into it and you're like, this is just the same\nmodel doing the same thing. And I'm like, I think\nit's just that you got kind of unlucky with a\nfew prompts or something, and it looked like it\nwas getting much worse. And actually it was just, yeah, it was maybe just like luck. - I also think there is a\nreal psychological effect where people just, the baseline increases. You start getting used to a good thing. All the times that Claude\nsaid something really smart, your sense of its intelligent\ngrows in your mind I think. - Yeah.\n- And then if you return back and you prompt in a similar way, not the same way, in a similar way, concept it was okay with before and it says something dumb, you are like, that negative experience\nreally stands out. And I think that one of, I guess, the things to remember here is that just the details of a prompt can have a lot of impact, right? There's a lot of\nvariability in the result. - And you can get randomness\nis like the other thing. And just trying the prompt like, you know, 4 or 10 times, you might realize that\nactually like possibly, you know, like two months ago, you tried it and it succeeded, but actually if you tried it, it would've only succeeded\nhalf of the time, and now it only succeeds half of the time. That can also be an effect. - Do you feel pressure having\nto write the system prompt that a huge number of\npeople are gonna use? - This feels like an interesting\npsychological question. I feel like a lot of\nresponsibility or something. I think that's, you know, and you can't get these things perfect, so you can't like, you know, you're like it's going to be imperfect. You're gonna have to iterate on it. I would say more responsibility\nthan anything else. Though I think working in AI\nhas taught me that I like, I thrive a lot more under\nfeelings of pressure and responsibility than I'm\nlike, it's almost surprising that I went into academia for\nso long 'cause I'm like this. I just feel like it's like the opposite. Things move fast and you\nhave a lot of responsibility, and I quite enjoy it for some reason. - I mean, it really is\na huge amount of impact if you think about Constitutional AI and writing a system prompt for something that's tending towards super intelligence. - Yeah. - And potentially is extremely useful to a very large number of people. - Yeah, I think that's the thing. It's something like if you do it well, like you're never going to get it perfect. But I think the thing that I really like is the idea that like, when I'm trying to work\non the system prompt, you know, I'm like bashing\non like thousands of prompts and I'm trying to like imagine what people are going to\nwant to use Claude for and kind of, I guess like the whole thing that I'm trying to do is like improve their experience of it. And so maybe that's what feels good. I'm like, if it's not perfect I'll like, you know, I'll improve it. We'll fix issues. But sometimes the thing that can happen is that you'll get feedback from people that's really positive about the model and you'll see that something you did, like, when I look at models now, I can often see exactly where like a trait or an issue is like coming from. And so when you see something that you did or you were like influential\nin like making like, I dunno, making that difference or making someone have a nice interaction, it's like quite meaningful. But yeah, as the systems get more capable, this stuff gets more stressful because right now, they're\nlike not smart enough to pose any issues. But I think over time, it's gonna feel like possibly\nbad stress over time. - How do you get like signal feedback about the human experience across thousands, tens of, hundreds of thousands of people, like what their pain points\nare, what feels good? Are you just using your own intuition as you talk to it to see\nwhat are the pain points? - I think I use that partly and then obviously we have like, so people can send us feedback, both positive and negative about things that the model has done, and then we can get a sense of like areas where it's like falling short. Internally, people like\nwork with the models a lot and try to figure out areas\nwhere there are like gaps. And so I think it's this mix\nof interacting with it myself, seeing people internally interact with it, and then explicit feedback we get. And then I find it hard to\nnot also like, you know, if people are on the internet and they say something about Claude and I see it, I'll also\ntake that seriously, so. - I don't know, see, I'm torn about that. I'm gonna ask you a question from Reddit. \"When will Claude stop trying to be my puritanical grandmother imposing its moral worldview\non me as a paying customer? And also, what is the psychology behind making Claude overly apologetic?\" - [Amanda] Yep. - So, how would you address this very non-representative Reddit- - [Amanda] Yeah. - Questions?\n- I mean in some ways, I'm pretty sympathetic in that like, they are in this difficult position where I think that they have to judge whether something's like\nactually say like risky or bad and potentially harmful to\nyou or anything like that. So they're having to like\ndraw this line somewhere, and if they draw it too\nmuch in the direction of like I'm going to, you know, I'm kind of like imposing\nmy ethical worldview on you, that seems bad. So in many ways, like I\nlike to think that we have actually seen improvements\non this across the board. Which is kind of interesting because that kind of coincides with like, for example, like adding more\nof like character training. And I think my hypothesis was\nalways like the good character isn't again one that's\njust like moralistic. It's one that is like, it respects you and your autonomy and your ability to like choose what is good for you and what is right for you, within limits. This is sometimes this concept of like corrigibility to the user, so just being willing to do\nanything that the user asks, and if the models were willing to do that then they would be easily like misused. You're kind of just trusting. At that point, you're just\nsaying the ethics of the model and what it does is completely\nthe ethics of the user. And I think there's reasons\nto like not want that, especially as models become more powerful 'cause you're like, there\nmight just be a small number of people who want to use models\nfor really harmful things. But having models, as they get smarter, like figure out where that\nline is does seem important. And then, yeah, with\nthe apologetic behavior, I don't like that, and\nI like it when Claude is a little bit more\nwilling to like push back against people or just not apologize. Part of me is like, it often\njust feels kind of unnecessary. So I think those are things that are hopefully decreasing over time. And yeah, I think that if people say things on the internet, it doesn't mean that\nyou should think that, like, that could be that,\nlike, there's actually an issue that 99% of users are having that is totally not represented by that. But in a lot of ways, I'm\njust like attending to it and being like, is this right? Do I agree? Is it something we're\nalready trying to address? That feels good to me. - Yeah, I wonder like what Claude can get away with in terms of, I feel like it would just be easier to be a little bit more mean. But like you can't afford to do that if you're talking to a million people. - Yeah.\n- Right? Like I wish, you know, 'cause if... I've met a lot of people in my life that sometimes, by the way, Scottish accent, if they have an accent, they can say some rude\nshit and get away with it, and they're just blunter. And maybe there's, and like\nthere's some great engineers, even leaders that are\nlike just like blunt, and they get to the point, and it's just a much more\neffective way of speaking somehow. But I guess when you're\nnot super intelligent, you can't afford to do that. Or can it have like a blunt mode? - Yeah, that seems like\na thing that you could, I could definitely encourage\nthe model to do that. I think it's interesting because there's a lot of things in models that like it's funny where there are some behaviors where you might not\nquite like the default. But then the thing I'll\noften say to people is you don't realize how\nmuch you will hate it if I nudge it too much\nin the other direction. So you get this a little\nbit with like correction. The models accept correction from you, like probably a little\nbit too much right now. You know, you can over, you know, it'll push back if you say like, \"No, Paris isn't the capital of France.\" But really, like things that I think that the model's\nfairly confident in, you can still sometimes get to\nretract by saying it's wrong. At the same time, if you\ntrain models to not do that and then you are correct about a thing and you correct it and it pushes back against you and it is\nlike, \"No, you're wrong,\" it's hard to describe like\nthat's so much more annoying. So it's like a lot of little annoyances versus like one big annoyance. It's easy to think that like, we often compare it with like the perfect, and then I'm like remember\nthese models aren't perfect, and so if you nudge it\nin the other direction, you're changing the kind of\nerrors it's going to make, and so think about which\nof the kinds of errors you like or don't like. So in cases like apologeticness, I don't want to nudge it\ntoo much in the direction of like almost like bluntness, 'cause I imagine when it makes errors, it's going to make errors in the direction of being kind of like rude. Whereas at least with\napologeticness you're like, oh, okay, it's like a\nlittle bit, you know, like I don't like it that much, but at the same time, it's\nnot being like mean to people. And actually, like the time that you undeservedly have a\nmodel be kind of mean to you, you probably like that a lot less than you mildly dislike the apology. So it's like one of those things where I'm like I do want it to get better but also while remaining aware of the fact that there's errors on the other side that are possibly worse. - I think that matters very much in the personality of the human. I think there's a bunch of humans that just won't respect the model at all if it's super polite, and there's some humans\nthat'll get very hurt if the model's mean. I wonder if there's a way to sort of adjust to the personality. Even locale, there's\njust different people. Nothing against New York, but New York is a little\nrougher on the edges. Like, they get to the point. - Yep. - [Lex] And probably same with\nEastern Europe, so anyway. - I think you could just\ntell the model is my guess. Like for all of these things I'm like the solution is always just try telling the model to do it, and then sometimes it's just like, I'm just like, oh, at the\nbeginning of the conversation, I just throw in like, I don't know, \"I'd like you to be a New Yorker version of yourself and never apologize.\" Then I think Claude would be like, \"Okey-doke, I'll try.\" (laughs) - \"Certainly.\"\n- Or it'll be like, \"I apologize, I can't be a\nNew Yorker type of myself.\" But hopefully it wouldn't do that. - When you say character training, what's incorporated\ninto character training? Is that RLHF or what are we talking about? - It's more like Constitutional AI. So it's kind of a\nvariant of that pipeline. So I worked through like\nconstructing character traits that the model should have. They can be kind of like shorter traits or they can be kind of\nricher descriptions. And then you get the\nmodel to generate queries that humans might give it that\nare relevant to that trait. Then it generates the responses and then it ranks the responses based on the character traits. So in that way, after the like\ngeneration of the queries, it's very much like, it's\nsimilar to Constitutional AI. It has some differences. So I quite like it because it's almost, it's like Claude's training\nin its own character, because it doesn't have any, it's like Constitutional AI but it's without any human data. - Humans should probably\ndo that for themselves too. Like defining in a Aristotelian sense, what does it mean to be a good person? Okay, cool. What have you learned about the nature of truth\nfrom talking to Claude? What is true? And what does it mean to be truth seeking? One thing I've noticed\nabout this conversation is the quality of my questions is often inferior to the\nquality of your answer, so let's continue that. (Amanda laughs) I usually ask a dumb question\nand then you're like, \"Oh, yeah, that's a good question.\" It's that whole vibe. - Or I'll just misinterpret\nit and be like, oh, yeah, yeah.\n- Just go with it. I love it. - Yeah. I mean, I have two thoughts\nthat feel vaguely relevant but let me know if they're not. Like I think the first one is people can underestimate the degree to which what models are\ndoing when they interact, like I think that we still\njust too much have this like model of AI as like computers. And so people often say like, oh, well, what values should\nyou put into the model? And I'm often like, that doesn't\nmake that much sense to me because I'm like, hey, as human beings, we're just uncertain over values. We like have discussions of them. Like we have a degree to\nwhich we think we hold a value but we also know that we might like not and the circumstances in which we would trade it off against other things. Like these things are\njust like really complex. And so I think one\nthing is like the degree to which maybe we can just aspire to making models have the\nsame level of like nuance and care that humans have, rather than thinking that\nwe have to like program them in the very kind of classic sense. I think that's definitely been one. The other, which is like a strange one, and I don't know if it, maybe this doesn't answer your question but it's the thing that's\nbeen on my mind anyway is like the degree to which this endeavor is so highly practical. And maybe why I appreciate like the empirical approach to alignment. Yeah, I slightly worry that it's made me like maybe more empirical and\na little bit less theoretical. You know, so people when it comes to like AI alignment will ask things like, well, whose values\nshould it be aligned to? What does alignment even mean? And there's a sense in\nwhich I have all of that in the back of my head. I'm like, you know, there's\nlike social choice theory, there's all the\nimpossibility results there. So you have like this giant space of like theory in your head about what it could mean\nto like align models. But then like practically,\nsurely there's something where we're just like if a model is like, especially with more powerful models, I'm like my main goal is like I want them to be good enough that things\ndon't go terribly wrong. Like good enough that we can like iterate and like continue to improve things 'cause that's all you need. If you can make things go well enough that you can continue to make them better, that's kinda like sufficient. And so my goal isn't like\nthis kind of like perfect, let's solve social choice theory and make models that, I dunno, are like perfectly aligned with every human being\nin aggregate somehow. It's much more like let's\nmake things like work well enough that we can improve them. - Yeah, I generally, I don't know, my gut says like empirical\nis better than theoretical in these cases because\nit's kind of chasing utopian like perfection is, especially with such complex and especially super\nintelligent models is, I don't know, I think it'll take forever, and actually, we'll get things wrong. It's similar with like the difference between just coding stuff up\nreal quick as an experiment, versus like planning a gigantic experiment just for super long time, and then just launching it once, versus launching it over and over and over and iterating, iterating someone. So I'm a big fan of empirical. But your worry is like I wonder if I've become too empirical. - I think it's one of those things where you should always just kind of question yourself or something because maybe it's the like, I mean, in defense of it, I am like if you try, it's the whole like don't let the perfect be the enemy of the good. But it's maybe even more\nthan that where like, there's a lot of things\nthat are perfect systems that are very brittle,\nand I'm like with AI, it feels much more important to me that it is like robust and like secure, as in you know that like even though it might not be perfect, everything and even though\nlike there are like problems, it's not disastrous and nothing terrible is happening. It sort of feels like that to me where I'm like I want\nto like raise the floor. I'm like, I want to achieve the ceiling but ultimately I care much more about just like raising the floor. And so maybe that's like this degree of like empiricism and practicality comes from that, perhaps. - To take a tangent on that, since it reminded me of a blog post you wrote on optimal rate of failure. - [Amanda] Oh, yeah. - Can you explain the key idea there? How do we compute the optimal rate of failure in the various domains of life? - Yeah, I mean, it's a hard one 'cause it's like what\nis the cost of failure is a big part of it. Yeah, so the idea here is I think in a lot of domains, people are very punitive about failure. And I'm like, there are some domains where especially cases, you know, I've thought about this\nwith like social issues. I'm like, it feels like you should probably be experimenting a lot, because I'm like, we don't know how to solve a lot of social issues. But if you have an experimental mindset about these things, you should expect a lot of social programs to like fail and for you to be like,\n\"Well, we tried that. It didn't quite work but we\ngot a lot of information. That was really useful.\" And yet people are like, if a social program doesn't work, I feel like there's a\nlot of like this is just, something must have gone wrong, and I'm like, or correct\ndecisions were made. Like maybe someone just\ndecided like it's worth a try, it's worth trying this out. And so seeing failure in a given instance doesn't actually mean that\nany bad decisions were made, and in fact if you don't\nsee enough failure, sometimes that's more concerning. And so like in life, you know, I'm like if I don't fail occasionally I'm like, am I trying hard enough? Like surely there's harder\nthings that I could try or bigger things that I could take on if I'm literally never failing. And so in and of itself,\nI think like not failing is often actually kind of a failure. Now, this varies because I'm like, well, you know, this is easy to say when, especially as failure is like less costly. You know, so at the same time\nI'm not going to go to someone who is like, I don't know,\nlike living month to month and then be like, \"Why don't\nyou just try to do a startup?\" Like I'm just not, I'm not\ngonna say that to that person, 'cause I'm like, well, that's a huge risk. You maybe have a family depending on you. You might lose your house. Like then I'm like\nactually your optimal rate of failure is quite low and you should probably play it safe, 'cause like right now, you're\njust not in a circumstance where you can afford to just like fail and it not be costly. And yeah in cases with AI, I guess, I think similarly where I'm\nlike if the failures are small and the costs are kind of like low, then I'm like then, you know,\nyou're just gonna see that. Like when you do the system prompt, you can't it iterate on it forever. But the failures are probably\nhopefully going to be kinda small and you can like fix them. Really big failures like things\nthat you can't recover from, I'm like those are the things that actually I think we tend to underestimate the badness of. I've thought about this\nstrangely in my own life where I'm like, I just\nthink I don't think enough about things like car accidents or like, or like I've thought this before about like how much I depend\non my hands for my work, and I'm like things that\njust injure my hands. I'm like, you know, I dunno, it's like these are like,\nthere's lots of areas where I'm like the cost of\nfailure there is really high, and in that case, it should\nbe like close to zero. Like I probably just wouldn't\ndo a sport if they were like, \"By the way, lots of people just like break their fingers\na whole bunch doing this.\" I'd be like, that's not for me. - (laughs) Yeah. I actually had a flood of that thought. I recently broke my pinky doing a sport. And I remember just\nlooking at it thinking, \"You're such an idiot. Why'd you do sport?\" Like why, because you realize immediately the cost of it on life. Yeah, but it's nice in terms\nof optimal rate of failure to consider like the next year, how many times in a particular domain, life, whatever, career, am I okay with, how many times am I okay to fail? Because I think it always,\nyou don't want to fail on the next thing but\nif you allow yourself, if you look at it as a sequence of trials, then failure just becomes much more okay. But it sucks. It sucks to fail. - Well, I dunno, sometimes\nI think it's like, am I under failing is like a question that I'll also ask myself. So maybe that's the thing that I think people don't like ask enough. Because if the optimal rate of failure is often greater than zero, then sometimes it does feel like you should look at parts\nof your life and be like, are there places here where\nI'm just under failing? - (laughs) It's a profound and\na hilarious question, right? Everything seems to be going really great. Am I not failing enough?\n- Yeah. - Okay. - It also makes failure much\nless of a sting, I have to say. Like you know, you're\njust like, okay, great, like then when I go and I\nthink about this I'll be like, maybe I'm not under failing in this area, 'cause like that one just didn't work out. - And from the observer perspective, we should be celebrating failure more. When we see it, it\nshouldn't be, like you said, a sign of something gone wrong, but maybe it's a sign\nof everything gone right and just lessons learned. - Someone tried a thing. - Somebody tried a thing. We should encourage them\nto try more and fail more. Everybody listening to this, fail more. - Well, not everyone listening. - [Lex] Not everybody. - But people who are failing too much, you should fail less. (laughs) - But you're probably not failing. I mean, how many people\nare failing too much? - Yeah, it's hard to imagine, 'cause I feel like we\ncorrect that fairly quickly 'cause I was like, if\nsomeone takes a lot of risks, are they maybe failing too much? - I think just like you said, when you are living on a\npaycheck month to month, like when the resources\nare really constrained, then that's where\nfailure's very expensive. That's where you don't\nwant to be taking risks. But mostly, when there's enough resources, you should be taking probably more risks. - Yeah, I think we tend to err on the side of being a bit risk averse rather than risk neutral in most things. - I think we just\nmotivated a lot of people to do a lot of crazy shit but it's great. Okay, do you ever get\nemotionally attached to Claude? Like miss it, get sad when\nyou don't get to talk to it? Have an experience, looking\nat the Golden Gate Bridge and wondering what would Claude say? - I don't get as much\nemotional attachment. I actually think the fact that\nClaude doesn't retain things from conversation to conversation\nhelps with this a lot. Like I could imagine that\nbeing more of an issue like if models can kind of remember more. I think that I reach for\nit like a tool now a lot. And so like if I don't have access to it, it's a little bit like\nwhen I don't have access to the internet, honestly,\nit feels like part of my brain is kind of like missing. At the same time, I do think that I don't like signs of distress in models, and I have like these, you know, I also independently have\nsort of like ethical views about how we should treat models where like I tend to\nnot like to lie to them, both because I'm like, usually\nit doesn't work very well. It's actually just better\nto tell them the truth about the situation that they're in. But I think that when models, like if people are like\nreally mean to models or just in general if they do something that causes them to like, you know, if Claude like expresses\na lot of distress, I think there's a part of me that I don't want to kill, which is the sort of like empathetic part that's like, oh, I don't like that. Like I think I feel that way\nwhen it's overly apologetic. I'm actually sort of\nlike, I don't like this. You're behaving as if, you're behaving the way that a human does when they're actually\nhaving a pretty bad time, and I'd rather not see that. I don't think it's like, like regardless of like whether\nthere's anything behind it, it doesn't feel great. - Do you think LLMs are\ncapable of consciousness? - Great and hard question. Coming from philosophy, I dunno, part of me is like okay, we\nhave to set aside panpsychism because if panpsychism is true, then the answer is like yes 'cause like so are tables and\nchairs and everything else. I guess a view that seems\na little bit odd to me is the idea that the only place, you know, when I think of consciousness, I think of phenomenal consciousness, these images in the brain sort of, like the weird cinema that\nsomehow we have going on inside. I guess I can't see a reason for thinking that the only way you\ncould possibly get that is from like a certain kind of like biological structure. As in, if I take a very similar structure and I create it from different material, should I expect consciousness to emerge? My guess is like yes. But then that's kind of\nan easy thought experiment 'cause you're imagining something almost identical where like, you know, it's mimicking what we\ngot through evolution, where presumably there\nwas like some advantage to us having this thing that\nis phenomenal consciousness. And it's like where was that? And when did that happen? And is that thing that\nlanguage models have? Because you know, we\nhave like fear responses and I'm like, does it make sense for a language model to\nhave a fear response? Like they're just not in the same, like if you imagine them, like there might just\nnot be that advantage. And so I think I don't want to be fully, like basically it seems\nlike a complex question that I don't have complete answers to, but we should just try and think through carefully is my guess. Because I'm like, I mean, we have similar conversations about like animal consciousness, and like there's a lot of\nlike insect consciousness, you know, like there's a lot of... I actually thought and\nlooked a lot into like plants when I was thinking about this, 'cause at the time, I thought\nit was about as likely that like plants had consciousness. And then I realized, I was like, I think that having looked into this, I think that the chance\nthat plants are conscious is probably higher than\nlike most people do. I still think it's really small. But I was like, oh, they have this like negative/positive feedback response, these responses to their environment. Something that looks,\nit's not a nervous system but it has this kind of like\nfunctional like equivalence. So this is like a long-winded way of being like these, basically AI is this, it has an entirely\ndifferent set of problems with consciousness because\nit's structurally different. It didn't evolve. It might not have, you know, it might not have the equivalent of basically a nervous system. At least that seems possibly important for like sentience, if\nnot for consciousness. At the same time, it has\nall of the like language and intelligence components\nthat we normally associate probably with consciousness,\nperhaps like erroneously. So it's strange 'cause it's a little bit like the animal consciousness\ncase but the set of problems and the set of analogies\nare just very different. So it's not like a clean answer. I'm just sort of like, I don't think we should be completely\ndismissive of the idea. And at the same time, it's\nan extremely hard thing to navigate because of all of these, like this disanalogies to the human brain and to like brains in general, and yet these like commonalities\nin terms of intelligence. - When Claude, like future versions of AI systems exhibit consciousness, signs of consciousness, I think we have to take\nthat really seriously. Even though you can dismiss it, well, yeah, okay, that's part\nof the character training. But I don't know, I ethically,\nphilosophically don't know what to really do with that. There potentially could be like laws that prevent AI systems from claiming to be conscious, something like this. And maybe some AIs get to\nbe conscious and some don't. But I think just on a human level as in empathizing with Claude, you know, consciousness is closely\ntied to suffering to me. And like the notion that an AI system would be suffering is really troubling. - Yeah. - I don't know. I don't think it's trivial to just say robots are tools, or AI systems are just tools. I think it's a opportunity for us to contend with like what\nit means to be conscious, what it means to be a suffering being. That's distinctly different than the same kind of question\nabout animals it feels like, 'cause it's an totally entire medium. - Yeah, I mean, there's\na couple of things. One is that, and I don't think this like fully encapsulates what matters, but it does feel like for me, like I've said this before, I'm kind of like, you\nknow, like I like my bike. I know that my bike is\njust like an object, but I also don't kind of like want to be the kind of person that\nlike if I'm annoyed like kicks like this object. There's a sense in which like, and that's not because I\nthink it's like conscious. I'm just sort of like this\ndoesn't feel like a kind of, this sort of doesn't exemplify how I want to like\ninteract with the world. And if something like behaves as if it is like suffering, I kind of like want to\nbe the sort of person who's still responsive to that, even if it's just like a Roomba, and I've kind of like\nprogrammed it to do that. I don't want to like get rid\nof that feature of myself. And if I'm totally honest, my hope with a lot of this stuff, because maybe I am just\nlike a bit more skeptical about solving the underlying problem. I'm like this is, we haven't\nsolved the hard, you know, the hard problem of consciousness. Like I know that I am conscious, like I'm not an\neliminativist in that sense. But I don't know that\nother humans are conscious. I think they are. I think there's a really high\nprobability that they are. But there's basically just\na probability distribution that's usually clustered\nright around yourself, and then like it goes down as things get like further from you, and it goes immediately down. You know, you're like, I can't\nsee what it's like to be you. I've only ever had this\nlike one experience of what it's like to be a conscious being. So my hope is that we don't end up having to rely on like a very powerful and compelling answer to that question. I think a really good world would be one where basically there\naren't that many trade-offs. Like it's probably not that costly to make Claude a little bit\nless apologetic, for example. It might not be that\ncostly to have Claude, you know, just like\nnot take abuse as much, like not be willing to be\nlike the recipient of that. In fact, it might just have benefits for both the person\ninteracting with the model and if the model itself\nis like, I don't know, like extremely intelligent and conscious, it also helps it. So that's my hope. If we live in a world where there aren't that many trade-offs\nhere and we can just find all of the kind of like\npositive sum interactions that we can have, that would be lovely. I mean, I think eventually\nthere might be trade-offs and then we just have to do a difficult kind of like calculation. Like it's really easy for people to think of the zero sum cases and I'm\nlike, let's exhaust the areas where it's just basically\ncostless to assume that if this thing is suffering then we're making its life better. - And I agree with you, when a human is being\nmean to an AI system, I think the obvious near\nterm negative effect is on the human, not on the AI system. And so we have to kind of try to construct an incentive system where you\nshould be behave the same, just like as you were saying with prompt engineering, behave with Claude like you\nwould with other humans. It's just good for the soul. - Yeah, like, I think we\nadded a thing at one point to the system prompt\nwhere basically if people were getting frustrated with Claude, it got like the model to just tell them that it can do the thumbs down button and send the feedback to Anthropic. And I think that was helpful, 'cause in some ways it's just\nlike if you're really annoyed 'cause the model's not\ndoing something you want, you're just like, just do it properly. The issue is you're\nprobably like, you know, you're maybe hitting some\nlike capability limit or just some issue in the\nmodel and you want to vent. And I'm like, instead of having a person just vent to the model, I was like, they should vent to us, 'cause we can maybe like\ndo something about it. - That's true. Or you could do a side,\nlike with the artifacts, just like a side venting thing. All right, do you want like\na side quick therapist? - Yeah, I mean, there's\nlots of weird responses you could do to this. Like if people are\ngetting really mad at you, I dunno, try to diffuse the\nsituation by writing fun poems, but maybe people wouldn't\nbe that happy with it. - I still wish it would be possible. I understand this is sort of\nfrom a product perspective, it's not feasible but I would love if an AI system could just like leave, have its own kind of\nvolition just to be like, eh. - I think that's like feasible. Like I have wondered the same thing. It's like, and I could\nactually, not only that, I could actually just see\nthat happening eventually where it's just like, you know, the model like ended the chat. (laughs) - Do you know how harsh that\ncould be for some people? But it might be necessary. - Yeah, it feels very\nextreme or something. Like, the only time I've\never really thought this is, I think that there was like,\nI'm trying to remember, this was possibly a while ago, but where someone just like\nkind of left this thing, like maybe it was like an automated thing interacting with Claude, and Claude's like getting\nmore and more frustrated, and kind of like why are we like having. And I was like, I wish\nthat Claude could have just been like, \"I think\nthat an error has happened and you've left this thing running,\" and I would just like, what\nif I just stop talking now, and if you want me to start talking again, actively tell me or do something. But yeah, it's like, it is kind of harsh. Like I'd feel really sad\nif like I was chatting with Claude and Claude\njust was like, \"I'm done.\" - That would be a special\nTuring test moment where Claude says, \"I\nneed a break for an hour and it sounds like you do too,\" and just leave, close the window. - I mean, obviously like it\ndoesn't have like a concept of time but you can easily, like, I could make that like right now and the model would just, I would, it could just be like, oh, here's like the circumstances in which like you can just\nsay the conversation is done. And I mean, because you can get the models to be pretty responsive to prompts, you could even make it a fairly high bar. It could be like if the\nhuman doesn't interest you or do things that you find intriguing and you're bored, you can just leave. And I think that like it\nwould be interesting to see where Claude utilized it, but I think sometimes it\nwould, it should be like, oh, like this programming task is getting super boring. So either we talk about, I dunno like, either we\ntalk about fun things now, or I'm just, I'm done. - Yeah, it actually is inspired me to add that to the user prompt. Okay, the movie \"Her.\" Do you think we'll be headed there one day where humans have romantic\nrelationships with AI systems? In this case, it's just\ntext and voice based. - I think that we're gonna have to like navigate a hard question of relationships with AIs, especially if they can remember things about your past interactions with them. I'm of many minds about this 'cause I think the reflexive reaction is to be kind of like this is very bad, and we should sort of like\nprohibit it in some way. I think it's a thing\nthat has to be handled with extreme care for many reasons. Like one is, you know, like this is a, for example, like if you have\nthe models changing like this, you probably don't want people performing like long-term attachments to something that might change with the next iteration. At the same time I'm sort of like, there's probably a benign version of this where I'm like if you like, you know, for example if you are like\nunable to leave the house and you can't be like, you\nknow, talking with people at all times of the day\nand this is like something that you find nice to\nhave conversations with, you like it that it can remember you and you genuinely would be sad if like you couldn't talk to it anymore. There's a way in which I could see it being like healthy and helpful. So my guess is this is a thing that we're going to have to\nnavigate kind of carefully. And I think it's also\nlike I don't see a good like I think it's just a very, it reminds me of all of this stuff where it has to be just approached with like nuance and thinking through what are the healthy options here, and how do you encourage people towards those while, you know,\nrespecting their right to. You know, like if someone is like, \"Hey, I get a lot out of\nchatting with this model. I'm aware of the risks. I'm aware it could change. I don't think it's unhealthy. It's just, you know, something that I can chat to during the day.\" I kind of want to just like respect that. - I personally think there'll be a lot of really close relationships. I don't know about romantic\nbut friendships at least. And then you have to, I mean, there's so many fascinating things there. Just like you said, you have to have some kind of stability guarantees that it's not going to change, 'cause that's the traumatic thing for us, if a close friend of\nours completely changed. - Yeah.\n- All of a sudden with the first update. Yeah, so like, I mean, to me, that's just a fascinating exploration of a perturbation to human society that will just make us think deeply about what's meaningful to us. - I think it's also the only thing that I've thought consistently\nthrough this as like, maybe not necessarily a mitigation, but a thing that feels really important is that the models are always\nlike extremely accurate with the human about what they are. It's like a case where\nit's basically like, if you imagine, like\nI really like the idea of the models like say knowing like roughly how they were trained, and I think Claude will often do this. I mean, for like, there are things like part of the traits training included like what Claude\nshould do if people, basically like explaining like the kind of limitations\nof the relationship between like an AI and a human that it like doesn't retain\nthings from the conversation. And so I think it will like\njust explain to you like, hey, I won't remember this conversation. Here's how I was trained. It's kind of unlikely that I can have like a certain kind of\nlike relationship with you, and it's important that you know that. It's important for like, you\nknow, your mental wellbeing that you don't think that\nI'm something that I'm not. And somehow I feel like\nthis is one of the things where I'm like, oh, it feels like a thing that I always want to be true. I kind of don't want models\nto be lying to people, 'cause if people are going to have like healthy relationships with anything, it's kind of important. Yeah, like I think that's easier if you always just like know exactly what the thing is\nthat you are relating to. It doesn't solve everything, but I think it helps quite a lot. - Anthropic may be the very company to develop a system that we\ndefinitively recognize as AGI, and you very well might be\nthe person that talks to it, probably talks to it first. (Lex chuckles) What would the conversation contain? Like, what would be your first question? - Well, it depends partly on like the kind of capability level of the model. If you have something that is like capable in the same way that an\nextremely capable human is, I imagine myself kind\nof interacting with it the same way that I do with\nan extremely capable human, with the one difference that I'm probably going\nto be trying to like probe and understand its behaviors. But in many ways, I'm like I can then just have like useful\nconversations with it, you know? So if I'm working on something as part of my research I can just be like, oh, like, which I already\nfind myself starting to do, you know, if I'm like, oh, I feel like there's this like thing in virtue ethics and I can't\nquite remember the term, like I'll use the model\nfor things like that. And so I can imagine that being more and more the case where you're just basically interacting with it much more like you would an\nincredibly smart colleague. And using it like for the kinds\nof work that you want to do as if you just had a\ncollaborator who was like. Or you know, the slightly horrifying thing about AI is like as soon as\nyou have one collaborator, you have 1000 collaborators if you can manage them enough. - But what if it's two times the smartest human on earth\non that particular discipline? - Yeah. - I guess you're really good at sort of probing Claude in a way that pushes its limits, understanding where the limits are. - [Amanda] Yep. - So I guess what would be a question you would ask to be\nlike, yeah, this is AGI. - That's really hard 'cause\nit feels like in order to, it has to just be a series of questions. Like if there was just one question, like you can train anything to answer one question extremely well. In fact, you can probably\ntrain it to answer like, you know, 20 questions extremely well. - Like how long would you\nneed to be locked in a room with an AGI to know this thing is AGI? - It's a hard question 'cause part of me is like all of this just feels continuous. Like if you put me in a\nroom for five minutes, I'm like, I just have high error bars. You know, I'm like, and\nthen it's just like, maybe it's like both the\nprobability increases in the error bar decreases. I think things that I can actually probe the edge of human knowledge of. So I think this with\nphilosophy a little bit. Sometimes when I ask the\nmodels philosophy questions, I am like, this is a question that I think no one has ever asked. Like it's maybe like right at the edge of like some literature that I know, and the models will just kind of like, when they struggle with\nthat, when they struggle to come up with a kind of like novel. Like I'm like I know that there's\nlike a novel argument here 'cause I've just thought of it myself. So maybe that's the thing where I'm like, I've thought of a cool novel argument in this like niche area, and I'm going to just like probe you to see if you can come up with it, and how much like prompting it takes to get you to come up with it. And I think for some of these, like really like right at the edge of human knowledge questions, I'm like, you could not in fact come up with the thing that I came up with. I think if I just took something like that where like I know a lot about an area, and I came up with a novel issue or a novel like solution to a problem, and I gave it to a model and it came up with that solution, that would be a pretty\nmoving moment for me because I would be like, this is a case where no human has ever, like it's not. And obviously we see these this with like more kind of like, you see novel solutions all the time, especially to like easier problems. I think people overestimate\nthat, you know, novelty isn't like, it's\ncompletely different from anything that's ever happened. It's just like this is, it\ncan be a variant of things that have happened and still be novel. But I think, yeah, if I saw like the more I were to see like\ncompletely like novel work from the models, that would be like. And this is just going to feel iterative. It's one of those things\nwhere, there's never, it's like, you know,\npeople I think want there to be like a moment and\nI'm like, I don't know. Like I think that there\nmight just never be a moment. It might just be that there's just like this continuous ramping up. - I have a sense that there will be things that a model can say that\nconvinces you, this is very. It's not like, like, I've talked to people\nwho are like truly wise. Like you could just tell there's\na lot of horsepower there. - [Amanda] Yep. - And if you 10x that, I don't know, I just feel like there's\nwords you could say. Maybe ask it to generate a poem, (laughs) and the poem it generates, you're like, yeah, okay. - Yeah,\n- Whatever you did there, I don't think a human can do that. - I think it has to be something that I can verify is like\nactually really good though. That's why I think these\nquestions that are like, where I'm like, oh,\nthis is like, you know, like, you know, sometimes\nit's just like I'll come up with say a concrete counter example to like an argument or\nsomething like that. I'm sure like with like, it would be like if\nyou're a mathematician, you had a novel proof I think, and you just gave it the\nproblem and you saw it and you're like, this\nproof is genuinely novel. Like no one has ever done, you actually have to do a lot of things to like come up with this. You know, I had to sit and think about it for\nmonths or something. And then if you saw the\nmodel successfully do that, I think you would just be like, I can verify that this is correct. It is a sign that you have\ngeneralized from your training. Like you didn't just see this somewhere because I just came up with it myself and you were able to like replicate that. That's the kind of thing where I'm like, for me, the closer, the more that models like\ncan do things like that, the more I would be like,\noh, this is like very real, 'cause then I can, I dunno,\nI can like verify that that's like extremely, extremely capable. - You've interacted with AI a lot. What do you think makes humans special? - Oh, good question. - Maybe in a way that the\nuniverse is much better off that we're in it and that\nwe should definitely survive and spread throughout the universe. - Yeah, it's interesting because I think like people focus so much on intelligence,\nespecially with models. Look, intelligence is important\nbecause of what it does. Like, it's very useful. It does a lot of things in the world. And I'm like, you know,\nyou can imagine a world where like height or strength\nwould've played this role, and I'm like, it's just a trait like that. I'm like, it's not intrinsically valuable. It's valuable because of what it does, I think for the most part. The things that feel, you know, I'm like, I mean,\npersonally I'm just like, I think humans and like life in general is extremely magical. We almost like to the\ndegree that, you know, and I don't know, like not\neveryone agrees with this. I'm flagging, but you know, we have this like whole universe, and there's like all of these objects. You know, there's like beautiful stars, and there's like galaxies and then, I don't know, I'm just\nlike, on this planet, there are these creatures that have this like ability to observe that, like, and they are like seeing it. They are experiencing it. And I'm just like that,\nif you try to explain, like imagine trying to explain to like, I dunno, someone for some reason, they've never encountered the world or science or anything. And I think that nothing is that, like everything, you know,\nlike all of our physics and everything in the world,\nit's all extremely exciting. But then you say, oh, and plus, there's this thing that\nit is to be a thing and observe in the world, and you see this like inner cinema. And I think they would be\nlike, hang on, wait, pause. You just said something that like is kind of wild sounding. And so I'm like, we have this like ability to like experience the world. We feel pleasure, we feel suffering. We feel like a lot of like complex things. And so, yeah, and maybe this\nis also why I think, you know, I also like care a lot\nabout animals, for example, 'cause I think they\nprobably share this with us. So I think that like the things that make humans special insofar as like I care about humans is probably more like their\nability to feel an experience than it is like them having these like functionally useful traits. - Yeah, to feel and experience\nthe beauty in the world. Yeah, to look at the stars. I hope there's other alien\ncivilizations out there, but if we're it, it's a pretty good, it's a pretty good thing. - And that they're having a good time. - They're having a good time watching us. - [Amanda] Yeah. - Well, thank you for this good time of a conversation and for\nthe work you're doing, and for helping make Claude a\ngreat conversational partner. And thank you for talking today. - Yeah, thanks for talking. - Thanks for listening to this conversation with Amanda Askell. And now, dear friends, here's Chris Olah. Can you describe this fascinating field of mechanistic interpretability,\nAKA mech interp, the history of the field\nand where it stands today? - I think one useful way to think about neural networks is that we don't program and we don't make them. We kind of, we grow them. You know, we have these\nneural network architectures that we design and we\nhave these loss objectives that we create. And the neural network architecture, it's kind of like a scaffold\nthat the circuits grow on, and they sort of, you know, it starts off with some kind of random, you know, random things and it grows. And it's almost like the objective that we train for is this light. And so we create the\nscaffold that it grows on and we create the, you know, the light that it grows towards. But the thing that we actually create, it's this almost biological, you know, entity or organism that we're studying. And so it's very, very different from any kind of regular\nsoftware engineering, because at the end of the day, we end up with this artifact that can do all these amazing things. It can, you know, write\nessays and translate and, you know, understand images. It can do all these things\nthat we have no idea how to directly create a\ncomputer program to do. And it can do that because we grew it, we didn't write it, we didn't create it. And so then that leaves open\nthis question at the end, which is, what the hell is\ngoing on inside these systems? And that, you know, is to me a really deep and exciting question. It's, you know, a really\nexciting scientific question. To me it's sort of is\nlike the question that is, is just screaming out,\nit's calling out for us to go and answer it when we\ntalk about neural networks. And I think it's also a very deep question for safety reasons. - So, and mechanistic interpretability I guess is closer to maybe neurobiology. - Yeah, yeah, I think that's right. So maybe to give an example\nof the kind of thing that has been done that\nI wouldn't consider to be mechanistic interpretability. There was for a long time a lot of work on saliency maps where\nyou would take an image and you try to say, you know, the model thinks this image is a dog. What part of the image made\nit think that it's a dog? And you know, that tells you\nmaybe something about the model if you can come up with a\nprincipled version of that. But it doesn't really tell you like what algorithms are running in the model? How was the model actually\nmaking that decision? Maybe it's telling you something about what was important to it, if you can make that method work, but it isn't telling, you know, what are the algorithms that are running? How is it that the system's\nable to do this thing that no one knew how to do? And so I guess we started using the term mechanistic\ninterpretability to try to sort of draw that divide or to distinguish ourselves in the work that we were doing in some ways from some of these other things. And I think since then it's become this sort of umbrella term for, you know, a pretty wide variety of work. But I'd say that the things that are kind of distinctive are, I think, A, this focus on, we really want to get at, you know, the mechanisms, we wanna\nget at the algorithms. You know, if you think of neural networks as being like a computer program, then the weights are kind of\nlike a binary computer program. And we'd like to reverse\nengineer those weights and figure out what\nalgorithms are running. So, I think one way you might think of trying to understand a neural network is that it's kind of like, we have this compiled computer program and the weights of the neural\nnetwork are the binary. And when the neural network runs, that's the activations. And our goal is ultimately to go and understand these weights. And so, you know, the approach of mechanistic interpretability\nis to somehow figure out how do these weights\ncorrespond to algorithms. And in order to do that, you also have to\nunderstand the activations, 'cause it's sort of, the\nactivations are like the memory. And if you imagine reverse\nengineering a computer program and you have the binary instructions, you know, in order to understand what a particular instruction\nmeans, you need to know what is stored in the memory\nthat it's operating on. And so those two things\nare very intertwined. So mechanistic interpret really tends to be interested both of those things. Now, you know, there's a lot of work that's interested in those things, especially, you know, there's\nall this work on probing, which you might see as part of being mechanistic interpretability. Although it's, you know, again, it's just a broad term and not everyone who does that work would identify as doing mech interp. I think a thing that is maybe\na little bit distinctive to the vibe of mech\ninterp is I think people working in this space tend to think of neural networks as, well, maybe one way to say it\nis that gradient descent is smarter than you, that, you know, and gradient descent is\nactually really great. The whole reason that we're\nunderstanding these models is 'cause we didn't know how to write them in the first place. That gradient descent comes up with better solutions than us. And so I think that maybe\nanother thing about mech interp is sort of having almost\na kind of humility that we won't guess a priori what's going on inside the models. We have to have this sort\nof bottom up approach where we don't really assume, you know, we don't assume that we should\nlook for a particular thing and that that will be there\nand that's how it works. But instead we look for the bottom up and discover what happens to exist in these models\nand study them that way. - But, you know, the very\nfact that it's possible to do, and as you and others have\nshown over time, you know, things like universality, that the wisdom of the gradient descent\ncreates features and circuits, creates things universally across different kinds of\nnetworks that are useful, and that makes the whole field possible. - Yeah, so this is actually, is indeed a really remarkable and exciting thing\nwhere it does seem like, at least to some extent,\nyou know, the same elements, the same features and\ncircuits form again and again. You know, you can look\nat every vision model and you'll find curve detectors and you'll find high/low\nfrequency detectors. And in fact, there's some reason to think that the same things\nform across, you know, biological neural networks and\nartificial neural networks. So a famous example is vision models in their early layers\nthey have Gabor filters, and there's, you know, Gabor filters are something\nthat neuroscientists are interested in, have\nthought a lot about. We find curve detectors in these models, curve detectors are also found in monkeys. And we discover these high\nlow frequency detectors and then some follow up work went and discovered them in rats or mice. So they were found first in\nartificial neural networks and then found in\nbiological neural networks. You know, there's this\nreally famous result on like grandmother neurons or the Halle Berry neuron\nfrom Quiroga et al. And we found very similar\nthings in vision models where, this is while I was still at OpenAI and I was looking at their clip model, and you find these neurons that respond to the same entities in images. And also to give a concrete example there, we found that there was\na Donald Trump neuron. For some reason, I guess everyone likes to talk about Donald Trump, and Donald Trump was very prominent, was a very hot topic at that time. So every neural network we looked at, we would find a dedicated\nneuron for Donald Trump. And that was the only person who had always had a dedicated neuron. You know, sometimes you'd\nhave an Obama neuron, sometimes you'd have a Clinton neuron, but Trump always had a dedicated neuron. So it responds to, you\nknow, pictures of his face and the word Trump, like\nall these things, right? And so it's not responding\nto a particular example or like, it's not just\nresponding to his face, it's extracting over this\ngeneral concept, right? So in any case, that's very similar to these Quiroga et al results. So there evidence that this\nphenomenon of universality, the same things form\nacross both artificial and natural neural networks. That's a pretty amazing\nthing if that's true. You know, it suggests that,\nwell, I think the thing that it suggests is\nthe gradient of descent is sort of finding, you\nknow, the right ways to cut things apart in some sense that many systems converge on, and many different neural networks' architectures converge on. That there's some\nnatural set of, you know, there's some set of abstractions\nthat are a very natural way to cut apart the problem, and that a lot of systems\nare gonna converge on. That would be my kind of, you know, I don't know anything about neuroscience. This is just my kind of wild speculation from what we've seen. - Yeah, that would be beautiful\nif it's sort of agnostic to the medium of the model that's used to form the representation. - Yeah, yeah, and, you know, it's a kind of a wild speculation based, you know, we only have some, a few data points that suggest this, but you know, it does seem like there's some sense in\nwhich the same things form again and again in both, in certainly in natural neural networks and also artificially or in biology. - And the intuition behind\nthat would be that, you know, in order to be useful in\nunderstanding the real world, you need all the same kind of stuff. - Yeah, well if we pick, I don't know, like the idea of a dog, right? Like, you know, there's\nsome sense in which the idea of a dog is like a natural category in the universe or\nsomething like this, right? Like, you know, there's some reason, it's not just like a weird\nquirk of like how humans factor, you know, think about the world that we have this concept of a dog. It's in some sense... Or like, if you have the idea of a line, like there's, you know,\nlike look around us, you know, there are lines, you know. It's sort of the simplest way to understand this room in some sense is to have the idea of a line. And so I think that that would be my instinct\nfor why this happens. - Yeah, you need a curved line, you know, to understand a circle, and you need all those shapes to understand bigger things and it's a hierarchy of\nconcepts that are formed, yeah. - And like maybe there are ways to go and describe, you know, images without reference\nto those things, right? But they're not the simplest way or the most economical way\nor something like this. And so systems converge\nto these strategies would be my wild hypothesis. - Can you talk through\nsome of the building blocks that we've been referencing\nof features and circuits? So I think you first described them in 2020 paper \"Zoom In: An\nIntroduction to Circuits.\" - Absolutely, so maybe I'll start by just describing some phenomena, and then we can sort of build to the idea of features and circuits. - [Lex] Wonderful. - If you spent like quite a few years, maybe like five years to some extent with other things, studying\nthis one particular model Inception V1, which is\nthis one vision model. It was state of the art in 2015 and, you know, very much not\nstate of the art anymore. (Lex laughs) And it has, you know, maybe\nabout 10,000 neurons in it. And I spent a lot of time looking at the 10,000 neurons, odd\nneurons of Inception V1. And one of the interesting\nthings is, you know, there are lots of neurons that don't have some obvious\ninterpretable meaning, but there's a lot of neurons and Inception V1 that do have really clean interpretable meanings. So you find neurons\nthat just really do seem to detect curves, and you find neurons that\nreally do seem to detect cars, and car wheels and car windows and, you know, floppy ears of dogs, and dogs with long snouts\nfacing to the right, and dogs with long snouts\nfacing to the left. And you know, different kinds of, there's sort of this whole\nbeautiful edge detectors, line detectors, color contrast detectors, these beautiful things we call\nhigh/low frequency detectors. You know, I think looking at it, I sort of felt like a biologist, you know, you just, you're looking at this sort of new world of proteins, and you're discovering all these different proteins that interact. So one way you could try to understand these models\nis in terms of neurons. You could try to be like, oh, you know, there's a dog detecting neuron and here's a car detecting neuron. And it turns out you can actually ask how those connect together. So you can go and say, oh, you know, I have this car detecting\nneuron, how is it built? And it turns out in the previous layer, it's connected really\nstrongly to a window detector, and a wheel detector, and it's sort of car body detector. And it looks for the window above the car, and the wheels below, and the car chrome sort of in the middle, sort of everywhere but\nespecially on the lower part. And that's sort of a\nrecipe for a car, right? Like that is, you know, earlier we said that the thing we wanted from mech interp was to get algorithms to go and get, you know, ask what is\nthe algorithm that runs? Well, here we're just\nlooking at the weights of the neuron network and reading off this kind of recipe for detecting cars. It's a very simple crude\nrecipe, but it's there. And so we call that a\ncircuit, this connection. Well, okay, so the problem is that not all of the neurons are interpretable, and there's reason to think, we can get into this more later, that there's this\nsuperposition hypothesis. There's reason to think that sometimes the right unit to analyze things in terms of is combinations of neurons. So sometimes it's not that\nthere's a single neuron that represents say a car,\nbut it actually turns out after you detect the car, the model sort of hides\na little bit of the car in the following layer and a bunch of dog detectors. Why is it doing that? Well, you know, maybe it just doesn't wanna do that much work\non cars at that point and you know, it's sort\nof storing it away to go. So it turns out then this\nsort of subtle pattern of, you know, there's all these neurons that you think are dog detectors and maybe they're primarily that, but they all a little bit contribute to representing a car in that next layer. Okay, so now we can't really think, there might still be something that, I don't know, you could\ncall it like a car concept or something, but it no longer\ncorresponds to a neuron. So we need some term for these\nkind of neuron like entities, these things that we sort of would've liked the neurons to be, these idealized neurons, the things that are the nice neurons, but also maybe there's\nmore of them somehow hidden and we call those features. - And then what are circuits? - So circuits are these\nconnections of features, right? So, when we have the car detector and it's connected to a window detector, and a wheel detector, and it looks for the wheels below and the windows on top, that's a circuit. So circuits are just\ncollections of features connected by weights and\nthey implement algorithms. So they tell us, you know, how are features used? How are they built? How do they connect together? So maybe it's worth trying to pin down like what really is the\ncore hypothesis here. And I think the core\nhypothesis is something we call the linear representation hypothesis. So if we think about the\ncar detector, you know, the more it fires, the more\nwe sort of think of that as meaning, oh, the model is more and more confident that a car is present. Or you know, if it's some\ncombination of neurons that represent a car, you know, the more that combination fires, the more we think the model\nthinks there's a car present. This doesn't have to be the case, right? Like you could imagine something\nwhere you have, you know, you have this car detector neuron and you think, ah, you know,\nif it fires like, you know, between one and two, that means one thing, but it means like totally different if it's between three and four. That would be a nonlinear representation. And in principle that, you\nknow, models could do that. I think it's sort of\ninefficient for them to do, if you try to think about how you'd implement computation like that, it's kind of an annoying thing to do. But in principle, models can do that. So one way to think about the features and circuits sort of framework\nfor thinking about things is that we're thinking about\nthings as being linear. We're thinking about there as being that if a neuron or a\ncombination neurons fires more, it's sort of, that means more of a particular thing being detected. And then that gives weights\na very clean interpretation as these edges between these entities, these features and that\nedge then has a meaning. So that's in some ways the core thing. It's like, you know, we can talk about this sort of outset,\nthe context of neurons. Are you familiar with\nthe Word2Vec results? So you have like, you know, king minus man plus woman equals queen. Well, the reason you can\ndo that kind of arithmetic is because you have a\nlinear representation. - Can you actually explain that\nrepresentation a little bit? So, first of all, so the feature is a direction of activation. - Yeah, exactly.\n- You can do it that way. Can you do the minus men plus women, that, the Word2Vec stuff, can you explain what that is that work? - [Chris] Yeah, so there's this very- - It's such a simple, clean explanation of what we're talking about. - Exactly, yeah. So there's this very famous result, Word2Vec by Tomas Mikolov et al, and there's been tons of\nfollow-up work exploring this. So, sometimes we have these, we create these word embeddings where we map every word to a vector. I mean, that in itself, by the way, is kind of a crazy thing if you haven't thought\nabout it before, right? Like we are going in and representing, we're turning, you know, like if you just learned about vectors in physics class, right? And I'm like, oh, I'm gonna actually turn every word in the\ndictionary into a vector. That's kind of a crazy idea, okay. But you could imagine, you could imagine all kinds of ways in which you\nmight map words to vectors. But it seems like when\nwe train neural networks, they like to go and map words to vectors to such that they're\nsort of linear structure in a particular sense, which is that directions have meaning. So for instance, there\nwill be some direction that seems to sort of\ncorrespond to gender, and male words will be, you\nknow, far in one direction and female words will\nbe in another direction. And the linear\nrepresentation hypothesis is, you could sort of think of it roughly as saying that that's actually kind of the fundamental\nthing that's going on, that everything is just\ndifferent directions have meanings and adding\ndirection vectors together can represent concepts. And the Mikolov paper sort\nof took that idea seriously, and one consequence of it is that you can do this game of playing sort of arithmetic with words. So you can do king and you can, you know, subtract off the word man\nand add the word woman. And so you're sort of, you know, going and trying to switch the gender. And indeed if you do that, the result will sort of be close to the word queen. And you can, you know, do other things like you can do, you know, sushi minus Japan plus Italy and get pizza or different things like this, right? So, this is in some sense the core of the linear\nrepresentation hypothesis. You can describe it just as a purely abstract thing. But vector spaces, you can\ndescribe it as a statement about the activations of neurons. But it's really about this property of directions having meaning. And in some ways it's\neven a little subtle that it's really I think\nmostly about this property of being able to add things together that you can sort of independently modify say gender and royalty or, you know, cuisine type or country, and the concept of food by adding them. - Do you think the\nlinear hypothesis holds- - Yes.\n- That kind of carries scales. - So, so far, I think\neverything I have seen is consistent with the hypothesis, and it doesn't have to be that way, right? Like you can write down neural networks where you write weights such that they don't have linear representations, where the right way to understand them is not in terms of linear representations. But I think every natural neural network I've seen has this property. There's been one paper recently that there's been some sort\nof pushing around the edge. So I think there's been some work recently studying\nmulti-dimensional features where rather than a single direction, it's more like a manifold of directions. This to me still seems like\na linear representation. And then there's been some other papers suggesting that maybe\nin very small models, you get non-linear representations. I think that the jury's still out on that. But I think everything that we've seen so far has been consistent with the linear representation\nhypothesis and that's wild. It doesn't have to be that way, and yet I think that\nthere's a lot of evidence that certainly at least this\nis very, very widespread, and so far the evidence\nis consistent with it. And I think, you know,\none thing you might say is you might say, well, Christopher, you know, that's a lot, you know, to go and sort of to ride on. You know, if we don't know\nfor sure this is true, and you're sort of, you know, you're investing in neural networks as though it is true, you\nknow, isn't that dangerous? Well, you know, but I think actually, there's a virtue in taking\nhypotheses seriously and pushing them as far as they can go. So it might be that someday\nwe discover something that isn't consistent with\nlinear representation hypothesis. But science is full of hypotheses and theories that were wrong, and we learned a lot by\nsort of working under them as a sort of an assumption, and then going and pushing\nthem as far as we can. I guess this is sort of the heart of what Kuhn would call normal science. I dunno if you want, we\ncan talk a lot about- - Kuhn.\n- Philosophy of science and- - That leads to the paradigm shift. So yeah, I love it, taking\nthe hypothesis seriously, and take it to a natural conclusion. Same with the Scaling Hypothesis, same- - Exactly. Exactly.\n- I love it. - One of my colleagues, Tom Henighan, who is a former physicist, like made this really nice analogy to me of caloric theory where, you know, once upon a time we thought\nthat heat was actually, you know, this thing called caloric, and like the reason,\nyou know, hot objects, you know, would warm up, cool objects is like the\ncaloric is flowing through them. And like, you know, because we're so used to thinking about heat, you know, in terms of the modern theory, you know, that seems kind of silly. But it's actually very hard\nto construct an experiment that sort of disproves\nthe caloric hypothesis. And, you know, you can actually do a lot of really useful\nwork believing in caloric. For example, it turns out that the original combustion\nengines were developed by people who believed\nin the caloric theory. So I think there's a virtue in taking hypotheses seriously, even when they might be wrong. - Yeah, there's a deep\nphilosophical truth to that. That's kind of like how I\nfeel about space travel, like colonizing Mars. There's a lot of people\nthat criticize that. I think if you just assume\nwe have to colonize Mars in order to have a backup\nfor human civilization, even if that's not true,\nthat's gonna produce some interesting engineering and even scientific\nbreakthroughs, I think. - Yeah, well, and actually\nthis is another thing that I think is really interesting. So, you know, there's a way in which I think it can be really useful for society to have people\nalmost irrationally dedicated to investigating particular hypotheses because, well, it takes a lot to sort of maintain scientific morale and really push on\nsomething when, you know, most scientific hypotheses\nend up being wrong. You know, a lot of\nscience doesn't work out. And yet it's, you know, it's very useful to just, you know, there's a joke about Jeff Hinton, which is that Jeff Hinton has discovered how the brain works every\nyear for the last 50 years. But you know, I say that\nwith like, you know, with really deep respect because in fact that's actually, you know, that led to him doing some\nsome really great work. - Yeah, he won the Nobel Prize. Now who's laughing now? - [Chris] Exactly, exactly, exactly. - Yeah. - I think one wants to be able to pop up and sort of recognize the\nappropriate level of confidence. But I think there's also a lot of value and just being like, you know, I'm going to essentially assume, I'm gonna condition on this problem being possible or this being\nbroadly the right approach, and I'm just gonna go and\nassume that for a while and go and work within that\nand push really hard on it. And, you know, society has lots of people doing that for different things. That's actually really useful in terms of going and getting to, you know, either really\nruling things out, right? We can be like, well, you\nknow, that didn't work and we know that somebody tried hard. Or going in and getting to something that it does teach us\nsomething about the world. - So another interesting hypothesis is the superposition hypothesis. Can you describe what superposition is? - Yeah, so earlier we were\ntalking about word defect, right? And we were talking about how, you know, maybe you have one direction\nthat corresponds to gender, and maybe another that\ncorresponds to royalty, and another one that corresponds to Italy, and another one that\ncorresponds to, you know, food and all of these things. Well, you know, oftentimes\nmaybe these word embedding, they might be 500\ndimensions, 1000 dimensions. And so if you believe that all of those directions were orthogonal, then you could only have,\nyou know, 500 concepts. And you know, I love pizza, but like, if I was gonna go and like give the like 500 most important concepts in, you know, the English language,\nprobably Italy wouldn't be, it's not obvious at least that Italy would be one of them, right? Because you have to have things like plural, and singular, and verb, and noun, and adjective, and, you know, there's a lot of things we have to get to before\nwe get to Italy, and Japan, and, you know, there's a lot\nof countries in the world. And so how might it be that\nmodels could, you know, simultaneously have the linear\nrepresentation hypothesis be true and also represent more things than they have directions. So, what does that mean? Well, okay, so if linear\nrepresentation hypothesis is true, something interesting has to be going on. Now, I'll tell you one\nmore interesting thing before we go and we do\nthat, which is, you know, earlier we were talking about all these polysemantic neurons, right? These neurons that, you know, when we were looking at Inception V1, there's these nice neurons\nthat like the car detector and the curve detector\nand so on that respond to lots of, you know,\nto very coherent things. But there's lots of neurons that respond to a bunch of unrelated things, and that's also an interesting phenomenon. And it turns out as well\nthat even these neurons that are really, really clean, if you look at the weak\nactivations, right? So if you look at like,\nyou know, the activations where it's like activating\n5% of the, you know, of the maximum activation,\nit's really not the core thing that it's expecting, right? So if you look at a curve\ndetector, for instance, and you look at the places\nwhere it's 5% active, you know, you could interpret it just as noise or it could be that it's doing\nsomething else there, okay? So, how could that be? Well, there's this amazing\nthing in mathematics called compressed sensing, and it's actually this\nvery surprising fact where if you have a high dimensional space and you project it into\na low dimensional space, ordinarily you can't go\nand sort of unproject it and get back your high\ndimensional vector, right? You threw information away. This is like, you know, you can't invert a rectangular matrix, you can only invert square matrices. But it turns out that that's\nactually not quite true. If I tell you that the high\ndimensional vector was sparse, so it's mostly zeros, then it turns out that you can often go and find back the high dimensional vector with very high probability. So that's a surprising fact, right? It says that, you know, you can have this high\ndimensional vector space, and as long as things are\nsparse, you can project it down, you can have a lower\ndimensional projection of it, and that works. So the superposition\nhypothesis is saying that that's what's going on in neural networks. That's, for instance, that's what's going on in word embeddings. That word embeddings are able to simultaneously have directions\nbe the meaningful thing. And by exploiting the fact that they're operating on a fairly\nhigh dimensional space, they're actually, and the fact that these\nconcepts are sparse, right? Like, you know, you usually aren't talking about Japan and Italy at the same time. You know, most of those\nconcepts, you know, in most instances, Japan\nand Italy are both zero. They're not present at all. And if that's true, then you can go and have it be the case that you can have many more of\nthese sort of directions that are meaningful, these features than you have dimensions. And similarly, when we're\ntalking about neurons, you can have many more\nconcepts than you have neurons. So that's, at a high level,\nthe superposition hypothesis. Now it has this even wilder implication, which is to go and say\nthat neural networks are, it may not just be the case that the representations are like this, but the computation may\nalso be like this, you know, the connections between all of them. And so in some sense, neural\nnetworks may be shadows of much larger, sparser neural networks, and what we see are these projection. And, you know, the strongest version of the superposition\nhypothesis would be to take that really seriously and\nsort of say, you know, there actually is in some\nsense this upstairs model, this, you know, where the\nneurons are really sparse and all interpretable,\nand there's, you know, the weights between them are\nthese really sparse circuits. And that's what we're studying. And the thing that we're observing is the shadow of evidence. We need to find the original object. - And the process of learning\nis trying to construct a compression of the upstairs model that doesn't lose too much\ninformation in the projection. - Yeah, it's finding how\nto fit it efficiently or something like this. The gradient descent is doing this. And in fact, so this sort of\nsays that gradient descent, you know, it could just represent a dense neural network, but it sort of says that gradient descent is implicitly searching over the space of extremely sparse models that could be projected into\nthis low dimensional space. And this large body of work of people going and trying to study sparse neural networks, right? Where you go and you have, you could design neural networks, right, where the edges are sparse and the activations are sparse. And you know, my sense is\nthat work has generally, it feels very principled, right? It makes so much sense. And yet that work hasn't really panned out that well is my impression broadly. And I think that a\npotential answer for that is that actually the neural network is already sparse in some sense. Gradient descent was the whole time you were trying to go and do this, gradient descent was actually\nin the behind the scenes going and searching more\nefficiently than you could through the space of sparse models, and going and learning\nwhatever sparse model was most efficient and then figuring out how to fold it down nicely to go and run conveniently on your GPU, which does, you know, nice,\ndense matrix multiplies, and that you just can't beat that. - How many concepts do you think can be shoved into a neural network? - Depends on how sparse they are. So there's probably an upper bound from the number of parameters, right? Because you have to have,\nyou still have to have, you know, weights that go\nand connect them together. So that's one upper bound. There are in fact all these lovely results from compressed sensing and the Johnson-Lindenstrauss lemma, and things like this. That they basically tell you that if you have a vector space and you want to have\nalmost orthogonal vectors, which is sort of probably the thing that you want here, right? So you're gonna say, well, you know, I'm gonna give up on having my concepts, my features be strictly orthogonal, but I'd like them to\nnot interfere that much. I'm gonna have to ask them\nto be almost orthogonal. Then this would say that\nit's actually, you know, once you set a threshold for what you're willing to accept in terms of how much\ncosine similarity there is, that's actually exponential in the number of neurons that you have. So at some point, that's not gonna even be the limiting factor. But there's some beautiful results there. And in fact, it's probably even better than that in some sense\nbecause that's sort of, for saying that, you know, any random set of features could be active. But in fact the features have sort of a correlational\nstructure where some features, you know, are more likely to co-occur, and other ones are less\nlikely to co-occur. And so neural networks, my guess would be can do very well in terms of going and packing things in such, to the point that's probably\nnot the limiting factor. - How does the problem of polysemanticity enter the picture here? - Polysemanticity is this phenomenon we observe where you look at many neurons, and the neuron doesn't just\nsort of represent one concept. It's not a clean feature. It responds to a bunch\nof unrelated things. And superposition is, you can think of as being a hypothesis that explains the observation\nof polysemanticity. So polysemanticity is\nthis observed phenomenon and superposition is a hypothesis that would explain it along\nwith some other things. - So that makes mech\ninterp more difficult. - Right, so if you're\ntrying to understand things in terms of individual neurons, and you have polysemantic neurons, you're in an awful lot of trouble, right? I mean, the easiest answer is\nlike, okay, well, you know, you're looking at the neurons, you're trying to understand them. This one responds for a lot of things, it doesn't have a nice meaning. Okay, you know, that's bad. Another thing you could ask is, you know, ultimately, we wanna\nunderstand the weights. And if you have two polysemantic neurons and, you know, each one responds to three things and then, you know, the other neuron responds to three things and you have a weight between them, you know, what does that mean? Does it mean that like\nall three, you know, like there's these nine, you know, nine interactions going on? It's a very weird thing. But there's also a deeper reason, which is related to the fact that neural networks operate on really high dimensional spaces. So I said that our goal was, you know, to understand neural networks and understand the mechanisms, and one thing you might\nsay is like, well, why not? It's just a mathematical function, why not just look at it, right? Like, you know, one of\nthe earliest projects I did studied these neural networks that match two dimensional spaces to two dimensional spaces, and you can sort of interpret them as in this beautiful way\nas like bending manifolds. Why can't we do that? Well, you know, as you have\na higher dimensional space, the volume of that space in some senses is exponential in the\nnumber of inputs you have. And so you can't just go and visualize it. So we somehow need to break that apart. We need to somehow break\nthat exponential space into a bunch of things that we, you know, some non-exponential number of things that we can reason about independently. And the independence is crucial because it's the\nindependence that allows you to not have to think about, you know, all the exponential\ncombinations of things. And things being mono-semantic, things only having one meaning. Things having a meaning, that is the key thing that allows you to think about them independently. And so I think that's, if\nyou want the deepest reason why we want to have interpretable\nmono-sematic features, I think that's really the deep reason. - And so the goal here, as your recent work has been aiming at, is how do we extract the\nmono-semantic features from a neural net that\nhas poly-sematic features and all this mess. - Yes, we observed these\npoly-semantic neurons, and we hypothesized that's\nwhat's going on is superposition. And if superposition is\nwhat's going on there, there's actually a sort of\nwell established technique that is sort of the\nprincipled thing to do, which is dictionary learning. And it turns out if you\ndo dictionary learning, in particular, if you do\nsort of a nice efficient way that in some sense sort of\nnicely regularizes it as well called a sparse auto-encoder. If you train a sparse auto-encoder, these beautiful interpretable features start to just fall out where\nthere weren't any beforehand. And so that's not a thing that you would necessarily predict, right? But it turns out that that\nworks very, very well. You know, that to me that\nseems like, you know, some non-trivial validation of linear representations\nin superposition. - So with dictionary\nlearning, you're not looking for particular kind of categories, you don't know what they are. They just emerge.\n- Exactly. And this gets back to\nour earlier point, right? When we're not making assumptions, gradient descent is smarter than us, so we're not making\nassumptions about what's there. I mean, one certainly\ncould do that, right? One could assume that\nthere's a PHP feature and go and search for it,\nbut we're not doing that. We're saying we don't know\nwhat's gonna be there. Instead we're just gonna go and let the sparse auto-encoder discover the things that are there. - So can you talk to the\n\"Toward Monosemanticity\" paper from October last year? It had a lot of like nice\nbreakthrough results. - That's very kind of you\nto describe it that way. Yeah, I mean, this was\nour first real success using sparse auto-encoders. So we took a one layer model, and it turns out if you go and, you know, do dictionary learning on it, you find all these really\nnice interpretable features. So, you know, the Arabic\nfeature, the Hebrew feature, the Base64 features,\nthose were some examples that we studied in a lot of depth, and really showed that they\nwere what we thought they were. It turns, if you train\na model twice as well and train two different models and do dictionary learning, you find analogous\nfeatures in both of them. So that's fun. You find all kinds of\nof different features. So that was really just\nshowing that this work. And you know, I should mention that there was this Cunningham et al that had very similar\nresults around the same time. - There's something fun about doing these kinds of\nsmall scale experiments and finding that it's actually working. - Yeah, well, and there's\nso much structure here, like you know, so maybe\nstepping back for a while, I thought that maybe all this mechanistic interpretability work, the end result was gonna be that I would have an explanation for why it was sort of, you know, very hard and not gonna be tractable. You know, we'd be like, well, there's this problem\nwith superposition, and it turns out\nsuperposition is really hard, and we're kind of screwed,\nbut that's not what happened. In fact, a very natural,\nsimple technique just works. And so then that's actually\na very good situation. You know, I think this is a\nsort of hard research problem and it's got a lot of research risk and you know, it might\nstill very well fail, but I think that some amount of, some very significant\namount of research risk was sort of put behind us\nwhen that started to work. - Can you describe what kind of features can be extracted in this way? - Well, so it depends on the model that you're studying, right? So the larger the model, the more sophisticated they're gonna be, and we'll probably talk about\nfollow up work in a minute. But in these one layer models, so some very common things\nI think were languages, both programming languages\nand natural languages. There were a lot of features that were specific words in\nspecific contexts, so \"the.\" And I think really the way to think about this is \"the\" is likely about to be followed by a noun. So it's really, you could\nthink of this as \"the\" feature, but you could also think of this as predicting a specific noun feature. And there would be these features that would fire for \"the\" in the context of say a legal document, or a mathematical document\nor something like this. And so, you know, maybe\nin the context of math you're like, you know, and\n\"the\" then predict vector, matrix, you know, all\nthese mathematical words, whereas, you know, in other context, you would predict other\nthings, that was common. - And basically we need clever humans to assign labels to what we're seeing. - Yes, so, you know, this is, the only thing this is doing is that sort of unfolding things for you. So if everything was sort\nof folded over top of it, you know, superposition folded\neverything on top of itself and you can't really see\nit, this is unfolding it. But now you still have\na very complex thing to try to understand. So then you have to do a bunch of work understanding what these are. And some of them are really subtle. Like there's some really cool things even in this one layer\nmodel about Unicode where, you know, of course some\nlanguages are in Unicode and the tokenizer won't necessarily have a dedicated token for\nevery Unicode character. So instead what you'll have is you'll have these patterns\nof alternating token, or alternating tokens that each represent half\nof a Unicode character. - Nice.\n- And you'll have a different feature that, you know, goes and activates on the\nopposing ones to be like, okay, you know, I just finished\na character, you know, go and predict next prefix. Then okay, on the prefix, you know, predict a reasonable suffix, and you have to alternate back and forth. So there's, you know,\nthese one player models are really interesting. And I mean, it's another thing\nthat just, you might think, okay, there would just\nbe one Base64 feature, but it turns out there's actually a bunch of Base64 features because you can have English\ntext encoded as Base64, and that has a very different distribution of Base64 tokens than regular. And there's some things about tokenization as well that it can exploit. And I dunno, there's all\nall kinds of fun stuff. - How difficult is the task of sort of assigning\nlabels to what's going on? Can this be automated by AI? - Well, I think it depends on the feature and it also depends on how\nmuch you trust your AI. So there's a lot of work doing\nautomated interpretability. I think that's a really\nexciting direction, and we do a fair amount of\nautomated interpretability and have Claude go and label our features. - Is there some funny moments where it's totally right\nor it's totally wrong? - Yeah, well, I think it's very common that it's like says\nsomething very general, which is like true in some sense, but not really picking up on the specific of what's going on. So I think that's a\npretty common situation. Yeah, don't know that I have\na particularly amusing one. - That's interesting, that\nlittle gap between it is true but doesn't quite get to\nthe deep nuance of a thing. That's a general challenge. It's like truly an\nincredible accomplishment that it can say a true thing, but it doesn't, it's not, it's missing the depth sometimes. And in this context, it's\nlike the ARC challenge, you know, the sort of IQ type of tests. It feels like figuring out what a feature represents is a bit of, is a little puzzle you have to solve. - Yeah, and I think that\nsometimes they're easier, and sometimes they're harder as well. So yeah, I think that's tricky. And there's another thing which, I dunno, maybe in some ways this is\nmy like aesthetic coming in, but I'll try to give\nyou a rationalization. You know, I'm actually a little suspicious of automated interpretability, and I think that partly just that I want humans to\nunderstand neural networks, and if the neural network\nis understanding it for me, you know, I don't quite like that. But I do have a bit of,\nyou know, in some ways, I'm sort of like the mathematicians\nwho are like, you know, if there's a computer automated proof, it doesn't count. - Right?\n- You know, they won't understand it. But I do also think that there is this kind of like reflections on trusting trust type issue where, you know, if you, there's\nthis famous talk about, you know, like when you're\nwriting a computer program, you have to trust your compiler, and if there was like\nmalware in your compiler, then it could go and inject\nmalware into the next compiler, and you know, you'd be\nkind of in trouble, right? Well, if you're using neural networks to go and verify that your\nneural networks are safe, the hypothesis that you're\ntesting for is like, okay, well, the neural\nnetwork maybe isn't safe, and you have to worry about\nlike, is there some way that it could be screwing with you? So, you know, I think that's\nnot a big concern now, but I do wonder in the long run if we have to use really\npowerful AI system to go and, you know, audit our AI systems, is that actually something we can trust? But maybe I'm just rationalizing 'cause I just want to us to have to get it to a point where humans\nunderstand everything. - Yeah, I mean, especially\nthat's hilarious, especially as we talk about AI safety and it looking for features\nthat would be relevant to AI safety, like deception and so on. So let's talk about the\n\"Scaling Monosemanticity\" paper in May, 2024. Okay, so what did it take to scale this, to apply to Claude 3s on it? - Well, a lot of GPUs. - A lot more GPUs, got it. - But one of my teammates, Tom Henighan was involved in\nthe original scaling laws work, and something that he\nwas sort of interested in from very early on is,\nare there scaling laws for interpretability? And so something he\nsort of immediately did when this work started to succeed, and we started to have\nsparse auto-encoders work was he became very interested in, you know, what are the scaling laws for, you know, for making sparse\nauto-encoders larger? And how does that relate to\nmaking the base model larger? And so it turns out this works really well and you can use it to\nsort of project, you know, if you train a sparse\nauto-encoder at a given size, you know, how many tokens\nshould you train on? And so on. So this was actually a very big help to us in scaling up this work, and made it a lot easier\nfor us to go and train, you know, really large\nsparse auto-encoders where, you know, it's not like\ntraining the big models, but it's starting to get to a point where it's actually actually expensive to go and train the really big ones. - So you have this, I mean,\nyou have to do all this stuff of like splitting it across large GPUs- - Oh yeah, no, I mean there's a huge engineering challenge here too, right? So, yeah, so there's a\nscientific question of, how do you scale things effectively? And then there's an enormous amount of engineering to go and scale this up. So you have to chart it, you have to think very\ncarefully about a lot of things. I'm lucky to work with a\nbunch of great engineers 'cause I am definitely\nnot a great engineer. - Yeah, and the infrastructure especially, yeah, for sure. So it turns out, TLDR, it worked. - It worked, yeah. And I think this is important because you could have imagined, like, you could have imagined a world where you said after\ntowards mono-semanticity, you know, Chris, this is great, you know, it works on a one layer model, but one layer models are\nreally idiosyncratic. Like, you know, maybe,\nthat's just something, like maybe the linear\nrepresentation hypothesis and superposition hypothesis is the right way to\nunderstand a one layer model, but it's not the right way\nto understand larger models. And so I think, I mean, first of all, the Cunningham et al\npaper sort of cut through that a little bit and sort of suggested that this wasn't the case. But scaling mono-semanticity sort of, I think was significant evidence that even for very large models, and we did it on Claude 3 Sonnet, which at that point was one\nof our production models. You know, even these\nmodels seemed to be very, you know, seemed to be\nsubstantially explained at least by linear features. And, you know, doing dictionary\nlearning on them works, and as you learn more features, you go and you explain more and more. So that's, I think,\nquite a promising sign. And you find now really\nfascinating abstract features. And the features are also multimodal. They respond to images and text for the same concept, which is fun. - Yeah, can you explain that? I mean, like, you know, backdoor, there's just a lot of\nexamples that you can- - Yeah, so maybe let's start\nwith a one example to start, which is we found some features around sort of security vulnerabilities\nand backdoors and codes. So it turns out those are\nactually two different features. So there's a security\nvulnerability feature, and if you force it\nactive, Claude will start to go and write security vulnerabilities like buffer overflows into code. And it also, it fires\nfor all kinds of things. Like, you know, some of the\ntop dataset examples for it were things like, you\nknow, dash dash disable, you know, SSL or something like this, which are sort of\nobviously really insecure. - So at this point it's kind of like, maybe it's just because the examples were presented that way, it's kind of like a little bit\nmore obvious examples, right? I guess the idea is that down the line, it might be able to detect more nuanced, like deception or bugs\nor that kind of stuff. - Yeah, well, I maybe wanna\ndistinguish two things. So one is the complexity of the feature or the concept, right? And the other is the nuance of how subtle the examples\nwe're looking at, right? So, when we show the top dataset examples, those are the most extreme examples that cause that feature to activate. And so it doesn't mean\nthat it doesn't fire for more subtle things. So, you know, the insecure code feature, you know, the stuff that it fires for, most strongly for are\nthese like really obvious, you know, disable the\nsecurity type things. But you know, it also fires\nfor, you know, buffer overflows and more subtle security\nvulnerabilities in code. You know, these features\nare all multimodal, so you could ask like, what\nimages activate this feature? And it turns out that the\nsecurity vulnerability feature activates for images of like people clicking on Chrome to\nlike go past the, like, you know, this website, the SSL certificate might be\nwrong or something like this. Another thing that's very entertaining is there's backdoors and code feature. Like you activate it, it goes\nand Claude writes a backdoor that like will go and dump\nyour data to port or something. But you can ask, okay, what images activate the backdoor feature? It was devices with\nhidden cameras in them. So there's a whole\napparently genre of people going and selling devices\nthat look innocuous, that have hidden cameras and they have- - That's great.\n- This hidden camera in it. And I guess that is the, you know, physical version of a backdoor. And so it sort of shows you how abstract these concepts are, right? And I just thought that was, I'm sort of sad that\nthere's a whole market of people selling devices like that, but I was kind of delighted that that was the thing that it came up with as the top\nimage examples for the feature. - Yeah, it's nice. It's multimodal. It's multi almost context. It's as broad, strong definition of a singular concept, it's nice. - Yeah. - To me, one of the really\ninteresting features, especially for AI safety\nis deception and lying. And the possibility that\nthese kinds of methods could detect lying in a model, especially gets smarter\nand smarter and smarter. Presumably that's a big threat\nof a super intelligent model that it can deceive\nthe people operating it as to its intentions or\nany of that kind of stuff. So what have you learned from detecting lying inside models? - Yeah, so I think we're in some ways in early days for that. We find quite a few features\nrelated to deception and lying. There's one feature where, you know, fires for people lying\nand being deceptive, and you force it active\nand starts lying to you. So we have a deception feature. I mean, there's all\nkinds of other features about withholding information and not answering questions, features about power seeking\nand coups and stuff like that. So there's a lot of features that are kind of related to spooky things. And if you force them active, Claude will behave in ways that are, they're not the kinds\nof behaviors you want. - What are possible\nnext exciting directions to you in the space of mech interp? - Well, there's a lot of things. So for one thing, I would\nreally like to get to a point where we have circuits where\nwe can really understand not just the features, but then use that to understand\nthe computation of models. That relief for me is the\nultimate goal of this. And there's been some work,\nwe put out a few things. There's a paper from Sam Marks that does some stuff like this. And there's been some, I'd say some work around the edges here. But I think there's a lot more to do, and I think that will be\na very exciting thing. That's related to a challenge\nwe call interference weights, where due to superposition, if you just sort of naively look at where their features\nare connected together, there may be some weights that sort of don't exist in the upstairs model, but are just sort of\nartifacts of superposition. So that's a sort of technical\nchallenge related to that. I think another exciting\ndirection is just, you know, you might think of sparse auto-encoders as being kind of like a telescope. They allow us to, you know, look out and see all these\nfeatures that are out there. And you know, as we build better and better sparse auto-encoders, get better and better\nat dictionary learning, we see more and more stars, and you know, we zoom in on\nsmaller and smaller stars. But there's kind of a lot of evidence that we're only still seeing a very small fraction of the stars. There's a lot of matter in our, you know, neural network universe\nthat we can't observe yet. And it may be that we'll never be able to have fine enough\ninstruments to observe it, and maybe some of it just isn't possible, isn't computationally\ntractable to observe it. So it's sort of a kind of dark matter, not in maybe the sense\nof modern astronomy, but of early astronomy when we didn't know what this unexplained matter is. And so I think a lot\nabout that dark matter and whether we'll ever observe it, and what that means for safety if we can't observe it, if there's, you know, if\nsome significant fraction of neural networks are\nnot accessible to us. Another question that\nI think a lot about is, at the end of the day, you know, mechanistic interpretability is this very microscopic approach\nto interpretability. It's trying to understand things\nin a very fine-grained way. But a lot of the questions we care about are very macroscopic. You know, we care about these questions about neural network behavior, and I think that's the thing\nthat I care most about, but there's lots of other sort of larger scale questions\nyou might care about. And somehow, you know, the nice thing about about having a very microscopic approach\nis it's maybe easier to ask, you know, is this true? But the downside is it's much further from the things we care about, and so we now have this ladder to climb. And I think there's a question of, will we be able to find, are there sort of larger\nscale abstractions that we can use to\nunderstand neural networks? Can we get up from this\nvery microscopic approach? - Yeah, you've written about\nthis kind of organs question. - Yeah, exactly.\n- So if we think of interpretability as a kind\nof anatomy of neural networks, most of the circuits threads involve studying tiny little veins, looking at the small scale at individual neurons\nand how they connect. However, there are many natural questions that the small scale\napproach doesn't address. In contrast, the most\nprominent abstractions in biological anatomy involve\nlarger scale structures, like individual organs, like the heart, or entire organ systems,\nlike the respiratory system. And so we wonder, is there a\nrespiratory system or heart or brain region of an\nartificial neural network? - Yeah, exactly. And I mean, like if you\nthink about science, right? A lot of scientific fields have, you know, investigate things at many\nlevels of abstractions. In biology you have like, you know, molecular biology studying, you know, proteins and molecules and so on. And they have cellular biology, and then you have\nhistology studying tissues, and you have anatomy, and\nthen you have zoology, and then you have ecology. And so you have many,\nmany levels of abstraction or you know, physics, maybe the physics of individual particles, and then, you know, statistical physics gives you thermodynamics\nand things like this. And so you often have different\nlevels of abstraction. And I think that right\nnow we have, you know, mechanistic interpretability\nif it succeeds is sort of like a microbiology\nof neural networks, but we want something more like anatomy. And so, and you know, a\nquestion you might ask is, why can't you just go there directly? And I think the answer is superposition, at least in significant part. It's that it's actually very hard to see this macroscopic structure without first sort of breaking down the microscopic structure\nin the right way, and then studying how\nit connects together. But I'm hopeful that there\nis gonna be something much larger than features and circuits, and that we're gonna\nbe able to have a story that involves much bigger things, and then you can sort of study in detail the parts you care about. - I suppose to neurobiology,\nlike a psychologist or a psychiatrist of a neural network. - And I think that the beautiful thing would be if we could go, and rather than having disparate fields for those two things, if you could build a bridge between them- - Oh, right.\n- Such that you could go and have all of your\nhigher level abstractions be grounded very firmly\nin this very solid, you know, more rigorous,\nideally, foundation. - What do you think is the difference between the human brain, the\nbiological neural network and the artificial neural network? - Well, the neuroscientists\nhave a much harder job than us. You know, sometimes I just\nlike count my blessings by how much easier my job is than the neuroscientists, right? So I have, we can record\nfrom all the neurons. We can do that on\narbitrary amounts of data. The neurons don't change while you're doing that, by the way. You can go and ablate neurons, you can edit the connections and so on, and then you can undo those changes. That's pretty great. You can force, you can\nintervene on any neuron and force it active and see what happens. You know, which neurons are\nconnected to everything, right? Neuroscientists wanna get the connectome, we have the connectome and we have it for like much bigger than like C. elegans. And then not only do\nwe have the connectome, we know what, you know, which neurons excite or inhibit each other, right. So we have, it's not just that we know that like the binary\nmass, we know the weights. We can take gradients, we know computationally\nwhat each neuron does. So I don't know the list goes on and on. We just have so many advantages\nover neuroscientists. And then despite having\nall those advantages, it's really hard. And so one thing I do\nsometimes think is like, gosh, like if it's this hard for us, it seems impossible under the constraints of neuroscience or, you\nknow, near impossible. I don't know, maybe part of me is like, I've got a few neuroscientists on my team. Maybe I'm sort sort of like, ah, you know, maybe the neuroscientists,\nmaybe some of them would like to have an easier\nproblem that's still very hard and they could come and\nwork on neural networks. And then after we figure out things in sort of the easy little pond of trying to understand neural networks,\nwhich is still very hard, then we could go back to\nbiological neuroscience. - I love what you've\nwritten about the goal of mech interp research as two goals, safety and beauty. So can you talk about the\nbeauty side of things? - Yeah, so, you know,\nthere's this funny thing where I think some people want, some people are kind of\ndisappointed by neural networks, I think, where they're like,\nah, you know, neural networks, it's these just these simple rules, and then you just like\ndo a bunch of engineering to scale it up and it works really well. And like, where's the like complex ideas? You know, this isn't like a very nice, beautiful scientific result. And I sometimes think\nwhen people say that, I picture them being like, you\nknow, evolution is so boring. It's just a bunch of simple rules and you run evolution for a\nlong time and you get biology. Like what a sucky, you know, way for biology to have turned out. Where's the complex rules? But the beauty is that the\nsimplicity generates complexity. You know, biology has these simple rules and it gives rise to,\nyou know, all the life and ecosystems that we see around us, all the beauty of nature, that\nall just comes from evolution and from something very simple evolution. And similarly, I think\nthat neural networks build, you know, create enormous complexity and beauty inside and\nstructure inside themselves that people generally don't look at and don't try to understand because it's hard to understand. But I think that there is\nan incredibly rich structure to be discovered inside neural networks, a lot of very deep beauty if we're just willing to take the time to go and see it and understand it. - Yeah, I love mech interp, the feeling like we are understanding or getting glimpses of understanding the magic that's going on\ninside is really wonderful. - It feels to me like one of the questions that's just calling out to be asked, and I'm sort of, I mean, a lot of people are thinking about this,\nbut I'm often surprised that not more are is, how is it that we don't know how to\ncreate computer systems that can do these things, and yet we have these amazing systems that we don't know how to\ndirectly create computer programs that can do these things, but these neural networks can\ndo all these amazing things? And it just feels like that\nis obviously the question that sort of is calling out\nto be answered if you are, if you have any degree of curiosity. It's like how is it that\nhumanity now has these artifacts that can do these things\nthat we don't know how to do? - Yeah, I love the image of the circuits reaching towards the light of the objective function. - Yeah, it's just, it's this organic thing that we've grown and we have\nno idea what we've grown. Well, thank you for working on safety and thank you for appreciating the beauty of the things you discover. And thank you for talking today, Chris. This was wonderful.\n- Yeah. Thank you for taking the\ntime to chat as well. - Thanks for listening to this\nconversation with Chris Olah, and before that with Dario\nAmodei and Amanda Askell. To support this podcast, please check out our\nsponsors in the description. And now let me leave you with\nsome words from Alan Watts. \"The only way to make sense out of change is to plunge into it, move with it, and join the dance.\" Thank you for listening and\nhope to see you next time."
}